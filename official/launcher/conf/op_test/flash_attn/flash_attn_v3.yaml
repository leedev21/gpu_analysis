# 1. transformer_engine_torch::generic_gemm ✅
# Compute GEMM (matrix-matrix multiply)
- name: flash_attn::flash_attn_interface.flash_attn_func
  size:
    B: [1, 2, 4, 8]
    MQKV: [2048, 4096, 8192, 16384, 32768, 65536, 131072, 75360]
    # NHQ: [8, 10, 32]
    NHK: [8, 10, 32, 40]
    D: [128]
    Drop: [0.0]
    S: [0.0]
    C: [False]
    W: [[-1, -1]]
    De: [False]
  input:
    - _tensor: [B, MQKV, NHK, D]
      precision: e4m3
    - _tensor: [B, MQKV, NHK, D]
      precision: e4m3
    - _tensor: [B, MQKV, NHK, D]
      precision: e4m3
    - qv: null
      softcap: S
      q_descale:
        _tensor: [B, NHK]
        precision: fp32
      k_descale:
        _tensor: [B, NHK]
        precision: fp32
      v_descale:
        _tensor: [B, NHK]
        precision: fp32
      causal: C
      window_size: W
      pack_gqa: False
      num_splits: 1


# 2. transformer_engine_torch::generic_gemm ✅
# Compute GEMM (matrix-matrix multiply)
- name: flash_attn::flash_attn_interface.flash_attn_func

# # # - name: flash_attn::flash_attn.flash_attn_with_kvcache
# #     # q,
# #     # k_cache,
# #     # v_cache,
# #     # k=None,
# #     # v=None,
# #     # rotary_cos=None,
# #     # rotary_sin=None,
# #     # cache_seqlens: Optional[Union[(int, torch.Tensor)]] = None,
# #     # cache_batch_idx: Optional[torch.Tensor] = None,
# #     # block_table: Optional[torch.Tensor] = None,
# #     # softmax_scale=None,
# #     # causal=False,
# #     # window_size=(-1, -1),  # -1 means infinite context window
# #     # rotary_interleaved=True,
# #     # alibi_slopes=None,