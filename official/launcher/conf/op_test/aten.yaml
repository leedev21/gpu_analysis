# base 1d
- name: aten::sum
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [M, N]
- name: aten::minimum
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [M, N]
    - [M, N]
- name: aten::pow
  size:
    B: [1]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [B, M, N]
    - exponent: 2
- name: aten::log
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [M, N]
- name: aten::gelu
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [M, N]
- name: aten::mean
  size:
    B: [1]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [B, M, N]
    - dim: -1
      keepdim: True
- name: aten::cumsum
  size:
    B: [1]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
  input:
    - [B, M]
    - dim: -1

# base embedding
- name: aten::index_select
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    E: [102400]
  input:
    - [E, N]
    - 0
    - _tensor: [M]
      high: E
      precision: long
- name: aten::embedding_dense_backward
  size:
    B: [1]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    E: [102400]
  input:
    - [B, M, N]
    - _tensor: [B, M]
      high: E
      precision: long
    - E
    - -1
    - false

# base 2d
- name: aten::linear
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    K: [6144]
    N: [8192]
  input:
    - [M, K]
    - [N, K]
    - [N]
- name: aten::convolution
  size:
    B: [1, 2, 4]
    C_IN: [1, 8, 16, 32, 64, 128, 512]
    iH: [128, 256, 384, 512, 1024]
    kH: [1, 2, 3, 16, 32]
    C_OUT: [8, 128, 256, 512, 1408, 4096]
  input:
    - [B, C_IN, iH, iH]
    - [C_OUT, C_IN, kH, kH]
    - [C_OUT]
    - stride: [2, 2]
    - padding: [0, 0]
    - dilation: [1, 1]
    - transposed: false
    - output_padding: [0, 0]
    - groups: 1
  precision: fp16
- name: aten::convolution_backward
  size:
    B: [1, 2, 4]
    C_IN: [1, 8, 16, 32, 64, 128, 512]
    kH: [1, 2, 3, 16, 32, 64, 128]
    C_OUT: [8, 128, 256, 512, 1408, 4096]
    Scale: [2]
  input:
    - [B, C_OUT, kH, kH]
    - [B, C_IN, kH*Scale, kH*Scale]
    - [C_OUT, C_IN, Scale, Scale]
    - _val: [C_OUT]
    - stride: [2, 2]
      padding: [0, 0]
      dilation: [1, 1]
      transposed: false
      output_padding: [0, 0]
      groups: 1
      output_mask: [True, True, True]
  precision: fp16

# advanced op
- name: aten::_softmax
  size:
    B: [1, 2, 4]
    M: [5, 10, 20, 40]
    # N: [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072] OOM on 40G
    N: [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192]
  input:
    - _tensor: [B, M, N, N]
      precision: fp16
    - -1
    - False
- name: aten::native_layer_norm
  size:
    B: [1]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    N: [768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    E: [1e-6, 1e-12]
  input:
    - [B, M, N]
    - _val: [N]
    - [N]
    - [N]
    - E
- name: aten::native_layer_norm_backward
  size:
    B: [1]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768]
    N: [768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [B, M, N]
    - [B, M, N]
    - _val: [N]
    - _tensor: [M, B, 1]
      precision: fp32
    - _tensor: [M, B, 1]
      precision: fp32
    - [N]
    - [N]
    - _val: [True, True, True]
- name: aten::tanh
  size:
    B: [8]
    # M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]
    # N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    N: [32, 64, 128, 512, 768]
  input:
    - [B, M, N]
- name: aten::tanh
  size:
    B: [8]
    # M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
    # N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    N: [32, 64, 128, 512]
  input:
    - [B, M, N, N]
- name: aten::tanh_backward
  size:
    B: [8]
    # M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
    # N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    N: [32, 64, 128, 512]
  input:
    - [B, M, N, N]
    - [B, M, N, N]
- name: aten::native_group_norm
  size:
    B: [1]
    # C: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    C: [32, 64, 128, 256, 512, 1024]
    # H: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
    H: [32, 64, 128, 512]
    W: [32, 64, 128, 512]
  input:
    - [B, C, H, W]
    - [C]
    - [C]
    - B
    - C
    - HxW: H*W
    - group: 32
    - eps: 9.9999999999999995e-07
- name: aten::linalg_vector_norm
  size:
    M: [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072]
    N: [32, 64, 128, 512, 768, 2048, 3072, 4096, 5120, 6144, 7168, 8192, 13824, 16384]
  input:
    - [M, N]
    - ord: 2.0
      dim: null
      keepdim: False
      dtype: null
- name: aten::upsample_bicubic2d
  size:
    B: [1, 2, 4]
    C: [1, 8, 16, 32, 64, 128, 512]
    iH: [128, 256, 384, 512, 1024]
    iW: [128, 256, 384, 512, 1024]
    kH: [224]
  input:
    - [B, C, iH, iW]
    - _val: [kH, kH]
    - align_corners: False
