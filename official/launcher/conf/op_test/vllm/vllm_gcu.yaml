# torch_custom_op_native ç®—å­æµ‹è¯•é…ç½®æ–‡ä»¶
# åŸºäº ok.yaml æµ‹è¯•ç»“æœç­›é€‰ï¼Œä¿ç•™å·²é€šè¿‡æµ‹è¯•çš„ç®—å­
# ç”Ÿæˆæ—¶é—´: 2025-06-07
# æœ€åæ›´æ–°: 2025-06-09

# ==================== æµ‹è¯•æ€»ç»“ ====================
# æ€»ç®—å­æ•°é‡: 22ä¸ª
# âœ… å·²æµ‹è¯•é€šè¿‡: 20ä¸ª 
# âŒ æµ‹è¯•å¤±è´¥: 2ä¸ª (å·²æ³¨é‡Š)
# ğŸ”„ å¾…æµ‹è¯•: 0ä¸ª

# === å·²æµ‹è¯•é€šè¿‡çš„ç®—å­ (16ä¸ªï¼Œå·²æ³¨é‡Š) ===
# 1. vllm.ops.NATIVE_C::silu_mul_per_token_group_quant - SiLU + Mul + åˆ†ç»„é‡åŒ–ç®—å­
# 2. vllm.ops.NATIVE_C::rms_norm_per_token_group_quant_fp8 - RMSNorm + åˆ†ç»„é‡åŒ–FP8ç®—å­
# 3. vllm.ops.NATIVE_C::rms_norm - RMSNormç®—å­
# 4. vllm.ops.NATIVE_C::paged_attention_v1 - åˆ†é¡µæ³¨æ„åŠ›V1ç®—å­
# 5. vllm.ops.NATIVE_C::fused_moe_quant_kernel - èåˆMOEé‡åŒ–å†…æ ¸ç®—å­
# 6. vllm.ops.NATIVE_C::fused_grouped_topk - èåˆåˆ†ç»„TopKç®—å­
# 7. vllm.ops.NATIVE_C::get_ep_indices - è·å–ä¸“å®¶å¹¶è¡Œç´¢å¼•ç®—å­
# 8. vllm.ops.NATIVE_C::fused_add_rms_norm_per_token_group_quant_fp8 - èåˆAdd+RMSNorm+åˆ†ç»„é‡åŒ–FP8ç®—å­
# 9. vllm.ops.NATIVE_C::fused_add_rms_norm - èåˆAdd+RMSNormç®—å­
# 10. vllm.ops.NATIVE_C::dynamic_per_token_group_fp8_quant - åŠ¨æ€åˆ†ç»„FP8é‡åŒ–ç®—å­
# 11. vllm.ops.NATIVE_C::concat_and_cache_mla - è¿æ¥å’Œç¼“å­˜MLAç®—å­
# 12. vllm.ops.NATIVE_C::silu_and_mul - SiLUå’ŒMulç®—å­ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰
# 13. vllm.ops.NATIVE_C::linear_quant - çº¿æ€§é‡åŒ–ç®—å­
# 14. vllm.ops.NATIVE_C::silu_mul_per_token_group_quant_with_size - SiLU + Mul + åˆ†ç»„é‡åŒ–ç®—å­ï¼ˆå¸¦sizeå‚æ•°ï¼‰
# 15. vllm.ops.NATIVE_C::silu_and_mul_pad - SiLUå’ŒMulç®—å­ï¼ˆå¸¦å¡«å……ï¼‰
# 16. vllm.ops.NATIVE_C::rotary_embedding_with_kv_cache - æ—‹è½¬åµŒå…¥ä¸KVç¼“å­˜ç®—å­
# 17. vllm.ops.NATIVE_C::moe_align_block_size - MOEå¯¹é½å—å¤§å°ç®—å­
# 19. vllm.ops.NATIVE_C::exts_moe_align_block_size - MOEå¯¹é½å—å¤§å°ç®—å­
# 21. vllm.ops.NATIVE_C::fused_dispatch_decode - èåˆåˆ†å‘è§£ç ç®—å­ 
# 22. vllm.ops.NATIVE_C::dynamic_split - åŠ¨æ€åˆ†å‰²ç®—å­

# === æµ‹è¯•å¤±è´¥çš„ç®—å­ (2ä¸ª) ===

# 18. vllm.ops.NATIVE_C::moe_sum - MOEæ±‚å’Œç®—å­ 
# 20. vllm.ops.NATIVE_C::fused_qkv_proj - èåˆQKVæŠ•å½±ç®—å­


# å·²æµ‹è¯•é€šè¿‡ç®—å­: 20ä¸ªï¼ˆå·²å¯¼å‡ºLOGï¼‰ï¼Œæµ‹è¯•å¤±è´¥ç®—å­: 2ä¸ªï¼ˆå·²æ³¨é‡Šï¼‰ï¼Œå…¨éƒ¨ç®—å­å·²æµ‹è¯•å®Œæˆ 


# ========== å·²æµ‹è¯•é€šè¿‡çš„ç®—å­ (20ä¸ª) ==========

# 1. VLLM SiLU + Mul + åˆ†ç»„é‡åŒ–ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::silu_mul_per_token_group_quant
  size:
    M: [8, 16, 32]   # tokens
    N: [64, 128, 256]  # input_dim (must be even)
    G: [8, 16, 32]   # group_size
  input:
    - out:
        _tensor: [M, N/2]  # quantized output tensor
        precision: e4m3
      scale:
        _tensor: [M, N/2/G]  # scale tensor
        precision: fp32
        ulp_dtype: bf16
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
      group_size: G  # quantization group size
  output: [out, scale]
  _name: vllmSiluMulPerTokenGroupQuant
  _mapping: [0, 1, 2, 3]

# 2. VLLM RMSNorm + åˆ†ç»„é‡åŒ–FP8ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::rms_norm_per_token_group_quant_fp8
  size:
    M: [64, 128, 256]  # tokens
    N: [512, 1024, 2048]  # hidden_dim
    G: [64, 128]  # group_size
  input:
    - out:
        _tensor: [M, N]  # quantized output tensor
        precision: e4m3
      scale:
        _tensor: [M, N/G]  # scaling factor tensor
        precision: fp32
        ulp_dtype: bf16
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
      weight:
        _tensor: [N]  # weight tensor
        distributions: _weight_normal
      epsilon: 1e-6  # numerical stability parameter
      group_size: G  # quantization group size
  output: [out, scale]
  _name: vllmRmsNormPerTokenGroupQuantFp8
  _mapping: [0, 1, 2, 3, 4, 5]

# 3. VLLM RMSNormç®—å­ âœ…
- name: vllm.ops.NATIVE_C::rms_norm
  size:
    M: [64, 128, 256]  # tokens
    N: [512, 1024, 2048]  # hidden_dim
  input:
    - result:
        _tensor: [M, N]  # output tensor
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
      weight:
        _tensor: [N]  # weight tensor
        distributions: _weight_normal
      epsilon: 1e-6  # numerical stability parameter
  output: [result]
  _name: vllmRmsNorm
  _mapping: [0, 1, 2, 3]

# 4. VLLM åˆ†é¡µæ³¨æ„åŠ›V1ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::paged_attention_v1
  size:
    B: [1, 2, 4]  # batch_size
    H: [4, 8, 16]  # num_heads
    S: [64, 128]  # seq_len
    D: [64, 128]  # head_dim
    BS: [16, 32]  # block_size
    MS: [128, 256]  # max_seq_len
    BT: [8, 16]  # max_blocks
  input:
    - out:
        _tensor: [B, H, D]  # output tensor
        precision: fp16
      query:
        _tensor: [B, H, D]  # query tensor
        precision: fp16
      key_cache:
        _tensor: [BT, H, 1, BS, D]  # key cache tensor
        precision: fp16
      value_cache:
        _tensor: [BT, H, D, BS]  # value cache tensor
        precision: fp16
      num_kv_heads: H  # number of key-value heads
      scale: 0.125  # attention scale factor
      block_tables:
        _tensor: [B, BT]  # block tables tensor
        precision: int32
        high: BT
        low: 0
      seq_lens:
        _tensor: [B]  # sequence lengths
        precision: int32
        high: S+1
        low: 1
      block_size: BS  # block size
      max_seq_len: MS  # maximum sequence length
      alibi_slopes: null  # optional alibi slopes
      kv_cache_dtype: "auto"  # kv cache data type
      k_scale: 1.0  # key scale
      v_scale: 1.0  # value scale
      tp_rank: 0  # tensor parallel rank
      blocksparse_local_blocks: 0  # blocksparse local blocks
      blocksparse_vert_stride: 0  # blocksparse vertical stride
      blocksparse_block_size: 0  # blocksparse block size
      blocksparse_head_sliding_step: 0  # blocksparse head sliding step
      k_zero: 0.0  # key zero point
      v_zero: 0.0  # value zero point
      out_scales: null  # optional output scales
  output: [out]
  _name: vllmPagedAttentionV1
  _mapping: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]

# 5. VLLM èåˆMOEé‡åŒ–å†…æ ¸ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::fused_moe_quant_kernel
  size:
    B: [8]  # batch_size
    M: [128]  # seq_length
    K: [64]  # input_dim
    N: [32]  # output_dim
    GS: [8]  # group_size
    E: [4]  # num_experts
    TK: [2]  # topk value
    BS: [4]  # block_size
    MT: [16]  # max_tokens
  input:
    - C: #0
        _tensor: [M, B, N]  # output tensor (modified in-place)
        precision: bf16
      A: #1
        _tensor: [M*B, K]  # input tensor A
        precision: e4m3
        distributions: _input_normal
      B: #2
        _tensor: [E, N, K]  # weight tensor B for all experts
        precision: e4m3
        distributions: _weight_linear
      A_scale: #3
        _tensor: [M*B, K/GS]  # A scale tensor (optional)
        precision: fp32
        distributions: _fp8_scale_t
      B_scale: #4
        _tensor: [E, N/GS, K/GS]  # B scale tensor
        precision: fp32
        distributions: _fp8_scale
      gs: GS  # group size # 5
      B_zp: #6
        _tensor: [E, N/GS, K/GS]  # B zero point tensor (optional)
        precision: fp32
      topk_weights: #7
        _tensor: [M, TK]  # topk weights
        precision: fp32
      topk_ids: #8
        _tensor: [M, TK]  # topk expert indices
        precision: int32
        high: E-1
        low: 0
      sorted_token_ids: #9
        _tensor: [MT]  # sorted token indices
        precision: int32
        high: M-1
        low: 0
      experts_ids: #10
        _tensor: [E*2]  # expert IDs
        precision: int32
        high: E-1
        low: 0
      num_tokens_post_pad: #11
        _tensor: [1]  # number of tokens after padding
        precision: int32
        high: MT
        low: 0
      mul_routed_weight: true  # whether to multiply routed weight #12
      topk: TK  # topk value #13
      block_size: BS  # block size #14
      bias: null  # bias tensor (optional) #15
      real_token_num: #16
        _tensor: [1]  # real token number (optional)
        precision: int32
        high: M*B
        low: M*B
  output: [C]  # æ— è¿”å›å€¼ï¼Œç›´æ¥ä¿®æ”¹Cå¼ é‡
  _name: vllmInvokeFusedMoeNonGatherQuantKernel
  _mapping: [0, 1, 2, 3, 4, 5, -1, 7, 8, 9, 10, 11, 13, 14, 15, 6, 12]

# 6. VLLM èåˆåˆ†ç»„TopKç®—å­ âœ… (åç§°å·²æ›´æ–°ï¼šgrouped_topk -> fused_grouped_topk)
- name: vllm.ops.NATIVE_C::fused_grouped_topk
  size:
    M: [256]  # batch_size * seq_len
    N: [4096]  # vocab_size or feature_dim
    K: [8]  # topk value
    NEG: [16]  # num_expert_group
    TG: [4]  # topk_group
  input:
    - topk_weights:
        _tensor: [M, K]  # output: top-k weights
        precision: fp32
      topk_ids:
        _tensor: [M, K]  # output: top-k indices
        precision: long
        high: 32768
      gating_output:
        _tensor: [M, N]  # input tensor for top-k selection
        precision: bf16
      topk: K  # number of top elements to select
      renormalize: true  # whether to renormalize weights
      num_expert_group: NEG  # number of expert groups
      topk_group: TG  # topk group parameter
      e_score_correction_bias:
        _tensor: [N]  # bias correction tensor
        precision: bf16
      scoring_func: "softmax"  # scoring function name
  output: [topk_weights, topk_ids]
  _name: vllmGroupedTopk
  _mapping: [0, 1, 2, 3, 4, 5, 6, 8, 7]

# 7. VLLM è·å–ä¸“å®¶å¹¶è¡Œç´¢å¼•ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::get_ep_indices
  size:
    M: [32, 64, 128]  # total_tokens
    K: [2, 4]  # topk value
    E: [2, 4, 8]  # expert_per_rank
    P: [2, 4]  # ep_size
    TI: [64, 128, 256]  # max token indices length
  input:
    - ep_count:
        _tensor: [P]  # output: count of tokens per rank
        precision: long
        high: 32768
      ep_token_indices:
        _tensor: [TI]  # output: token indices for each rank
        precision: long
        high: 32768
      ep_valid_token_indices:
        _tensor: [1]  # output: total valid token count
        precision: long
        high: 32768
      topk_ids:
        _tensor: [M, K]  # input: expert indices for each token
        precision: long
        high: 32768
      expert_per_rank: E  # number of experts per rank
      ep_size: P  # number of expert ranks/partitions
  output: [ep_count, ep_token_indices, ep_valid_token_indices]
  _name: vllmGetEPIndices
  _mapping: [0, 1, 2, 3, 4, 5]

# 8. VLLM èåˆAdd+RMSNorm+åˆ†ç»„é‡åŒ–FP8ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::fused_add_rms_norm_per_token_group_quant_fp8
  size:
    M: [8]  # tokens
    N: [7168]  # hidden_dim
    G: [128]  # group_size
  input:
    - out:
        _tensor: [M, N]  # output tensor (quantized)
        precision: e4m3
      residual:
        _tensor: [M, N]  # residual tensor (both input and output)
        distributions: _input_normal
      scale:
        _tensor: [M, N/G]  # scale tensor for each group
        precision: fp32
        ulp_dtype: bf16
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
      weight:
        _tensor: [N]  # weight tensor for scaling
        distributions: _weight_normal
      epsilon: 1e-6  # ç›´æ¥å®šä¹‰ä¸ºç®€å•å€¼ï¼Œä¸ä½¿ç”¨å¤æ‚ç»“æ„
      group_size: G  # quantization group size
  output: [out, residual, scale]
  _name: vllmFusedAddRmsNormPerTokenGroupQuantFp8
  _mapping: [0, 1, 2, 3, 5, 6, 7]

# 9. VLLM èåˆAdd+RMSNormç®—å­ âœ…
- name: vllm.ops.NATIVE_C::fused_add_rms_norm
  size:
    M: [32, 64, 128]  # batch_size
    N: [256, 512, 1024]  # hidden_dim
  input:
    - input:
        _tensor: [M, N]  # input tensor (modified in-place)
        distributions: _input_normal
      residual:
        _tensor: [M, N]  # residual tensor (modified in-place)
        distributions: _input_normal
      weight:
        _tensor: [N]  # weight vector
        distributions: _weight_normal
      epsilon: 1e-5  # epsilon for numerical stability
  output: [input, residual]
  _name: vllmFusedAddRmsNorm
  _mapping: [0, 1, 2, 3]

# 10. VLLM åŠ¨æ€åˆ†ç»„FP8é‡åŒ–ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::dynamic_per_token_group_fp8_quant
  size:
    M: [3453]  # tokens
    N: [7168]  # hidden_dim
    G: [128]  # group_size
  input:
    - out:
        _tensor: [M, N]  # quantized output tensor
        precision: e4m3
      scale:
        _tensor: [M, N/G]  # total group scale factors
        precision: fp32
        ulp_dtype: bf16
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
      group_size: G  # quantization group size
  output: [out, scale]
  _name: vllmDynamicPerTokenGroupFP8Quant
  _mapping: [0, 1, 2, 3]

# 11. VLLM è¿æ¥å’Œç¼“å­˜MLAç®—å­ âœ…
- name: vllm.ops.NATIVE_C::concat_and_cache_mla
  size:
    T: [4, 8, 16]  # num_tokens
    KR: [8, 16, 32]  # kv_lora_rank
    PD: [16, 32, 64]  # pe_dim
    NB: [2, 4, 8]  # num_blocks
    BS: [4, 8, 16]  # block_size
  input:
    - kv_cache:
        _tensor: [NB, BS, KR+PD]  # kv cache tensor (modified in-place)
      kv_c:
        _tensor: [T, KR]  # kv_c tensor
      k_pe:
        _tensor: [T, PD]  # k_pe tensor
      slot_mapping:
        _tensor: [T]  # slot mapping tensor
        precision: int32
        high: NB*BS
        low: -1
      kv_cache_dtype: "float32"  # kv cache data type
      scale:
        _tensor: [1]  # scale tensor
        precision: fp32
  output: [kv_cache]
  _name: vllmConcatAndCacheMla
  _mapping: [0, 1, 2, 3, 4, 5]

# 12. SiLUå’ŒMulç®—å­ï¼ˆé€šç”¨ç‰ˆæœ¬ï¼‰âœ…
- name: vllm.ops.NATIVE_C::silu_and_mul
  size:
    M: [16, 32, 64]  # batch_size
    N: [128, 256, 512]  # hidden_dim (must be even)
  input:
    - out:
        _tensor: [M, N/2]  # output tensor
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
  output: [out]
  _name: vllmSiluAndMul
  _mapping: [0, 1]

# 13. çº¿æ€§é‡åŒ–ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::linear_quant
  size:
    M: [8]  # batch_size
    K: [64]  # input_dim
    N: [32]  # output_dim
    GS: [8]  # group_size
  input:
    - out:
        _tensor: [M, N]  # output tensor
        precision: bf16
      lhs:
        _tensor: [M, K]  # left hand side tensor
        precision: e4m3
      rhs:
        _tensor: [N, K]  # right hand side tensor
        precision: e4m3
      bias: null  # bias tensor (optional)
      lhs_scale:
        _tensor: [M, K/GS]  # left scale tensor
        precision: fp32
      rhs_scale:
        _tensor: [N, K/GS]  # right scale tensor
        precision: fp32
  output: [out]
  _name: atenLinearQuant
  _mapping: [0, 1, 2, -1, 3, 4]

# 14. SiLU + Mul + åˆ†ç»„é‡åŒ–ç®—å­ï¼ˆå¸¦sizeå‚æ•°ï¼‰âœ… (åŸok.yamlä¸­çš„å‘½åç©ºé—´åº”ä¸ºexts)
- name: vllm.ops.NATIVE_C::silu_mul_per_token_group_quant_with_size
  size:
    B: [8557]   # tokens (batch_size)
    M: [8]    # seq_len (ä¸­é—´ç»´åº¦)
    K: [4096]  # input_dim (must be even)
    OK: [2048] # output_dim
    G: [128]  # group_size
    SK: [16] # scale group size
  input:
    - out:
        _tensor: [B, M, OK]  # quantized output tensor
        precision: e4m3
      scale:
        _tensor: [B, M, SK]  # scale tensor
        precision: fp32
        ulp_dtype: bf16
      input:
        _tensor: [B, M, K]  # input tensor
        distributions: _input_normal
      size:
        _val: [B]
        precision: long
      group_size: G  # quantization group size
  output: [out, scale]
  _name: extsSiluMulPerTokenGroupQuant
  _mapping: [0, 1, 2, 3, 4]

# 15. SiLUå’ŒMulç®—å­ï¼ˆå¸¦å¡«å……ï¼‰âœ…
- name: vllm.ops.NATIVE_C::silu_and_mul_pad
  size:
    M: [16, 32, 64]  # batch_size
    N: [128, 256, 512]  # hidden_dim (must be even)
  input:
    - out:
        _tensor: [M, N/2]  # output tensor
      input:
        _tensor: [M, N]  # input tensor
        distributions: _input_normal
      size:
        _tensor: [1]  # size tensor
        precision: long
        high: M+1
        low: 1
  output: [out]
  _name: extsSiluAndMulPad
  _mapping: [0, 1, 2]

# 16. æ—‹è½¬åµŒå…¥ä¸KVç¼“å­˜ç®—å­ âœ… (æ ¹æ®å®é™…å‚æ•°è°ƒç”¨ä¿®æ”¹)
- name: vllm.ops.NATIVE_C::rotary_embedding_with_kv_cache
  size:
    T: [100]  # tokens 
    H: [100]    # num_heads (æ·»åŠ headç»´åº¦)
    QD: [64]  # Q_DIM 
    KVD: [576]  # KV_DIM 
    RD: [512]  # RMS_DIM 
  input:
    # å‚æ•°1: q_out (è¾“å‡ºå¼ é‡)
    - q_out:
        _tensor: [T, QD]  # query output tensor [tokens, Q_DIM]
        precision: fp32
        _role: output
      # å‚æ•°2: kv_cache (è¾“å‡ºå¼ é‡) - ä¿®æ”¹ä¸º3ç»´å¼ é‡
      kv_cache:
        _tensor: [T, H, KVD]  # kv cache output tensor [tokens, num_heads, KV_DIM]
        precision: fp32
        _role: output
      # å‚æ•°3: q (è¾“å…¥å¼ é‡)
      q:
        _tensor: [T, QD]  # query input tensor [tokens, Q_DIM]
        precision: fp32
        _role: input
      # å‚æ•°4: kv (è¾“å…¥å¼ é‡)
      kv:
        _tensor: [T, KVD]  # kv input tensor [tokens, KV_DIM]
        precision: fp32
        _role: input
      # å‚æ•°5: positions (è¾“å…¥å¼ é‡)
      positions:
        _tensor: [T]  # position ids [tokens]
        precision: int32
        _role: input
        high: T-1
        low: 0
      # å‚æ•°6: cos_sin_cache (è¾“å…¥å¼ é‡)
      cos_sin_cache:
        _tensor: [T, QD]  # cosine sine cache [tokens, Q_DIM]
        precision: fp32
        _role: input
      # å‚æ•°7: weight (è¾“å…¥å¼ é‡)
      weight:
        _tensor: [RD]  # weight tensor [RMS_DIM]
        precision: fp32
        _role: input
      # å‚æ•°8: slot_mapping (è¾“å…¥å¼ é‡)
      slot_mapping:
        _tensor: [T]  # slot mapping [tokens]
        precision: int32
        _role: input
        high: T-1
        low: 0
      # å‚æ•°9: scale (è¾“å…¥å¼ é‡)
      scale:
        _tensor: [1]  # scale tensor [1]
        precision: fp32
        _role: input
      # å‚æ•°10: epsilon (æ ‡é‡)
      epsilon: 0.6  # epsilon value (float)
      # å‚æ•°11: split_size (åˆ—è¡¨)
      split_size: [RD, QD]  # split sizes [RMS_DIM, Q_DIM] = [512, 64]
      # å‚æ•°12: kv_cache_dtype (å­—ç¬¦ä¸²)
      kv_cache_dtype: "auto"  # kv cache data type (string)
  output: [q_out, kv_cache]
  _name: extsRotaryEmbeddingWithKVCache
  _mapping: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]

# 21. èåˆåˆ†å‘è§£ç ç®—å­ âœ…
- name: vllm.ops.NATIVE_C::fused_dispatch_decode
  size:
    B: [4]  # batch_sizeï¼Œåªä¿ç•™åŸºç¡€æµ‹è¯•æ•°æ®
  input:
    # åŸºç¡€é…ç½®: æ ¹æ®æµ‹è¯•ä»£ç å®é™…å‚æ•°è®¾ç½®
    - outputs:
        _tensor_list: [[4, 64], [4, 96], [4, 96]]  # è¾“å‡ºå¼ é‡åˆ—è¡¨ï¼Œä¸‰ä¸ªå¼ é‡
      recv_packed:
        _tensor: [4, 256]  # è¾“å…¥å¼ é‡ï¼Œtotal_dim=256
      sp_split_size:
        _val: [2, 1, 1]  # å¯¹åº”æµ‹è¯•ä»£ç ä¸­çš„torch.tensor([2, 1, 1])
        precision: int32
      split_sizes: [64, 96, 96]  # åˆ†å‰²å¤§å°ï¼Œå¯¹åº”ä¸‰ä¸ªè¾“å‡ºå¼ é‡çš„åˆ—ç»´åº¦
  output: [outputs]
  _name: extsFusedDispatchDecode
  _mapping: [0, 1, 2, 3]

# 17. MOEæ±‚å’Œç®—å­ âœ… (åœ¨no.yamlä¸­å¾…æµ‹è¯•) - æš‚æ—¶æ³¨é‡Šï¼Œå†…éƒ¨å®ç°æœ‰é—®é¢˜
- name: vllm.ops.NATIVE_C::moe_sum
  size:
    M: [16]  # batch_size
    R: [8]  # reduce_dim
    N: [32]  # feature_dim
  input:
    - out:
        _tensor: [M, N]  # output tensor (keepdim=False case)
      input:
        _tensor: [M, R, N]  # input tensor
      size:
        _val: [M]
        precision: int64
      dim: 1  # dimension to sum over
      keepdim: false  # whether to keep dimensions
  output: [out]  # æ— è¿”å›å€¼ï¼Œç›´æ¥ä¿®æ”¹è¾“å‡ºå¼ é‡
  _name: atenValidSum
  _mapping: [0, 1, 2, 3, 4]

# # ========== å¾…æµ‹è¯•ç®—å­ (2ä¸ªï¼Œå·²å¯ç”¨) ==========

# # 15. VLLM MOEå¯¹é½å—å¤§å°ç®—å­ âŒ
# - name: vllm.ops.NATIVE_C::moe_align_block_size
#   size:
#     M: [8, 16]  # total_tokens
#     K: [2, 4]  # topk value  
#     E: [4, 8]  # num_experts
#     B: [4, 8]  # block_size
#   input:
#     - topk_ids:
#         _tensor: [M, K]  # input: expert indices for each token
#         precision: int32  # å¿…é¡»æ˜¯int32ç±»å‹ï¼Œä¸èƒ½æ˜¯long
#         high: E-1
#         low: 0
#       num_experts: E  # number of experts
#       block_size: B  # block size
#       sorted_token_ids:
#         _tensor: [M*K*2]  # output: sorted token indices (é¢„åˆ†é…ç©ºé—´)
#         precision: int32  # å¿…é¡»æ˜¯int32ç±»å‹ï¼Œä¸èƒ½æ˜¯int64
#         high: M*K
#         low: 0
#       experts_ids:
#         _tensor: [M*K//B + E*2]  # output: expert IDs (é¢„åˆ†é…ç©ºé—´)
#         precision: int32
#         high: E-1
#         low: 0
#       num_tokens_post_pad:
#         _tensor: [1]  # output: total tokens after padding (é¢„åˆ†é…ç©ºé—´)
#         precision: int32
#         high: 32768
#         low: 0
#   output: []  # æ— è¿”å›å€¼ï¼Œç›´æ¥ä¿®æ”¹è¾“å‡ºå¼ é‡
#   _name: vllmMoeAlignBlockSize
#   _mapping: [0, 1, 2, 3, 4, 5]

# # 19. MOEå¯¹é½å—å¤§å°ç®—å­ï¼ˆæ‰©å±•ç‰ˆæœ¬ï¼‰âŒ
# - name: vllm.ops.NATIVE_C::exts_moe_align_block_size
#   size:
#     M: [8, 16]  # total_tokens
#     K: [2, 4]  # topk value
#     E: [4, 8]  # num_experts
#     B: [4, 8]  # block_size
#   input:
#     - sorted_token_ids:
#         _tensor: [M*K + B]  # output: sorted token indices (è®¡ç®—å¾—åˆ°: M*K + B)
#         precision: int32
#         high: M*K
#         low: 0
#       experts_ids:
#         _tensor: [M*K//B + 1]  # output: expert IDs (ç®€åŒ–å…¬å¼)
#         precision: long
#         high: E-1
#         low: 0
#       num_tokens_post_pad:
#         _tensor: [1]  # output: total tokens after padding
#         precision: int32
#         high: M*K + K*(B-1)
#         low: 0
#       topk_ids:
#         _tensor: [M, K]  # input: expert indices for each token
#         precision: int32
#         high: E-1
#         low: 0
#       real_token_num:
#         _tensor: [1]  # å®é™…tokenæ•°é‡
#         precision: int32
#         high: M+1  # å°†ä¸Šé™å¢åŠ 1ï¼Œç¡®ä¿high > low
#         low: M
#       expert_map:
#         _tensor: [E]  # ä¸“å®¶æ˜ å°„æ•°ç»„
#         precision: int32
#         high: E-1
#         low: 0
#       num_experts: E  # number of experts
#       block_size: B  # block size
#   output: []  # æ— è¿”å›å€¼ï¼Œç›´æ¥ä¿®æ”¹è¾“å‡ºå¼ é‡
#   _name: extsMoeAlignBlockSize
#   _mapping: [0, 1, 2, 3, 4, 5, 6, 7]

# # 20. èåˆQKVæŠ•å½±ç®—å­ âŒ (å·²ç®€åŒ–é…ç½®ï¼Œä¿®æ­£å‘½åç©ºé—´å’Œå‚æ•°é…ç½®)
# - name: vllm.ops.NATIVE_C::fused_qkv_proj
#   size:
#     B: [2, 4]  # batch_size (ç®€åŒ–)
#     S: [4, 8]  # seq_len (ç®€åŒ–)
#     H: [64, 128]  # hidden_size (ç®€åŒ–)
#     N0: [32, 64]  # q output dim (ç®€åŒ–)
#     N1: [32, 64]  # kv output dim (ç®€åŒ–)
#     G: [16, 32]  # group_size (ç®€åŒ–)
#   input:
#     - q:
#         _tensor: [B, S, N0]  # query output tensor (in-placeä¿®æ”¹)
#         precision: fp32
#       kv:
#         _tensor: [B, S, N1]  # key-value output tensor (in-placeä¿®æ”¹)
#         precision: fp32
#       x:
#         _tensor: [B, S, H]  # input tensor
#         precision: fp32
#       weight:
#         _tensor: [N0+N1, H]  # combined qkv weight matrix
#         precision: fp32
#       x_scale:
#         _tensor: [B*S, (H+G-1)//G]  # æ­£ç¡®çš„ceiling divisionè®¡ç®—
#         precision: fp32
#       weight_scale:
#         _tensor: [((N0+N1)+G-1)//G, (H+G-1)//G]  # æ­£ç¡®çš„ceiling divisionè®¡ç®—
#         precision: fp32
#       group_size: G  # quantization group size
#   output: [q, kv]
#   _name: extsFusedQKVProj
#   _mapping: [0, 1, 2, 3, 4, 5, 6]

# # 22. åŠ¨æ€åˆ†å‰²ç®—å­ âŒ
# - name: vllm.ops.NATIVE_C::dynamic_split
#   size:
#     M: [32, 64, 128]  # input_tokens
#   input:
#     # Configuration 1: N=128, splits=[64, 32, 32] (sum=128)
#     - out:
#         _tensor_list: [[M, 64], [M, 32], [M, 32]]  # output tensor list
#         precision: fp32
#       input:
#         _tensor: [M, 128]  # input tensor
#         precision: fp32
#       size:
#         _val: [M]  # size value (å›ºå®šä¸ºMï¼Œä¸ä½¿ç”¨éšæœºèŒƒå›´)
#         precision: int64
#       split_sizes: [64, 32, 32]  # split sizes array (sum=128)
#       dim: 1  # dimension to split
#     # Configuration 2: N=256, splits=[128, 64, 64] (sum=256)
#     - out:
#         _tensor_list: [[M, 128], [M, 64], [M, 64]]  # output tensor list
#         precision: fp32
#       input:
#         _tensor: [M, 256]  # input tensor
#         precision: fp32
#       size:
#         _val: [M]  # size value (å›ºå®šä¸ºMï¼Œä¸ä½¿ç”¨éšæœºèŒƒå›´)
#         precision: int64
#       split_sizes: [128, 64, 64]  # split sizes array (sum=256)
#       dim: 1  # dimension to split
#     # Configuration 3: N=512, splits=[256, 128, 128] (sum=512)
#     - out:
#         _tensor_list: [[M, 256], [M, 128], [M, 128]]  # output tensor list
#         precision: fp32
#       input:
#         _tensor: [M, 512]  # input tensor
#         precision: fp32
#       size:
#         _val: [M]  # size value (å›ºå®šä¸ºMï¼Œä¸ä½¿ç”¨éšæœºèŒƒå›´)
#         precision: int64
#       split_sizes: [256, 128, 128]  # split sizes array (sum=512)
#       dim: 1  # dimension to split
#   output: [out]
#   _name: extsDynamicSplit
#   _mapping: [0, 1, 2, 3, 4]