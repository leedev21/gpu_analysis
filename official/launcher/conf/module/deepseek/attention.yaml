layer:
  name: deepseek_attention
  type:
    - MLA_Fused
    # - MLA
  config:
    MLA:
      - use_mla
    Fused:
      - use_fused_op
  seq_length:
    - 1
    # - 30000
  input:
    - [1, seq_length, 7168, 8192, False]
  init:
    - hidden_size: 7168
      num_attention_heads: 128
      qk_nope_head_dim: 128
      qk_rope_head_dim: 64
      v_head_dim: 128
      q_lora_rank: 1536
      kv_lora_rank: 512
      cache_config: 12
      quant_config: 0.0
      rms_norm_eps: 1e-5
  micro_batch_size: 1
  precision: bf16