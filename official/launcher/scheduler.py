import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
import hydra
import importlib
from launcher.utils import Event, record, print_rank_0, _get_env_cfg
import torch
import time
from omegaconf import OmegaConf, DictConfig
from report import report
from tqdm import tqdm  # å¯¼å…¥tqdmåº“ç”¨äºé«˜æ•ˆæ˜¾ç¤ºè¿›åº¦æ¡
import traceback
from moprobe.advanced_compare import draw_manager
from moprobe.utils import acc_check_helper
import logging
from datetime import datetime
import re
from copy import deepcopy
try:
    from torchtrace.torchtrace import set_torchtrace, update
except ImportError:
    def set_torchtrace(**kwargs):
        pass

    def update(*args, **kwargs):
        pass

def get_module_name(args):
    if args.get("stages"):
        return args['stages']
    return args['name']


def test_module(runner, model, train_iterator, forward_backward_func, args):
    acc = None
    steps = args.run.max_steps
    time_befor_step_cuda = Event()
    time_after_step_cuda = Event()

    time_start = time.perf_counter()
    record(time_befor_step_cuda)

    if model['cuda_graph']:
        model['cuda_graph'].replay()
    else:
        acc = runner.run_iter(model['model'], train_iterator, forward_backward_func, steps, args)

    step_time_cuda = record(time_befor_step_cuda, time_after_step_cuda)
    time_end = time.perf_counter()
    return (time_end - time_start) * 1000 / steps, step_time_cuda * 1000 / steps, acc

nsys_profile_started = False
def run_test_case(runner, _i, total_cases, device, hw_name, test_case, args, error_logger):
    global nsys_profile_started
    acc = None
    test_success = False
    failed_case_num = 0
    _mean, _min, _max = 0, 0, 0
    try:
        # è·å–ç®—å­åç§°
        op_name = test_case.get('name', 'Unknown')
        model, data_iterator, forward_backward_func = runner.prepare_test_case(test_case, args)
        if not data_iterator:
            return None, 0.0, 0.0, 0.0, test_success, 1

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†çœŸå®æ•°æ®
        if hasattr(data_iterator, 'use_real_data') and data_iterator.use_real_data:
            print(f"ğŸ”„ [{_i+1}/{total_cases}] ä½¿ç”¨çœŸå®æ•°æ®æµ‹è¯•ç®—å­: {op_name}")
        else:
            print(f"ğŸ”„ [{_i+1}/{total_cases}] ä½¿ç”¨éšæœºæ•°æ®æµ‹è¯•ç®—å­: {op_name}")

        time_cuda_list = []
        if 'cuda_graph' in args.run and args.run.cuda_graph:
            try:
                with torch.cuda.graph(model['cuda_graph']):
                    runner.run_iter(model['model'], data_iterator, forward_backward_func, args.run.max_steps, args)
            except:
                model['cuda_graph'] = None
        else:
            model['cuda_graph'] = None

        # æ‰§è¡Œæµ‹è¯•å¾ªç¯
        for i in range(args.run.loop_time):
            if not nsys_profile_started and device.type == 'cuda' and args.run.nsys_profile.enabled:
                if i == args.run.nsys_profile.start_step and rank in args.run.nsys_profile.ranks:
                    print_rank_0("====== Start nsys profiling ======")
                    torch.cuda.cudart().cudaProfilerStart()
                    if args.run.nsys_profile.gen_shape:
                        torch.autograd.profiler.emit_nvtx(record_shapes=True).__enter__()
                    nsys_profile_started = True
            time_cpu, time_cuda, acc = test_module(runner, model, data_iterator, forward_backward_func, args)
            time_cuda_list.append(time_cuda)
            if acc and args.run.loop_time > 10:
                acc_check_helper.add(_i, model['model'][0], test_case, hw_name, acc)
                if i > 0:
                    detail_msg = acc_check_helper.check_this(args)
                    failed_case_num += print_info(0, test_case, hw_name, args, runner.get_report_format(args), time_cuda, time_cuda, time_cuda, detail_msg)

        del data_iterator
        torch.cuda.empty_cache()
        time_cuda_list.sort(reverse=True)
        sum_start = args.run.warm_up if hasattr(args.run, 'warm_up') and len(time_cuda_list) > args.run.warm_up else 0
        _mean = torch.tensor(sum(time_cuda_list[sum_start:]) / len(time_cuda_list[sum_start:]), device=device)
        _min = torch.tensor(min(time_cuda_list[sum_start:]), device=device)
        _max = torch.tensor(max(time_cuda_list[sum_start:]), device=device)
        if torch.distributed.is_initialized():
            if torch.distributed.get_backend() == 'gloo':
                torch.distributed.all_reduce(_min.cpu(), torch.distributed.ReduceOp.MIN)
                torch.distributed.all_reduce(_max.cpu(), torch.distributed.ReduceOp.MAX)
            else:
                if torch.distributed.get_backend() != 'eccl':
                    torch.distributed.all_reduce(_mean, torch.distributed.ReduceOp.AVG)
                torch.distributed.all_reduce(_min, torch.distributed.ReduceOp.MIN)
                torch.distributed.all_reduce(_max, torch.distributed.ReduceOp.MAX)
        _mean = _mean.detach().item()
        _min = _min.detach().item()
        _max = _max.detach().item()
        if acc:
            if 'op' in args.stages:
                failed_case_num += print_info(_i, test_case, hw_name, args, runner.get_report_format(args), _mean, _min, _max, acc)
            elif args.run.loop_time <= 10:
                acc_check_helper.add(_i, model['model'][0], test_case, hw_name, acc)
        else:
            report(test_case, _mean, _min, _max, acc)
            report.instant_report(args, {'hw': hw_name, 'format': runner.get_report_format(args)})

        test_success = True

    except Exception as e:
        # è®°å½•é”™è¯¯ä¿¡æ¯åˆ°å¯¹åº”ç®—å­çš„æ—¥å¿—æ–‡ä»¶
        traceback_str = traceback.format_exc()
        failed_case_num += 1

        # ä½¿ç”¨æ–°çš„é”™è¯¯æ—¥å¿—è®°å½•å™¨
        log_case = deepcopy(test_case)
        for k in ['init', 'input']:
            if k in log_case:
                if 'load' in log_case[k]:
                    del log_case[k]['load']
        error_logger.log_error(op_name, _i, log_case, e, traceback_str)

        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºç®€åŒ–çš„é”™è¯¯ä¿¡æ¯
        print(f"\nâŒ æµ‹è¯•ç”¨ä¾‹ #{_i+1} å¤±è´¥: {op_name}")
        print(f"   é”™è¯¯: {str(e)}")
        print(f"   è¯¦ç»†ä¿¡æ¯å·²è®°å½•åˆ°ç®—å­ä¸“ç”¨æ—¥å¿—æ–‡ä»¶")

        # å¦‚æœå¯ç”¨äº†çœŸå®æ•°æ®ï¼Œé¢å¤–æ˜¾ç¤ºæ•°æ®æ¥æºä¿¡æ¯
        if hasattr(args.run, 'enable_real_data') and args.run.enable_real_data:
            print(f"   æ•°æ®æ¥æº: çœŸå®æ•°æ®ï¼ˆå¦‚å¯ç”¨ï¼‰")

        # æ¸…ç†èµ„æº
        try:
            if 'data_iterator' in locals():
                del data_iterator
            torch.cuda.empty_cache()
        except:
            pass

    return acc, _mean, _min, _max, test_success, failed_case_num


def print_info(_i, case, hw_name, args, report_format, _mean, _min, _max, acc):
    test_case = deepcopy(case)
    for k in ['init', 'input']:
        if k in test_case:
            if 'load' in test_case[k]:
                del test_case[k]['load']
    failed_case_num = 0
    formatted_string_acc = " \n" + "\n".join(
        "\n ".join(f"{header}: {value}" for header, value in zip(report_format, row))
        for row in acc
    ) if acc else ''
    if int(os.environ.get('RANK', 0)) == 0:
        print('\n', _i, test_case, _mean, _min, _max, formatted_string_acc)
    else:
        print_rank_0('\n', _i, test_case, _mean, _min, _max, formatted_string_acc)
    report(test_case, _mean, _min, _max, acc)
    report.instant_report(args, {'hw': hw_name, 'format': report_format})
    if acc:
        for res in acc:
            if res[0] != 'pass':
                failed_case_num += 1
    if acc and args.run.draw:
        draw_manager.draw({'hw': hw_name, 'format': report_format})
    return failed_case_num


def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤ä¸åˆæ³•çš„å­—ç¬¦"""
    # æ›¿æ¢ä¸åˆæ³•çš„æ–‡ä»¶åå­—ç¬¦
    filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
    # æ›¿æ¢åŒå†’å·ä¸ºå•ä¸‹åˆ’çº¿
    filename = filename.replace('::', '_')
    # ç§»é™¤å¼€å¤´çš„ç‚¹å·
    filename = filename.lstrip('.')
    # é™åˆ¶æ–‡ä»¶åé•¿åº¦
    if len(filename) > 100:
        filename = filename[:100]
    return filename


class OperatorErrorLogger:
    """æŒ‰ç®—å­åç§°åˆ†ç±»çš„é”™è¯¯æ—¥å¿—è®°å½•å™¨"""

    def __init__(self):
        self.error_log_dir = "error_logs"
        os.makedirs(self.error_log_dir, exist_ok=True)

        self.timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        self.loggers = {}  # å­˜å‚¨æ¯ä¸ªç®—å­çš„æ—¥å¿—è®°å½•å™¨
        self.log_files = {}  # å­˜å‚¨æ¯ä¸ªç®—å­çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„

        # åˆ›å»ºæ€»ä½“é”™è¯¯ç»Ÿè®¡æ—¥å¿—
        self.summary_log_file = os.path.join(self.error_log_dir, f"error_summary_{self.timestamp}.log")
        self.summary_logger = self._create_logger('error_summary', self.summary_log_file)

    def _create_logger(self, logger_name, log_file):
        """åˆ›å»ºå•ä¸ªæ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger(logger_name)
        logger.setLevel(logging.ERROR)

        # æ¸…é™¤å·²æœ‰çš„å¤„ç†å™¨
        logger.handlers.clear()

        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.ERROR)

        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)

        # æ·»åŠ å¤„ç†å™¨åˆ°æ—¥å¿—è®°å½•å™¨
        logger.addHandler(file_handler)

        return logger

    def get_operator_logger(self, op_name):
        """è·å–æˆ–åˆ›å»ºæŒ‡å®šç®—å­çš„æ—¥å¿—è®°å½•å™¨"""
        if op_name not in self.loggers:
            # æ¸…ç†ç®—å­åç§°ä½œä¸ºæ–‡ä»¶å
            safe_op_name = sanitize_filename(op_name)
            log_file = os.path.join(self.error_log_dir, f"{safe_op_name}_errors_{self.timestamp}.log")

            # åˆ›å»ºç®—å­ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
            logger_name = f"error_{safe_op_name}"
            logger = self._create_logger(logger_name, log_file)

            self.loggers[op_name] = logger
            self.log_files[op_name] = log_file

        return self.loggers[op_name]

    def log_error(self, op_name, test_case_index, test_case, error, traceback_str):
        """è®°å½•ç®—å­é”™è¯¯"""
        # è·å–ç®—å­ä¸“ç”¨çš„æ—¥å¿—è®°å½•å™¨
        op_logger = self.get_operator_logger(op_name)

        # è¯¦ç»†é”™è¯¯ä¿¡æ¯
        error_msg = f"""
æµ‹è¯•ç”¨ä¾‹ #{test_case_index+1} æ‰§è¡Œå¤±è´¥:
ç®—å­åç§°: {op_name}
æµ‹è¯•å‚æ•°: {test_case}
é”™è¯¯ç±»å‹: {type(error).__name__}
é”™è¯¯ä¿¡æ¯: {str(error)}
å †æ ˆè·Ÿè¸ª:
{traceback_str}
{'='*80}
"""

        # è®°å½•åˆ°ç®—å­ä¸“ç”¨æ—¥å¿—
        op_logger.error(error_msg)

        # è®°å½•åˆ°æ€»ä½“ç»Ÿè®¡æ—¥å¿—
        summary_msg = f"ç®—å­ {op_name} - æµ‹è¯•ç”¨ä¾‹ #{test_case_index+1} å¤±è´¥: {str(error)}"
        self.summary_logger.error(summary_msg)

    def log_info(self, message):
        """è®°å½•ä¿¡æ¯åˆ°æ€»ä½“æ—¥å¿—"""
        self.summary_logger.info(message)

    def get_log_files_info(self):
        """è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶ä¿¡æ¯"""
        return {
            'summary': self.summary_log_file,
            'operators': self.log_files.copy()
        }


@hydra.main(config_path="conf", config_name="config", version_base="1.2")
def main(args) -> None:
    global nsys_profile_started
    OmegaConf.resolve(args)
    n_device, hw_name = _get_env_cfg()
    if args.run.draw:
        draw_manager.set('draw_ulp', True)
        draw_manager.set('hw_name', hw_name)
        if args.run.load_op_summary:
            import json
            user_config_summary_file = isinstance(args.run.load_op_summary, str)
            summary_file = args.run.load_op_summary if user_config_summary_file else draw_manager.summary_file
            with open(summary_file, 'r') as f:
                if user_config_summary_file:
                    draw_manager.set('summary_file', summary_file)
                draw_manager.set('summary', json.load(f))

    failed_case_num = 0
    successful_case_num = 0
    error_case_num = 0

    # åˆ›å»ºæŒ‰ç®—å­åˆ†ç±»çš„é”™è¯¯æ—¥å¿—è®°å½•å™¨
    error_logger = OperatorErrorLogger()

    if hw_name == 'None':
        print('HW not support!!!', hw_name)
        exit(1)
    print(args)

    # è®°å½•æµ‹è¯•å¼€å§‹ä¿¡æ¯
    error_logger.log_info(f"å¼€å§‹æµ‹è¯• - ç¡¬ä»¶: {hw_name}, é…ç½®: {args}")

    # myTorchTraceMode.__enter__()
    for stage in get_module_name(args):
        import_path = 'runner.' + stage + '.function'
        runner = importlib.import_module(import_path)

        # åˆå§‹åŒ–çœŸå®æ•°æ®åŠ è½½å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(args.run, 'enable_real_data') and args.run.enable_real_data:
            dump_json_path = getattr(args.run, 'dump_json_path', None)
            pt_data_dir = getattr(args.run, 'pt_data_dir', None)

            if dump_json_path and pt_data_dir:
                # è®¾ç½®çœŸå®æ•°æ®åŠ è½½å™¨
                if hasattr(runner, 'data_loader'):
                    runner.data_loader.set_real_data_loader(
                        dump_json_path=dump_json_path,
                        pt_data_dir=pt_data_dir,
                        enable_real_data=args.run.enable_real_data
                    )
                    print(f"âœ… çœŸå®æ•°æ®åŠ è½½å™¨å·²å¯ç”¨:")
                    print(f"   dump.json è·¯å¾„: {dump_json_path}")
                    print(f"   pt æ–‡ä»¶ç›®å½•: {pt_data_dir}")
                else:
                    print("âš ï¸  å½“å‰runneræ¨¡å—ä¸æ”¯æŒçœŸå®æ•°æ®åŠ è½½å™¨")
            else:
                print("âš ï¸  å¯ç”¨çœŸå®æ•°æ®åŠ è½½éœ€è¦åŒæ—¶æŒ‡å®š dump_json_path å’Œ pt_data_dir")

        device, rank = runner.initialize_distributed(args)
        runner.manual_seed(rank)

        nsys_profile_started = False

        all_test_cases = list(runner.splite_test_case(args))
        total_cases = len(all_test_cases)

        test_start_time = time.time()

        print(f"\nå¼€å§‹æµ‹è¯• {total_cases} ä¸ªæµ‹è¯•ç”¨ä¾‹...")
        print(f"é”™è¯¯æ—¥å¿—å°†ä¿å­˜åˆ°: {error_logger.error_log_dir}/")

        with tqdm(total=total_cases, desc="å¤„ç†æµ‹è¯•ç”¨ä¾‹", unit="case", ncols=100) as pbar:
            # Running the model for n iterations
            acc = None
            for _i, test_case in enumerate(all_test_cases):
                if report.check_limit(test_case, {'hw': hw_name}, getattr(args.run, 'duration_limit')):
                    pbar.update(1)
                    continue

                if 'acc_check' in args.run and args.run.acc_check and args.run.loop_time > 10:
                    acc_check_helper.clear()
                vllm_enable = True if 'use_vllm_backend' in test_case else False

                if vllm_enable:
                    from launcher.vllm_backend import init_distributed_vllm
                    from vllm.model_executor.layers.quantization.fp8 import Fp8Config
                    from vllm.config import VllmConfig, ParallelConfig, CompilationConfig, set_current_vllm_config, get_current_vllm_config
                    quant_config = Fp8Config(is_checkpoint_fp8_serialized=True, weight_block_size=[128, 128])
                    parallel_config = ParallelConfig(data_parallel_size=8, pipeline_parallel_size=1, enable_expert_parallel=True)
                    vllm_config = VllmConfig(quant_config=quant_config, parallel_config=parallel_config)
                    with set_current_vllm_config(vllm_config):
                        init_distributed_vllm()
                        acc, _mean, _min, _max, test_success, _failed_case_num = run_test_case(runner, _i, total_cases, device, hw_name, test_case, args, error_logger)
                else:
                    acc, _mean, _min, _max, test_success, _failed_case_num = run_test_case(runner, _i, total_cases, device, hw_name, test_case, args, error_logger)
                failed_case_num += _failed_case_num
                if test_success:
                    successful_case_num += 1
                else:
                    error_case_num += 1

                pbar.update(1)
            if acc:
                if 'op' not in args.stages:
                    if args.run.loop_time <= 10:
                        detail_msg = acc_check_helper.check(args)
                        failed_case_num += print_info(0, test_case, hw_name, args, runner.get_report_format(args), _mean, _min, _max, detail_msg)
                draw_manager.save_summary()

        test_end_time = time.time()
        total_test_time = test_end_time - test_start_time
        hours, remainder = divmod(total_test_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        time_format = ""
        if hours > 0:
            time_format += f"{int(hours)}å°æ—¶"
        if minutes > 0:
            time_format += f"{int(minutes)}åˆ†é’Ÿ"
        time_format += f"{seconds:.2f}ç§’"

        print("\n\n" + "="*80)
        print("æµ‹è¯•å®Œæˆç»Ÿè®¡:")
        print(f"æ€»æµ‹è¯•ç”¨ä¾‹æ•°: {total_cases}")
        print(f"âœ… æˆåŠŸ: {successful_case_num}")
        print(f"âŒ å¤±è´¥: {error_case_num}")
        print(f"âš ï¸  å…¶ä»–å¤±è´¥: {failed_case_num}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {time_format} ({total_test_time:.2f}ç§’)")

        # æ˜¾ç¤ºæ—¥å¿—æ–‡ä»¶ä¿¡æ¯
        log_files_info = error_logger.get_log_files_info()
        print(f"\nğŸ“‹ æ—¥å¿—æ–‡ä»¶:")
        print(f"  ğŸ“„ æ€»ä½“é”™è¯¯ç»Ÿè®¡: {log_files_info['summary']}")
        if log_files_info['operators']:
            print(f"  ğŸ“ ç®—å­ä¸“ç”¨é”™è¯¯æ—¥å¿—:")
            for op_name, log_file in log_files_info['operators'].items():
                print(f"    - {op_name}: {log_file}")
        else:
            print(f"  ğŸ‰ æ²¡æœ‰ç®—å­é”™è¯¯æ—¥å¿—ï¼ˆæ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸï¼ï¼‰")

        print("="*80)

        if hasattr(report, 'op_log_files'):
            print(f"ç”Ÿæˆçš„å…¶ä»–æ—¥å¿—æ–‡ä»¶:")
            for op, path in report.op_log_files.items():
                print(f"  - {op}: {path}")

        if device.type == 'cuda' and args.run.nsys_profile.enabled:
            if rank in args.run.nsys_profile.ranks:
                print_rank_0("====== End nsys profiling ======")
                torch.cuda.cudart().cudaProfilerStop()

    # è®°å½•æµ‹è¯•ç»“æŸä¿¡æ¯
    error_logger.log_info(f"æµ‹è¯•ç»“æŸ - æ€»ç”¨ä¾‹: {total_cases}, æˆåŠŸ: {successful_case_num}, å¤±è´¥: {error_case_num}")

    # æ ¹æ®æ˜¯å¦æœ‰é”™è¯¯å†³å®šé€€å‡ºç ï¼Œä½†ä¸å¼ºåˆ¶é€€å‡º
    if error_case_num > 0:
        print(f"\nâš ï¸  æœ‰ {error_case_num} ä¸ªæµ‹è¯•ç”¨ä¾‹å¤±è´¥ï¼Œè¯¦ç»†ä¿¡æ¯è¯·æŸ¥çœ‹å¯¹åº”çš„ç®—å­é”™è¯¯æ—¥å¿—æ–‡ä»¶")
        # ä¸å†å¼ºåˆ¶é€€å‡ºï¼Œè®©ç”¨æˆ·å†³å®š
        # exit(1)
    else:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹éƒ½æˆåŠŸæ‰§è¡Œï¼")

if __name__ == "__main__":
    if set_torchtrace and os.getenv('RUN_TYPE', '') != 'test':
        try:
            from torchtrace.torchtrace import set_torchtrace, update
            import subprocess
            def is_card():
                try:
                    result = subprocess.run(
                        "dpkg -l | grep sdk",
                        shell=True,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.PIPE,
                        encoding='utf-8'
                    )
                    return "sdk" in result.stdout
                except Exception:
                    return False

            if is_card():
                update('customer_op', {'flash_attn': 'flash_attn_device'})
            else:
                update('customer_op', {'flash_attn': 'flash_attn_2_cuda'})
        except:
            pass
        set_torchtrace(torch_dispatch_trace=True, torch_api_trace=False, save_pt=False, sync_mode=True, save_to=os.path.abspath(os.getenv('SAVE_PATH', './')))

    main()

    if set_torchtrace and os.getenv('RUN_TYPE', '') != 'test':
        set_torchtrace(torch_dispatch_trace=False, torch_api_trace=False)
