- 1_1_fwd_module::Qwen2ForCausalLM:
  - param_name: qwen
  - inputs: ([], {'input_ids': '<1xi32>{1}', 'positions': '<1xint64>{1}', 'intermediate_tensors': 'None:NoneType', 'inputs_embeds': 'None:NoneType'})
  - 1_1_fwd_module::VocabParallelEmbedding:
    - param_name: qwen.model.embed_tokens
    - inputs: (['<1xi32>{1}'], {})
    - 1_1_fwd_module::VocabParallelEmbedding:
      - param_name: qwen.lm_head
      - inputs: (['<1xi32>{1}'], {})
      - name: aten::embedding
        inputs: (weight:<151936x2048xf16>{2048, 1}, indices:<1xint64>{1})
        outputs: <1x2048xf16>{2048, 1}
        duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.0
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', 'None:NoneType'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.0.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.0.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.0.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.0.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_0_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.0.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.0.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.0.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.0.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.0.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.0.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.1
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.1.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.1.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.1.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.1.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_1_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.1.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.1.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.1.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.1.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.1.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.1.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.2
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.2.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.2.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.2.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.2.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_2_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.2.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.2.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.2.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.2.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.2.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.2.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.3
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.3.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.3.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.3.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.3.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_3_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.3.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.3.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.3.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.3.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.3.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.3.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.4
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.4.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.4.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.4.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.4.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_4_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.4.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.4.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.4.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.4.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.4.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.4.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.5
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.5.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.5.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.5.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.5.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_5_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.5.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.5.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.5.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.5.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.5.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.5.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.6
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.6.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.6.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.6.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.6.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_6_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.6.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.6.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.6.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.6.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.6.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.6.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.7
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.7.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.7.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.7.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.7.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_7_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.7.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.7.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.7.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.7.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.7.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.7.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.8
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.8.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.8.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.8.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.8.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_8_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.8.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.8.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.8.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.8.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.8.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.8.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.9
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.9.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.9.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.9.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.9.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_9_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.9.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.9.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.9.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.9.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.9.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.9.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.10
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.10.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.10.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.10.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.10.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_10_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.10.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.10.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.10.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.10.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.10.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.10.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.11
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.11.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.11.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.11.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.11.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_11_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.11.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.11.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.11.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.11.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.11.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.11.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.12
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.12.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.12.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.12.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.12.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_12_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.12.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.12.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.12.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.12.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.12.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.12.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.13
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.13.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.13.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.13.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.13.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_13_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.13.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.13.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.13.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.13.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.13.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.13.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.14
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.14.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.14.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.14.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.14.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_14_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.14.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.14.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.14.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.14.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.14.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.14.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.15
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.15.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.15.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.15.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.15.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_15_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.15.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.15.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.15.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.15.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.15.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.15.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.16
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.16.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.16.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.16.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.16.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_16_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.16.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.16.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.16.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.16.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.16.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.16.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.17
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.17.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.17.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.17.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.17.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_17_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.17.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.17.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.17.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.17.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.17.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.17.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.18
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.18.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.18.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.18.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.18.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_18_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.18.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.18.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.18.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.18.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.18.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.18.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.19
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.19.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.19.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.19.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.19.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_19_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.19.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.19.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.19.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.19.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.19.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.19.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.20
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.20.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.20.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.20.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.20.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_20_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.20.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.20.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.20.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.20.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.20.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.20.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.21
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.21.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.21.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.21.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.21.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_21_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.21.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.21.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.21.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.21.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.21.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.21.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.22
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.22.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.22.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.22.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.22.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_22_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.22.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.22.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.22.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.22.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.22.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.22.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.23
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.23.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.23.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.23.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.23.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_23_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.23.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.23.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.23.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.23.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.23.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.23.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.24
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.24.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.24.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.24.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.24.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_24_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.24.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.24.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.24.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.24.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.24.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.24.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.25
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.25.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.25.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.25.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.25.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_25_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.25.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.25.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.25.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.25.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.25.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.25.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.26
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.26.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.26.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.26.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.26.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_26_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.26.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.26.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.26.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.26.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.26.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.26.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.27
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.27.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.27.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.27.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.27.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_27_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.27.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.27.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.27.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.27.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.27.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.27.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.28
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.28.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.28.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.28.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.28.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_28_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.28.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.28.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.28.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.28.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.28.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.28.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.29
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.29.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.29.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.29.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.29.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_29_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.29.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.29.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.29.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.29.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.29.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.29.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.30
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.30.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.30.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.30.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.30.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_30_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.30.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.30.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.30.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.30.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.30.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.30.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.31
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.31.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.31.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.31.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.31.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_31_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.31.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.31.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.31.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.31.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.31.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.31.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.32
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.32.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.32.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.32.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.32.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_32_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.32.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.32.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.32.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.32.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.32.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.32.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.33
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.33.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.33.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.33.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.33.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_33_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.33.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.33.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.33.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.33.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.33.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.33.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.34
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.34.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.34.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.34.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.34.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_34_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.34.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.34.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.34.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.34.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.34.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.34.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::Qwen2DecoderLayer:
  - param_name: qwen.model.layers.35
  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::RMSNorm:
    - param_name: qwen.model.layers.35.input_layernorm
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
    - name: aten::add
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::pow
      inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mean
      inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::add
      inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::rsqrt
      inputs: (self:<1x1xf32>{1, 1})
      outputs: <1x1xf32>{1, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
      outputs: <1x2048xf32>{2048, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
  - 1_1_fwd_module::Qwen2Attention:
    - param_name: qwen.model.layers.35.self_attn
    - inputs: ([], {'positions': '<1xint64>{1}', 'hidden_states': '<1x2048xf16>{2048, 1}'})
    - 1_1_fwd_module::QKVParallelLinear:
      - param_name: qwen.model.layers.35.self_attn.qkv_proj
      - inputs: (['<1x2048xf16>{2048, 1}'], {})
      - name: _C::gptq_marlin_gemm
        inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x5120xi32>{5120, 1}, b_scales:<16x2560xf16>{2560, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x320xi32>{320, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2560:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
      - name: aten::add_
        inputs: (self:<1x2560xf16>{2560, 1}, other:<2560xf16>{1})
        outputs: <1x2560xf16>{2560, 1}
        duration: -1
    - 1_1_fwd_module::RotaryEmbedding:
      - param_name: qwen.model.layers.0.self_attn.rotary_emb
      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
      - 1_1_fwd_module::RotaryEmbedding:
        - param_name: qwen.model.layers.1.self_attn.rotary_emb
        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
        - 1_1_fwd_module::RotaryEmbedding:
          - param_name: qwen.model.layers.2.self_attn.rotary_emb
          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
          - 1_1_fwd_module::RotaryEmbedding:
            - param_name: qwen.model.layers.3.self_attn.rotary_emb
            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
            - 1_1_fwd_module::RotaryEmbedding:
              - param_name: qwen.model.layers.4.self_attn.rotary_emb
              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
              - 1_1_fwd_module::RotaryEmbedding:
                - param_name: qwen.model.layers.5.self_attn.rotary_emb
                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                - 1_1_fwd_module::RotaryEmbedding:
                  - param_name: qwen.model.layers.6.self_attn.rotary_emb
                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                  - 1_1_fwd_module::RotaryEmbedding:
                    - param_name: qwen.model.layers.7.self_attn.rotary_emb
                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                    - 1_1_fwd_module::RotaryEmbedding:
                      - param_name: qwen.model.layers.8.self_attn.rotary_emb
                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                      - 1_1_fwd_module::RotaryEmbedding:
                        - param_name: qwen.model.layers.9.self_attn.rotary_emb
                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                        - 1_1_fwd_module::RotaryEmbedding:
                          - param_name: qwen.model.layers.10.self_attn.rotary_emb
                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                          - 1_1_fwd_module::RotaryEmbedding:
                            - param_name: qwen.model.layers.11.self_attn.rotary_emb
                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                            - 1_1_fwd_module::RotaryEmbedding:
                              - param_name: qwen.model.layers.12.self_attn.rotary_emb
                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                              - 1_1_fwd_module::RotaryEmbedding:
                                - param_name: qwen.model.layers.13.self_attn.rotary_emb
                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                - 1_1_fwd_module::RotaryEmbedding:
                                  - param_name: qwen.model.layers.14.self_attn.rotary_emb
                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                  - 1_1_fwd_module::RotaryEmbedding:
                                    - param_name: qwen.model.layers.15.self_attn.rotary_emb
                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                    - 1_1_fwd_module::RotaryEmbedding:
                                      - param_name: qwen.model.layers.16.self_attn.rotary_emb
                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                      - 1_1_fwd_module::RotaryEmbedding:
                                        - param_name: qwen.model.layers.17.self_attn.rotary_emb
                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                        - 1_1_fwd_module::RotaryEmbedding:
                                          - param_name: qwen.model.layers.18.self_attn.rotary_emb
                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                          - 1_1_fwd_module::RotaryEmbedding:
                                            - param_name: qwen.model.layers.19.self_attn.rotary_emb
                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                            - 1_1_fwd_module::RotaryEmbedding:
                                              - param_name: qwen.model.layers.20.self_attn.rotary_emb
                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                              - 1_1_fwd_module::RotaryEmbedding:
                                                - param_name: qwen.model.layers.21.self_attn.rotary_emb
                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                - 1_1_fwd_module::RotaryEmbedding:
                                                  - param_name: qwen.model.layers.22.self_attn.rotary_emb
                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                    - param_name: qwen.model.layers.23.self_attn.rotary_emb
                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                      - param_name: qwen.model.layers.24.self_attn.rotary_emb
                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                        - param_name: qwen.model.layers.25.self_attn.rotary_emb
                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                          - param_name: qwen.model.layers.26.self_attn.rotary_emb
                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                            - param_name: qwen.model.layers.27.self_attn.rotary_emb
                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                            - 1_1_fwd_module::RotaryEmbedding:
                                                              - param_name: qwen.model.layers.28.self_attn.rotary_emb
                                                              - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                              - 1_1_fwd_module::RotaryEmbedding:
                                                                - param_name: qwen.model.layers.29.self_attn.rotary_emb
                                                                - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                - 1_1_fwd_module::RotaryEmbedding:
                                                                  - param_name: qwen.model.layers.30.self_attn.rotary_emb
                                                                  - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                  - 1_1_fwd_module::RotaryEmbedding:
                                                                    - param_name: qwen.model.layers.31.self_attn.rotary_emb
                                                                    - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                    - 1_1_fwd_module::RotaryEmbedding:
                                                                      - param_name: qwen.model.layers.32.self_attn.rotary_emb
                                                                      - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                      - 1_1_fwd_module::RotaryEmbedding:
                                                                        - param_name: qwen.model.layers.33.self_attn.rotary_emb
                                                                        - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                        - 1_1_fwd_module::RotaryEmbedding:
                                                                          - param_name: qwen.model.layers.34.self_attn.rotary_emb
                                                                          - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                          - 1_1_fwd_module::RotaryEmbedding:
                                                                            - param_name: qwen.model.layers.35.self_attn.rotary_emb
                                                                            - inputs: (['<1xint64>{1}', '<1x2048xf16>{2560, 1}', '<1x256xf16>{2560, 1}+2048'], {})
                                                                            - name: aten::index_select
                                                                              inputs: (self:<32768x128xf16>{128, 1}, dim:0:int, index:<1xint64>{1})
                                                                              outputs: <1x128xf16>{128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}+64, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x16x64xf16>{2048, 128, 1}, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x16x64xf16>{1024, 64, 1}, other:<1x16x64xf16>{1024, 64, 1})
                                                                              outputs: <1x16x64xf16>{1024, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x64xf16>{1024, 64, 1}', '<1x16x64xf16>{1024, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x16x128xf16>{2048, 128, 1}', '<1x16x0xf16>{2048, 128, 1}+128'], dim:-1:int)
                                                                              outputs: <1x16x128xf16>{2048, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::sub
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2112, other:<1x1x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::mul
                                                                              inputs: (self:<1x2x64xf16>{256, 128, 1}+2048, other:<1x1x64xf16>{128, 64, 1}+64)
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::add
                                                                              inputs: (self:<1x2x64xf16>{128, 64, 1}, other:<1x2x64xf16>{128, 64, 1})
                                                                              outputs: <1x2x64xf16>{128, 64, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x64xf16>{128, 64, 1}', '<1x2x64xf16>{128, 64, 1}'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
                                                                            - name: aten::cat
                                                                              inputs: (tensors:['<1x2x128xf16>{256, 128, 1}', '<1x2x0xf16>{256, 128, 1}+2176'], dim:-1:int)
                                                                              outputs: <1x2x128xf16>{256, 128, 1}
                                                                              duration: -1
  - 1_1_fwd_module::Attention:
    - param_name: qwen.model.layers.35.self_attn.attn
    - inputs: (['<1x2048xf16>{2048, 1}', '<1x256xf16>{256, 1}', '<1x256xf16>{2560, 1}+2304'], {})
    - name: vllm::unified_attention_with_output
      inputs: (query:<1x16x128xf16>{2048, 128, 1}, key:<1x2x128xf16>{256, 128, 1}, value:<1x2x128xf16>{256, 128, 1}+2304, output:<1x16x128xf16>{2048, 128, 1}, layer_name:model_layers_35_self_attn_attn:str)
      outputs: None:NoneType
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.35.self_attn.o_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x4096xi32>{4096, 1}, b_scales:<16x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.layers.35.post_attention_layernorm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- 1_1_fwd_module::Qwen2MLP:
  - param_name: qwen.model.layers.35.mlp
  - inputs: (['<1x2048xf16>{2048, 1}'], {})
  - 1_1_fwd_module::MergedColumnParallelLinear:
    - param_name: qwen.model.layers.35.mlp.gate_up_proj
    - inputs: (['<1x2048xf16>{2048, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x2048xf16>{2048, 1}, c_or_none:None:NoneType, b_q_weight:<128x44032xi32>{44032, 1}, b_scales:<16x22016xf16>{22016, 1}, global_scale:None:NoneType, b_zeros_or_none:<16x2752xi32>{2752, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:22016:int, size_k:2048:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x22016xf16>{22016, 1}
      duration: -1
  - 1_1_fwd_module::SiluAndMul:
    - param_name: qwen.model.layers.35.mlp.act_fn
    - inputs: (['<1x22016xf16>{22016, 1}'], {})
    - name: aten::silu
      inputs: (self:<1x11008xf16>{22016, 1})
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
    - name: aten::mul
      inputs: (self:<1x11008xf16>{11008, 1}, other:<1x11008xf16>{22016, 1}+11008)
      outputs: <1x11008xf16>{11008, 1}
      duration: -1
  - 1_1_fwd_module::RowParallelLinear:
    - param_name: qwen.model.layers.35.mlp.down_proj
    - inputs: (['<1x11008xf16>{11008, 1}'], {})
    - name: _C::gptq_marlin_gemm
      inputs: (a:<1x11008xf16>{11008, 1}, c_or_none:None:NoneType, b_q_weight:<688x4096xi32>{4096, 1}, b_scales:<86x2048xf16>{2048, 1}, global_scale:None:NoneType, b_zeros_or_none:<86x256xi32>{256, 1}, g_idx_or_none:<0xi32>{1}, perm_or_none:<0xi32>{1}, workspace:<108xi32>{1}, b_q_type:1125899906843648:int, size_m:1:int, size_n:2048:int, size_k:11008:int, is_k_full:True:bool, use_atomic_add:False:bool, use_fp32_reduce:True:bool, is_zp_float:False:bool)
      outputs: <1x2048xf16>{2048, 1}
      duration: -1
- 1_1_fwd_module::RMSNorm:
  - param_name: qwen.model.norm
  - inputs: (['<1x2048xf16>{2048, 1}', '<1x2048xf16>{2048, 1}'], {})
  - name: aten::add
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x2048xf32>{2048, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::pow
    inputs: (self:<1x2048xf32>{2048, 1}, exponent:2:int)
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mean
    inputs: (self:<1x2048xf32>{2048, 1}, dim:['-1:int'], keepdim:True:bool)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::add
    inputs: (self:<1x1xf32>{1, 1}, other:1e-06:float)
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::rsqrt
    inputs: (self:<1x1xf32>{1, 1})
    outputs: <1x1xf32>{1, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf32>{2048, 1}, other:<1x1xf32>{1, 1})
    outputs: <1x2048xf32>{2048, 1}
    duration: -1
  - name: aten::mul
    inputs: (self:<1x2048xf16>{2048, 1}, other:<2048xf16>{1})
    outputs: <1x2048xf16>{2048, 1}
    duration: -1
- name: aten::index
  inputs: (self:<1x2048xf16>{2048, 1}, indices:['<1xi32>{1}'])
  outputs: <1x2048xf16>{2048, 1}
  duration: -1
- 1_1_fwd_module::LogitsProcessor:
  - param_name: qwen.logits_processor
  - inputs: (['VocabParallelEmbedding(num_embeddings=151936,_embedding_dim=2048,_org_vocab_size=151936,_num_embeddings_padded=151936,_tp_size=1):VocabParallelEmbedding', '<1x2048xf16>{2048, 1}', 'None:NoneType'], {})
  - name: aten::linear
    inputs: (input:<1x2048xf16>{2048, 1}, weight:<151936x2048xf16>{2048, 1})
    outputs: <1x151936xf16>{151936, 1}
    duration: -1
- name: aten::argmax
  inputs: (self:<1x151936xf32>{151936, 1}, dim:-1:int)
  outputs: <1xint64>{1}
  duration: -1
- name: aten::copy_
  inputs: (self:<1x2048xi32>{2048, 1}, src:<1x2048xi32>{2048, 1}, non_blocking:True:bool)
  outputs: <1x2048xi32>{2048, 1}
  duration: -1
- name: aten::index_select
  inputs: (self:<8388608xi32>{1}, dim:0:int, index:<1xint64>{1}, out:<1xi32>{1})
  outputs: <1xi32>{1}
  duration: -1
- name: aten::index
  inputs: (self:<524288xi32>{1}, indices:['<1xint64>{1}'])
  outputs: <1xi32>{1}
  duration: -1
- name: aten::copy_
  inputs: (self:<1xi32>{1}, src:<1xi32>{1}, non_blocking:True:bool)
  outputs: <1xi32>{1}
  duration: -1
- name: aten::copy_
  inputs: (self:<1xint64>{1}, src:<1xint64>{1}, non_blocking:True:bool)
  outputs: <1xint64>{1}
  duration: -1
- name: aten::copy_
  inputs: (self:<2xi32>{1}, src:<2xi32>{1}, non_blocking:True:bool)
  outputs: <2xi32>{1}
  duration: -1
- name: aten::copy_
  inputs: (self:<1xi32>{1}, src:<1xi32>{1}, non_blocking:True:bool)
  outputs: <1xi32>{1}
  duration: -1
- name: aten::fill_
  inputs: (self:<255xi32>{1}+1, value:0:int)
  outputs: <255xi32>{1}+1
  duration: -1
- name: aten::fill_
  inputs: (self:<255xi32>{1}+2, value:-1:int)
  outputs: <255xi32>{1}+2
  duration: -1
- name: aten::copy_
  inputs: (self:<1xint64>{1}, src:<1xint64>{1}, non_blocking:True:bool)
  outputs: <1xint64>{1}
  duration: -1
- name: aten::fill_
  inputs: (self:<8191xint64>{1}+1, value:-1:int)
  outputs: <8191xint64>{1}+1
  duration: -1
- name: aten::sub
  inputs: (self:<1xi32>{1}+1, other:1:int)
  outputs: <1xi32>{1}
  duration: -1