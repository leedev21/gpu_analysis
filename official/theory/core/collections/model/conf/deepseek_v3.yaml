model:
  name: DeepSeek_v3_A37B
  size: 671B
  num_layers: 61
  hidden_size: 7168
  num_attention_heads: 128
  num_hidden_dim: 128
  kv_lora_rank: 512
  q_lora_rank: 1536
  qk_nope_head_dim: 128
  qk_rope_head_dim: 64
  ffn_hidden_size: 18432
  n_shared_experts: 1
  n_routed_experts: 256
  moe_intermediate_size: 2048
  moe_router_topk: 8
  n_expert_groups: 8
  n_limited_groups: 4
  swiglu: true
  moe_layer_freq: 1
  first_k_dense_replace: 3
  vocab_size: 129280
  attention_type: mla
  position_embedding_type: rope
  normalization: RMSNorm
  score_func: sigmoid
  routed_scaling_factor: 2.5
  per_group_quant_fp8: 128