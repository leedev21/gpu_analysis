model:
  name: qwen2_5_vl_72b
  size: 72
  num_layers: 80
  hidden_size: 8192
  ffn_hidden_size: 29568
  num_attention_heads: 64
  num_hidden_dim: 128
  num_query_groups: 8
  vocab_size: 152064
  swiglu: true
  normalization: rmsnorm
  # position_embedding_type: rope
  attention_type: multihead
  max_window_layers: 80
  num_key_value_heads: 8
  sliding_window: 32768
  vision_config:
    depth: 32
    hidden_act: silu
    hidden_size: 1280
    in_chans: 3
    intermediate_size: 3456
    num_heads: 16
    out_hidden_size: 8192
    patch_size: 14
    spatial_merge_size: 2
    spatial_patch_size: 14
    temporal_patch_size: 2
    tokens_per_second: 2
    window_size: 112