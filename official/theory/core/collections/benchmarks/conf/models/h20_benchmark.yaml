H20_70B_Node4_4K:
  model: llama2_70b
  size: 70
  hw: H20SXM
  sw: Megatron-LM
  n_device: 32
  n_nodes: 4
  tensor_model_parallel_size: 8
  pipeline_model_parallel_size: 4
  context_model_parallel_size: 2
  micro_batch_size: 2
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 128
  precision: bf16
  dataset: oscar
  tps_per_gpu: 305.92
H20_70B_Node4_4K_fp8:
  model: llama2_70b
  size: 70
  hw: H20SXM
  sw: Megatron-LM
  n_device: 32
  n_nodes: 4
  tensor_model_parallel_size: 8
  pipeline_model_parallel_size: 4
  context_model_parallel_size: 1
  micro_batch_size: 2
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 128
  precision: fp8
  dataset: oscar
  tps_per_gpu: 444.16
H20_8B_Node1_8K:
  model: llama3_8b
  size: 8
  hw: H20SXM
  sw: Megatron-LM
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 2
  context_model_parallel_size: 1
  micro_batch_size: 1
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 64
  precision: bf16
  dataset: oscar
  tps_per_gpu: 2283
H20_8B_Node1_8K_fp8:
  model: llama3_8b
  size: 8
  hw: H20SXM
  sw: Megatron-LM
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 2
  context_model_parallel_size: 1
  micro_batch_size: 1
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 64
  precision: fp8
  dataset: oscar
  tps_per_gpu: 3379
H20_13B_Node1_4K:
  model: llama2_13b
  size: 13
  hw: H20SXM
  sw: Megatron-LM
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 4
  context_model_parallel_size: 1
  micro_batch_size: 1
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 128
  precision: bf16
  dataset: oscar
  tps_per_gpu: 1464.32
H20_13B_Node1_4K_fp8:
  model: llama2_13b
  size: 13
  hw: H20SXM
  sw: Megatron-LM
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 4
  context_model_parallel_size: 1
  micro_batch_size: 1
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 128
  precision: fp8
  dataset: oscar
  tps_per_gpu: 2406.4
H20_7B_Node1_4K:
  model: llama2_7b
  size: 7
  hw: H20SXM
  sw: Megatron-LM
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  context_model_parallel_size: 1
  micro_batch_size: 1
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 32
  precision: bf16
  dataset: oscar
  tps_per_gpu: 2928.64
H20_7B_Node1_4K_fp8:
  model: llama2_7b
  size: 7
  hw: H20SXM
  sw: Megatron-LM
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  context_model_parallel_size: 1
  micro_batch_size: 1
  seq_length: 4096
  global_batch_size: 256
  gradient_accumulation_steps: 32
  precision: fp8
  dataset: oscar
  tps_per_gpu: 4771.84