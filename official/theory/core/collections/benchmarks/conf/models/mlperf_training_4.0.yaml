H100_175B_Node1344_2K:
  model: gpt175b
  size: 175
  hw: H100SXM
  sw: Nemo24.04
  n_device: 10752
  n_nodes: 1344
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 8
  micro_batch_size: 1
  seq_length: 2048
  global_batch_size: 2688
  gradient_accumulation_steps: 6
  precision: fp8
  trained_samples: 1211105280
  train_epoch_timing: 234.93810319900513
  train_step_timing: 0.9296703338623047
  eval_samples: 11590004
  validation_epoch_timing: 1.0206727981567383
  validation_step_timing: 0.20831823348999023
  dataset: c4
  log_perplexity: 2.69
  throughput: 5154997.267404211
  total_time_min: 3.709
H100_175B_Node1452_2K:
  model: gpt175b
  size: 175
  hw: H100SXM
  sw: Nemo24.04
  n_device: 11616
  n_nodes: 1452
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 8
  micro_batch_size: 1
  seq_length: 2048
  global_batch_size: 2904
  gradient_accumulation_steps: 8
  precision: fp8
  trained_samples: 1211105280
  train_epoch_timing: 234.93810319900513
  train_step_timing: 0.9296703338623047
  eval_samples: 11590004
  validation_epoch_timing: 1.0206727981567383
  validation_step_timing: 0.20831823348999023
  dataset: c4
  log_perplexity: 2.69
  throughput: 5154997.267404211
  total_time_min: 3.444
H100_70B_Node128_2K:
  model: llama2_70b
  size: 70
  hw: H100SXM
  sw: Nemo24.04
  n_device: 1024
  n_nodes: 128
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_model_parallel_size: 2
  micro_batch_size: 1
  seq_length: 8192
  global_batch_size: 128
  gradient_accumulation_steps: 1
  precision: fp8
  max_steps: 1024
  trained_samples: 3901
  eval_samples: 173
  dataset: GovRep
  log_perplexity: 0.9246432185173035
  throughput: 199.57991844871646
  total_time_min: 1.494
H100_70B_Node128_8K:
  model: llama2_70b
  size: 70
  hw: H100SXM
  sw: Nemo24.04
  n_device: 1024
  n_nodes: 128
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_model_parallel_size: 2
  micro_batch_size: 1
  seq_length: 8192
  global_batch_size: 128
  gradient_accumulation_steps: 1
  precision: fp8
  max_steps: 1024
  trained_samples: 3901
  eval_samples: 173
  dataset: GovRep
  log_perplexity: 0.9246432185173035
  throughput: 199.57991844871646
  total_time_min: 1.494
H100_70B_Node1_8K:
  model: llama2_70b
  size: 70
  hw: H100SXM
  sw: Nemo24.04
  n_device: 8
  n_nodes: 1
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_model_parallel_size: 2
  micro_batch_size: 1
  seq_length: 8192
  global_batch_size: 128
  gradient_accumulation_steps: 1
  precision: fp8
  max_steps: 1024
  trained_samples: 3901
  eval_samples: 173
  dataset: GovRep
  log_perplexity: 0.9246432185173035
  throughput: 199.57991844871646
  total_time_min: 28.163
A100_70B_Node8_8K:
  model: llama2_70b
  size: 70
  hw: A100SXM
  sw: deepspeed0.13.2
  n_device: 64
  n_nodes: 8
  tensor_model_parallel_size: 4
  pipeline_model_parallel_size: 1
  context_model_parallel_size: 2
  micro_batch_size: 1
  seq_length: 8192
  global_batch_size: 64
  gradient_accumulation_steps: 1
  precision: bf16
  max_steps: 1024
  trained_samples: 3901
  eval_samples: 173
  dataset: GovRep
  log_perplexity: 0.9246432185173035
  train_loss: 1.287
  total_time_min: 75.469