================================================== trace ops ==================================================
aten::to 21
        %torch.2_7_0:0% aten::to(self:<8xi32>{1}, dtype:torch_int64:dtype) -> <8xi64>{1}
        %torch.2_7_0:0% aten::to(self:<8x1536xbf16>{1536, 1}, dtype:torch_bfloat16:dtype) -> <8x1536xbf16>{1536, 1}
        %torch.2_7_0:0% aten::to(self:<8x24576xbf16>{24576, 1}, dtype:torch_bfloat16:dtype) -> <8x24576xbf16>{24576, 1}
        %torch.2_7_0:0% aten::to(self:<8x576xbf16>{576, 1}, dtype:torch_bfloat16:dtype) -> <8x576xbf16>{576, 1}
        %torch.2_7_0:0% aten::to(self:<8x32768xbf16>{32768, 1}, dtype:torch_bfloat16:dtype) -> <8x32768xbf16>{32768, 1}
        %torch.2_7_0:0% aten::to(self:<8x7168xbf16>{7168, 1}, dtype:torch_bfloat16:dtype) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::to(self:<8x36864xbf16>{36864, 1}, dtype:torch_bfloat16:dtype) -> <8x36864xbf16>{36864, 1}
        %torch.2_7_0:0% aten::to(self:<8x4096xbf16>{4096, 1}, dtype:torch_bfloat16:dtype) -> <8x4096xbf16>{4096, 1}
        %torch.2_7_0:0% aten::to(self:<1x129280xbf16>{129280, 1}, dtype:torch_float32:dtype) -> <1x129280xf32>{129280, 1}
        %torch.2_7_0:0% aten::to(self:<1xi64>{1}, dtype:torch_int64:dtype) -> <1xi64>{1}
        %torch.2_7_0:0% aten::to(self:<1xi64>{1}, dtype:torch_int32:dtype) -> <1xi32>{1}
        %torch.2_7_0:0% aten::to(self:<1x1xi32>{1, 1}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cpu:device) -> <1x1xi32>{1, 1}
        %torch.2_7_0:0% aten::to(self:<1xi64>{1}, device:cpu:device, dtype:torch_int64:dtype) -> <1xi64>{1}
        %torch.2_7_0:0% aten::to(self:<1xi32>{1}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cpu:device) -> <1xi32>{1}
        %torch.2_7_0:0% aten::to(self:<1xi32>{1}, dtype:torch_int64:dtype) -> <1xi64>{1}
        %torch.2_7_0:0% aten::to(self:<1x1536xbf16>{1536, 1}, dtype:torch_bfloat16:dtype) -> <1x1536xbf16>{1536, 1}
        %torch.2_7_0:0% aten::to(self:<1x24576xbf16>{24576, 1}, dtype:torch_bfloat16:dtype) -> <1x24576xbf16>{24576, 1}
        %torch.2_7_0:0% aten::to(self:<1x576xbf16>{576, 1}, dtype:torch_bfloat16:dtype) -> <1x576xbf16>{576, 1}
        %torch.2_7_0:0% aten::to(self:<1x7168xbf16>{7168, 1}, dtype:torch_bfloat16:dtype) -> <1x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::to(self:<1x36864xbf16>{36864, 1}, dtype:torch_bfloat16:dtype) -> <1x36864xbf16>{36864, 1}
        %torch.2_7_0:0% aten::to(self:<1x4096xbf16>{4096, 1}, dtype:torch_bfloat16:dtype) -> <1x4096xbf16>{4096, 1}
aten::embedding 2
        %torch.2_7_0:0% aten::embedding(weight:<129280x7168xbf16>{7168, 1}, indices:<8xi64>{1}) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::embedding(weight:<129280x7168xbf16>{7168, 1}, indices:<1xi64>{1}) -> <1x7168xbf16>{7168, 1}
_C::rms_norm 6
        %torch.2_7_0:0% _C::rms_norm(result:<8x7168xbf16>{7168, 1}, input:<8x7168xbf16>{7168, 1}, weight:<7168xbf16>{1}, epsilon:1e-06) -> None
        %torch.2_7_0:0% _C::rms_norm(result:<8x1536xbf16>{1536, 1}, input:<8x1536xbf16>{1536, 1}, weight:<1536xbf16>{1}, epsilon:1e-06) -> None
        %torch.2_7_0:0% _C::rms_norm(result:<8x512xbf16>{512, 1}, input:<8x512xbf16>{512, 1}, weight:<512xbf16>{1}, epsilon:1e-06) -> None
        %torch.2_7_0:0% _C::rms_norm(result:<1x7168xbf16>{7168, 1}, input:<1x7168xbf16>{7168, 1}, weight:<7168xbf16>{1}, epsilon:1e-06) -> None
        %torch.2_7_0:0% _C::rms_norm(result:<1x1536xbf16>{1536, 1}, input:<1x1536xbf16>{1536, 1}, weight:<1536xbf16>{1}, epsilon:1e-06) -> None
        %torch.2_7_0:0% _C::rms_norm(result:<1x512xbf16>{576, 1}, input:<1x512xbf16>{576, 1}, weight:<512xbf16>{1}, epsilon:1e-06) -> None
aten::pad 2
        %torch.2_7_0:0% aten::pad(self:<8x7168xbf16>{7168, 1}, pad:[0, 0, 0, 0], mode:constant, value:0.0) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::pad(self:<1x7168xbf16>{7168, 1}, pad:[0, 0, 0, 0], mode:constant, value:0.0) -> <1x7168xbf16>{7168, 1}
aten::chunk 2
        %torch.2_7_0:0% aten::chunk(self:<8x7168xbf16>{7168, 1}, chunks:1:int) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::chunk(self:<1x7168xbf16>{7168, 1}, chunks:1:int) -> <1x7168xbf16>{7168, 1}
aten::clone 6
        %torch.2_7_0:0% aten::clone(self:<8x7168xbf16>{7168, 1}) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::clone(self:<8x128x64xbf16>{24576, 192, 1}+128) -> <8x128x64xbf16>{8192, 64, 1}
        %torch.2_7_0:0% aten::clone(self:<8x1x64xbf16>{576, 64, 1}+512) -> <8x1x64xbf16>{64, 64, 1}
        %torch.2_7_0:0% aten::clone(self:<1x7168xbf16>{7168, 1}) -> <1x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::clone(self:<1x128x64xbf16>{24576, 192, 1}+128) -> <1x128x64xbf16>{8192, 64, 1}
        %torch.2_7_0:0% aten::clone(self:<1x1x64xbf16>{576, 64, 1}+512) -> <1x1x64xbf16>{576, 64, 1}
_C::dynamic_per_token_group_fp8_quant 11
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<8x7168xf8e4m3>{7168, 1}, scale:<8x56xf32>{56, 1}, input:<8x7168xbf16>{7168, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<8x1536xf8e4m3>{1536, 1}, scale:<8x12xf32>{12, 1}, input:<8x1536xbf16>{1536, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<8x512xf8e4m3>{512, 1}, scale:<8x4xf32>{4, 1}, input:<8x512xbf16>{512, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<8x16384xf8e4m3>{16384, 1}, scale:<8x128xf32>{128, 1}, input:<8x16384xbf16>{16384, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<8x18432xf8e4m3>{18432, 1}, scale:<8x144xf32>{144, 1}, input:<8x18432xbf16>{18432, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<8x2048xf8e4m3>{2048, 1}, scale:<8x16xf32>{16, 1}, input:<8x2048xbf16>{2048, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<1x7168xf8e4m3>{7168, 1}, scale:<1x56xf32>{56, 1}, input:<1x7168xbf16>{7168, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<1x1536xf8e4m3>{1536, 1}, scale:<1x12xf32>{12, 1}, input:<1x1536xbf16>{1536, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<1x16384xf8e4m3>{16384, 1}, scale:<1x128xf32>{128, 1}, input:<1x16384xbf16>{16384, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<1x18432xf8e4m3>{18432, 1}, scale:<1x144xf32>{144, 1}, input:<1x18432xbf16>{18432, 1}, group_size:128) -> None
        %torch.2_7_0:0% _C::dynamic_per_token_group_fp8_quant(out:<1x2048xf8e4m3>{2048, 1}, scale:<1x16xf32>{16, 1}, input:<1x2048xbf16>{2048, 1}, group_size:128) -> None
_C::linear_quant 17
        %torch.2_7_0:0% _C::linear_quant(out:<8x1536xbf16>{1536, 1}, lhs:<8x7168xf8e4m3>{7168, 1}, rhs:<1536x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<8x56xf32>{56, 1}, rhs_scale:<12x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x24576xbf16>{24576, 1}, lhs:<8x1536xf8e4m3>{1536, 1}, rhs:<24576x1536xf8e4m3>{1536, 1}, bias:None, lhs_scale:<8x12xf32>{12, 1}, rhs_scale:<192x12xf32>{12, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x576xbf16>{576, 1}, lhs:<8x7168xf8e4m3>{7168, 1}, rhs:<576x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<8x56xf32>{56, 1}, rhs_scale:<5x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x32768xbf16>{32768, 1}, lhs:<8x512xf8e4m3>{512, 1}, rhs:<32768x512xf8e4m3>{512, 1}, bias:None, lhs_scale:<8x4xf32>{4, 1}, rhs_scale:<256x4xf32>{4, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x7168xbf16>{7168, 1}, lhs:<8x16384xf8e4m3>{16384, 1}, rhs:<7168x16384xf8e4m3>{16384, 1}, bias:None, lhs_scale:<8x128xf32>{128, 1}, rhs_scale:<56x128xf32>{128, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x36864xbf16>{36864, 1}, lhs:<8x7168xf8e4m3>{7168, 1}, rhs:<36864x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<8x56xf32>{56, 1}, rhs_scale:<288x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x7168xbf16>{7168, 1}, lhs:<8x18432xf8e4m3>{18432, 1}, rhs:<7168x18432xf8e4m3>{18432, 1}, bias:None, lhs_scale:<8x144xf32>{144, 1}, rhs_scale:<56x144xf32>{144, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x4096xbf16>{4096, 1}, lhs:<8x7168xf8e4m3>{7168, 1}, rhs:<4096x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<8x56xf32>{56, 1}, rhs_scale:<32x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<8x7168xbf16>{7168, 1}, lhs:<8x2048xf8e4m3>{2048, 1}, rhs:<7168x2048xf8e4m3>{2048, 1}, bias:None, lhs_scale:<8x16xf32>{16, 1}, rhs_scale:<56x16xf32>{16, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x1536xbf16>{1536, 1}, lhs:<1x7168xf8e4m3>{7168, 1}, rhs:<1536x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<1x56xf32>{56, 1}, rhs_scale:<12x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x24576xbf16>{24576, 1}, lhs:<1x1536xf8e4m3>{1536, 1}, rhs:<24576x1536xf8e4m3>{1536, 1}, bias:None, lhs_scale:<1x12xf32>{12, 1}, rhs_scale:<192x12xf32>{12, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x576xbf16>{576, 1}, lhs:<1x7168xf8e4m3>{7168, 1}, rhs:<576x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<1x56xf32>{56, 1}, rhs_scale:<5x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x7168xbf16>{7168, 1}, lhs:<1x16384xf8e4m3>{16384, 1}, rhs:<7168x16384xf8e4m3>{16384, 1}, bias:None, lhs_scale:<1x128xf32>{128, 1}, rhs_scale:<56x128xf32>{128, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x36864xbf16>{36864, 1}, lhs:<1x7168xf8e4m3>{7168, 1}, rhs:<36864x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<1x56xf32>{56, 1}, rhs_scale:<288x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x7168xbf16>{7168, 1}, lhs:<1x18432xf8e4m3>{18432, 1}, rhs:<7168x18432xf8e4m3>{18432, 1}, bias:None, lhs_scale:<1x144xf32>{144, 1}, rhs_scale:<56x144xf32>{144, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x4096xbf16>{4096, 1}, lhs:<1x7168xf8e4m3>{7168, 1}, rhs:<4096x7168xf8e4m3>{7168, 1}, bias:None, lhs_scale:<1x56xf32>{56, 1}, rhs_scale:<32x56xf32>{56, 1}) -> None
        %torch.2_7_0:0% _C::linear_quant(out:<1x7168xbf16>{7168, 1}, lhs:<1x2048xf8e4m3>{2048, 1}, rhs:<7168x2048xf8e4m3>{2048, 1}, bias:None, lhs_scale:<1x16xf32>{16, 1}, rhs_scale:<56x16xf32>{16, 1}) -> None
aten::split_with_sizes 4
        %torch.2_7_0:0% aten::split_with_sizes(self:<8x576xbf16>{576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> <8x512xbf16>{576, 1}, <8x64xbf16>{576, 1}+512
        %torch.2_7_0:0% aten::split_with_sizes(self:<8x128x256xbf16>{32768, 256, 1}, split_sizes:list{128:int, 128:int}, dim:-1:int) -> <8x128x128xbf16>{32768, 256, 1}, <8x128x128xbf16>{32768, 256, 1}+128
        %torch.2_7_0:0% aten::split_with_sizes(self:<1x576xbf16>{576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> <1x512xbf16>{576, 1}, <1x64xbf16>{576, 1}+512
        %torch.2_7_0:0% aten::split_with_sizes(self:<1x128x192xbf16>{24576, 192, 1}, split_sizes:list{128:int, 64:int}, dim:-1:int) -> <1x128x128xbf16>{24576, 192, 1}, <1x128x64xbf16>{24576, 192, 1}+128
aten::contiguous 1
        %torch.2_7_0:0% aten::contiguous(self:<8x512xbf16>{576, 1}) -> <8x512xbf16>{512, 1}
aten::unsqueeze 3
        %torch.2_7_0:0% aten::unsqueeze(self:<8x64xbf16>{576, 1}+512, dim:1:int) -> <8x1x64xbf16>{576, 64, 1}+512
        %torch.2_7_0:0% aten::unsqueeze(self:<1xi32>{1}, dim:-1:int) -> <1x1xi32>{1, 1}
        %torch.2_7_0:0% aten::unsqueeze(self:<1x64xbf16>{576, 1}+512, dim:1:int) -> <1x1x64xbf16>{576, 64, 1}+512
aten::slice 31
        %torch.2_7_0:0% aten::slice(self:<8x128x192xbf16>{24576, 192, 1}, dim:2:int, start:128:int, end:9223372036854775807:int) -> <8x128x64xbf16>{24576, 192, 1}+128
        %torch.2_7_0:0% aten::slice(self:<8x128x192xbf16>{24576, 192, 1}, dim:0:int, start:0:int, end:0:int) -> <0x128x192xbf16>{24576, 192, 1}
        %torch.2_7_0:0% aten::slice(self:<8x128x192xbf16>{24576, 192, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x128x192xbf16>{24576, 192, 1}
        %torch.2_7_0:0% aten::slice(self:<8x1x64xbf16>{64, 64, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x1x64xbf16>{64, 64, 1}
        %torch.2_7_0:0% aten::slice(self:<8x512xbf16>{512, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x512xbf16>{512, 1}
        %torch.2_7_0:0% aten::slice(self:<8x16384xbf16>{16384, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x16384xbf16>{16384, 1}
        %torch.2_7_0:0% aten::slice(self:<8x7168xbf16>{7168, 1}, dim:0:int, start:0:int, end:8:int) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::slice(self:<458752xbf16>{1}, dim:0:int, start:0:int, end:57344:int) -> <57344xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<458752xbf16>{1}, dim:0:int, start:0:int, end:262144:int) -> <262144xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<458752xbf16>{1}, dim:0:int, start:0:int, end:65536:int) -> <65536xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<458752xbf16>{1}, dim:0:int, start:0:int, end:458752:int) -> <458752xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<1x1024xi32>{1024, 1}, dim:0:int, start:0:int, end:1:int) -> <1x1024xi32>{1024, 1}
        %torch.2_7_0:0% aten::slice(self:<16384xi32>{1}, dim:0:int, start:0:int, end:1:int) -> <1xi32>{1}
        %torch.2_7_0:0% aten::slice(self:<16384xi64>{1}, dim:0:int, start:0:int, end:1:int) -> <1xi64>{1}
        %torch.2_7_0:0% aten::slice(self:<2xi32>{1}, dim:0:int, start:0:int, end:2:int) -> <2xi32>{1}
        %torch.2_7_0:0% aten::slice(self:<1xi32>{1}, dim:0:int, start:0:int, end:1:int) -> <1xi32>{1}
        %torch.2_7_0:0% aten::slice(self:<1xi32>{1}, dim:0:int, start:1:int, end:9223372036854775807:int) -> <0xi32>{1}+1
        %torch.2_7_0:0% aten::slice(self:<2xi32>{1}, dim:0:int, start:2:int, end:9223372036854775807:int) -> <0xi32>{1}+2
        %torch.2_7_0:0% aten::slice(self:<16384xi64>{1}, dim:0:int, start:1:int, end:9223372036854775807:int) -> <16383xi64>{1}+1
        %torch.2_7_0:0% aten::slice(self:<2xi32>{1}, dim:0:int, start:1:int, end:9223372036854775807:int) -> <1xi32>{1}+1
        %torch.2_7_0:0% aten::slice(self:<1x128x192xbf16>{24576, 192, 1}, dim:2:int, start:128:int, end:9223372036854775807:int) -> <1x128x64xbf16>{24576, 192, 1}+128
        %torch.2_7_0:0% aten::slice(self:<1x128x192xbf16>{24576, 192, 1}, dim:0:int, start:0:int, end:1:int) -> <1x128x192xbf16>{24576, 192, 1}
        %torch.2_7_0:0% aten::slice(self:<1x128x192xbf16>{24576, 192, 1}, dim:0:int, start:1:int, end:9223372036854775807:int) -> <0x128x192xbf16>{24576, 192, 1}+24576
        %torch.2_7_0:0% aten::slice(self:<1x1x64xbf16>{64, 64, 1}, dim:0:int, start:1:int, end:9223372036854775807:int) -> <0x1x64xbf16>{64, 64, 1}+64
        %torch.2_7_0:0% aten::slice(self:<1x512xbf16>{576, 1}, dim:0:int, start:1:int, end:9223372036854775807:int) -> <0x512xbf16>{576, 1}+576
        %torch.2_7_0:0% aten::slice(self:<1x7168xbf16>{7168, 1}, dim:0:int, start:0:int, end:1:int) -> <1x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::slice(self:<57344xbf16>{1}, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<57344xbf16>{1}, dim:0:int, start:0:int, end:32768:int) -> <32768xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<57344xbf16>{1}, dim:0:int, start:0:int, end:8192:int) -> <8192xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<57344xbf16>{1}, dim:0:int, start:0:int, end:57344:int) -> <57344xbf16>{1}
        %torch.2_7_0:0% aten::slice(self:<1xf32>{1}, dim:0:int, start:0:int, end:0:int) -> <0xf32>{1}
aten::reshape 9
        %torch.2_7_0:0% aten::reshape(self:<8x128x64xbf16>{8192, 64, 1}, shape:list{8:int, 8192:int}) -> <8x8192xbf16>{8192, 1}
        %torch.2_7_0:0% aten::reshape(self:<8x1x64xbf16>{64, 64, 1}, shape:list{8:int, 64:int}) -> <8x64xbf16>{64, 1}
        %torch.2_7_0:0% aten::reshape(self:<8x8192xbf16>{8192, 1}, shape:list{8:int, 128:int, 64:int}) -> <8x128x64xbf16>{8192, 64, 1}
        %torch.2_7_0:0% aten::reshape(self:<8x64xbf16>{64, 1}, shape:list{8:int, 1:int, 64:int}) -> <8x1x64xbf16>{64, 64, 1}
        %torch.2_7_0:0% aten::reshape(self:<1x128x64xbf16>{8192, 64, 1}, shape:list{1:int, 8192:int}) -> <1x8192xbf16>{8192, 1}
        %torch.2_7_0:0% aten::reshape(self:<1x1x64xbf16>{576, 64, 1}, shape:list{1:int, 64:int}) -> <1x64xbf16>{64, 1}
        %torch.2_7_0:0% aten::reshape(self:<1x8192xbf16>{8192, 1}, shape:list{1:int, 128:int, 64:int}) -> <1x128x64xbf16>{8192, 64, 1}
        %torch.2_7_0:0% aten::reshape(self:<1x64xbf16>{64, 1}, shape:list{1:int, 1:int, 64:int}) -> <1x1x64xbf16>{64, 64, 1}
        %torch.2_7_0:0% aten::reshape(self:<1x128x128xbf16>{128, 128, 1}, shape:list{-1:int, 16384:int}) -> <1x16384xbf16>{16384, 1}
_C::rotary_embedding 2
        %torch.2_7_0:0% _C::rotary_embedding(positions:<8xi64>{1}, query:<8x8192xbf16>{8192, 1}, key:<8x64xbf16>{64, 1}, head_size:64, cos_sin_cache:<163840x64xbf16>{64, 1}, is_neox:False) -> None
        %torch.2_7_0:0% _C::rotary_embedding(positions:<1xi64>{1}, query:<1x8192xbf16>{8192, 1}, key:<1x64xbf16>{64, 1}, head_size:64, cos_sin_cache:<163840x64xbf16>{64, 1}, is_neox:False) -> None
aten::copy_ 10
        %torch.2_7_0:0% aten::copy_(self:<8x128x64xbf16>{24576, 192, 1}+128, src:<8x128x64xbf16>{8192, 64, 1}) -> <8x128x64xbf16>{24576, 192, 1}+128
        %torch.2_7_0:0% aten::copy_(self:<8x16384xbf16>{16384, 1}, src:<8x16384xbf16>{16384, 1}) -> <8x16384xbf16>{16384, 1}
        %torch.2_7_0:0% aten::copy_(self:<8x7168xbf16>{7168, 1}, src:<8x7168xbf16>{7168, 1}) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::copy_(self:<1x1024xi32>{1024, 1}, src:<1x1024xi32>{1024, 1}, non_blocking:True:bool) -> <1x1024xi32>{1024, 1}
        %torch.2_7_0:0% aten::copy_(self:<1xi32>{1}, src:<1xi32>{1}, non_blocking:True:bool) -> <1xi32>{1}
        %torch.2_7_0:0% aten::copy_(self:<1xi64>{1}, src:<1xi64>{1}, non_blocking:True:bool) -> <1xi64>{1}
        %torch.2_7_0:0% aten::copy_(self:<2xi32>{1}, src:<2xi32>{1}, non_blocking:True:bool) -> <2xi32>{1}
        %torch.2_7_0:0% aten::copy_(self:<1x128x64xbf16>{24576, 192, 1}+128, src:<1x128x64xbf16>{8192, 64, 1}) -> <1x128x64xbf16>{24576, 192, 1}+128
        %torch.2_7_0:0% aten::copy_(self:<1x16384xbf16>{16384, 1}, src:<1x16384xbf16>{16384, 1}) -> <1x16384xbf16>{16384, 1}
        %torch.2_7_0:0% aten::copy_(self:<1x7168xbf16>{7168, 1}, src:<1x7168xbf16>{7168, 1}) -> <1x7168xbf16>{7168, 1}
aten::zeros 3
        %torch.2_7_0:0% aten::zeros(size:[8, 16384], dtype:torch.bfloat16, device:device:0, pin_memory:False) -> <8x16384xbf16>{16384, 1}
        %torch.2_7_0:0% aten::zeros(size:[1, 16384], dtype:torch.bfloat16, device:device:0, pin_memory:False) -> <1x16384xbf16>{16384, 1}
        %torch.2_7_0:0% aten::zeros(size:[1, 128, 512], dtype:torch.bfloat16, device:device:0, pin_memory:False) -> <1x128x512xbf16>{65536, 512, 1}
aten::squeeze 2
        %torch.2_7_0:0% aten::squeeze(self:<8x1x64xbf16>{64, 64, 1}, dim:1:int) -> <8x64xbf16>{64, 1}
        %torch.2_7_0:0% aten::squeeze(self:<1x1x64xbf16>{64, 64, 1}, dim:1:int) -> <1x64xbf16>{64, 1}
aten::flatten 7
        %torch.2_7_0:0% aten::flatten(self:<8xi64>{1}) -> <8xi64>{1}
        %torch.2_7_0:0% aten::flatten(self:<8x128x128xbf16>{16384, 128, 1}, start_dim:-2:int) -> <8x16384xbf16>{16384, 1}
        %torch.2_7_0:0% aten::flatten(self:<458752xbf16>{1}) -> <458752xbf16>{1}
        %torch.2_7_0:0% aten::flatten(self:<1x65536xi32>{65536, 1}) -> <65536xi32>{1}
        %torch.2_7_0:0% aten::flatten(self:<1x1024xi32>{1024, 1}) -> <1024xi32>{1}
        %torch.2_7_0:0% aten::flatten(self:<1xi64>{1}) -> <1xi64>{1}
        %torch.2_7_0:0% aten::flatten(self:<57344xbf16>{1}) -> <57344xbf16>{1}
_C_cache_ops::concat_and_cache_mla 2
        %torch.2_7_0:0% _C_cache_ops::concat_and_cache_mla(kv_c:<8x512xbf16>{512, 1}, k_pe:<8x64xbf16>{64, 1}, kv_cache:<210762x64x576xbf16>{36864, 576, 1}, slot_mapping:<8xi64>{1}, kv_cache_dtype:auto, scale:<1xf32>{1}) -> None
        %torch.2_7_0:0% _C_cache_ops::concat_and_cache_mla(kv_c:<1x512xbf16>{576, 1}, k_pe:<1x64xbf16>{64, 1}, kv_cache:<210762x64x576xbf16>{36864, 576, 1}, slot_mapping:<1xi64>{1}, kv_cache_dtype:auto, scale:<1xf32>{1}) -> None
aten::cat 2
        %torch.2_7_0:0% aten::cat(tensors:['<8x128x128xbf16>{32768, 256, 1}', '<8x128x64xbf16>{64, 0, 1}'], dim:-1) -> <8x128x192xbf16>{24576, 192, 1}
        %torch.2_7_0:0% aten::cat(tensors:['<1x128x512xbf16>{512, 512, 1}', '<1x128x64xbf16>{24576, 192, 1}+128'], dim:-1) -> <1x128x576xbf16>{73728, 576, 1}
flash_attn::varlen_fwd 1
        %torch.2_7_0:0% flash_attn::varlen_fwd(<8x128x192xbf16>{24576, 192, 1}, <8x128x192xbf16>{24576, 192, 1}, <8x128x128xbf16>{32768, 256, 1}+128, None, <2xi32>{1}, <2xi32>{1}, None, None, None, None, 8, 8, 0.0, 0.1352337788608801, False, True, -1, -1, 0.0, False, None) -> <8x128x128xbf16>{16384, 128, 1}, <8x128x192xbf16>{24576, 192, 1}, <8x128x192xbf16>{24576, 192, 1}, <8x128x128xbf16>{32768, 256, 1}+128, <8x128x128xbf16>{16384, 128, 1}, <128x8xf32>{8, 1}, <0xf32>{1}, <2xCUSTOM_DATA_TYPE>{1}
c10d::_reduce_scatter_base_ 2
        %torch.2_7_0:0% c10d::_reduce_scatter_base_(output_tensor:<8x7168xbf16>{7168, 1}, input_tensor:<8x7168xbf16>{7168, 1}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, asyncOp:False, timeout:-1) -> <8x7168xbf16>{7168, 1}, Work:distributed
        %torch.2_7_0:0% c10d::_reduce_scatter_base_(output_tensor:<1x7168xbf16>{7168, 1}, input_tensor:<1x7168xbf16>{7168, 1}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, asyncOp:False, timeout:-1) -> <1x7168xbf16>{7168, 1}, Work:distributed
_C::fused_add_rms_norm 2
        %torch.2_7_0:0% _C::fused_add_rms_norm(input:<8x7168xbf16>{7168, 1}, residual:<8x7168xbf16>{7168, 1}, weight:<7168xbf16>{1}, epsilon:1e-06) -> None
        %torch.2_7_0:0% _C::fused_add_rms_norm(input:<1x7168xbf16>{7168, 1}, residual:<1x7168xbf16>{7168, 1}, weight:<7168xbf16>{1}, epsilon:1e-06) -> None
_C::silu_and_mul 4
        %torch.2_7_0:0% _C::silu_and_mul(result:<8x18432xbf16>{18432, 1}, input:<8x36864xbf16>{36864, 1}) -> None
        %torch.2_7_0:0% _C::silu_and_mul(result:<8x2048xbf16>{2048, 1}, input:<8x4096xbf16>{4096, 1}) -> None
        %torch.2_7_0:0% _C::silu_and_mul(result:<1x18432xbf16>{18432, 1}, input:<1x36864xbf16>{36864, 1}) -> None
        %torch.2_7_0:0% _C::silu_and_mul(result:<1x2048xbf16>{2048, 1}, input:<1x4096xbf16>{4096, 1}) -> None
aten::linear 3
        %torch.2_7_0:0% aten::linear(input:<8x7168xbf16>{7168, 1}, weight:<256x7168xbf16>{7168, 1}) -> <8x256xbf16>{256, 1}
        %torch.2_7_0:0% aten::linear(input:<1x7168xbf16>{7168, 1}, weight:<129280x7168xbf16>{7168, 1}) -> <1x129280xbf16>{129280, 1}
        %torch.2_7_0:0% aten::linear(input:<1x7168xbf16>{7168, 1}, weight:<256x7168xbf16>{7168, 1}) -> <1x256xbf16>{256, 1}
_C::fused_grouped_topk 2
        %torch.2_7_0:0% _C::fused_grouped_topk(topk_weights:<8x8xf32>{8, 1}, topk_ids:<8x8xi32>{8, 1}, gating_output:<8x256xbf16>{256, 1}, topk:8, renormalize:True, num_expert_group:8, topk_group:4, e_score_correction_bias:<256xbf16>{1}, scoring_func:sigmoid) -> None
        %torch.2_7_0:0% _C::fused_grouped_topk(topk_weights:<1x8xf32>{8, 1}, topk_ids:<1x8xi32>{8, 1}, gating_output:<1x256xbf16>{256, 1}, topk:8, renormalize:True, num_expert_group:8, topk_group:4, e_score_correction_bias:<256xbf16>{1}, scoring_func:sigmoid) -> None
aten::full 2
        %torch.2_7_0:0% aten::full(size:[1], fill_value:8, dtype:torch.int32, device:device:0, pin_memory:False) -> <1xi32>{1}
        %torch.2_7_0:0% aten::full(size:[1], fill_value:1, dtype:torch.int32, device:device:0, pin_memory:False) -> <1xi32>{1}
_C::moe_align_block_size_pad 2
        %torch.2_7_0:0% _C::moe_align_block_size_pad(_0:<8x8xi32>{8, 1}, _1:<1xi32>{1}, _2:256, _3:64, _4:<16192xi32>{1}, _5:<253xi32>{1}, _6:<1xi32>{1}) -> None
        %torch.2_7_0:0% _C::moe_align_block_size_pad(_0:<1x8xi32>{8, 1}, _1:<1xi32>{1}, _2:256, _3:64, _4:<16136xi32>{1}, _5:<252xi32>{1}, _6:<1xi32>{1}) -> None
_C::fused_moe_quant_kernel 4
        %torch.2_7_0:0% _C::fused_moe_quant_kernel(_0:<8x8x4096xbf16>{32768, 4096, 1}, _1:<8x7168xf8e4m3>{7168, 1}, _2:<256x4096x7168xf8e4m3>{29360128, 7168, 1}, _3:<8x56xf32>{56, 1}, _4:<256x32x56xf32>{1792, 56, 1}, _5:128, _6:None, _7:<8x8xf32>{8, 1}, _8:<8x8xi32>{8, 1}, _9:<16192xi32>{1}, _10:<253xi32>{1}, _11:<1xi32>{1}, _12:False, _13:8, _14:64, _15:None, _16:<1xi32>{1}) -> None
        %torch.2_7_0:0% _C::fused_moe_quant_kernel(_0:<8x8x7168xbf16>{57344, 7168, 1}, _1:<64x2048xf8e4m3>{2048, 1}, _2:<256x7168x2048xf8e4m3>{14680064, 2048, 1}, _3:<64x16xf32>{16, 1}, _4:<256x56x16xf32>{896, 16, 1}, _5:128, _6:None, _7:<8x8xf32>{8, 1}, _8:<8x8xi32>{8, 1}, _9:<16192xi32>{1}, _10:<253xi32>{1}, _11:<1xi32>{1}, _12:True, _13:1, _14:64, _15:None, _16:<1xi32>{1}) -> None
        %torch.2_7_0:0% _C::fused_moe_quant_kernel(_0:<1x8x4096xbf16>{32768, 4096, 1}, _1:<1x7168xf8e4m3>{7168, 1}, _2:<256x4096x7168xf8e4m3>{29360128, 7168, 1}, _3:<1x56xf32>{56, 1}, _4:<256x32x56xf32>{1792, 56, 1}, _5:128, _6:None, _7:<1x8xf32>{8, 1}, _8:<1x8xi32>{8, 1}, _9:<16136xi32>{1}, _10:<252xi32>{1}, _11:<1xi32>{1}, _12:False, _13:8, _14:64, _15:None, _16:<1xi32>{1}) -> None
        %torch.2_7_0:0% _C::fused_moe_quant_kernel(_0:<1x8x7168xbf16>{57344, 7168, 1}, _1:<8x2048xf8e4m3>{2048, 1}, _2:<256x7168x2048xf8e4m3>{14680064, 2048, 1}, _3:<8x16xf32>{16, 1}, _4:<256x56x16xf32>{896, 16, 1}, _5:128, _6:None, _7:<1x8xf32>{8, 1}, _8:<1x8xi32>{8, 1}, _9:<16136xi32>{1}, _10:<252xi32>{1}, _11:<1xi32>{1}, _12:True, _13:1, _14:64, _15:None, _16:<1xi32>{1}) -> None
_C::silu_mul_per_token_group_quant_with_size 2
        %torch.2_7_0:0% _C::silu_mul_per_token_group_quant_with_size(out:<8x8x2048xf8e4m3>{16384, 2048, 1}, scale:<8x8x16xf32>{128, 16, 1}, input:<8x8x4096xbf16>{32768, 4096, 1}, size:<1xi32>{1}, group_size:128) -> None
        %torch.2_7_0:0% _C::silu_mul_per_token_group_quant_with_size(out:<1x8x2048xf8e4m3>{16384, 2048, 1}, scale:<1x8x16xf32>{128, 16, 1}, input:<1x8x4096xbf16>{32768, 4096, 1}, size:<1xi32>{1}, group_size:128) -> None
_moe_C::moe_sum_pad 2
        %torch.2_7_0:0% _moe_C::moe_sum_pad(out:<8x7168xbf16>{7168, 1}, input:<8x8x7168xbf16>{57344, 7168, 1}, size:<1xi32>{1}, dim:1, keepdim:False) -> None
        %torch.2_7_0:0% _moe_C::moe_sum_pad(out:<1x7168xbf16>{7168, 1}, input:<1x8x7168xbf16>{57344, 7168, 1}, size:<1xi32>{1}, dim:1, keepdim:False) -> None
aten::mul_ 2
        %torch.2_7_0:0% aten::mul_(self:<8x7168xbf16>{7168, 1}, other:2.5) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::mul_(self:<1x7168xbf16>{7168, 1}, other:2.5) -> <1x7168xbf16>{7168, 1}
aten::add_ 2
        %torch.2_7_0:0% aten::add_(self:<8x7168xbf16>{7168, 1}, other:<8x7168xbf16>{7168, 1}) -> <8x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::add_(self:<1x7168xbf16>{7168, 1}, other:<1x7168xbf16>{7168, 1}) -> <1x7168xbf16>{7168, 1}
aten::index 3
        %torch.2_7_0:0% aten::index(self:<8x7168xbf16>{7168, 1}, indices:['<1xi32>{1}']) -> <1x7168xbf16>{7168, 1}
        %torch.2_7_0:0% aten::index(self:<1024xi32>{1}, indices:['<1xi64>{1}']) -> <1xi32>{1}
        %torch.2_7_0:0% aten::index(self:<1x7168xbf16>{7168, 1}, indices:['<1xi32>{1}']) -> <1x7168xbf16>{7168, 1}
aten::argmax 1
        %torch.2_7_0:0% aten::argmax(self:<1x129280xf32>{129280, 1}, dim:-1) -> <1xi64>{1}
aten::resolve_conj 2
        %torch.2_7_0:0% aten::resolve_conj(self:<1x1xi32>{1, 1}) -> <1x1xi32>{1, 1}
        %torch.2_7_0:0% aten::resolve_conj(self:<1xi32>{1}) -> <1xi32>{1}
aten::resolve_neg 2
        %torch.2_7_0:0% aten::resolve_neg(self:<1x1xi32>{1, 1}) -> <1x1xi32>{1, 1}
        %torch.2_7_0:0% aten::resolve_neg(self:<1xi32>{1}) -> <1xi32>{1}
aten::index_select 1
        %torch.2_7_0:0% aten::index_select(self:<65536xi32>{1}, dim:0, index:<1xi64>{1}, out:<1xi32>{1}) -> <1xi32>{1}
aten::fill_ 3
        %torch.2_7_0:0% aten::fill_(self:<0xi32>{1}+1, value:0) -> <0xi32>{1}+1
        %torch.2_7_0:0% aten::fill_(self:<0xi32>{1}+2, value:1) -> <0xi32>{1}+2
        %torch.2_7_0:0% aten::fill_(self:<16383xi64>{1}+1, value:-1) -> <16383xi64>{1}+1
aten::max 1
        %torch.2_7_0:0% aten::max(self:<1xi32>{1}) -> <1xi32>{1}
aten::sub 1
        %torch.2_7_0:0% aten::sub(self:<1xi32>{1}+1, other:1) -> <1xi32>{1}
aten::transpose 4
        %torch.2_7_0:0% aten::transpose(self:<1x128x128xbf16>{24576, 192, 1}, dim0:0, dim1:1) -> <128x1x128xbf16>{192, 24576, 1}
        %torch.2_7_0:0% aten::transpose(self:<128x1x512xbf16>{512, 512, 1}, dim0:0, dim1:1) -> <1x128x512xbf16>{512, 512, 1}
        %torch.2_7_0:0% aten::transpose(self:<1x128x512xbf16>{65536, 512, 1}, dim0:0, dim1:1) -> <128x1x512xbf16>{512, 65536, 1}
        %torch.2_7_0:0% aten::transpose(self:<128x1x128xbf16>{128, 128, 1}, dim0:0, dim1:1) -> <1x128x128xbf16>{128, 128, 1}
aten::bmm 2
        %torch.2_7_0:0% aten::bmm(self:<128x1x128xbf16>{192, 24576, 1}, mat2:<128x128x512xbf16>{65536, 512, 1}) -> <128x1x512xbf16>{512, 512, 1}
        %torch.2_7_0:0% aten::bmm(self:<128x1x512xbf16>{512, 65536, 1}, mat2:<128x512x128xbf16>{65536, 128, 1}) -> <128x1x128xbf16>{128, 128, 1}
_C::paged_attention_v1 15
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:9, alibi_slopes:None, kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:10, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:11, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:12, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:13, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:14, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:15, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:16, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:17, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:18, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None
        %torch.2_7_0:0% _C::paged_attention_v1(out:<1x128x512xbf16>{65536, 512, 1}, query:<1x128x576xbf16>{73728, 576, 1}, key_cache:<210762x64x576xbf16>{36864, 576, 1}, value_cache:None, num_kv_heads:1, scale:0.1352337788608801, block_tables:<1x1024xi32>{1024, 1}, seq_lens:<1xi32>{1}, block_size:64, max_seq_len:19, alibi_slopes:None,kv_cache_dtype:auto, k_scale:1.0, v_scale:1.0, tp_rank:0, blocksparse_local_blocks:0, blocksparse_vert_stride:0, blocksparse_block_size:64, blocksparse_head_sliding_step:0, k_zero:0.0, v_zero:0.0, out_scales:None, query_scales:None) -> None