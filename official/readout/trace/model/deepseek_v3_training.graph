- 1_0_fwd_module::GPTModel:
  - param_name: deepseek_v3
  - inputs: ['<1x4096xi64>{4096, 1}', '<1x4096xi64>{4096, 1}', '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}'], {'labels': '<1x4096xi64>{4096, 1}', 'packed_seq_params': 'None:', 'loss_mask': '<1x4096xf32>{4096, 1}'}
  - 1_0_fwd_module::LanguageModelEmbedding:
    - param_name: deepseek_v3.embedding
    - inputs: [], {'input_ids': '<1x4096xi64>{4096, 1}', 'position_ids': '<1x4096xi64>{4096, 1}'}
    - 1_0_fwd_module::VocabParallelEmbedding:
      - param_name: deepseek_v3.embedding.word_embeddings
      - inputs: ['<1x4096xi64>{4096, 1}'], {}
      2025-10-09 13:08:31.343658 %681140:0:0% aten::embedding(weight:<129280x7168xbf16>{7168, 1}+2075734016, indices:<1x4096xi64>{4096, 1}) -> <1x4096x7168xbf16>{29360128, 7168, 1}
      - name: api::_ReduceFromModelParallelRegion
        inputs: <torch.autograd.function._ReduceFromModelParallelRegionBackward object at 0x7f5348b1c6b0>, <1x4096x7168xbf16>{29360128, 7168, 1}, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
        file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
    2025-10-09 13:08:31.344881 %681140:0:0% aten::transpose(self:<1x4096x7168xbf16>{29360128, 7168, 1}, dim0:0:int, dim1:1:int) -> <4096x1x7168xbf16>{7168, 29360128, 1}
    - 1_0_fwd_module::Dropout:
      - param_name: deepseek_v3.embedding.embedding_dropout
      - inputs: ['<4096x1x7168xbf16>{7168, 29360128, 1}'], {}
  - 1_0_fwd_module::TransformerBlock:
    - param_name: deepseek_v3.decoder
    - inputs: [], {'hidden_states': '<4096x1x7168xbf16>{7168, 7168, 1}', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'inference_context': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
    - 1_0_fwd_module::TransformerLayer:
      - param_name: deepseek_v3.decoder.layers.0
      - inputs: [], {'hidden_states': '<4096x1x7168xbf16>{7168, 7168, 1}', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'context': 'None:', 'context_mask': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'attention_bias': 'None:', 'inference_context': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
      - 1_0_fwd_module::RMSNorm:
        - param_name: deepseek_v3.decoder.layers.0.input_layernorm
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
      - 1_0_fwd_module::MLASelfAttention:
        - param_name: deepseek_v3.decoder.layers.0.self_attention
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'inference_context': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'attention_bias': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
        - 1_0_fwd_module::YarnRotaryEmbedding:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.rotary_pos_emb
          - inputs: ['4096:int'], {'packed_seq': 'False:bool'}
          2025-10-09 13:08:31.349847 %681140:None:0% aten::arange(end:32:int, dtype:torch_float32:dtype, device:cpu:device, pin_memory:False:bool) -> <32xf32>{1}
          2025-10-09 13:08:31.350206 %681140:None:0% aten::sub(self:<32xf32>{1}, other:10:int) -> <32xf32>{1}
          2025-10-09 13:08:31.350484 %681140:None:0% aten::div(self:<32xf32>{1}, other:13:int) -> <32xf32>{1}
          2025-10-09 13:08:31.350774 %681140:None:0% aten::clamp(self:<32xf32>{1}, min:0:int, max:1:int) -> <32xf32>{1}
          2025-10-09 13:08:31.351236 %681140:None:0% aten::_to_copy(self:<32xf32>{1}, dtype:torch_float32:dtype, device:cuda_0:device) -> <32xf32>{1}
          2025-10-09 13:08:31.351583 %681140:0:0% aten::rsub(self:<32xf32>{1}, other:1_0:float) -> <32xf32>{1}
          2025-10-09 13:08:31.351866 %681140:0:0% aten::rsub(self:<32xf32>{1}, other:1:int) -> <32xf32>{1}
          2025-10-09 13:08:31.352141 %681140:0:0% aten::mul(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
          2025-10-09 13:08:31.352342 %681140:0:0% aten::mul(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
          2025-10-09 13:08:31.352647 %681140:0:0% aten::add(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
          2025-10-09 13:08:31.353264 %681140:None:0% aten::arange(end:4096:int, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <4096xf32>{1}
          2025-10-09 13:08:31.353581 %681140:0:0% aten::add(self:<4096xf32>{1}, other:0:int) -> <4096xf32>{1}
          2025-10-09 13:08:31.353898 %681140:0:0% aten::mul(self:<4096x1xf32>{1, 1}, other:<32xf32>{1}) -> <4096x32xf32>{32, 1}
          2025-10-09 13:08:31.354298 %681140:None:0% aten::cat(tensors:list{<4096x32xf32>{32, 1}, <4096x32xf32>{32, 1}}, dim:-1:int) -> <4096x64xf32>{64, 1}
          2025-10-09 13:08:31.354651 %681140:0:0% aten::slice(self:<4096x64xf32>{64, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <4096x64xf32>{64, 1}
          2025-10-09 13:08:31.354892 %681140:0:0% aten::unsqueeze(self:<4096x64xf32>{64, 1}, dim:1:int) -> <4096x1x64xf32>{64, 64, 1}
          2025-10-09 13:08:31.355126 %681140:0:0% aten::unsqueeze(self:<4096x1x64xf32>{64, 64, 1}, dim:2:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:31.355455 %681140:0:0% aten::slice(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dim:3:int, start:0:int, end:9223372036854775807:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        - 1_0_fwd_module::TELinear:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_q_down_proj
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f53308c3130>, <1536x7168xbf16>{7168, 1}+1947276288, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533f51fef0_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, None:, 1:int, False:bool, False:bool, torch_bfloat16:dtype, None:, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TELinear():TELinear, None:, None:, False:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.358593 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <1536x7168xbf16>{7168, 1}+1947276288, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.360142 %681140:0:0% te::quantize(<1536x7168xbf16>{7168, 1}+1947276288, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.389583 %681140:0:0% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x1536xbf16>{1536, 1536, 1}, None:, None:, None:
        - 1_0_fwd_module::TELinear:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_kv_down_proj
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f53308c3680>, <576x7168xbf16>{7168, 1}+1905397248, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c3c3a10_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, None:, 1:int, False:bool, False:bool, torch_bfloat16:dtype, None:, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TELinear():TELinear, None:, None:, False:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.391708 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <576x7168xbf16>{7168, 1}+1905397248, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.392473 %681140:0:0% te::quantize(<576x7168xbf16>{7168, 1}+1905397248, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.393977 %681140:0:0% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x576xbf16>{576, 576, 1}, None:, None:, None:
        2025-10-09 13:08:31.394738 %681140:0:0% aten::split_with_sizes(self:<4096x1x576xbf16>{576, 576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> <4096x1x512xbf16>{576, 576, 1}, <4096x1x64xbf16>{576, 576, 1}+512
        - 1_0_fwd_module::TELayerNormColumnParallelLinear:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_q_up_proj
          - inputs: ['<4096x1x1536xbf16>{1536, 1536, 1}'], {}
          - name: api::_LayerNormLinear
            inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f53308c3bd0>, <4096x1x1536xbf16>{1536, 1536, 1}, <1536xbf16>{1}+1947274752, None:, <24576x1536xbf16>{1536, 1}+1909526016, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c3c3ec0_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=1536,_out_features=24576,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <24576x1536xbf16>{1536, 1}+1909526016, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.396909 %681140:0:0% te::quantize(<24576x1536xbf16>{1536, 1}+1909526016, <True, True, False>:Float8BlockQuantizer) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.399296 %681140:0:0% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x24576xbf16>{24576, 1}, None:, None:, None:
        - 1_0_fwd_module::TELayerNormColumnParallelLinear:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_kv_up_proj
          - inputs: ['<4096x1x512xbf16>{576, 576, 1}'], {}
          2025-10-09 13:08:31.400314 %681140:0:0% aten::clone(self:<4096x1x512xbf16>{576, 512, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x512xbf16>{512, 512, 1}
          - name: api::_LayerNormLinear
            inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5330940050>, <4096x1x512xbf16>{512, 512, 1}, <512xbf16>{1}+1905396736, None:, <32768x512xbf16>{512, 1}+1888619520, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c3c2f90_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=512,_out_features=32768,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <32768x512xbf16>{512, 1}+1888619520, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.402102 %681140:0:0% te::quantize(<32768x512xbf16>{512, 1}+1888619520, <True, True, False>:Float8BlockQuantizer) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.403873 %681140:0:0% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32768xbf16>{32768, 1}, None:, None:, None:
        2025-10-09 13:08:31.404499 %681140:0:0% aten::unsqueeze(self:<4096x1x64xbf16>{576, 576, 1}+512, dim:-2:int) -> <4096x1x1x64xbf16>{576, 576, 64, 1}+512
        2025-10-09 13:08:31.404861 %681140:0:0% aten::slice(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dim:0:int, start:0:int, end:4096:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.405258 %681140:0:0% aten::split_with_sizes(self:<4096x1x128x192xbf16>{24576, 24576, 192, 1}, split_sizes:list{128:int, 64:int}, dim:-1:int) -> <4096x1x128x128xbf16>{24576, 24576, 192, 1}, <4096x1x128x64xbf16>{24576, 24576, 192, 1}+128
        2025-10-09 13:08:31.405625 %681140:0:0% aten::split_with_sizes(self:<4096x1x128x256xbf16>{32768, 32768, 256, 1}, split_sizes:list{128:int, 128:int}, dim:-1:int) -> <4096x1x128x128xbf16>{32768, 32768, 256, 1}, <4096x1x128x128xbf16>{32768, 32768, 256, 1}+128
        2025-10-09 13:08:31.406033 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:64:int, end:9223372036854775807:int) -> <4096x1x128x0xbf16>{24576, 24576, 192, 1}+192
        2025-10-09 13:08:31.406391 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x32xbf16>{24576, 24576, 192, 2}+128
        2025-10-09 13:08:31.406729 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x32xbf16>{24576, 24576, 192, 2}+129
        2025-10-09 13:08:31.407368 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{24576, 24576, 192, 2}+128, <4096x1x128x32xbf16>{24576, 24576, 192, 2}+129}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.408061 %681140:0:0% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.408378 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.408855 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.409130 %681140:0:0% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.409361 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.409731 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.410165 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.410494 %681140:0:0% aten::split(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, split_size:32:int, dim:-1:int) -> <4096x1x128x32xbf16>{8192, 8192, 64, 1}, <4096x1x128x32xbf16>{8192, 8192, 64, 1}+32
        2025-10-09 13:08:31.416235 %681140:0:0% aten::neg(self:<4096x1x128x32xbf16>{8192, 8192, 64, 1}+32) -> <4096x1x128x32xbf16>{4096, 4096, 32, 1}
        2025-10-09 13:08:31.416852 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{4096, 4096, 32, 1}, <4096x1x128x32xbf16>{8192, 8192, 64, 1}}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.417251 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.417649 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.418169 %681140:None:0% aten::cat(tensors:list{<4096x1x128x64xbf16>{8192, 8192, 64, 1}, <4096x1x128x0xbf16>{24576, 24576, 192, 1}+192}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.418558 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:64:int, end:9223372036854775807:int) -> <4096x1x1x0xbf16>{576, 576, 64, 1}+576
        2025-10-09 13:08:31.418905 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x32xbf16>{576, 576, 64, 2}+512
        2025-10-09 13:08:31.419239 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x32xbf16>{576, 576, 64, 2}+513
        2025-10-09 13:08:31.419568 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{576, 576, 64, 2}+512, <4096x1x1x32xbf16>{576, 576, 64, 2}+513}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.419775 %681140:0:0% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.420011 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.420391 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.420576 %681140:0:0% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.421593 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.421999 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.422320 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.422646 %681140:0:0% aten::split(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, split_size:32:int, dim:-1:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}, <4096x1x1x32xbf16>{64, 64, 64, 1}+32
        2025-10-09 13:08:31.422887 %681140:0:0% aten::neg(self:<4096x1x1x32xbf16>{64, 64, 64, 1}+32) -> <4096x1x1x32xbf16>{32, 32, 32, 1}
        2025-10-09 13:08:31.423221 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{32, 32, 32, 1}, <4096x1x1x32xbf16>{64, 64, 64, 1}}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.423437 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.423747 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.424077 %681140:None:0% aten::cat(tensors:list{<4096x1x1x64xbf16>{64, 64, 64, 1}, <4096x1x1x0xbf16>{576, 576, 64, 1}+576}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.425126 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{24576, 24576, 192, 1}, <4096x1x128x64xbf16>{8192, 8192, 64, 1}}, dim:-1:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
        2025-10-09 13:08:31.426235 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{32768, 32768, 256, 1}, <4096x1x128x64xbf16>{64, 64, 0, 1}}, dim:-1:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
        2025-10-09 13:08:31.426821 %681140:0:0% aten::clone(self:<4096x1x128x128xbf16>{32768, 32768, 256, 1}+128, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
        - 1_0_fwd_module::TEDotProductAttention:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.core_attention
          - inputs: ['<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x128xbf16>{16384, 16384, 128, 1}', '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}'], {'packed_seq_params': 'None:', 'attn_mask_type': 'AttnMaskType_causal:AttnMaskType'}
          2025-10-09 13:08:31.428019 %681140:None:0% aten::arange(start:0:int, end:8192:int, step:4096:int, dtype:torch_int32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <2xi32>{1}
          2025-10-09 13:08:31.430933 %681140:None:0% te::get_fused_attn_backend(True:bool, DType_kBFloat16:DType, DType_kBFloat16:DType, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, 0_0:float, 128:int, 128:int, 4096:int, 4096:int, 192:int, 128:int, -1:int, 0:int) -> NVTE_Fused_Attn_Backend_NVTE_No_Backend:NVTE_Fused_Attn_Backend
          - 1_0_fwd_module::UnfusedDotProductAttention:
            - param_name: deepseek_v3.decoder.layers.0.self_attention.core_attention.unfused_attention
            - inputs: ["_'_num_heads'__None,_'_alibi_slopes'__None,_'_ma__seqlen_q'__None,_'_ma__seqlen_kv'__None,_'_bottom_right_alignment'__True,_'_alibi_bias'__None,_'_alibi_slopes_require_update'__False,_'_alibi_bias_require_update'__False_:dict", '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x128xbf16>{16384, 16384, 128, 1}'], {'qkv_layout': 'sbhd_sbhd_sbhd:str', 'cu_seqlens_q': '<2xi32>{1}', 'cu_seqlens_kv': '<2xi32>{1}', 'attn_mask_type': 'causal:str', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'window_size': 'tuple{-1:int, 0:int}', 'core_attention_bias_type': 'no_bias:str', 'core_attention_bias': 'None:', 'alibi_slopes': 'None:', 'inference_params': 'None:'}
            2025-10-09 13:08:31.432151 %681140:None:0% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
            2025-10-09 13:08:31.432490 %681140:None:0% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
            2025-10-09 13:08:31.432980 %681140:0:0% aten::sub(self:<1x1x4096x1xi32>{4096, 4096, 1, 1}, other:<1x1x1x4096xi32>{4096, 4096, 4096, 1}) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.433399 %681140:0:0% aten::sub(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:4096:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.433763 %681140:0:0% aten::add(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.434231 %681140:0:0% aten::le(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.434572 %681140:0:0% aten::lt(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.443582 %681140:0:0% aten::bitwise_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.449350 %681140:0:0% aten::bitwise_and(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.449794 %681140:0:0% aten::logical_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.459222 %681140:0:0% aten::logical_or(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.460675 %681140:0:0% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
            2025-10-09 13:08:31.460914 %681140:0:0% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
            2025-10-09 13:08:31.461195 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
            2025-10-09 13:08:31.529352 %681140:0:0% aten::baddbmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, batch1:<128x4096x192xbf16>{192, 24576, 1}, batch2:<128x192x4096xbf16>{192, 1, 24576}, beta:0_0:float, alpha:0_1352337788608801:float) -> <128x4096x4096xbf16>{16777216, 4096, 1}
            - name: api::ScaledMaskedSoftmax
              inputs: <torch.autograd.function.ScaledMaskedSoftmaxBackward object at 0x7f5330940c00>, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, 1_0:float
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/attention/dot_product_attention/softmax.py
              2025-10-09 13:08:31.542827 %681140:0:0% te::scaled_masked_softmax_forward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
            - 1_0_fwd_module::Dropout:
              - param_name: deepseek_v3.decoder.layers.0.self_attention.core_attention.unfused_attention.attention_dropout
              - inputs: ['<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}'], {}
            2025-10-09 13:08:31.543746 %681140:None:0% aten::eq(self:<5056xu8>{1}, other:<5056xu8>{1}) -> <5056xbool>{1}
            2025-10-09 13:08:31.544006 %681140:None:0% aten::all(self:<5056xbool>{1}) -> <1xbool>{1}
            2025-10-09 13:08:31.544395 %681140:0:0% aten::transpose(self:<4096x128x128xbf16>{16384, 128, 1}, dim0:0:int, dim1:1:int) -> <128x4096x128xbf16>{128, 16384, 1}
            2025-10-09 13:08:31.551689 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
            2025-10-09 13:08:31.552171 %681140:0:0% aten::permute(self:<1x128x4096x128xbf16>{67108864, 524288, 128, 1}, dims:list{2:int, 0:int, 1:int, 3:int}) -> <4096x1x128x128xbf16>{128, 67108864, 524288, 1}
            2025-10-09 13:08:31.552806 %681140:0:0% aten::clone(self:<4096x1x128x128xbf16>{128, 67108864, 524288, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
        - 1_0_fwd_module::TERowParallelLinear:
          - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_proj
          - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f5330941590>, <7168x16384xbf16>{16384, 1}+1958286336, <4096x1x16384xbf16>{16384, 16384, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c3c3080_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, row:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TERowParallelLinear(in_features=16384,_out_features=7168,_bias=False,_TP=1):TERowParallelLinear, None:, None:, True:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.555244 %681140:0:0% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <7168x16384xbf16>{16384, 1}+1958286336, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.556119 %681140:0:0% te::quantize(<7168x16384xbf16>{16384, 1}+1958286336, <True, True, False>:Float8BlockQuantizer) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.560845 %681140:0:0% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
      2025-10-09 13:08:31.567774 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
      2025-10-09 13:08:31.568233 %681140:0:0% triton::bias_dropout_add_fused_train(tuple{<4096x1x7168xbf16>{7168, 7168, 1}, None:}, <4096x1x7168xbf16>{7168, 7168, 1}, 0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}
      - 1_0_fwd_module::MLP:
        - param_name: deepseek_v3.decoder.layers.0.mlp
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
        - 1_0_fwd_module::TELayerNormColumnParallelLinear:
          - param_name: deepseek_v3.decoder.layers.0.mlp.linear_fc1
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - name: api::_LayerNormLinear
            inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5330941d00>, <4096x1x7168xbf16>{7168, 7168, 1}, <7168xbf16>{1}+1888612352, None:, <36864x7168xbf16>{7168, 1}+1624371200, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c418ec0_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=7168,_out_features=36864,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <36864x7168xbf16>{7168, 1}+1624371200, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.570956 %681140:0:0% te::quantize(<36864x7168xbf16>{7168, 1}+1624371200, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.579987 %681140:0:0% te::generic_gemm(tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x36864xbf16>{36864, 1}, None:, None:, None:
        - name: api::SwiGLUFunction
          inputs: <torch.autograd.function.SwiGLUFunctionBackward object at 0x7f5330942030>, <4096x36864xbf16>{36864, 1}, False:bool, False:bool
          file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
          2025-10-09 13:08:31.581537 %681140:0:0% aten::split(self:<4096x36864xbf16>{36864, 1}, split_size:18432:int, dim:-1:int) -> <4096x18432xbf16>{36864, 1}, <4096x18432xbf16>{36864, 1}+18432
          2025-10-09 13:08:31.582281 %681140:0:0% aten::silu(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
          2025-10-09 13:08:31.582916 %681140:0:0% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{36864, 1}+18432) -> <4096x18432xbf16>{18432, 1}
        - 1_0_fwd_module::TERowParallelLinear:
          - param_name: deepseek_v3.decoder.layers.0.mlp.linear_fc2
          - inputs: ['<4096x1x18432xbf16>{18432, 18432, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f5330942360>, <7168x18432xbf16>{18432, 1}+1492250624, <4096x1x18432xbf16>{18432, 18432, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c418e00_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, row:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TERowParallelLinear(in_features=18432,_out_features=7168,_bias=False,_TP=1):TERowParallelLinear, None:, None:, False:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x18432xbf16>{18432, 18432, 1}, <True, True, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.584735 %681140:0:0% te::quantize(<4096x1x18432xbf16>{18432, 18432, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<18432x4096x1xu8>, <32x18432xf32>, <4096x1x18432xu8>, <144x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <7168x18432xbf16>{18432, 1}+1492250624, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.585627 %681140:0:0% te::quantize(<7168x18432xbf16>{18432, 1}+1492250624, <True, True, False>:Float8BlockQuantizer) -> tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.590779 %681140:0:0% te::generic_gemm(tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<18432x4096x1xu8>, <32x18432xf32>, <4096x1x18432xu8>, <144x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
      2025-10-09 13:08:31.591763 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
      2025-10-09 13:08:31.591851 %681140:0:0% triton::bias_dropout_add_fused_train(tuple{<4096x1x7168xbf16>{7168, 7168, 1}, None:}, <4096x1x7168xbf16>{7168, 7168, 1}, 0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}
    - 1_0_fwd_module::TransformerLayer:
      - param_name: deepseek_v3.decoder.layers.1
      - inputs: [], {'hidden_states': '<4096x1x7168xbf16>{7168, 7168, 1}', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'context': 'None:', 'context_mask': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'attention_bias': 'None:', 'inference_context': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
      - 1_0_fwd_module::RMSNorm:
        - param_name: deepseek_v3.decoder.layers.1.input_layernorm
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
      - 1_0_fwd_module::MLASelfAttention:
        - param_name: deepseek_v3.decoder.layers.1.self_attention
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'inference_context': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'attention_bias': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
        - 1_0_fwd_module::YarnRotaryEmbedding:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.rotary_pos_emb
          - inputs: ['4096:int'], {'packed_seq': 'False:bool'}
          2025-10-09 13:08:31.593960 %681140:None:0% aten::arange(end:32:int, dtype:torch_float32:dtype, device:cpu:device, pin_memory:False:bool) -> <32xf32>{1}
          2025-10-09 13:08:31.594240 %681140:None:0% aten::sub(self:<32xf32>{1}, other:10:int) -> <32xf32>{1}
          2025-10-09 13:08:31.594455 %681140:None:0% aten::div(self:<32xf32>{1}, other:13:int) -> <32xf32>{1}
          2025-10-09 13:08:31.594680 %681140:None:0% aten::clamp(self:<32xf32>{1}, min:0:int, max:1:int) -> <32xf32>{1}
          2025-10-09 13:08:31.595084 %681140:None:0% aten::_to_copy(self:<32xf32>{1}, dtype:torch_float32:dtype, device:cuda_0:device) -> <32xf32>{1}
          2025-10-09 13:08:31.595351 %681140:0:0% aten::rsub(self:<32xf32>{1}, other:1_0:float) -> <32xf32>{1}
          2025-10-09 13:08:31.595593 %681140:0:0% aten::rsub(self:<32xf32>{1}, other:1:int) -> <32xf32>{1}
          2025-10-09 13:08:31.595784 %681140:0:0% aten::mul(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
          2025-10-09 13:08:31.595975 %681140:0:0% aten::mul(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
          2025-10-09 13:08:31.596204 %681140:0:0% aten::add(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
          2025-10-09 13:08:31.596533 %681140:None:0% aten::arange(end:4096:int, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <4096xf32>{1}
          2025-10-09 13:08:31.596779 %681140:0:0% aten::add(self:<4096xf32>{1}, other:0:int) -> <4096xf32>{1}
          2025-10-09 13:08:31.597024 %681140:0:0% aten::mul(self:<4096x1xf32>{1, 1}, other:<32xf32>{1}) -> <4096x32xf32>{32, 1}
          2025-10-09 13:08:31.597306 %681140:None:0% aten::cat(tensors:list{<4096x32xf32>{32, 1}, <4096x32xf32>{32, 1}}, dim:-1:int) -> <4096x64xf32>{64, 1}
          2025-10-09 13:08:31.597588 %681140:0:0% aten::slice(self:<4096x64xf32>{64, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <4096x64xf32>{64, 1}
          2025-10-09 13:08:31.597779 %681140:0:0% aten::unsqueeze(self:<4096x64xf32>{64, 1}, dim:1:int) -> <4096x1x64xf32>{64, 64, 1}
          2025-10-09 13:08:31.597964 %681140:0:0% aten::unsqueeze(self:<4096x1x64xf32>{64, 64, 1}, dim:2:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:31.598224 %681140:0:0% aten::slice(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dim:3:int, start:0:int, end:9223372036854775807:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        - 1_0_fwd_module::TELinear:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_q_down_proj
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f5330942f10>, <1536x7168xbf16>{7168, 1}+1363792896, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c419400_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, None:, 1:int, False:bool, False:bool, torch_bfloat16:dtype, None:, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TELinear():TELinear, None:, None:, False:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.600281 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <1536x7168xbf16>{7168, 1}+1363792896, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.600978 %681140:0:0% te::quantize(<1536x7168xbf16>{7168, 1}+1363792896, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.602374 %681140:0:0% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x1536xbf16>{1536, 1536, 1}, None:, None:, None:
        - 1_0_fwd_module::TELinear:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_kv_down_proj
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f5330943350>, <576x7168xbf16>{7168, 1}+1321913856, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c418b60_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, None:, 1:int, False:bool, False:bool, torch_bfloat16:dtype, None:, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TELinear():TELinear, None:, None:, False:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.604127 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <576x7168xbf16>{7168, 1}+1321913856, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.604818 %681140:0:0% te::quantize(<576x7168xbf16>{7168, 1}+1321913856, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.606080 %681140:0:0% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x576xbf16>{576, 576, 1}, None:, None:, None:
        2025-10-09 13:08:31.606664 %681140:0:0% aten::split_with_sizes(self:<4096x1x576xbf16>{576, 576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> <4096x1x512xbf16>{576, 576, 1}, <4096x1x64xbf16>{576, 576, 1}+512
        - 1_0_fwd_module::TELayerNormColumnParallelLinear:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_q_up_proj
          - inputs: ['<4096x1x1536xbf16>{1536, 1536, 1}'], {}
          - name: api::_LayerNormLinear
            inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f53309438a0>, <4096x1x1536xbf16>{1536, 1536, 1}, <1536xbf16>{1}+1363791360, None:, <24576x1536xbf16>{1536, 1}+1326042624, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41a090_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=1536,_out_features=24576,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <24576x1536xbf16>{1536, 1}+1326042624, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.608630 %681140:0:0% te::quantize(<24576x1536xbf16>{1536, 1}+1326042624, <True, True, False>:Float8BlockQuantizer) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.610803 %681140:0:0% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x24576xbf16>{24576, 1}, None:, None:, None:
        - 1_0_fwd_module::TELayerNormColumnParallelLinear:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_kv_up_proj
          - inputs: ['<4096x1x512xbf16>{576, 576, 1}'], {}
          2025-10-09 13:08:31.611633 %681140:0:0% aten::clone(self:<4096x1x512xbf16>{576, 512, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x512xbf16>{512, 512, 1}
          - name: api::_LayerNormLinear
            inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5330943ce0>, <4096x1x512xbf16>{512, 512, 1}, <512xbf16>{1}+1321913344, None:, <32768x512xbf16>{512, 1}+1305136128, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c418b30_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=512,_out_features=32768,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <32768x512xbf16>{512, 1}+1305136128, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.613294 %681140:0:0% te::quantize(<32768x512xbf16>{512, 1}+1305136128, <True, True, False>:Float8BlockQuantizer) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.614853 %681140:0:0% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32768xbf16>{32768, 1}, None:, None:, None:
        2025-10-09 13:08:31.615395 %681140:0:0% aten::unsqueeze(self:<4096x1x64xbf16>{576, 576, 1}+512, dim:-2:int) -> <4096x1x1x64xbf16>{576, 576, 64, 1}+512
        2025-10-09 13:08:31.615685 %681140:0:0% aten::slice(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dim:0:int, start:0:int, end:4096:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.616006 %681140:0:0% aten::split_with_sizes(self:<4096x1x128x192xbf16>{24576, 24576, 192, 1}, split_sizes:list{128:int, 64:int}, dim:-1:int) -> <4096x1x128x128xbf16>{24576, 24576, 192, 1}, <4096x1x128x64xbf16>{24576, 24576, 192, 1}+128
        2025-10-09 13:08:31.616315 %681140:0:0% aten::split_with_sizes(self:<4096x1x128x256xbf16>{32768, 32768, 256, 1}, split_sizes:list{128:int, 128:int}, dim:-1:int) -> <4096x1x128x128xbf16>{32768, 32768, 256, 1}, <4096x1x128x128xbf16>{32768, 32768, 256, 1}+128
        2025-10-09 13:08:31.616629 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:64:int, end:9223372036854775807:int) -> <4096x1x128x0xbf16>{24576, 24576, 192, 1}+192
        2025-10-09 13:08:31.616899 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x32xbf16>{24576, 24576, 192, 2}+128
        2025-10-09 13:08:31.617178 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x32xbf16>{24576, 24576, 192, 2}+129
        2025-10-09 13:08:31.617689 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{24576, 24576, 192, 2}+128, <4096x1x128x32xbf16>{24576, 24576, 192, 2}+129}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.618121 %681140:0:0% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.618369 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.618760 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.618959 %681140:0:0% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.619180 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.619533 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.619900 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.620161 %681140:0:0% aten::split(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, split_size:32:int, dim:-1:int) -> <4096x1x128x32xbf16>{8192, 8192, 64, 1}, <4096x1x128x32xbf16>{8192, 8192, 64, 1}+32
        2025-10-09 13:08:31.620425 %681140:0:0% aten::neg(self:<4096x1x128x32xbf16>{8192, 8192, 64, 1}+32) -> <4096x1x128x32xbf16>{4096, 4096, 32, 1}
        2025-10-09 13:08:31.620915 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{4096, 4096, 32, 1}, <4096x1x128x32xbf16>{8192, 8192, 64, 1}}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.621294 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.621607 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.622006 %681140:None:0% aten::cat(tensors:list{<4096x1x128x64xbf16>{8192, 8192, 64, 1}, <4096x1x128x0xbf16>{24576, 24576, 192, 1}+192}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
        2025-10-09 13:08:31.622314 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:64:int, end:9223372036854775807:int) -> <4096x1x1x0xbf16>{576, 576, 64, 1}+576
        2025-10-09 13:08:31.622582 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x32xbf16>{576, 576, 64, 2}+512
        2025-10-09 13:08:31.622846 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x32xbf16>{576, 576, 64, 2}+513
        2025-10-09 13:08:31.623123 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{576, 576, 64, 2}+512, <4096x1x1x32xbf16>{576, 576, 64, 2}+513}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.623302 %681140:0:0% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.623517 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.623869 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.624052 %681140:0:0% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.624437 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        2025-10-09 13:08:31.624797 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.625010 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.625263 %681140:0:0% aten::split(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, split_size:32:int, dim:-1:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}, <4096x1x1x32xbf16>{64, 64, 64, 1}+32
        2025-10-09 13:08:31.625450 %681140:0:0% aten::neg(self:<4096x1x1x32xbf16>{64, 64, 64, 1}+32) -> <4096x1x1x32xbf16>{32, 32, 32, 1}
        2025-10-09 13:08:31.625720 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{32, 32, 32, 1}, <4096x1x1x32xbf16>{64, 64, 64, 1}}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.625925 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.626168 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.626433 %681140:None:0% aten::cat(tensors:list{<4096x1x1x64xbf16>{64, 64, 64, 1}, <4096x1x1x0xbf16>{576, 576, 64, 1}+576}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        2025-10-09 13:08:31.627419 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{24576, 24576, 192, 1}, <4096x1x128x64xbf16>{8192, 8192, 64, 1}}, dim:-1:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
        2025-10-09 13:08:31.628436 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{32768, 32768, 256, 1}, <4096x1x128x64xbf16>{64, 64, 0, 1}}, dim:-1:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
        2025-10-09 13:08:31.628944 %681140:0:0% aten::clone(self:<4096x1x128x128xbf16>{32768, 32768, 256, 1}+128, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
        - 1_0_fwd_module::TEDotProductAttention:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.core_attention
          - inputs: ['<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x128xbf16>{16384, 16384, 128, 1}', '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}'], {'packed_seq_params': 'None:', 'attn_mask_type': 'AttnMaskType_causal:AttnMaskType'}
          - 1_0_fwd_module::UnfusedDotProductAttention:
            - param_name: deepseek_v3.decoder.layers.1.self_attention.core_attention.unfused_attention
            - inputs: ["_'_num_heads'__None,_'_alibi_slopes'__None,_'_ma__seqlen_q'__None,_'_ma__seqlen_kv'__None,_'_bottom_right_alignment'__True,_'_alibi_bias'__None,_'_alibi_slopes_require_update'__False,_'_alibi_bias_require_update'__False_:dict", '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x128xbf16>{16384, 16384, 128, 1}'], {'qkv_layout': 'sbhd_sbhd_sbhd:str', 'cu_seqlens_q': '<2xi32>{1}', 'cu_seqlens_kv': '<2xi32>{1}', 'attn_mask_type': 'causal:str', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'window_size': 'tuple{-1:int, 0:int}', 'core_attention_bias_type': 'no_bias:str', 'core_attention_bias': 'None:', 'alibi_slopes': 'None:', 'inference_params': 'None:'}
            2025-10-09 13:08:31.630292 %681140:None:0% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
            2025-10-09 13:08:31.630618 %681140:None:0% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
            2025-10-09 13:08:31.630984 %681140:0:0% aten::sub(self:<1x1x4096x1xi32>{4096, 4096, 1, 1}, other:<1x1x1x4096xi32>{4096, 4096, 4096, 1}) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.631289 %681140:0:0% aten::sub(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:4096:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.631579 %681140:0:0% aten::add(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.631843 %681140:0:0% aten::le(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.632110 %681140:0:0% aten::lt(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.632316 %681140:0:0% aten::bitwise_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.632541 %681140:0:0% aten::bitwise_and(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.632734 %681140:0:0% aten::logical_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.632962 %681140:0:0% aten::logical_or(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
            2025-10-09 13:08:31.633269 %681140:0:0% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
            2025-10-09 13:08:31.633471 %681140:0:0% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
            2025-10-09 13:08:31.633666 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
            2025-10-09 13:08:31.640566 %681140:0:0% aten::baddbmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, batch1:<128x4096x192xbf16>{192, 24576, 1}, batch2:<128x192x4096xbf16>{192, 1, 24576}, beta:0_0:float, alpha:0_1352337788608801:float) -> <128x4096x4096xbf16>{16777216, 4096, 1}
            - name: api::ScaledMaskedSoftmax
              inputs: <torch.autograd.function.ScaledMaskedSoftmaxBackward object at 0x7f53307c47c0>, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, 1_0:float
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/attention/dot_product_attention/softmax.py
              2025-10-09 13:08:31.652135 %681140:0:0% te::scaled_masked_softmax_forward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
            - 1_0_fwd_module::Dropout:
              - param_name: deepseek_v3.decoder.layers.1.self_attention.core_attention.unfused_attention.attention_dropout
              - inputs: ['<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}'], {}
            2025-10-09 13:08:31.652791 %681140:None:0% aten::eq(self:<5056xu8>{1}, other:<5056xu8>{1}) -> <5056xbool>{1}
            2025-10-09 13:08:31.652974 %681140:None:0% aten::all(self:<5056xbool>{1}) -> <1xbool>{1}
            2025-10-09 13:08:31.653277 %681140:0:0% aten::transpose(self:<4096x128x128xbf16>{16384, 128, 1}, dim0:0:int, dim1:1:int) -> <128x4096x128xbf16>{128, 16384, 1}
            2025-10-09 13:08:31.657530 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
            2025-10-09 13:08:31.657866 %681140:0:0% aten::permute(self:<1x128x4096x128xbf16>{67108864, 524288, 128, 1}, dims:list{2:int, 0:int, 1:int, 3:int}) -> <4096x1x128x128xbf16>{128, 67108864, 524288, 1}
            2025-10-09 13:08:31.658406 %681140:0:0% aten::clone(self:<4096x1x128x128xbf16>{128, 67108864, 524288, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
        - 1_0_fwd_module::TERowParallelLinear:
          - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_proj
          - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
          - name: api::_Linear
            inputs: <torch.autograd.function._LinearBackward object at 0x7f53307c5150>, <7168x16384xbf16>{16384, 1}+1374802944, <4096x1x16384xbf16>{16384, 16384, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c4195b0_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, row:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TERowParallelLinear(in_features=16384,_out_features=7168,_bias=False,_TP=1):TERowParallelLinear, None:, None:, True:bool, False:bool
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
            - name: api::_QuantizeFunc
              inputs: None:, <4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.660625 %681140:0:0% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}
            - name: api::_QuantizeFunc
              inputs: None:, <7168x16384xbf16>{16384, 1}+1374802944, <True, True, False>:Float8BlockQuantizer
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
              2025-10-09 13:08:31.661481 %681140:0:0% te::quantize(<7168x16384xbf16>{16384, 1}+1374802944, <True, True, False>:Float8BlockQuantizer) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
            2025-10-09 13:08:31.665963 %681140:0:0% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
      2025-10-09 13:08:31.667079 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
      2025-10-09 13:08:31.667167 %681140:0:0% triton::bias_dropout_add_fused_train(tuple{<4096x1x7168xbf16>{7168, 7168, 1}, None:}, <4096x1x7168xbf16>{7168, 7168, 1}, 0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}
      - 1_0_fwd_module::RMSNorm:
        - param_name: deepseek_v3.decoder.layers.1.pre_mlp_layernorm
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
      - 1_0_fwd_module::MoELayer:
        - param_name: deepseek_v3.decoder.layers.1.mlp
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
        - 1_0_fwd_module::TopKRouter:
          - param_name: deepseek_v3.decoder.layers.1.mlp.router
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          2025-10-09 13:08:31.668583 %681140:0:0% aten::_to_copy(self:<32xbf16>{1}, dtype:torch_float32:dtype) -> <32xf32>{1}
          - name: api::RouterGatingLinearFunction
            inputs: <torch.autograd.function.RouterGatingLinearFunctionBackward object at 0x7f53307c5d00>, <4096x1x7168xbf16>{7168, 7168, 1}, <32x7168xbf16>{7168, 1}+1304899584, torch_float32:dtype
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
            2025-10-09 13:08:31.695168 %681140:0:0% te::generic_gemm(<32x7168xbf16>{7168, 1}+1304899584, True:bool, <4096x7168xbf16>{7168, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32xf32>{32, 1}, None:, None:, None:
          - name: api::RandomSTE
            inputs: <torch.autograd.function.RandomSTEBackward object at 0x7f53307c5e10>, <4096x1x32xf32>{32, 32, 1}
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
            2025-10-09 13:08:31.696325 %681140:0:0% aten::clone(self:<4096x1x32xf32>{32, 32, 1}) -> <4096x1x32xf32>{32, 32, 1}
            2025-10-09 13:08:31.696827 %681140:0:0% aten::normal_(self:<4096x1x32xf32>{32, 32, 1}, generator:%290:tuple{seed=42:i32, offset=None:NoneType}) -> <4096x1x32xf32>{32, 32, 1}
          - name: api::FusedTopkScoreFunction
            inputs: <torch.autograd.function.FusedTopkScoreFunctionBackward object at 0x7f53307c6030>, <4096x32xf32>{32, 1}, 8:int, True:bool, 2:int, 1:int, 2_5:float, sigmoid:str, <32xf32>{1}
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
            2025-10-09 13:08:31.698304 %681140:0:0% te::fused_topk_with_score_function_fwd(<4096x32xf32>{32, 1}, 8:int, True:bool, 2:int, 1:int, 2_5:float, sigmoid:str, <32xf32>{1}) -> <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
          - name: api::FusedComputeScoresForMoEAuxLoss
            inputs: <torch.autograd.function.FusedComputeScoresForMoEAuxLossBackward object at 0x7f53307c6250>, <4096x32xf32>{32, 1}, 8:int, sigmoid:str
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
            2025-10-09 13:08:31.699246 %681140:None:0% te::fused_score_for_moe_aux_loss_fwd(logits:<4096x32xf32>{32, 1}, topk:8:int, score_function:sigmoid:str) -> <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
          2025-10-09 13:08:31.704626 %681140:0:0% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
          - name: api::_ReduceFromModelParallelRegion
            inputs: <torch.autograd.function._ReduceFromModelParallelRegionBackward object at 0x7f53307c6580>, <32xi64>{1}, <rank:0, size:1, group_name:55, backend:nccl>:ProcessGroup
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
          - name: api::FusedAuxLoss
            inputs: <torch.autograd.function.FusedAuxLossBackward object at 0x7f53307c6580>, <4096x32xf32>{32, 1}, <32xi64>{1}, 4096:int, 32:int, 8:int, 0_0001:float
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
            2025-10-09 13:08:31.705798 %681140:None:0% te::fused_moe_aux_loss_fwd(probs:<4096x32xf32>{32, 1}, tokens_per_expert:<32xi64>{1}, total_num_tokens:4096:int, num_experts:32:int, num_rows:4096:int, num_cols:32:int, topk:8:int, coeff:0_0001:float) -> <1xf32>{1}, <1xf32>{1}
          2025-10-09 13:08:31.706169 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
          2025-10-09 13:08:31.706441 %681140:0:0% aten::div(self:<1xf32>{1}, other:0_0001:float) -> <1xf32>{1}
          2025-10-09 13:08:31.706859 %681140:None:0% aten::zeros(size:list{3:int}, device:cuda_0:device, pin_memory:False:bool) -> <3xf32>{1}
          2025-10-09 13:08:31.707314 %681140:0:0% aten::add_(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
          2025-10-09 13:08:31.707591 %681140:0:0% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
          - name: api::MoEAuxLossAutoScaler
            inputs: <torch.autograd.function.MoEAuxLossAutoScalerBackward object at 0x7f53307c67a0>, <4096x32xf32>{32, 1}, <1xf32>{1}
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
          2025-10-09 13:08:31.708173 %681140:0:0% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
          2025-10-09 13:08:31.708533 %681140:0:0% aten::add_(self:<32xbf16>{1}, other:<32xi64>{1}) -> <32xbf16>{1}
        2025-10-09 13:08:31.723548 %681140:0:0% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int) -> <4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}
        - name: api::FusedDispatch
          inputs: <torch.autograd.function.FusedDispatchBackward object at 0x7f53307c69c0>, <4096x7168xbf16>{7168, 1}, <4096x8xi64>{8, 1}, <4096x8xf32>{8, 1}, 32:int, <rank:0, size:8, group_name:72, backend:nccl>:ProcessGroup, True:bool, True:bool
          file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
          2025-10-09 13:08:31.726602 %681140:None:0% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <5xu8>{1}
          2025-10-09 13:08:31.727107 %681140:None:0% aten::_to_copy(self:<1xi64>{1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1xi64>{1}
          2025-10-09 13:08:31.727551 %681140:None:0% aten::zeros(size:list{8:int}, dtype:torch_int64:dtype, device:cuda:device, pin_memory:False:bool) -> <8xi64>{1}
          2025-10-09 13:08:31.727853 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}
          2025-10-09 13:08:31.728120 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+1
          2025-10-09 13:08:31.728364 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+2
          2025-10-09 13:08:31.728603 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+3
          2025-10-09 13:08:31.728834 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+4
          2025-10-09 13:08:31.729072 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+5
          2025-10-09 13:08:31.729321 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+6
          2025-10-09 13:08:31.729559 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+7
          2025-10-09 13:08:32.568714 %681140:None:0% c10d::allgather_(output_tensors:list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, input_tensors:list{<1xi64>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, Work:distributed
          2025-10-09 13:08:32.569178 %681140:0:0% aten::gt(self:<1xi64>{1}+1, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.569533 %681140:0:0% aten::gt(self:<1xi64>{1}+2, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.569815 %681140:0:0% aten::gt(self:<1xi64>{1}+3, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.570107 %681140:0:0% aten::gt(self:<1xi64>{1}+4, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.570385 %681140:0:0% aten::gt(self:<1xi64>{1}+5, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.570667 %681140:0:0% aten::gt(self:<1xi64>{1}+6, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.570945 %681140:0:0% aten::gt(self:<1xi64>{1}+7, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.571343 %681140:0:0% aten::resize_(self:<5xu8>{1}, size:list{5:int}) -> <5xu8>{1}
          2025-10-09 13:08:32.571767 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:0:int, end:5:int) -> <5xu8>{1}
          2025-10-09 13:08:32.572097 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:5:int, end:10:int) -> <5xu8>{1}+5
          2025-10-09 13:08:32.572398 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:10:int, end:15:int) -> <5xu8>{1}+10
          2025-10-09 13:08:32.572704 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:15:int, end:20:int) -> <5xu8>{1}+15
          2025-10-09 13:08:32.573015 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:20:int, end:25:int) -> <5xu8>{1}+20
          2025-10-09 13:08:32.573322 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:25:int, end:30:int) -> <5xu8>{1}+25
          2025-10-09 13:08:32.573616 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:30:int, end:35:int) -> <5xu8>{1}+30
          2025-10-09 13:08:32.573913 %681140:0:0% aten::slice(self:<40xu8>{1}, dim:0:int, start:35:int, end:40:int) -> <5xu8>{1}+35
          2025-10-09 13:08:32.575101 %681140:None:0% c10d::allgather_(output_tensors:list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, input_tensors:list{<5xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, Work:distributed
          2025-10-09 13:08:32.575643 %681140:0:0% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.576201 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+5, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.576683 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+10, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.577176 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+15, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.577655 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+20, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.578149 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+25, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.578623 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+30, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.579100 %681140:0:0% aten::_to_copy(self:<5xu8>{1}+35, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
          2025-10-09 13:08:32.579730 %681140:None:0% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <108xu8>{1}
          2025-10-09 13:08:32.580128 %681140:None:0% aten::_to_copy(self:<1xi64>{1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1xi64>{1}
          2025-10-09 13:08:32.580484 %681140:None:0% aten::zeros(size:list{8:int}, dtype:torch_int64:dtype, device:cuda:device, pin_memory:False:bool) -> <8xi64>{1}
          2025-10-09 13:08:32.580716 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}
          2025-10-09 13:08:32.580914 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+1
          2025-10-09 13:08:32.581117 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+2
          2025-10-09 13:08:32.581305 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+3
          2025-10-09 13:08:32.581493 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+4
          2025-10-09 13:08:32.581681 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+5
          2025-10-09 13:08:32.581873 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+6
          2025-10-09 13:08:32.582066 %681140:0:0% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+7
          2025-10-09 13:08:32.583060 %681140:None:0% c10d::allgather_(output_tensors:list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, input_tensors:list{<1xi64>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, Work:distributed
          2025-10-09 13:08:32.583296 %681140:0:0% aten::gt(self:<1xi64>{1}+1, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.583524 %681140:0:0% aten::gt(self:<1xi64>{1}+2, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.583741 %681140:0:0% aten::gt(self:<1xi64>{1}+3, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.583963 %681140:0:0% aten::gt(self:<1xi64>{1}+4, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.584178 %681140:0:0% aten::gt(self:<1xi64>{1}+5, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.584389 %681140:0:0% aten::gt(self:<1xi64>{1}+6, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.584599 %681140:0:0% aten::gt(self:<1xi64>{1}+7, other:<1xi64>{1}) -> <1xbool>{1}
          2025-10-09 13:08:32.584960 %681140:0:0% aten::resize_(self:<108xu8>{1}, size:list{108:int}) -> <108xu8>{1}
          2025-10-09 13:08:32.585321 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:0:int, end:108:int) -> <108xu8>{1}
          2025-10-09 13:08:32.585632 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:108:int, end:216:int) -> <108xu8>{1}+108
          2025-10-09 13:08:32.585951 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:216:int, end:324:int) -> <108xu8>{1}+216
          2025-10-09 13:08:32.586261 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:324:int, end:432:int) -> <108xu8>{1}+324
          2025-10-09 13:08:32.586573 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:432:int, end:540:int) -> <108xu8>{1}+432
          2025-10-09 13:08:32.586888 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:540:int, end:648:int) -> <108xu8>{1}+540
          2025-10-09 13:08:32.587198 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:648:int, end:756:int) -> <108xu8>{1}+648
          2025-10-09 13:08:32.587495 %681140:0:0% aten::slice(self:<864xu8>{1}, dim:0:int, start:756:int, end:864:int) -> <108xu8>{1}+756
          2025-10-09 13:08:32.588570 %681140:None:0% c10d::allgather_(output_tensors:list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, input_tensors:list{<108xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, Work:distributed
          2025-10-09 13:08:32.589061 %681140:0:0% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.589586 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+108, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.590091 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+216, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.590584 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+324, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.591086 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+432, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.591561 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+540, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.592048 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+648, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.592521 %681140:0:0% aten::_to_copy(self:<108xu8>{1}+756, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
          2025-10-09 13:08:32.720905 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.721247 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.721500 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.721732 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.721960 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.722182 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.722430 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.722661 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.731763 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.732018 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.732208 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.732388 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.732608 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.732831 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.733062 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.733281 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.733503 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.733726 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.733952 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.734168 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.734343 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.734515 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.734728 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.734958 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.735134 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.735301 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.735509 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.735728 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.735896 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.736070 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.736236 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.736399 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.736607 %681140:0:270142432% aten::record_stream(self:<15712x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.736830 %681140:0:270142432% aten::record_stream(self:<15712x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:32.737049 %681140:0:270142432% aten::record_stream(self:<15712x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:32.737265 %681140:0:270142432% aten::record_stream(self:<15712x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
        - 1_0_fwd_module::SharedExpertMLP:
          - param_name: deepseek_v3.decoder.layers.1.mlp.shared_experts
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - 1_0_fwd_module::TEColumnParallelLinear:
            - param_name: deepseek_v3.decoder.layers.1.mlp.shared_experts.linear_fc1
            - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
            - name: api::_Linear
              inputs: <torch.autograd.function._LinearBackward object at 0x7f53307c7240>, <4096x7168xbf16>{7168, 1}+1275539456, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41aed0_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TEColumnParallelLinear(in_features=7168,_out_features=4096,_bias=False,_TP=1):TEColumnParallelLinear, None:, None:, True:bool, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, False, True>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:32.739613 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <4096x7168xbf16>{7168, 1}+1275539456, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:32.740350 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+1275539456, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:32.742593 %681140:0:0% te::generic_gemm(tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x4096xbf16>{4096, 4096, 1}, None:, None:, None:
          - name: api::SwiGLUFunction
            inputs: <torch.autograd.function.SwiGLUFunctionBackward object at 0x7f53307c7570>, <4096x4096xbf16>{4096, 1}, False:bool, False:bool
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
            2025-10-09 13:08:32.743487 %681140:0:0% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048
            2025-10-09 13:08:32.743808 %681140:0:0% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
            2025-10-09 13:08:32.744134 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
          - 1_0_fwd_module::TERowParallelLinear:
            - param_name: deepseek_v3.decoder.layers.1.mlp.shared_experts.linear_fc2
            - inputs: ['<4096x1x2048xbf16>{2048, 2048, 1}'], {}
            - name: api::_Linear
              inputs: <torch.autograd.function._LinearBackward object at 0x7f53307c78a0>, <7168x2048xbf16>{2048, 1}+1260859392, <4096x1x2048xbf16>{2048, 2048, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41ade0_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, row:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TERowParallelLinear(in_features=2048,_out_features=7168,_bias=False,_TP=1):TERowParallelLinear, None:, None:, False:bool, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:32.745781 %681140:0:0% te::quantize(<4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<2048x4096x1xu8>, <32x2048xf32>, <4096x1x2048xu8>, <16x4096xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <7168x2048xbf16>{2048, 1}+1260859392, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:32.746492 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+1260859392, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:32.748180 %681140:0:0% te::generic_gemm(tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<2048x4096x1xu8>, <32x2048xf32>, <4096x1x2048xu8>, <16x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
        - name: api::IndicesToMultihot
          inputs: <torch.autograd.function.IndicesToMultihotBackward object at 0x7f53307c7ce0>, <15712x8xi64>{8, 1}, <15712x8xf32>{8, 1}, 4:int
          file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_indices_converter.py
        2025-10-09 13:08:33.103420 %681140:None:0% aten::div(self:<4xi64>{1}, other:16:int) -> <4xf32>{1}
        2025-10-09 13:08:33.103718 %681140:None:0% aten::ceil(self:<4xf32>{1}) -> <4xf32>{1}
        2025-10-09 13:08:33.103994 %681140:None:0% aten::mul(self:<4xf32>{1}, other:16:int) -> <4xf32>{1}
        2025-10-09 13:08:33.104429 %681140:None:0% aten::_to_copy(self:<4xf32>{1}, dtype:torch_int64:dtype) -> <4xi64>{1}
        2025-10-09 13:08:33.104691 %681140:None:0% aten::le(self:<4xi64>{1}, other:15712:int) -> <4xbool>{1}
        2025-10-09 13:08:33.104908 %681140:None:0% aten::all(self:<4xbool>{1}) -> <1xbool>{1}
        2025-10-09 13:08:33.105336 %681140:0:0% aten::transpose(self:<15712x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x15712xbool>{1, 4}
        2025-10-09 13:08:33.105757 %681140:0:0% aten::clone(self:<4x15712xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x15712xbool>{15712, 1}
        2025-10-09 13:08:33.106255 %681140:0:0% aten::_to_copy(self:<4x15712xbool>{15712, 1}, dtype:torch_int32:dtype) -> <4x15712xi32>{15712, 1}
        2025-10-09 13:08:33.110237 %681140:0:0% aten::transpose(self:<4x15712xi32>{15712, 1}, dim0:0:int, dim1:1:int) -> <15712x4xi32>{1, 15712}
        2025-10-09 13:08:33.110550 %681140:None:0% aten::sum(self:<4xi64>{1}) -> <1xi64>{1}
        - name: api::_moe_permute_mask_map
          inputs: <torch.autograd.function._moe_permute_mask_mapBackward object at 0x7f53308408d0>, <15712x7168xbf16>{7168, 1}, <15712x4xi32>{1, 15712}, 32736:int, <15712x4xf32>{4, 1}
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
          2025-10-09 13:08:33.126806 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.128548 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.130253 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.131975 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.133679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.155546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.157290 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.159010 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.160724 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.162442 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.164160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.165864 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.167582 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.169292 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.171001 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.172705 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.174415 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.176124 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.177830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.179530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.181238 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.182962 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.184672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.186374 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.188094 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.189801 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.191515 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.193228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.194940 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.196644 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.198353 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.200070 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.201778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.203493 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.205200 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.206904 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.208613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.210319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.212026 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.213735 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.215448 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.217147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.218845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.220546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.222248 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.223971 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.225695 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.227403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.229114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.230830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.232537 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.234251 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.235957 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.237658 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.239364 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.241070 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.242784 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.244492 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.246195 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.247895 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.249598 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.251300 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.256805 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.257851 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.258861 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.259871 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.260881 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.279766 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.280804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.281832 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.282866 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.283894 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.284917 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.285946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.286965 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.287990 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.289011 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.290026 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.291046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.292067 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.293092 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.294113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.295130 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.296148 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.297166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.298174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.299190 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.300198 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.301209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.302217 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.303233 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.304245 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.305258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.306272 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.307282 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.308300 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.309319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.310340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.311351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.312361 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.313377 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.314398 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.315409 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.316413 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.317431 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.318441 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.319455 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.320465 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.321474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.322486 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.323504 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.324523 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.325535 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.326551 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.327565 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.328576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.329585 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.330595 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.331604 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.332612 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.333625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.334637 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.335652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.336664 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.337671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.338679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.339686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.340690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.341703 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.342712 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.343726 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.344743 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.345754 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.346761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.347769 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.348781 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.349800 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.350823 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.351839 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.352853 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.353870 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.354887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.355912 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.356933 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.357951 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.358969 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.359988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.361001 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.362010 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.363020 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.364036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.365049 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.366076 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.367095 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.368113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.369131 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.370151 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.371168 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.372181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.373190 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.374208 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.375218 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.376228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.377237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.381999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.382766 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.383516 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.384254 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.384986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.401403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.402181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.402943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.403701 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.404453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.405202 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.405954 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.406703 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.407453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.408201 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.408948 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.409690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.410435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.411180 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.411922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.412669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.413410 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.414162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.414903 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.415648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.416387 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.417135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.417869 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.418621 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.419366 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.420109 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.420852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.421593 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.422339 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.423090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.423840 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.424586 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.425337 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.426088 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.426826 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.427570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.428317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.429065 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.429804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.430545 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.431285 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.432027 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.432768 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.433514 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.434267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.435014 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.435764 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.436506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.437246 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.437987 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.438727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.439477 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.440222 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.440968 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.441709 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.442451 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.443205 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.443962 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.444705 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.445446 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.446186 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.446921 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.447662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.448399 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.449144 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.449886 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.450635 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.451378 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.452123 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.452860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.453607 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.454352 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.455105 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.455848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.456595 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.457340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.458090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.458830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.459570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.460312 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.461053 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.461802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.462546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.463287 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.464038 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.464778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.465518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.466260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.467006 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.467742 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.468483 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.469222 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.469966 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.470707 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.471445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.472184 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.472919 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.473662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.474403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.475150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.475892 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.476653 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.477399 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.478147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.478883 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.479631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.480372 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.481116 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.481853 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.482609 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.483346 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.484097 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.484843 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.485592 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.486337 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.487090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.487833 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.488583 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.489325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.490076 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.490821 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.491565 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.492307 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.493048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.493789 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.494532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.495276 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.496020 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.496758 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.497496 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.498245 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.498989 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.503216 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.503811 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.504400 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.504979 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.505548 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.519309 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.519920 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.520521 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.521120 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.521703 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.522288 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.522872 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.523467 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.524050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.524638 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.525223 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.525803 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.526385 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.526972 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.527549 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.528135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.528712 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.529292 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.529870 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.530454 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.531037 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.531617 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.532197 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.532769 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.533356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.533946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.534537 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.535123 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.535707 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.536282 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.536858 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.537441 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.538018 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.538588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.539169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.539743 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.540319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.540892 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.541469 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.542052 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.542631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.543211 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.543785 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.544364 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.544952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.545532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.546114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.546687 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.547259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.547828 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.548404 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.548981 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.549554 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.550135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.550721 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.551297 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.551870 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.552450 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.553026 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.553598 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.554171 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.554750 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.555325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.555900 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.556485 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.557065 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.557638 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.558209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.558780 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.559355 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.559940 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.560521 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.561098 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.561682 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.562254 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.562826 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.563401 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.563982 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.564561 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.565145 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.565728 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.566318 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.566889 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.567461 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.568044 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.568619 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.569206 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.569780 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.570356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.570934 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.571507 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.572086 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.572659 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.573231 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.573806 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.574380 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.574972 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.575546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.576128 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.576697 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.577270 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.577844 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.578418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.578994 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.579569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.580155 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.580726 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.581301 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.581875 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.582457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.583031 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.583602 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.584175 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.584756 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.585333 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.585905 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.586490 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.587073 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.587648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.588228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.588807 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.589382 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.589961 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.590533 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.591113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.591684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.592260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.592834 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.593412 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.594000 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.594577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.595166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.595748 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.596325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.596901 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.597481 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.598051 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.598623 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.599200 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.599770 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.600345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.600920 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.601507 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.602091 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.602663 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.603245 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.603817 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.604394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.604982 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.605558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.606139 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.606714 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.607302 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.607874 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.608457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.609031 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.609614 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.610190 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.610765 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.611341 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.611918 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.612499 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.613081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.613654 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.614230 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.614806 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.615396 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.615977 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.620407 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.620969 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.621489 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.622000 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.622506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.634778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.635331 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.635860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.636394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.636918 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.637456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.637985 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.638508 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.639033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.639555 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.640082 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.640606 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.641128 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.641646 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.642167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.642684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.643207 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.643727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.644258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.644776 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.645312 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.645830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.646348 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.646869 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.647398 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.647915 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.648439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.648958 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.649475 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.650009 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.650527 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.651045 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.651564 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.652090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.652610 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.653145 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.653662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.654182 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.654700 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.655237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.655755 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.656273 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.656788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.657305 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.657829 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.658352 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.658869 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.659393 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.659912 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.660433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.660951 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.661466 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.661987 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.662503 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.663017 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.663532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.664048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.664566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.665092 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.665617 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.666142 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.666656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.667176 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.667688 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.668204 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.668719 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.669233 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.669752 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.670272 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.670784 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.671314 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.671825 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.672342 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.672854 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.673369 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.673879 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.674398 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.674915 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.675443 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.675960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.676473 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.676993 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.677506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.678022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.678533 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.679050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.679568 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.680090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.680606 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.681125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.681637 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.682157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.682671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.683186 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.683697 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.684212 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.684733 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.685268 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.685793 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.686311 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.686827 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.687345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.687853 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.688366 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.688878 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.689395 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.689907 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.690427 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.690946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.691468 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.691986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.692501 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.693018 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.693533 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.694054 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.694570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.695087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.695611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.696134 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.696645 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.697161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.697683 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.698198 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.698709 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.699228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.699743 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.700259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.700774 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.701294 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.701808 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.702329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.702842 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.703363 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.703879 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.704401 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.704919 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.705448 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.705974 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.706489 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.707013 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.707536 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.708064 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.708583 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.709101 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.709617 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.710147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.710673 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.711195 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.711707 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.712222 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.712735 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.713260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.713771 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.714283 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.714795 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.715312 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.715829 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.716344 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.716852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.717367 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.717882 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.718408 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.718922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.719440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.719959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.720475 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.720993 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.721506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.722021 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.722534 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.723048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.723561 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.724084 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.724599 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.725120 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.725644 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.726164 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.726679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.727198 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.727718 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.728242 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.728750 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.729276 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.729789 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.730308 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.730823 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.731340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.735804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.736335 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.736848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.737351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.737848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.749761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.750296 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.750808 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.751322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.751832 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.752347 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.752854 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.753363 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.753869 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.754380 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.754883 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.755393 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.755901 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.756416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.756917 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.757428 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.757937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.758439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.758953 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.759462 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.759975 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.760483 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.760995 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.761499 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.761998 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.762499 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.763004 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.763509 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.764017 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.764518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.765021 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.765524 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.766036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.766545 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.767053 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.767558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.768069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.768576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.769083 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.769590 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.770099 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.770602 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.771116 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.771618 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.772125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.772626 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.773132 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.773631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.774138 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.774640 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.775156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.775657 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.776163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.776667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.777171 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.777671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.778177 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.778675 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.779176 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.779679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.780180 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.780679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.781181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.781680 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.782178 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.782679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.783181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.783691 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.784191 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.784694 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.785201 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.785697 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.786211 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.786711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.787214 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.787718 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.788221 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.788727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.789229 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.789728 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.790226 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.790727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.791228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.791727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.792229 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.792725 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.793227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.793731 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.794234 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.794733 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.795239 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.795741 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.796255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.796772 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.797283 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.797789 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.798293 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.798795 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.799299 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.799800 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.800301 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.800801 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.801303 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.801805 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.802312 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.802816 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.803317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.803817 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.804322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.804824 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.805329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.805832 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.806341 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.806841 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.807351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.807857 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.808365 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.808865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.809379 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.809880 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.810390 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.810890 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.811397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.811909 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.812420 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.812926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.813437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.813941 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.814443 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.814950 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.815454 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.815963 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.816479 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.816988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.817494 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.818022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.818529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.819033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.819536 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.820037 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.820542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.821060 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.821560 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.822072 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.822575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.823081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.823580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.824086 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.824585 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.825088 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.825588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.826106 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.826613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.827122 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.827620 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.828130 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.828632 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.829138 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.829640 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.830147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.830645 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.831154 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.831655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.832157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.832657 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.833159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.833661 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.834161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.834662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.835172 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.835671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.836179 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.836681 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.837191 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.837690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.838194 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.838698 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.839210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.839709 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.840213 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.840714 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.841216 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.841714 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.842213 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.842717 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.843219 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.843716 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.844233 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.844736 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.845238 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.849845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.850369 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.850871 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.851373 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.851864 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.863985 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.864520 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.865035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.865545 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.866051 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.866562 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.867081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.867588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.868103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.868612 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.869131 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.869636 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.870150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.870651 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.871159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.871659 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.872161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.872662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.873161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.873660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.874162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.874668 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.875166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.875667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.876169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.876669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.877167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.877672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.878174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.878674 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.879173 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.879671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.880169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.880666 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.881169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.881667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.882165 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.882662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.883163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.883660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.884160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.884656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.885156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.885655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.886165 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.886671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.887170 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.887671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.888172 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.888671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.889171 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.889667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.890170 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.890667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.891171 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.891668 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.892265 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.892774 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.893281 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.893777 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.894282 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.894785 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.895291 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.895805 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.896308 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.896822 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.897325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.897822 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.898319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.898814 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.899313 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.899812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.900313 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.900811 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.901316 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.901825 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.902325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.902820 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.903320 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.903818 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.904322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.904825 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.905326 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.905821 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.906322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.906824 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.907329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.907828 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.908349 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.908847 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.909351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.909850 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.910356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.910865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.911372 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.911879 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.912389 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.912888 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.913400 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.913901 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.914409 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.914909 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.915423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.915921 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.916430 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.916942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.917449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.917963 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.918465 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.918972 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.919473 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.919981 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.920479 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.920989 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.921497 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.921997 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.922498 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.923007 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.923504 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.924004 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.924507 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.925005 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.925506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.926007 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.926510 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.927019 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.927524 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.928030 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.928535 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.929033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.929530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.930032 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.930530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.931033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.931543 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.932044 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.932544 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.933050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.933552 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.934058 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.934553 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.935067 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.935566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.936071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.936573 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.937091 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.937597 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.938102 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.938609 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.939117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.939611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.940119 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.940618 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.941125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.941628 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.942129 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.942628 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.943145 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.943647 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.944147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.944641 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.945149 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.945646 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.946148 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.946648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.947161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.947661 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.948157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.948658 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.949158 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.949655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.950155 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.950657 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.951155 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.951649 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.952157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.952660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.953162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.953667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.954167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.954663 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.955160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.955663 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.956162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.956658 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.957189 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.957684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.958181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.958682 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.959188 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.959690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.960190 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.960690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:33.961183 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
        - 1_0_fwd_module::TEGroupedMLP:
          - param_name: deepseek_v3.decoder.layers.1.mlp.experts
          - inputs: ['<32736x7168xbf16>{7168, 1}', '<4xi64>{1}', '<32736xf32>{1}'], {}
          - 1_0_fwd_module::Fp8Padding:
            - param_name: deepseek_v3.decoder.layers.1.mlp.experts.fp8_padding
            - inputs: ['<32736x7168xbf16>{7168, 1}', 'list{8288:int, 8208:int, 8080:int, 8160:int}'], {}
          2025-10-09 13:08:33.963895 %681140:0:0% aten::unsqueeze(self:<32736xf32>{1}, dim:-1:int) -> <32736x1xf32>{1, 1}
          - 1_0_fwd_module::Fp8Padding:
            - param_name: deepseek_v3.decoder.layers.1.mlp.experts.fp8_padding
            - inputs: ['<32736x1xf32>{1, 1}', 'list{8288:int, 8208:int, 8080:int, 8160:int}'], {}
          - 1_0_fwd_module::TEColumnParallelGroupedLinear:
            - param_name: deepseek_v3.decoder.layers.1.mlp.experts.linear_fc1
            - inputs: ['<32736x7168xbf16>{7168, 1}', 'list{8288:int, 8208:int, 8080:int, 8160:int}'], {}
            - name: api::_GroupedLinear
              inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f53308427a0>, <32736x7168xbf16>{7168, 1}, list{8288:int, 8208:int, 8080:int, 8160:int}, False:bool, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c77e780_:WeightGradStore, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, list{None:, None:, None:, None:}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, True:bool, False:bool, False:bool, torch_bfloat16:dtype, True:bool, TEColumnParallelGroupedLinear():TEColumnParallelGroupedLinear, None:, False:bool, <4096x7168xbf16>{7168, 1}+322961408, <4096x7168xbf16>{7168, 1}+293601280, <4096x7168xbf16>{7168, 1}+264241152, <4096x7168xbf16>{7168, 1}+234881024, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
              2025-10-09 13:08:33.967901 %681140:0:0% te::split_quantize(<32736x7168xbf16>{7168, 1}, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <4096x7168xbf16>{7168, 1}+322961408, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.968564 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+322961408, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <4096x7168xbf16>{7168, 1}+293601280, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.969297 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+293601280, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <4096x7168xbf16>{7168, 1}+264241152, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.970011 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+264241152, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <4096x7168xbf16>{7168, 1}+234881024, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.970713 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+234881024, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:33.971043 %681140:None:0% te::get_num_cublas_streams() -> 4:int
              2025-10-09 13:08:33.971614 %681140:None:0% aten::_to_copy(self:<0xf32>{1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <0xf32>{1}
              2025-10-09 13:08:33.983757 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32736x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
          - name: api::WeightedSwiGLUFunction
            inputs: <torch.autograd.function.WeightedSwiGLUFunctionBackward object at 0x7f53308428b0>, <32736x4096xbf16>{4096, 1}, <32736x1xf32>{1, 1}, False:bool
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
            2025-10-09 13:08:33.985024 %681140:0:0% aten::split(self:<32736x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32736x2048xbf16>{4096, 1}, <32736x2048xbf16>{4096, 1}+2048
            2025-10-09 13:08:33.985611 %681140:0:0% aten::silu(self:<32736x2048xbf16>{4096, 1}) -> <32736x2048xbf16>{2048, 1}
            2025-10-09 13:08:33.986195 %681140:0:0% aten::mul(self:<32736x2048xbf16>{2048, 1}, other:<32736x2048xbf16>{4096, 1}+2048) -> <32736x2048xbf16>{2048, 1}
            2025-10-09 13:08:33.986995 %681140:0:0% aten::mul(self:<32736x2048xbf16>{2048, 1}, other:<32736x1xf32>{1, 1}) -> <32736x2048xf32>{2048, 1}
            2025-10-09 13:08:33.987677 %681140:0:0% aten::_to_copy(self:<32736x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <32736x2048xbf16>{2048, 1}
          - 1_0_fwd_module::TERowParallelGroupedLinear:
            - param_name: deepseek_v3.decoder.layers.1.mlp.experts.linear_fc2
            - inputs: ['<32736x2048xbf16>{2048, 1}', 'list{8288:int, 8208:int, 8080:int, 8160:int}'], {}
            - name: api::_GroupedLinear
              inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f5330843240>, <32736x2048xbf16>{2048, 1}, list{8288:int, 8208:int, 8080:int, 8160:int}, False:bool, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41ab10_:WeightGradStore, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, list{None:, None:, None:, None:}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, True:bool, False:bool, False:bool, torch_bfloat16:dtype, True:bool, TERowParallelGroupedLinear():TERowParallelGroupedLinear, None:, False:bool, <7168x2048xbf16>{2048, 1}+220200960, <7168x2048xbf16>{2048, 1}+205520896, <7168x2048xbf16>{2048, 1}+190840832, <7168x2048xbf16>{2048, 1}+176160768, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
              2025-10-09 13:08:33.990362 %681140:0:0% te::split_quantize(<32736x2048xbf16>{2048, 1}, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<2048x8288xu8>, <65x2048xf32>, <8288x2048xu8>, <16x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, <8208x2048xu8>, <16x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8080xu8>, <64x2048xf32>, <8080x2048xu8>, <16x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, <8160x2048xu8>, <16x8160xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <7168x2048xbf16>{2048, 1}+220200960, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.990953 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+220200960, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <7168x2048xbf16>{2048, 1}+205520896, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.991650 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+205520896, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <7168x2048xbf16>{2048, 1}+190840832, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.992334 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+190840832, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <7168x2048xbf16>{2048, 1}+176160768, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:33.993023 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+176160768, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:33.998874 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<2048x8288xu8>, <65x2048xf32>, <8288x2048xu8>, <16x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, <8208x2048xu8>, <16x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8080xu8>, <64x2048xf32>, <8080x2048xu8>, <16x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, <8160x2048xu8>, <16x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32736x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
          - 1_0_fwd_module::Fp8Unpadding:
            - param_name: deepseek_v3.decoder.layers.1.mlp.experts.fp8_unpadding
            - inputs: ['<32736x7168xbf16>{7168, 1}', 'list{8288:int, 8208:int, 8080:int, 8160:int}'], {}
        - name: api::_moe_unpermute_mask_map
          inputs: <torch.autograd.function._moe_unpermute_mask_mapBackward object at 0x7f5330843ce0>, <32736x7168xbf16>{7168, 1}, <15712x9xi32>{9, 1}, None:, torch_Size([15712,_7168]):Size
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
          2025-10-09 13:08:34.006288 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.008234 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.010151 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.012068 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.013986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.035736 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.037700 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.039630 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.041559 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.043490 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.045416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.047340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.049276 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.051195 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.053121 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.055046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.056977 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.058904 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.060842 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.062767 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.064707 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.066627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.068556 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.070491 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.072419 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.074345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.076262 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.078193 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.080123 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.082048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.083980 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.085901 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.087838 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.089763 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.091686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.093604 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.095521 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.097444 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.099373 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.101295 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.103214 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.105167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.107100 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.109024 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.110962 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.112883 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.114811 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.116730 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.118652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.120575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.122501 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.124418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.126338 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.128266 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.130191 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.132111 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.137924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.139103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.140247 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.141384 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.142518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.162125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.163302 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.164458 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.165611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.166762 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.167914 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.169090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.170241 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.171386 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.172529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.173678 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.174824 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.175986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.177135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.178287 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.179433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.180584 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.181730 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.182877 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.184034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.185182 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.186325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.187469 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.188627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.189783 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.190937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.192091 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.193234 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.194384 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.195529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.196674 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.197817 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.198971 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.200126 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.201267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.202409 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.203550 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.204703 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.205864 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.207025 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.208170 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.209321 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.210472 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.211620 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.212766 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.213919 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.215084 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.216234 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.217387 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.218542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.219691 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.220836 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.221991 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.223140 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.224284 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.225433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.226580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.227730 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.228881 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.230040 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.231196 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.232347 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.233489 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.234639 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.235784 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.236938 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.238096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.239245 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.240397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.241544 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.242689 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.243834 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.244983 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.246135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.247281 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.248423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.249572 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.250716 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.251862 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.253016 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.254161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.255299 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.256440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.257590 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.258746 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.259897 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.264844 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.265630 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.266401 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.267166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.267927 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.284314 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.285124 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.285906 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.286693 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.287473 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.288259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.289057 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.289835 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.290622 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.291399 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.292174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.292952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.293724 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.294504 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.295282 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.296058 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.296833 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.297606 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.298380 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.299167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.299941 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.300715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.301487 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.302259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.303023 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.303788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.304559 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.305339 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.306115 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.306885 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.307665 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.308437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.309212 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.309988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.310771 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.311551 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.312323 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.313102 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.313874 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.314654 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.315435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.316220 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.316992 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.317757 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.318524 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.319299 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.320074 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.320855 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.321638 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.322404 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.323167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.323946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.324715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.325484 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.326253 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.327019 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.327789 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.328561 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.329338 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.330126 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.330895 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.331673 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.332440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.333208 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.333978 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.334746 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.335513 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.336278 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.337046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.337810 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.338575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.339350 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.340124 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.340890 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.341667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.342442 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.343215 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.343985 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.344750 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.345519 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.346287 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.347051 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.347822 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.348591 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.349383 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.350161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.350939 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.351706 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.352473 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.353239 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.354003 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.354769 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.355541 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.356310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.357087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.357859 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.358634 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.359403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.360172 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.360944 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.361712 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.362487 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.363254 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.364023 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.364794 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.365570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.366340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.367115 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.367881 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.368655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.369436 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.370210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.370983 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.371755 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.372525 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.373294 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.374069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.374834 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.375600 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.376367 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.377139 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.377908 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.378678 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.379453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.380222 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.380988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.381751 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.386015 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.386634 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.387239 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.387834 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.388433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.402622 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.403258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.403872 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.404495 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.405129 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.405738 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.406351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.406967 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.407578 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.408192 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.408800 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.409407 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.410022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.410627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.411234 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.411837 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.412445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.413049 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.413656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.414263 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.414866 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.415476 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.416085 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.416687 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.417289 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.417889 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.418501 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.419108 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.419716 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.420323 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.420926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.421539 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.422147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.422745 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.423349 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.423959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.424562 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.425163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.425761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.426369 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.426974 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.427582 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.428186 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.428786 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.429384 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.429996 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.430601 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.431209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.431816 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.432423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.433027 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.433626 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.434232 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.434835 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.435440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.436043 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.436643 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.437246 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.437842 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.438445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.439045 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.439648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.440258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.440865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.441474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.442087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.442688 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.443294 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.443892 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.444496 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.445101 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.445700 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.446309 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.446908 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.447515 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.448119 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.448718 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.449320 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.449922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.450532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.451139 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.451734 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.452336 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.452943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.453542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.454145 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.454738 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.455334 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.455942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.456544 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.457150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.457769 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.458376 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.458984 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.459586 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.460196 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.460796 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.461396 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.462003 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.462605 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.463209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.463804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.464408 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.465013 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.465613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.466218 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.466817 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.467416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.468019 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.468623 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.469227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.469827 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.470435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.471038 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.471635 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.472238 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.472843 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.473450 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.474049 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.474648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.475250 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.475848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.476447 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.477047 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.477647 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.478248 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.478846 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.479447 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.480057 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.480661 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.481265 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.481860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.482458 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.483073 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.483670 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.484276 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.484874 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.485481 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.486087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.486686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.487284 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.487887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.488496 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.489114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.489715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.490317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.490917 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.491529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.492145 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.492745 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.493352 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.493956 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.494557 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.495159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.495756 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.496358 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.496962 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.497569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.498170 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.498771 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.499374 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.499981 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.504178 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.504720 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.505243 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.505757 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.506270 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.518608 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.519167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.519698 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.520229 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.520763 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.521292 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.521812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.522340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.522862 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.523396 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.523916 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.524459 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.524989 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.525512 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.526041 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.526565 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.527089 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.527607 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.528132 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.528654 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.529176 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.529697 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.530228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.530758 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.531281 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.531804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.532329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.532848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.533370 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.533888 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.534417 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.534942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.535466 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.535990 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.536515 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.537035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.537566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.538095 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.538617 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.539143 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.539669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.540199 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.540729 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.541255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.541778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.542303 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.542822 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.543345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.543864 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.544391 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.544911 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.545443 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.545966 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.546484 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.547007 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.547526 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.548046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.548566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.549094 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.549615 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.550141 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.550674 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.551202 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.551724 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.552250 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.552773 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.553296 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.553814 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.554336 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.554858 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.555377 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.555896 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.556429 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.556960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.557481 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.557999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.558520 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.559041 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.559558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.560081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.560603 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.561133 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.561655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.562176 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.562692 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.563215 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.563732 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.564260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.564778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.565298 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.565814 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.566333 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.566862 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.567394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.567926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.568451 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.568969 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.569481 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.569999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.570514 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.571045 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.571563 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.572089 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.572618 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.573150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.573667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.574186 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.574699 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.575223 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.575748 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.576272 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.576793 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.577318 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.577849 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.578374 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.578895 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.579422 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.579947 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.580474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.581009 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.581530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.582050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.582570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.583094 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.583611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.584135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.584653 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.585169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.585686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.586209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.586726 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.587243 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.587758 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.588278 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.588794 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.589312 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.589825 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.590340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.590871 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.591396 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.591909 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.592435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.592956 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.593474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.593999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.594518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.595043 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.595557 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.596082 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.596598 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.597120 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.597633 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.598153 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.598668 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.599188 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.599711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.600229 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.600746 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.601270 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.601787 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.602310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.602828 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.603345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.603868 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.604391 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.604906 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.605428 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.605947 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.606465 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.606988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.607511 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.608035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.608552 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.609081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.609605 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.610128 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.610639 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.611161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.611675 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.612192 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.612707 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.613228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.613752 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.614274 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.614787 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.615304 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.615821 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.619973 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.620520 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.621057 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.621578 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.622103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.634515 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.635078 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.635613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.636163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.636698 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.637233 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.637763 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.638294 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.638833 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.639363 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.639888 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.640425 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.640961 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.641497 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.642021 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.642545 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.643075 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.643601 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.644134 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.644656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.645183 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.645706 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.646235 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.646758 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.647285 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.647809 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.648337 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.648860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.649392 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.649916 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.650461 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.650991 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.651520 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.652072 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.652601 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.653141 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.653664 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.654190 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.654714 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.655241 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.655761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.656291 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.656823 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.657347 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.657868 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.658397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.658920 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.659457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.659987 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.660512 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.661041 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.661576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.662111 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.662631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.663161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.663680 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.664207 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.664729 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.665267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.665793 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.666319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.666841 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.667366 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.667885 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.668416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.668942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.669464 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.669987 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.670510 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.671033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.671565 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.672096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.672617 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.673148 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.673672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.674195 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.674711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.675232 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.675753 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.676279 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.676803 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.677329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.677854 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.678394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.678933 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.679456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.679985 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.680504 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.681027 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.681558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.682088 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.682610 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.683145 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.683672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.684197 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.684715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.685236 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.685754 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.686281 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.686801 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.687324 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.687843 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.688365 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.688887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.689413 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.689937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.690457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.690987 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.691519 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.692047 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.692581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.693114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.693646 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.694169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.694688 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.695210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.695732 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.696256 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.696774 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.697300 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.697830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.698356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.698877 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.699404 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.699926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.700456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.700986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.701516 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.702043 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.702567 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.703095 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.703614 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.704141 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.704661 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.705183 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.705708 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.706231 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.706748 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.707274 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.707796 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.708321 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.708845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.709367 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.709887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.710416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.710945 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.711481 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.712010 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.712530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.713055 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.713582 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.714111 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.714629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.715157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.715672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.716192 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.716715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.717238 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.717788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.718311 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.718840 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.719361 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.719885 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.720415 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.720938 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.721457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.721988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.722512 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.723036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.723556 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.724084 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.724608 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.725144 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.725662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.726183 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.726701 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.727226 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.727745 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.728273 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.728791 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.729310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.729827 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.730356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.730875 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.740225 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.740820 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.741390 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.741966 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.742528 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.755792 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.756389 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.756973 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.757554 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.758134 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.758705 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.759277 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.759845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.760420 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.761007 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.761577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.762160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.762734 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.763306 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.763870 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.764442 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.765005 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.765570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.766138 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.766701 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.767274 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.767838 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.768402 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.768972 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.769540 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.770114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.770679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.771246 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.771815 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.772384 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.772963 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.773531 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.774103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.774670 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.775249 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.775818 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.776385 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.776956 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.777529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.778098 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.778660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.779225 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.779786 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.780357 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.780920 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.781499 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.782085 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.782655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.783224 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.783788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.784354 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.784918 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.785489 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.786061 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.786624 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.787189 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.787755 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.788322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.788886 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.789466 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.790028 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.790592 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.791163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.791734 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.792307 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.792885 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.793455 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.794019 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.794587 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.795161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.795721 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.796293 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.796853 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.797421 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.797988 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.798554 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.799123 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.799686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.800260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.800824 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.801390 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.801959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.802535 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.803103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.803671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.804254 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.804816 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.805382 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.805949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.806518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.807089 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.807652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.808225 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.808795 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.809359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.809921 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.810495 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.811066 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.811629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.812208 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.812773 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.813338 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.813903 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.814472 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.815034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.815597 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.816164 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.816725 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.817287 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.817860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.818435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.819007 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.819581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.820147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.820708 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.821269 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.821833 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.822408 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.822978 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.823549 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.824137 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.824706 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.825267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.825832 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.826397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.826975 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.827552 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.828128 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.828690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.829265 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.829825 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.830387 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.830955 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.831518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.832090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.832663 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.833240 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.833802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.834365 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.834925 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.835499 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.836070 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.836631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.837204 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.837768 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.838333 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.838891 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.839468 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.840035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.840594 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.841163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.841724 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.842292 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.842864 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.843436 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.843999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.844562 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.845139 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.845704 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.846268 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.846836 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.847404 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.847971 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.848529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.849101 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.849666 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.850226 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.850790 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.851359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
          2025-10-09 13:08:34.851921 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
        - name: api::FusedCombine
          inputs: <torch.autograd.function.FusedCombineBackward object at 0x7f5331eb4e20>, <15712x7168xbf16>{7168, 1}, <rank:0, size:8, group_name:72, backend:nccl>:ProcessGroup, tuple{<8x8xi32>{8, 1}, <8x10xi32>{10, 1}, <8x10xi32>{10, 1}, <15712xi32>{1}, <4096x8xbool>{8, 1}, <4096x8xi32>{8, 1}}, True:bool, True:bool
          file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
          2025-10-09 13:08:34.857697 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:34.857900 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:34.858094 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:34.858271 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:34.858449 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:34.858625 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:34.858807 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:34.858988 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:34.859161 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:34.859329 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:34.859499 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
          2025-10-09 13:08:34.859671 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
        2025-10-09 13:08:34.860206 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
      2025-10-09 13:08:34.860985 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
      2025-10-09 13:08:34.861072 %681140:0:0% triton::bias_dropout_add_fused_train(tuple{<4096x1x7168xbf16>{7168, 7168, 1}, None:}, <4096x1x7168xbf16>{7168, 7168, 1}, 0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}
    - 1_0_fwd_module::RMSNorm:
      - param_name: deepseek_v3.decoder.final_layernorm
      - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  - 1_0_fwd_module::MultiTokenPredictionBlock:
    - param_name: deepseek_v3.mtp
    - inputs: [], {'input_ids': '<1x4096xi64>{4096, 1}', 'position_ids': '<1x4096xi64>{4096, 1}', 'hidden_states': '<4096x1x7168xbf16>{7168, 7168, 1}', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'inference_params': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:', 'embedding': 'NotSurpot:LanguageModelEmbedding'}
    2025-10-09 13:08:34.863124 %681140:0:0% aten::split(self:<4096x1x7168xbf16>{7168, 7168, 1}, split_size:4096:int) -> <4096x1x7168xbf16>{7168, 7168, 1}
    - 1_0_fwd_module::MultiTokenPredictionLayer:
      - param_name: deepseek_v3.mtp.layers.0
      - inputs: [], {'input_ids': '<1x4096xi64>{4096, 1}', 'position_ids': '<1x4096xi64>{4096, 1}', 'hidden_states': '<4096x1x7168xbf16>{7168, 7168, 1}', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'inference_params': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:', 'embedding': 'NotSurpot:LanguageModelEmbedding'}
      2025-10-09 13:08:34.864287 %681140:0:0% aten::roll(self:<1x4096xi64>{4096, 1}, shifts:list{-1:int}, dims:list{-1:int}) -> <1x4096xi64>{4096, 1}
      2025-10-09 13:08:34.864661 %681140:0:0% aten::fill_(self:<1xi64>{4096}+4095, value:0:int) -> <1xi64>{4096}+4095
      2025-10-09 13:08:34.865034 %681140:0:0% aten::sum(self:<1x4096xi64>{4096, 1}) -> <1xi64>{1}
      2025-10-09 13:08:34.865333 %681140:0:0% aten::roll(self:<1x4096xi64>{4096, 1}, shifts:list{-1:int}, dims:list{-1:int}) -> <1x4096xi64>{4096, 1}
      2025-10-09 13:08:34.865579 %681140:0:0% aten::fill_(self:<1xi64>{4096}+4095, value:0:int) -> <1xi64>{4096}+4095
      2025-10-09 13:08:34.865805 %681140:0:0% aten::sum(self:<1x4096xi64>{4096, 1}) -> <1xi64>{1}
      - 1_0_fwd_module::LanguageModelEmbedding:
        - param_name: deepseek_v3.embedding
        - inputs: [], {'input_ids': '<1x4096xi64>{4096, 1}', 'position_ids': '<1x4096xi64>{4096, 1}'}
        - 1_0_fwd_module::VocabParallelEmbedding:
          - param_name: deepseek_v3.embedding.word_embeddings
          - inputs: ['<1x4096xi64>{4096, 1}'], {}
          2025-10-09 13:08:34.866593 %681140:0:0% aten::embedding(weight:<129280x7168xbf16>{7168, 1}+2075734016, indices:<1x4096xi64>{4096, 1}) -> <1x4096x7168xbf16>{29360128, 7168, 1}
          - name: api::_ReduceFromModelParallelRegion
            inputs: <torch.autograd.function._ReduceFromModelParallelRegionBackward object at 0x7f5331eb56a0>, <1x4096x7168xbf16>{29360128, 7168, 1}, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
        2025-10-09 13:08:34.867182 %681140:0:0% aten::transpose(self:<1x4096x7168xbf16>{29360128, 7168, 1}, dim0:0:int, dim1:1:int) -> <4096x1x7168xbf16>{7168, 29360128, 1}
        - 1_0_fwd_module::Dropout:
          - param_name: deepseek_v3.embedding.embedding_dropout
          - inputs: ['<4096x1x7168xbf16>{7168, 29360128, 1}'], {}
      - 1_0_fwd_module::RMSNorm:
        - param_name: deepseek_v3.mtp.layers.0.enorm
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
      - 1_0_fwd_module::RMSNorm:
        - param_name: deepseek_v3.mtp.layers.0.hnorm
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
      2025-10-09 13:08:34.869619 %681140:None:0% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}}, dim:-1:int) -> <4096x1x14336xbf16>{14336, 14336, 1}
      - 1_0_fwd_module::TEColumnParallelLinear:
        - param_name: deepseek_v3.mtp.layers.0.eh_proj
        - inputs: ['<4096x1x14336xbf16>{14336, 14336, 1}'], {}
        - name: api::_Linear
          inputs: <torch.autograd.function._LinearBackward object at 0x7f5331eb6cf0>, <7168x14336xbf16>{14336, 1}+1158077440, <4096x1x14336xbf16>{14336, 14336, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41b800_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TEColumnParallelLinear(in_features=14336,_out_features=7168,_bias=False,_TP=1):TEColumnParallelLinear, None:, None:, False:bool, False:bool
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
          - name: api::_QuantizeFunc
            inputs: None:, <4096x1x14336xbf16>{14336, 14336, 1}, <True, True, True>:Float8BlockQuantizer
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
            2025-10-09 13:08:34.871364 %681140:0:0% te::quantize(<4096x1x14336xbf16>{14336, 14336, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<14336x4096x1xu8>, <32x14336xf32>, <4096x1x14336xu8>, <112x4096xf32>, 1D:Mode, GEMM_READY:Format}
          - name: api::_QuantizeFunc
            inputs: None:, <7168x14336xbf16>{14336, 1}+1158077440, <True, True, False>:Float8BlockQuantizer
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
            2025-10-09 13:08:34.872232 %681140:0:0% te::quantize(<7168x14336xbf16>{14336, 1}+1158077440, <True, True, False>:Float8BlockQuantizer) -> tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}
          2025-10-09 13:08:34.876594 %681140:0:0% te::generic_gemm(tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<14336x4096x1xu8>, <32x14336xf32>, <4096x1x14336xu8>, <112x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
      - name: api::_GatherFromModelParallelRegion
        inputs: <torch.autograd.function._GatherFromModelParallelRegionBackward object at 0x7f5331eb7020>, <4096x1x7168xbf16>{7168, 7168, 1}, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
        file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
      - 1_0_fwd_module::TransformerLayer:
        - param_name: deepseek_v3.mtp.layers.0.transformer_layer
        - inputs: [], {'hidden_states': '<4096x1x7168xbf16>{7168, 7168, 1}', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'context': 'None:', 'context_mask': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'attention_bias': 'None:', 'inference_params': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
        - 1_0_fwd_module::RMSNorm:
          - param_name: deepseek_v3.mtp.layers.0.transformer_layer.input_layernorm
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
        - 1_0_fwd_module::MLASelfAttention:
          - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'inference_context': 'None:', 'rotary_pos_emb': 'None:', 'rotary_pos_cos': 'None:', 'rotary_pos_sin': 'None:', 'attention_bias': 'None:', 'packed_seq_params': 'None:', 'sequence_len_offset': 'None:'}
          - 1_0_fwd_module::YarnRotaryEmbedding:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.rotary_pos_emb
            - inputs: ['4096:int'], {'packed_seq': 'False:bool'}
            2025-10-09 13:08:34.878769 %681140:None:0% aten::arange(end:32:int, dtype:torch_float32:dtype, device:cpu:device, pin_memory:False:bool) -> <32xf32>{1}
            2025-10-09 13:08:34.879059 %681140:None:0% aten::sub(self:<32xf32>{1}, other:10:int) -> <32xf32>{1}
            2025-10-09 13:08:34.879285 %681140:None:0% aten::div(self:<32xf32>{1}, other:13:int) -> <32xf32>{1}
            2025-10-09 13:08:34.879520 %681140:None:0% aten::clamp(self:<32xf32>{1}, min:0:int, max:1:int) -> <32xf32>{1}
            2025-10-09 13:08:34.879910 %681140:None:0% aten::_to_copy(self:<32xf32>{1}, dtype:torch_float32:dtype, device:cuda_0:device) -> <32xf32>{1}
            2025-10-09 13:08:34.880193 %681140:0:0% aten::rsub(self:<32xf32>{1}, other:1_0:float) -> <32xf32>{1}
            2025-10-09 13:08:34.880438 %681140:0:0% aten::rsub(self:<32xf32>{1}, other:1:int) -> <32xf32>{1}
            2025-10-09 13:08:34.880641 %681140:0:0% aten::mul(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
            2025-10-09 13:08:34.880826 %681140:0:0% aten::mul(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
            2025-10-09 13:08:34.881068 %681140:0:0% aten::add(self:<32xf32>{1}, other:<32xf32>{1}) -> <32xf32>{1}
            2025-10-09 13:08:34.881375 %681140:None:0% aten::arange(end:4096:int, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <4096xf32>{1}
            2025-10-09 13:08:34.881633 %681140:0:0% aten::add(self:<4096xf32>{1}, other:0:int) -> <4096xf32>{1}
            2025-10-09 13:08:34.881874 %681140:0:0% aten::mul(self:<4096x1xf32>{1, 1}, other:<32xf32>{1}) -> <4096x32xf32>{32, 1}
            2025-10-09 13:08:34.882785 %681140:None:0% aten::cat(tensors:list{<4096x32xf32>{32, 1}, <4096x32xf32>{32, 1}}, dim:-1:int) -> <4096x64xf32>{64, 1}
            2025-10-09 13:08:34.883102 %681140:0:0% aten::slice(self:<4096x64xf32>{64, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <4096x64xf32>{64, 1}
            2025-10-09 13:08:34.883304 %681140:0:0% aten::unsqueeze(self:<4096x64xf32>{64, 1}, dim:1:int) -> <4096x1x64xf32>{64, 64, 1}
            2025-10-09 13:08:34.883490 %681140:0:0% aten::unsqueeze(self:<4096x1x64xf32>{64, 64, 1}, dim:2:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
            2025-10-09 13:08:34.883754 %681140:0:0% aten::slice(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dim:3:int, start:0:int, end:9223372036854775807:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          - 1_0_fwd_module::TELinear:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_q_down_proj
            - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
            - name: api::_Linear
              inputs: <torch.autograd.function._LinearBackward object at 0x7f5331eb78a0>, <1536x7168xbf16>{7168, 1}+1029619712, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41bd40_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, None:, 1:int, False:bool, False:bool, torch_bfloat16:dtype, None:, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TELinear():TELinear, None:, None:, False:bool, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.885441 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <1536x7168xbf16>{7168, 1}+1029619712, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.886160 %681140:0:0% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:34.887553 %681140:0:0% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x1536xbf16>{1536, 1536, 1}, None:, None:, None:
          - 1_0_fwd_module::TELinear:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_kv_down_proj
            - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
            - name: api::_Linear
              inputs: <torch.autograd.function._LinearBackward object at 0x7f5331eb7ce0>, <576x7168xbf16>{7168, 1}+987740672, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41b3b0_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, None:, 1:int, False:bool, False:bool, torch_bfloat16:dtype, None:, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TELinear():TELinear, None:, None:, False:bool, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.892472 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <576x7168xbf16>{7168, 1}+987740672, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.893294 %681140:0:0% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:34.894546 %681140:0:0% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x576xbf16>{576, 576, 1}, None:, None:, None:
          2025-10-09 13:08:34.895163 %681140:0:0% aten::split_with_sizes(self:<4096x1x576xbf16>{576, 576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> <4096x1x512xbf16>{576, 576, 1}, <4096x1x64xbf16>{576, 576, 1}+512
          - 1_0_fwd_module::TELayerNormColumnParallelLinear:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_q_up_proj
            - inputs: ['<4096x1x1536xbf16>{1536, 1536, 1}'], {}
            - name: api::_LayerNormLinear
              inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5331f18270>, <4096x1x1536xbf16>{1536, 1536, 1}, <1536xbf16>{1}+1029618176, None:, <24576x1536xbf16>{1536, 1}+991869440, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41be60_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=1536,_out_features=24576,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <24576x1536xbf16>{1536, 1}+991869440, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.897193 %681140:0:0% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, False>:Float8BlockQuantizer) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:34.899380 %681140:0:0% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x24576xbf16>{24576, 1}, None:, None:, None:
          - 1_0_fwd_module::TELayerNormColumnParallelLinear:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_kv_up_proj
            - inputs: ['<4096x1x512xbf16>{576, 576, 1}'], {}
            2025-10-09 13:08:34.900232 %681140:0:0% aten::clone(self:<4096x1x512xbf16>{576, 512, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x512xbf16>{512, 512, 1}
            - name: api::_LayerNormLinear
              inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5331f186b0>, <4096x1x512xbf16>{512, 512, 1}, <512xbf16>{1}+987740160, None:, <32768x512xbf16>{512, 1}+970962944, None:, 1e-06:float, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c4300e0_:WeightGradStore, True:bool, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, False:bool, False:bool, True:bool, 0:int, 0:int, False:bool, RMSNorm:str, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, None:, TELayerNormColumnParallelLinear(in_features=512,_out_features=32768,_bias=False,_TP=1):TELayerNormColumnParallelLinear, None:, None:, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <32768x512xbf16>{512, 1}+970962944, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.901950 %681140:0:0% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, False>:Float8BlockQuantizer) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:34.903558 %681140:0:0% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32768xbf16>{32768, 1}, None:, None:, None:
          2025-10-09 13:08:34.904104 %681140:0:0% aten::unsqueeze(self:<4096x1x64xbf16>{576, 576, 1}+512, dim:-2:int) -> <4096x1x1x64xbf16>{576, 576, 64, 1}+512
          2025-10-09 13:08:34.904401 %681140:0:0% aten::slice(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dim:0:int, start:0:int, end:4096:int) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.904711 %681140:0:0% aten::split_with_sizes(self:<4096x1x128x192xbf16>{24576, 24576, 192, 1}, split_sizes:list{128:int, 64:int}, dim:-1:int) -> <4096x1x128x128xbf16>{24576, 24576, 192, 1}, <4096x1x128x64xbf16>{24576, 24576, 192, 1}+128
          2025-10-09 13:08:34.905018 %681140:0:0% aten::split_with_sizes(self:<4096x1x128x256xbf16>{32768, 32768, 256, 1}, split_sizes:list{128:int, 128:int}, dim:-1:int) -> <4096x1x128x128xbf16>{32768, 32768, 256, 1}, <4096x1x128x128xbf16>{32768, 32768, 256, 1}+128
          2025-10-09 13:08:34.905331 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:64:int, end:9223372036854775807:int) -> <4096x1x128x0xbf16>{24576, 24576, 192, 1}+192
          2025-10-09 13:08:34.905622 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x32xbf16>{24576, 24576, 192, 2}+128
          2025-10-09 13:08:34.905894 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{24576, 24576, 192, 1}+128, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x32xbf16>{24576, 24576, 192, 2}+129
          2025-10-09 13:08:34.906419 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{24576, 24576, 192, 2}+128, <4096x1x128x32xbf16>{24576, 24576, 192, 2}+129}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
          2025-10-09 13:08:34.906625 %681140:0:0% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.909361 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.909772 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.909981 %681140:0:0% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.910206 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.910567 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.910939 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
          2025-10-09 13:08:34.911211 %681140:0:0% aten::split(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, split_size:32:int, dim:-1:int) -> <4096x1x128x32xbf16>{8192, 8192, 64, 1}, <4096x1x128x32xbf16>{8192, 8192, 64, 1}+32
          2025-10-09 13:08:34.911483 %681140:0:0% aten::neg(self:<4096x1x128x32xbf16>{8192, 8192, 64, 1}+32) -> <4096x1x128x32xbf16>{4096, 4096, 32, 1}
          2025-10-09 13:08:34.911984 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{4096, 4096, 32, 1}, <4096x1x128x32xbf16>{8192, 8192, 64, 1}}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
          2025-10-09 13:08:34.912356 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
          2025-10-09 13:08:34.912664 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
          2025-10-09 13:08:34.913076 %681140:None:0% aten::cat(tensors:list{<4096x1x128x64xbf16>{8192, 8192, 64, 1}, <4096x1x128x0xbf16>{24576, 24576, 192, 1}+192}, dim:-1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
          2025-10-09 13:08:34.913395 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:64:int, end:9223372036854775807:int) -> <4096x1x1x0xbf16>{576, 576, 64, 1}+576
          2025-10-09 13:08:34.913668 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x32xbf16>{576, 576, 64, 2}+512
          2025-10-09 13:08:34.913944 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{576, 576, 64, 1}+512, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x32xbf16>{576, 576, 64, 2}+513
          2025-10-09 13:08:34.914218 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{576, 576, 64, 2}+512, <4096x1x1x32xbf16>{576, 576, 64, 2}+513}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.914403 %681140:0:0% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.914625 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.915002 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.915184 %681140:0:0% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.916110 %681140:0:0% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
          2025-10-09 13:08:34.916492 %681140:0:0% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.916703 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.916958 %681140:0:0% aten::split(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, split_size:32:int, dim:-1:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}, <4096x1x1x32xbf16>{64, 64, 64, 1}+32
          2025-10-09 13:08:34.917149 %681140:0:0% aten::neg(self:<4096x1x1x32xbf16>{64, 64, 64, 1}+32) -> <4096x1x1x32xbf16>{32, 32, 32, 1}
          2025-10-09 13:08:34.917416 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{32, 32, 32, 1}, <4096x1x1x32xbf16>{64, 64, 64, 1}}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.917629 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.917868 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.918145 %681140:None:0% aten::cat(tensors:list{<4096x1x1x64xbf16>{64, 64, 64, 1}, <4096x1x1x0xbf16>{576, 576, 64, 1}+576}, dim:-1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
          2025-10-09 13:08:34.919131 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{24576, 24576, 192, 1}, <4096x1x128x64xbf16>{8192, 8192, 64, 1}}, dim:-1:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
          2025-10-09 13:08:34.920153 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{32768, 32768, 256, 1}, <4096x1x128x64xbf16>{64, 64, 0, 1}}, dim:-1:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
          2025-10-09 13:08:34.920668 %681140:0:0% aten::clone(self:<4096x1x128x128xbf16>{32768, 32768, 256, 1}+128, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
          - 1_0_fwd_module::TEDotProductAttention:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.core_attention
            - inputs: ['<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x128xbf16>{16384, 16384, 128, 1}', '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}'], {'packed_seq_params': 'None:', 'attn_mask_type': 'AttnMaskType_causal:AttnMaskType'}
            - 1_0_fwd_module::UnfusedDotProductAttention:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.core_attention.unfused_attention
              - inputs: ["_'_num_heads'__None,_'_alibi_slopes'__None,_'_ma__seqlen_q'__None,_'_ma__seqlen_kv'__None,_'_bottom_right_alignment'__True,_'_alibi_bias'__None,_'_alibi_slopes_require_update'__False,_'_alibi_bias_require_update'__False_:dict", '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x192xbf16>{24576, 24576, 192, 1}', '<4096x1x128x128xbf16>{16384, 16384, 128, 1}'], {'qkv_layout': 'sbhd_sbhd_sbhd:str', 'cu_seqlens_q': '<2xi32>{1}', 'cu_seqlens_kv': '<2xi32>{1}', 'attn_mask_type': 'causal:str', 'attention_mask': '<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}', 'window_size': 'tuple{-1:int, 0:int}', 'core_attention_bias_type': 'no_bias:str', 'core_attention_bias': 'None:', 'alibi_slopes': 'None:', 'inference_params': 'None:'}
              2025-10-09 13:08:34.922010 %681140:None:0% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
              2025-10-09 13:08:34.922339 %681140:None:0% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
              2025-10-09 13:08:34.922677 %681140:0:0% aten::sub(self:<1x1x4096x1xi32>{4096, 4096, 1, 1}, other:<1x1x1x4096xi32>{4096, 4096, 4096, 1}) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.922999 %681140:0:0% aten::sub(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:4096:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.923292 %681140:0:0% aten::add(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.923560 %681140:0:0% aten::le(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.923816 %681140:0:0% aten::lt(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.924023 %681140:0:0% aten::bitwise_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.924247 %681140:0:0% aten::bitwise_and(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.924448 %681140:0:0% aten::logical_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.924670 %681140:0:0% aten::logical_or(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
              2025-10-09 13:08:34.924988 %681140:0:0% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
              2025-10-09 13:08:34.925197 %681140:0:0% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
              2025-10-09 13:08:34.925398 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
              2025-10-09 13:08:34.933049 %681140:0:0% aten::baddbmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, batch1:<128x4096x192xbf16>{192, 24576, 1}, batch2:<128x192x4096xbf16>{192, 1, 24576}, beta:0_0:float, alpha:0_1352337788608801:float) -> <128x4096x4096xbf16>{16777216, 4096, 1}
              - name: api::ScaledMaskedSoftmax
                inputs: <torch.autograd.function.ScaledMaskedSoftmaxBackward object at 0x7f5331f19370>, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, 1_0:float
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/attention/dot_product_attention/softmax.py
                2025-10-09 13:08:34.944625 %681140:0:0% te::scaled_masked_softmax_forward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
              - 1_0_fwd_module::Dropout:
                - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.core_attention.unfused_attention.attention_dropout
                - inputs: ['<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}'], {}
              2025-10-09 13:08:34.945309 %681140:None:0% aten::eq(self:<5056xu8>{1}, other:<5056xu8>{1}) -> <5056xbool>{1}
              2025-10-09 13:08:34.945487 %681140:None:0% aten::all(self:<5056xbool>{1}) -> <1xbool>{1}
              2025-10-09 13:08:34.945788 %681140:0:0% aten::transpose(self:<4096x128x128xbf16>{16384, 128, 1}, dim0:0:int, dim1:1:int) -> <128x4096x128xbf16>{128, 16384, 1}
              2025-10-09 13:08:34.950067 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
              2025-10-09 13:08:34.950394 %681140:0:0% aten::permute(self:<1x128x4096x128xbf16>{67108864, 524288, 128, 1}, dims:list{2:int, 0:int, 1:int, 3:int}) -> <4096x1x128x128xbf16>{128, 67108864, 524288, 1}
              2025-10-09 13:08:34.950959 %681140:0:0% aten::clone(self:<4096x1x128x128xbf16>{128, 67108864, 524288, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
          - 1_0_fwd_module::TERowParallelLinear:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_proj
            - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
            - name: api::_Linear
              inputs: <torch.autograd.function._LinearBackward object at 0x7f5331f19d00>, <7168x16384xbf16>{16384, 1}+1040629760, <4096x1x16384xbf16>{16384, 16384, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c41bb30_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, row:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TERowParallelLinear(in_features=16384,_out_features=7168,_bias=False,_TP=1):TERowParallelLinear, None:, None:, True:bool, False:bool
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
              - name: api::_QuantizeFunc
                inputs: None:, <4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.953194 %681140:0:0% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}
              - name: api::_QuantizeFunc
                inputs: None:, <7168x16384xbf16>{16384, 1}+1040629760, <True, True, False>:Float8BlockQuantizer
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                2025-10-09 13:08:34.954050 %681140:0:0% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, False>:Float8BlockQuantizer) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
              2025-10-09 13:08:34.958525 %681140:0:0% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
        2025-10-09 13:08:34.959636 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        2025-10-09 13:08:34.959724 %681140:0:0% triton::bias_dropout_add_fused_train(tuple{<4096x1x7168xbf16>{7168, 7168, 1}, None:}, <4096x1x7168xbf16>{7168, 7168, 1}, 0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}
        - 1_0_fwd_module::RMSNorm:
          - param_name: deepseek_v3.mtp.layers.0.transformer_layer.pre_mlp_layernorm
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
        - 1_0_fwd_module::MoELayer:
          - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp
          - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
          - 1_0_fwd_module::TopKRouter:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.router
            - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
            2025-10-09 13:08:34.961070 %681140:0:0% aten::_to_copy(self:<32xbf16>{1}, dtype:torch_float32:dtype) -> <32xf32>{1}
            - name: api::RouterGatingLinearFunction
              inputs: <torch.autograd.function.RouterGatingLinearFunctionBackward object at 0x7f5331f1a9c0>, <4096x1x7168xbf16>{7168, 7168, 1}, <32x7168xbf16>{7168, 1}+970726400, torch_float32:dtype
              file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
              2025-10-09 13:08:34.962181 %681140:0:0% te::generic_gemm(<32x7168xbf16>{7168, 1}+970726400, True:bool, <4096x7168xbf16>{7168, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32xf32>{32, 1}, None:, None:, None:
            - name: api::RandomSTE
              inputs: <torch.autograd.function.RandomSTEBackward object at 0x7f5331f1aad0>, <4096x1x32xf32>{32, 32, 1}
              file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
              2025-10-09 13:08:34.962627 %681140:0:0% aten::clone(self:<4096x1x32xf32>{32, 32, 1}) -> <4096x1x32xf32>{32, 32, 1}
              2025-10-09 13:08:34.963042 %681140:0:0% aten::normal_(self:<4096x1x32xf32>{32, 32, 1}, generator:%672:tuple{seed=42:i32, offset=None:NoneType}) -> <4096x1x32xf32>{32, 32, 1}
            - name: api::FusedTopkScoreFunction
              inputs: <torch.autograd.function.FusedTopkScoreFunctionBackward object at 0x7f5331f1acf0>, <4096x32xf32>{32, 1}, 8:int, True:bool, 2:int, 1:int, 2_5:float, sigmoid:str, <32xf32>{1}
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
              2025-10-09 13:08:34.963864 %681140:0:0% te::fused_topk_with_score_function_fwd(<4096x32xf32>{32, 1}, 8:int, True:bool, 2:int, 1:int, 2_5:float, sigmoid:str, <32xf32>{1}) -> <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
            - name: api::FusedComputeScoresForMoEAuxLoss
              inputs: <torch.autograd.function.FusedComputeScoresForMoEAuxLossBackward object at 0x7f5331f1af10>, <4096x32xf32>{32, 1}, 8:int, sigmoid:str
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
              2025-10-09 13:08:34.964502 %681140:None:0% te::fused_score_for_moe_aux_loss_fwd(logits:<4096x32xf32>{32, 1}, topk:8:int, score_function:sigmoid:str) -> <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
            2025-10-09 13:08:34.965035 %681140:0:0% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
            - name: api::_ReduceFromModelParallelRegion
              inputs: <torch.autograd.function._ReduceFromModelParallelRegionBackward object at 0x7f5331f1b240>, <32xi64>{1}, <rank:0, size:1, group_name:55, backend:nccl>:ProcessGroup
              file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
            - name: api::FusedAuxLoss
              inputs: <torch.autograd.function.FusedAuxLossBackward object at 0x7f5331f1b240>, <4096x32xf32>{32, 1}, <32xi64>{1}, 4096:int, 32:int, 8:int, 0_0001:float
              file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
              2025-10-09 13:08:34.965782 %681140:None:0% te::fused_moe_aux_loss_fwd(probs:<4096x32xf32>{32, 1}, tokens_per_expert:<32xi64>{1}, total_num_tokens:4096:int, num_experts:32:int, num_rows:4096:int, num_cols:32:int, topk:8:int, coeff:0_0001:float) -> <1xf32>{1}, <1xf32>{1}
            2025-10-09 13:08:34.966086 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
            2025-10-09 13:08:34.966315 %681140:0:0% aten::div(self:<1xf32>{1}, other:0_0001:float) -> <1xf32>{1}
            2025-10-09 13:08:34.966618 %681140:0:0% aten::add_(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
            2025-10-09 13:08:34.966848 %681140:0:0% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
            - name: api::MoEAuxLossAutoScaler
              inputs: <torch.autograd.function.MoEAuxLossAutoScalerBackward object at 0x7f5331f1b460>, <4096x32xf32>{32, 1}, <1xf32>{1}
              file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
            2025-10-09 13:08:34.967402 %681140:0:0% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
            2025-10-09 13:08:34.967647 %681140:0:0% aten::add_(self:<32xbf16>{1}, other:<32xi64>{1}) -> <32xbf16>{1}
          2025-10-09 13:08:34.968401 %681140:0:0% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int) -> <4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}
          - name: api::FusedDispatch
            inputs: <torch.autograd.function.FusedDispatchBackward object at 0x7f5331f1b680>, <4096x7168xbf16>{7168, 1}, <4096x8xi64>{8, 1}, <4096x8xf32>{8, 1}, 32:int, <rank:0, size:8, group_name:72, backend:nccl>:ProcessGroup, True:bool, True:bool
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
            2025-10-09 13:08:34.969088 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.969282 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.969461 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.969637 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.969809 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.969988 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.970160 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.970330 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.978430 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.978626 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.978829 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.979012 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.979185 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.979361 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.979531 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.979700 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.979920 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.980154 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.980381 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.980602 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.980780 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.980958 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.981130 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.981300 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.981469 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.981633 %681140:0:270142432% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.981798 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.981969 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.982138 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.982300 %681140:0:270142432% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.982464 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.982626 %681140:0:270142432% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.982838 %681140:0:270142432% aten::record_stream(self:<15713x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.983065 %681140:0:270142432% aten::record_stream(self:<15713x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:34.983297 %681140:0:270142432% aten::record_stream(self:<15713x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:34.983513 %681140:0:270142432% aten::record_stream(self:<15713x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          - 1_0_fwd_module::SharedExpertMLP:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.shared_experts
            - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
            - 1_0_fwd_module::TEColumnParallelLinear:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.shared_experts.linear_fc1
              - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
              - name: api::_Linear
                inputs: <torch.autograd.function._LinearBackward object at 0x7f5331f1bac0>, <4096x7168xbf16>{7168, 1}+941366272, <4096x1x7168xbf16>{7168, 7168, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c431460_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, column:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TEColumnParallelLinear(in_features=7168,_out_features=4096,_bias=False,_TP=1):TEColumnParallelLinear, None:, None:, True:bool, False:bool
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <True, False, True>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:34.985501 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x7168xbf16>{7168, 1}+941366272, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:34.986254 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
                2025-10-09 13:08:34.988246 %681140:0:0% te::generic_gemm(tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x4096xbf16>{4096, 4096, 1}, None:, None:, None:
            - name: api::SwiGLUFunction
              inputs: <torch.autograd.function.SwiGLUFunctionBackward object at 0x7f5331d3c050>, <4096x4096xbf16>{4096, 1}, False:bool, False:bool
              file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
              2025-10-09 13:08:34.989016 %681140:0:0% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048
              2025-10-09 13:08:34.989286 %681140:0:0% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
              2025-10-09 13:08:34.989545 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
            - 1_0_fwd_module::TERowParallelLinear:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.shared_experts.linear_fc2
              - inputs: ['<4096x1x2048xbf16>{2048, 2048, 1}'], {}
              - name: api::_Linear
                inputs: <torch.autograd.function._LinearBackward object at 0x7f5331d3c380>, <7168x2048xbf16>{2048, 1}+926686208, <4096x1x2048xbf16>{2048, 2048, 1}, None:, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c431340_:WeightGradStore, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, None:, None:, None:, <True, True, True>:Float8BlockQuantizer, True:bool, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, 1:int, False:bool, False:bool, torch_bfloat16:dtype, row:str, True:bool, False:bool, False:bool, False:bool, False:bool, False:bool, False:bool, None:, False:bool, None:, TERowParallelLinear(in_features=2048,_out_features=7168,_bias=False,_TP=1):TERowParallelLinear, None:, None:, False:bool, False:bool
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:34.991117 %681140:0:0% te::quantize(<4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<2048x4096x1xu8>, <32x2048xf32>, <4096x1x2048xu8>, <16x4096xf32>, 1D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <7168x2048xbf16>{2048, 1}+926686208, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:34.991844 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
                2025-10-09 13:08:34.993373 %681140:0:0% te::generic_gemm(tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<2048x4096x1xu8>, <32x2048xf32>, <4096x1x2048xu8>, <16x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
          - name: api::IndicesToMultihot
            inputs: <torch.autograd.function.IndicesToMultihotBackward object at 0x7f5331d3c7c0>, <15713x8xi64>{8, 1}, <15713x8xf32>{8, 1}, 4:int
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_indices_converter.py
          2025-10-09 13:08:34.994529 %681140:None:0% aten::div(self:<4xi64>{1}, other:16:int) -> <4xf32>{1}
          2025-10-09 13:08:34.994699 %681140:None:0% aten::ceil(self:<4xf32>{1}) -> <4xf32>{1}
          2025-10-09 13:08:34.994903 %681140:None:0% aten::mul(self:<4xf32>{1}, other:16:int) -> <4xf32>{1}
          2025-10-09 13:08:34.995270 %681140:None:0% aten::_to_copy(self:<4xf32>{1}, dtype:torch_int64:dtype) -> <4xi64>{1}
          2025-10-09 13:08:34.995524 %681140:None:0% aten::le(self:<4xi64>{1}, other:15713:int) -> <4xbool>{1}
          2025-10-09 13:08:34.995687 %681140:None:0% aten::all(self:<4xbool>{1}) -> <1xbool>{1}
          2025-10-09 13:08:34.996035 %681140:0:0% aten::transpose(self:<15713x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x15713xbool>{1, 4}
          2025-10-09 13:08:34.996351 %681140:0:0% aten::clone(self:<4x15713xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x15713xbool>{15713, 1}
          2025-10-09 13:08:34.996785 %681140:0:0% aten::_to_copy(self:<4x15713xbool>{15713, 1}, dtype:torch_int32:dtype) -> <4x15713xi32>{15713, 1}
          2025-10-09 13:08:35.010438 %681140:0:0% aten::transpose(self:<4x15713xi32>{15713, 1}, dim0:0:int, dim1:1:int) -> <15713x4xi32>{1, 15713}
          2025-10-09 13:08:35.010692 %681140:None:0% aten::sum(self:<4xi64>{1}) -> <1xi64>{1}
          - name: api::_moe_permute_mask_map
            inputs: <torch.autograd.function._moe_permute_mask_mapBackward object at 0x7f5331d3cc00>, <15713x7168xbf16>{7168, 1}, <15713x4xi32>{1, 15713}, 32704:int, <15713x4xf32>{4, 1}
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
          - 1_0_fwd_module::TEGroupedMLP:
            - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts
            - inputs: ['<32704x7168xbf16>{7168, 1}', '<4xi64>{1}', '<32704xf32>{1}'], {}
            - 1_0_fwd_module::Fp8Padding:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.fp8_padding
              - inputs: ['<32704x7168xbf16>{7168, 1}', 'list{8080:int, 8176:int, 8240:int, 8208:int}'], {}
            2025-10-09 13:08:35.019581 %681140:0:0% aten::unsqueeze(self:<32704xf32>{1}, dim:-1:int) -> <32704x1xf32>{1, 1}
            - 1_0_fwd_module::Fp8Padding:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.fp8_padding
              - inputs: ['<32704x1xf32>{1, 1}', 'list{8080:int, 8176:int, 8240:int, 8208:int}'], {}
            - 1_0_fwd_module::TEColumnParallelGroupedLinear:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.linear_fc1
              - inputs: ['<32704x7168xbf16>{7168, 1}', 'list{8080:int, 8176:int, 8240:int, 8208:int}'], {}
              - name: api::_GroupedLinear
                inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f5331d3d9d0>, <32704x7168xbf16>{7168, 1}, list{8080:int, 8176:int, 8240:int, 8208:int}, False:bool, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c430350_:WeightGradStore, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, list{None:, None:, None:, None:}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, True:bool, False:bool, False:bool, torch_bfloat16:dtype, True:bool, TEColumnParallelGroupedLinear():TEColumnParallelGroupedLinear, None:, False:bool, <4096x7168xbf16>{7168, 1}+146800640, <4096x7168xbf16>{7168, 1}+117440512, <4096x7168xbf16>{7168, 1}+88080384, <4096x7168xbf16>{7168, 1}+58720256, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
                2025-10-09 13:08:35.023290 %681140:0:0% te::split_quantize(<32704x7168xbf16>{7168, 1}, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x7168xbf16>{7168, 1}+146800640, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.023944 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x7168xbf16>{7168, 1}+117440512, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.024697 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x7168xbf16>{7168, 1}+88080384, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.025434 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <4096x7168xbf16>{7168, 1}+58720256, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.026160 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
                2025-10-09 13:08:35.035625 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
            - name: api::WeightedSwiGLUFunction
              inputs: <torch.autograd.function.WeightedSwiGLUFunctionBackward object at 0x7f5331d3dae0>, <32704x4096xbf16>{4096, 1}, <32704x1xf32>{1, 1}, False:bool
              file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
              2025-10-09 13:08:35.036646 %681140:0:0% aten::split(self:<32704x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32704x2048xbf16>{4096, 1}, <32704x2048xbf16>{4096, 1}+2048
              2025-10-09 13:08:35.037217 %681140:0:0% aten::silu(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
              2025-10-09 13:08:35.037785 %681140:0:0% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x2048xbf16>{4096, 1}+2048) -> <32704x2048xbf16>{2048, 1}
              2025-10-09 13:08:35.038529 %681140:0:0% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x1xf32>{1, 1}) -> <32704x2048xf32>{2048, 1}
              2025-10-09 13:08:35.039119 %681140:0:0% aten::_to_copy(self:<32704x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <32704x2048xbf16>{2048, 1}
            - 1_0_fwd_module::TERowParallelGroupedLinear:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.linear_fc2
              - inputs: ['<32704x2048xbf16>{2048, 1}', 'list{8080:int, 8176:int, 8240:int, 8208:int}'], {}
              - name: api::_GroupedLinear
                inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f5331d3e470>, <32704x2048xbf16>{2048, 1}, list{8080:int, 8176:int, 8240:int, 8208:int}, False:bool, True:bool, True:bool, False:bool, _transformer_engine_pytorch_module__common_WeightGradStore_object_at_0_7f533c430c20_:WeightGradStore, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, list{None:, None:, None:, None:}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}, True:bool, False:bool, False:bool, torch_bfloat16:dtype, True:bool, TERowParallelGroupedLinear():TERowParallelGroupedLinear, None:, False:bool, <7168x2048xbf16>{2048, 1}+44040192, <7168x2048xbf16>{2048, 1}+29360128, <7168x2048xbf16>{2048, 1}+14680064, <7168x2048xbf16>{2048, 1}, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}, <0xbf16>{1}
                file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
                2025-10-09 13:08:35.041760 %681140:0:0% te::split_quantize(<32704x2048xbf16>{2048, 1}, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<2048x8080xu8>, <64x2048xf32>, <8080x2048xu8>, <16x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8176xu8>, <64x2048xf32>, <8176x2048xu8>, <16x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8240xu8>, <65x2048xf32>, <8240x2048xu8>, <16x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, <8208x2048xu8>, <16x8208xf32>, 1D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <7168x2048xbf16>{2048, 1}+44040192, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.042367 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <7168x2048xbf16>{2048, 1}+29360128, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.043091 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <7168x2048xbf16>{2048, 1}+14680064, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.043823 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
                - name: api::_QuantizeFunc
                  inputs: None:, <7168x2048xbf16>{2048, 1}, <True, True, False>:Float8BlockQuantizer
                  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
                  2025-10-09 13:08:35.044541 %681140:0:0% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
                2025-10-09 13:08:35.050380 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<2048x8080xu8>, <64x2048xf32>, <8080x2048xu8>, <16x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8176xu8>, <64x2048xf32>, <8176x2048xu8>, <16x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8240xu8>, <65x2048xf32>, <8240x2048xu8>, <16x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, <8208x2048xu8>, <16x8208xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
            - 1_0_fwd_module::Fp8Unpadding:
              - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.fp8_unpadding
              - inputs: ['<32704x7168xbf16>{7168, 1}', 'list{8080:int, 8176:int, 8240:int, 8208:int}'], {}
          - name: api::_moe_unpermute_mask_map
            inputs: <torch.autograd.function._moe_unpermute_mask_mapBackward object at 0x7f5331d3ef10>, <32704x7168xbf16>{7168, 1}, <15713x9xi32>{9, 1}, None:, torch_Size([15713,_7168]):Size
            file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
          - name: api::FusedCombine
            inputs: <torch.autograd.function.FusedCombineBackward object at 0x7f5331d3f130>, <15713x7168xbf16>{7168, 1}, <rank:0, size:8, group_name:72, backend:nccl>:ProcessGroup, tuple{<8x8xi32>{8, 1}, <8x10xi32>{10, 1}, <8x10xi32>{10, 1}, <15713xi32>{1}, <4096x8xbool>{8, 1}, <4096x8xi32>{8, 1}}, True:bool, True:bool
            file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
            2025-10-09 13:08:35.053697 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:35.053897 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:35.054089 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:35.054268 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:35.054441 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:35.054614 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:35.054783 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:35.054955 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:35.055130 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:35.055306 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
            2025-10-09 13:08:35.055475 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
            2025-10-09 13:08:35.055644 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
          2025-10-09 13:08:35.056137 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        2025-10-09 13:08:35.056833 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        2025-10-09 13:08:35.056916 %681140:0:0% triton::bias_dropout_add_fused_train(tuple{<4096x1x7168xbf16>{7168, 7168, 1}, None:}, <4096x1x7168xbf16>{7168, 7168, 1}, 0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}
      - 1_0_fwd_module::RMSNorm:
        - param_name: deepseek_v3.mtp.layers.0.final_layernorm
        - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
    2025-10-09 13:08:35.058865 %681140:None:0% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}}) -> <8192x1x7168xbf16>{7168, 7168, 1}
  2025-10-09 13:08:35.059500 %681140:0:0% aten::clone(self:<1x4096xi64>{4096, 1}) -> <1x4096xi64>{4096, 1}
  2025-10-09 13:08:35.059825 %681140:0:0% aten::split(self:<8192x1x7168xbf16>{7168, 7168, 1}, split_size:4096:int) -> <4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}+29360128
  - 1_0_fwd_module::ColumnParallelLinear:
    - param_name: deepseek_v3.output_layer
    - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}+29360128'], {'weight': 'None:', 'runtime_gather_output': 'None:'}
    - name: api::_CopyToModelParallelRegion
      inputs: <torch.autograd.function._CopyToModelParallelRegionBackward object at 0x7f5331d3fce0>, <4096x1x7168xbf16>{7168, 7168, 1}+29360128, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
    - name: api::LinearWithGradAccumulationAndAsyncCommunication
      inputs: <torch.autograd.function.LinearWithGradAccumulationAndAsyncCommunicationBackward object at 0x7f5331d78050>, <4096x1x7168xbf16>{7168, 7168, 1}+29360128, <129280x7168xbf16>{7168, 1}, None:, True:bool, False:bool, False:bool, None:, None:, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/layers.py
      2025-10-09 13:08:35.117084 %681140:0:0% aten::mm(self:<4096x7168xbf16>{7168, 1}+29360128, mat2:<7168x129280xbf16>{1, 7168}) -> <4096x129280xbf16>{129280, 1}
  2025-10-09 13:08:35.118016 %681140:0:0% aten::roll(self:<1x4096xi64>{4096, 1}, shifts:list{-1:int}, dims:list{-1:int}) -> <1x4096xi64>{4096, 1}
  2025-10-09 13:08:35.118316 %681140:0:0% aten::fill_(self:<1xi64>{4096}+4095, value:0:int) -> <1xi64>{4096}+4095
  2025-10-09 13:08:35.118575 %681140:0:0% aten::sum(self:<1x4096xi64>{4096, 1}) -> <1xi64>{1}
  2025-10-09 13:08:35.118967 %681140:0:0% aten::roll(self:<1x4096xf32>{4096, 1}, shifts:list{-1:int}, dims:list{-1:int}) -> <1x4096xf32>{4096, 1}
  2025-10-09 13:08:35.119289 %681140:0:0% aten::fill_(self:<1xf32>{4096}+4095, value:0:int) -> <1xf32>{4096}+4095
  2025-10-09 13:08:35.119595 %681140:0:0% aten::sum(self:<1x4096xf32>{4096, 1}) -> <1xf32>{1}
  2025-10-09 13:08:35.119885 %681140:0:0% aten::transpose(self:<1x4096xi64>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xi64>{1, 4096}
  - name: api::CrossEntropyFunction
    inputs: <torch.autograd.function.CrossEntropyFunctionBackward object at 0x7f5331d78380>, <4096x1x129280xbf16>{129280, 129280, 1}, <4096x1xi64>{1, 1}, 0_0:float, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, -100:int, False:bool
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cross_entropy.py
    2025-10-09 13:08:35.120646 %681140:None:0% aten::zeros(size:list{4096:int}, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <4096xf32>{1}
    2025-10-09 13:08:35.121020 %681140:None:0% aten::zeros(size:list{12288:int}, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <12288xf32>{1}
  2025-10-09 13:08:35.131845 %681140:0:0% aten::transpose(self:<4096x1xf32>{1, 1}, dim0:0:int, dim1:1:int) -> <1x4096xf32>{1, 1}
  2025-10-09 13:08:35.132157 %681140:0:0% aten::mul(self:<1x4096xf32>{4096, 1}, other:<1x4096xf32>{1, 1}) -> <1x4096xf32>{4096, 1}
  2025-10-09 13:08:35.132427 %681140:0:0% aten::sum(self:<1x4096xf32>{4096, 1}) -> <1xf32>{1}
  2025-10-09 13:08:35.132986 %681140:0:0% aten::div(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
  2025-10-09 13:08:35.133385 %681140:None:0% aten::zeros(size:list{1:int}, device:cuda_0:device, pin_memory:False:bool) -> <1xf32>{1}
  2025-10-09 13:08:35.133702 %681140:0:0% aten::add_(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
  2025-10-09 13:08:35.133945 %681140:0:0% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
  2025-10-09 13:08:35.134255 %681140:0:0% aten::mul(self:<1x4096xf32>{4096, 1}, other:0_1:float) -> <1x4096xf32>{4096, 1}
  2025-10-09 13:08:35.134546 %681140:0:0% aten::div(self:<1x4096xf32>{4096, 1}, other:<1xf32>{1}) -> <1x4096xf32>{4096, 1}
  - name: api::MTPLossAutoScaler
    inputs: <torch.autograd.function.MTPLossAutoScalerBackward object at 0x7f5331d789e0>, <4096x1x7168xbf16>{7168, 7168, 1}, <1x4096xf32>{4096, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/multi_token_prediction.py
  - 1_0_fwd_module::ColumnParallelLinear:
    - param_name: deepseek_v3.output_layer
    - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {'weight': 'None:', 'runtime_gather_output': 'None:'}
    - name: api::_CopyToModelParallelRegion
      inputs: <torch.autograd.function._CopyToModelParallelRegionBackward object at 0x7f5331d78c00>, <4096x1x7168xbf16>{7168, 7168, 1}, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
    - name: api::LinearWithGradAccumulationAndAsyncCommunication
      inputs: <torch.autograd.function.LinearWithGradAccumulationAndAsyncCommunicationBackward object at 0x7f5331d78d10>, <4096x1x7168xbf16>{7168, 7168, 1}, <129280x7168xbf16>{7168, 1}, None:, True:bool, False:bool, False:bool, None:, None:, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/layers.py
      2025-10-09 13:08:35.190039 %681140:0:0% aten::mm(self:<4096x7168xbf16>{7168, 1}, mat2:<7168x129280xbf16>{1, 7168}) -> <4096x129280xbf16>{129280, 1}
  2025-10-09 13:08:35.190767 %681140:0:0% aten::transpose(self:<1x4096xi64>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xi64>{1, 4096}
  - name: api::CrossEntropyFunction
    inputs: <torch.autograd.function.CrossEntropyFunctionBackward object at 0x7f5331d79150>, <4096x1x129280xbf16>{129280, 129280, 1}, <4096x1xi64>{1, 1}, 0_0:float, False:bool, <rank:0, size:1, group_name:21, backend:nccl>:ProcessGroup, -100:int, False:bool
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cross_entropy.py
    2025-10-09 13:08:35.191511 %681140:None:0% aten::zeros(size:list{4096:int}, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <4096xf32>{1}
    2025-10-09 13:08:35.191839 %681140:None:0% aten::zeros(size:list{12288:int}, dtype:torch_float32:dtype, device:cuda_0:device, pin_memory:False:bool) -> <12288xf32>{1}
  2025-10-09 13:08:35.193624 %681140:0:0% aten::transpose(self:<4096x1xf32>{1, 1}, dim0:0:int, dim1:1:int) -> <1x4096xf32>{1, 1}
2025-10-09 13:08:35.194300 %681140:0:0% aten::mul(self:<4096xf32>{1}, other:<4096xf32>{1}) -> <4096xf32>{1}
2025-10-09 13:08:35.194601 %681140:0:0% aten::sum(self:<4096xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:35.194835 %681140:0:0% aten::sum(self:<4096xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:35.198699 %681140:None:0% aten::stack(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
2025-10-09 13:08:35.208972 %681140:0:0% aten::isnan(self:<2xf32>{1}) -> <2xbool>{1}
2025-10-09 13:08:35.216062 %681140:0:0% aten::any(self:<2xbool>{1}) -> <1xbool>{1}
2025-10-09 13:08:35.216500 %681140:0:0% aten::unbind(self:<2xf32>{1}) -> <1xf32>{1}, <1xf32>{1}
2025-10-09 13:08:35.216800 %681140:0:0% aten::clone(self:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:35.217099 %681140:0:0% aten::clone(self:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:35.217466 %681140:None:0% aten::cat(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
2025-10-09 13:08:36.217671 %681140:None:0% c10d::allreduce_(tensors:list{<2xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<2xf32>{1}}, Work:distributed
2025-10-09 13:08:36.218169 %681140:0:0% aten::div(self:<2xf32>{1}, other:8:int) -> <2xf32>{1}
2025-10-09 13:08:36.218524 %681140:0:0% aten::div(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.218845 %681140:0:0% aten::mul(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.219163 %681140:0:0% aten::mul_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.219451 %681140:0:0% aten::div_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.219882 %681140:None:0% aten::ones(size:list{1:int}, device:cuda_0:device, pin_memory:False:bool) -> <1xf32>{1}
2025-10-09 13:08:36.220166 %681140:0:0% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.220397 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.220684 %681140:0:0% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.221019 %681140:None:0% aten::ones(size:list{1:int}, device:cuda_0:device, pin_memory:False:bool) -> <1xf32>{1}
2025-10-09 13:08:36.221221 %681140:0:0% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.221441 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.221776 %681140:0:0% aten::add_(self:<1xi32>{1}, other:<1xi32>{1}) -> <1xi32>{1}
2025-10-09 13:08:36.221990 %681140:0:0% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.223813 %681140:0:0% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.224227 %681140:0:0% aten::sum(self:<1xf32>{1}) -> <1xf32>{1}
2025-10-09 13:08:36.224515 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.224755 %681140:0:0% aten::mul(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.224992 %681140:0:0% aten::mul(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
2025-10-09 13:08:36.225504 %681140:0:0% aten::select_backward(grad_output:<1xf32>{1}, input_sizes:list{2:int}, dim:0:int, index:0:int) -> <2xf32>{1}
2025-10-09 13:08:36.225858 %681140:0:0% aten::mul(self:<4096xf32>{0}, other:<4096xf32>{1}) -> <4096xf32>{1}
2025-10-09 13:08:36.226293 %681140:0:0% aten::transpose(self:<1x4096xf32>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xf32>{1, 4096}
- name: api::CrossEntropyFunction
  inputs: <torch.autograd.function.CrossEntropyFunctionBackward object at 0x7f5331d79150>, <4096x1xf32>{1, 4096}
  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cross_entropy.py
  2025-10-09 13:08:36.226780 %681140:0:0% aten::equal(self:<4096x1xf32>{1, 4096}, other:<1xf32>{1}) -> False:bool
- 1_0_bwd_module::ColumnParallelLinear:
  - param_name: deepseek_v3.output_layer
  - inputs: <4096x1x129280xbf16>{129280, 129280, 1}, None:
  - name: api::LinearWithGradAccumulationAndAsyncCommunication
    inputs: <torch.autograd.function.LinearWithGradAccumulationAndAsyncCommunicationBackward object at 0x7f5331d78d10>, <4096x1x129280xbf16>{129280, 129280, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/layers.py
    2025-10-09 13:08:36.291689 %681140:0:0% aten::mm(self:<4096x129280xbf16>{129280, 1}, mat2:<129280x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}
    2025-10-09 13:08:36.347859 %681140:0:0% apex:fused_weight_gradient_mlp_cuda::wgrad_gemm_accum_fp16(<4096x7168xbf16>{7168, 1}, <4096x129280xbf16>{129280, 1}, <129280x7168xbf16>{7168, 1}) -> None:
  - name: api::_CopyToModelParallelRegion
    inputs: <torch.autograd.function._CopyToModelParallelRegionBackward object at 0x7f5331d78c00>, <4096x1x7168xbf16>{7168, 7168, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
- name: api::MTPLossAutoScaler
  inputs: <torch.autograd.function.MTPLossAutoScalerBackward object at 0x7f5331d789e0>, <4096x1x7168xbf16>{7168, 7168, 1}
  file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/multi_token_prediction.py
  2025-10-09 13:08:36.349433 %681140:0:0% aten::mul(self:<1x4096xf32>{4096, 1}, other:<1xf32>{1}) -> <1x4096xf32>{4096, 1}
2025-10-09 13:08:36.349780 %681140:0:0% aten::div(self:<1x4096xf32>{4096, 1}, other:<1xf32>{1}) -> <1x4096xf32>{4096, 1}
2025-10-09 13:08:36.350059 %681140:0:0% aten::mul(self:<1x4096xf32>{4096, 1}, other:0_1:float) -> <1x4096xf32>{4096, 1}
2025-10-09 13:08:36.350325 %681140:0:0% aten::mul(self:<1x4096xf32>{4096, 1}, other:<1x4096xf32>{4096, 1}) -> <1x4096xf32>{4096, 1}
2025-10-09 13:08:36.350584 %681140:0:0% aten::transpose(self:<1x4096xf32>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xf32>{1, 4096}
- name: api::CrossEntropyFunction
  inputs: <torch.autograd.function.CrossEntropyFunctionBackward object at 0x7f5331d78380>, <4096x1xf32>{1, 4096}
  file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/cross_entropy.py
  2025-10-09 13:08:36.350971 %681140:0:0% aten::equal(self:<4096x1xf32>{1, 4096}, other:<1xf32>{1}) -> False:bool
- 1_0_bwd_module::ColumnParallelLinear:
  - param_name: deepseek_v3.output_layer
  - inputs: <4096x1x129280xbf16>{129280, 129280, 1}, None:
  - name: api::LinearWithGradAccumulationAndAsyncCommunication
    inputs: <torch.autograd.function.LinearWithGradAccumulationAndAsyncCommunicationBackward object at 0x7f5331d78050>, <4096x1x129280xbf16>{129280, 129280, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/layers.py
    2025-10-09 13:08:36.408564 %681140:0:0% aten::mm(self:<4096x129280xbf16>{129280, 1}, mat2:<129280x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}
    2025-10-09 13:08:36.462661 %681140:0:0% apex:fused_weight_gradient_mlp_cuda::wgrad_gemm_accum_fp16(<4096x7168xbf16>{7168, 1}+29360128, <4096x129280xbf16>{129280, 1}, <129280x7168xbf16>{7168, 1}) -> None:
  2025-10-09 13:08:36.466059 %681140:0:270202352% aten::add(self:<129280x7168xbf16>{7168, 1}, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
  - name: api::_CopyToModelParallelRegion
    inputs: <torch.autograd.function._CopyToModelParallelRegionBackward object at 0x7f5331d3fce0>, <4096x1x7168xbf16>{7168, 7168, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
2025-10-09 13:08:36.467014 %681140:None:0% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}}) -> <8192x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::MultiTokenPredictionBlock:
  - param_name: deepseek_v3.mtp
  - inputs: ['<8192x1x7168xbf16>{7168, 7168, 1}'], {}
2025-10-09 13:08:36.467555 %681140:0:0% aten::slice(self:<8192x1x7168xbf16>{7168, 7168, 1}, dim:0:int, start:0:int, end:4096:int) -> <4096x1x7168xbf16>{7168, 7168, 1}
2025-10-09 13:08:36.467877 %681140:0:0% aten::slice(self:<8192x1x7168xbf16>{7168, 7168, 1}, dim:0:int, start:4096:int, end:8192:int) -> <4096x1x7168xbf16>{7168, 7168, 1}+29360128
2025-10-09 13:08:36.468347 %681140:None:0% aten::zeros(size:list{1:int, 4096:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1x4096xi64>{4096, 1}
2025-10-09 13:08:36.468692 %681140:None:0% aten::zeros(size:list{1:int, 4096:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1x4096xi64>{4096, 1}
- 1_0_bwd_module::MultiTokenPredictionLayer:
  - param_name: deepseek_v3.mtp.layers.0
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}+29360128', 'None:', 'None:'], {}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.mtp.layers.0.final_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}+29360128'], {}
  2025-10-09 13:08:36.470707 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+926679040, other:<7168xbf16>{1}) -> <7168xbf16>{1}+926679040
- 1_0_bwd_module::TransformerLayer:
  - param_name: deepseek_v3.mtp.layers.0.transformer_layer
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
- 1_0_bwd_module::MoELayer:
  - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - name: api::FusedCombine
    inputs: <torch.autograd.function.FusedCombineBackward object at 0x7f5331d3f130>, <4096x7168xbf16>{7168, 1}, None:
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
    2025-10-09 13:08:36.475058 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.475254 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.475432 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.475604 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.475784 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.475960 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.476130 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.476294 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.476465 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.476632 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.476801 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.476972 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.477138 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.477301 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.477464 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.477626 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.477802 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.477973 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:36.478138 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:36.478298 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
  - name: api::_moe_unpermute_mask_map
    inputs: <torch.autograd.function._moe_unpermute_mask_mapBackward object at 0x7f5331d3ef10>, <15713x7168xbf16>{7168, 1}
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
    2025-10-09 13:08:36.484685 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.486104 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.487489 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.488862 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.490232 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.510632 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.512071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.513468 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.514859 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.516252 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.517638 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.519024 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.520406 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.521802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.523192 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.524576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.525966 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.527358 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.528743 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.530132 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.531514 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.532905 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.534295 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.535674 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.537069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.538457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.539838 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.541216 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.542614 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.544006 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.545391 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.546788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.548168 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.549546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.550924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.552317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.553703 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.555090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.556471 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.557863 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.559248 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.560625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.562016 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.563394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.564767 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.566147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.567543 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.568916 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.570307 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.571698 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.573081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.574459 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.575839 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.577227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.578604 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.579999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.581395 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.582773 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.584156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.585537 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.586939 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.588319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.589705 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.591104 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.592483 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.593858 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.595243 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.596632 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.598012 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.599390 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.600768 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.602165 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.603544 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.604920 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.606319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.607701 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.613602 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.614491 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.615367 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.616239 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.617108 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.635046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.635946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.636835 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.637724 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.638607 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.639495 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.640376 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.641256 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.642137 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.643016 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.643894 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.644772 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.645651 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.646524 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.647398 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.648272 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.649169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.650052 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.650933 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.651806 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.652689 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.653573 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.654456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.655332 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.656214 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.657090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.657968 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.658852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.659725 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.660593 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.661468 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.662350 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.663223 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.664113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.664994 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.665874 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.666751 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.667622 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.668502 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.669375 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.670257 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.671130 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.672013 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.672891 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.673771 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.674655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.675525 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.676397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.677270 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.678159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.679034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.679906 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.680792 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.681667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.682538 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.683411 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.684291 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.685162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.686035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.686914 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.687788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.688659 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.689534 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.690418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.691294 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.692168 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.693040 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.693918 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.694795 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.695664 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.696551 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.697421 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.698290 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.699162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.700048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.700922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.701800 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.702686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.703559 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.704436 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.705317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.706202 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.707089 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.707966 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.708856 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.709729 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.710604 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.711490 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.712372 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.713255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.714135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.715022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.715899 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.716776 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.717654 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.718539 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.719412 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.720285 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.721162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.722050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.722922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.723802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.724686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.725558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.726437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.727310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.728194 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.729071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.729955 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.730835 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.731709 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.732578 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.737900 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.738559 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.739188 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.739812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.740433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.755092 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.755760 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.756417 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.757067 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.757708 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.758349 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.758989 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.759620 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.760258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.760894 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.761532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.762169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.762802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.763437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.764078 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.764710 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.765342 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.765978 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.766608 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.767242 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.767870 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.768508 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.769144 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.769774 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.770403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.771031 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.771656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.772314 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.772948 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.773587 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.774217 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.774847 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.775476 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.776113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.776752 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.777382 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.778012 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.778652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.779280 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.779908 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.780548 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.781189 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.781812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.782439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.783087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.783720 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.784377 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.785012 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.785652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.786283 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.786911 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.787543 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.788189 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.788817 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.789452 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.790099 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.790736 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.791371 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.791998 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.792636 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.793262 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.793888 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.794535 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.795168 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.795792 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.796419 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.797069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.797702 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.798333 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.798979 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.799609 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.800237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.800863 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.801505 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.802138 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.802761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.803394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.804021 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.804660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.805293 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.805943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.806572 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.807201 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.807827 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.808468 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.809106 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.809731 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.810371 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.811000 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.811631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.812257 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.812891 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.813527 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.814158 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.814793 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.815417 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.816043 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.816674 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.817315 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.817943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.818568 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.819203 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.819828 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.820455 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.821088 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.821731 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.822365 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.822991 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.823628 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.824253 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.824879 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.825511 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.826153 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.826780 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.827416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.828044 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.828681 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.829313 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.829943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.830581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.831216 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.831849 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.832480 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.833124 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.833748 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.834379 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.835019 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.835646 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.836274 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.836899 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.837551 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.838181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.838802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.839439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.840078 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.840711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.841344 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.841986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.842610 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.843236 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.843874 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.844504 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.845143 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.845771 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.846411 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.847042 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.847669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.848300 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.848943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.849573 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.850202 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.850840 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.851467 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.852097 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.857193 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.857746 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.858281 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.858808 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.859335 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.872149 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.872710 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.873267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.873807 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.874353 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.874886 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.875423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.875962 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.876495 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.877029 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.877566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.878117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.878657 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.879191 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.879718 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.880250 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.880781 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.881314 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.881845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.882375 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.882906 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.883445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.883979 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.884505 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.885039 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.885569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.886106 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.886636 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.887164 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.887688 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.888219 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.888752 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.889286 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.889821 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.890356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.890887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.891423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.891958 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.892487 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.893025 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.893552 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.894096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.894628 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.895160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.895689 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.896217 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.896741 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.897276 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.897804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.898336 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.898864 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.899401 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.899936 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.900473 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.901011 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.901543 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.902084 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.902613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.903149 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.903676 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.904207 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.904737 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.905272 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.905805 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.906336 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.906861 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.907390 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.907949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.908483 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.909014 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.909542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.910078 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.910607 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.911147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.911672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.912204 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.912743 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.913272 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.913802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.914331 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.914865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.915411 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.915944 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.916491 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.917022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.917546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.918079 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.918612 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.919151 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.919678 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.920210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.920744 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.921283 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.921812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.922353 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.922876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.923417 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.923960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.924512 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.925047 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.925576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.926125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.926655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.927181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.927710 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.928248 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.928777 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.929305 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.929848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.930384 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.930914 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.931451 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.931994 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.932526 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.933063 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.933603 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.934138 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.934665 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.935195 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.935733 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.936264 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.936790 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.937322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.937859 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.938397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.938922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.939462 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.939990 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.940516 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.941046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.941588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.942123 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.942655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.943196 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.943727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.944258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.944785 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.945327 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.945854 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.946386 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.946924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.947465 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.948003 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.948536 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.949081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.949608 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.950144 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.950684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.951219 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.951749 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.952279 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.952814 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.953348 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.953874 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.954406 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.954947 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.955476 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.956010 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.956552 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.957098 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.957631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.958166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.958704 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.959231 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.959756 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.960297 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.960824 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.961354 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.961886 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.962435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.962970 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.963496 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.964034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.964564 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.965098 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.965627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.966166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.966690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.967220 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.967752 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.968282 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.968811 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.973787 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.974310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.974812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.975310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.975807 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.987833 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.988361 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.988876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.989398 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.989913 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.990430 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.990935 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.991440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.991946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.992446 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.992960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.993467 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.993974 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.994473 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.994980 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.995479 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.995984 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.996485 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.996989 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.997491 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.997995 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.998499 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.999018 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:36.999524 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.000024 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.000532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.001034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.001535 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.002037 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.002532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.003035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.003537 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.004036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.004536 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.005045 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.005548 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.006050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.006544 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.007043 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.007542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.008050 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.008554 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.009059 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.009560 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.010066 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.010567 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.011068 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.011572 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.012079 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.012582 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.013096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.013602 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.014107 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.014604 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.015112 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.015611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.016117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.016615 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.017118 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.017614 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.018116 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.018613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.019118 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.019615 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.020117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.020612 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.021116 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.021613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.022116 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.022612 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.023113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.023611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.024113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.024607 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.025111 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.025608 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.026115 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.026614 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.027130 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.027631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.028132 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.028629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.029131 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.029626 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.030125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.030622 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.031125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.031622 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.032133 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.032629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.033132 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.033627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.034129 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.034626 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.035125 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.035621 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.036122 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.036620 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.037118 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.037616 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.038118 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.038616 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.039117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.039644 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.040147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.040648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.041163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.041662 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.042168 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.042665 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.043175 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.043674 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.044169 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.044671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.045174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.045668 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.046171 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.046684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.047183 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.047680 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.048180 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.048689 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.049185 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.049684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.050190 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.050684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.051180 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.051675 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.052184 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.052683 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.053182 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.053687 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.054184 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.054678 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.055171 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.055682 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.056187 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.056684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.057210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.057711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.058209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.058702 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.059211 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.059711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.060209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.060714 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.061215 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.061709 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.062214 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.062722 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.063223 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.063718 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.064215 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.064721 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.065222 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.065716 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.066225 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.066717 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.067220 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.067716 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.068229 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.068728 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.069227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.069741 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.070247 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.070744 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.071243 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.071748 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.072245 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.072745 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.073262 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.073765 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.074262 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.074757 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.075267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.075765 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.076260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.076760 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.077264 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.077761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.078259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.078765 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.079264 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.079762 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.080260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.080772 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.081271 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.081768 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.082275 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.082778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.083288 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.083792 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.084312 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.084809 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.089632 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.090156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.090656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.091157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.091647 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.103589 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.104122 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.104632 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.105150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.105654 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.106160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.106659 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.107159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.107658 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.108159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.108656 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.109161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.109660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.110163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.110659 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.111160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.111658 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.112156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.112653 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.113158 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.113651 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.114162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.114652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.115152 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.115646 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.116153 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.116647 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.117146 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.117637 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.118143 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.118636 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.119136 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.119629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.120135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.120634 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.121134 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.121627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.122128 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.122620 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.123117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.123613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.124113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.124610 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.125113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.125606 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.126114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.126605 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.127102 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.127594 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.128097 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.128590 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.129092 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.129585 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.130093 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.130591 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.131089 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.131582 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.132081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.132577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.133079 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.133576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.134077 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.134572 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.135074 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.135566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.136065 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.136559 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.137062 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.137556 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.138052 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.138545 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.139040 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.139532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.140035 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.140535 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.141036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.141533 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.142038 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.142528 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.143022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.143516 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.144009 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.144502 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.144995 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.145486 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.145984 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.146476 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.146978 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.147474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.147972 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.148465 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.148967 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.149462 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.149959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.150454 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.150957 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.151453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.151950 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.152454 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.152957 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.153456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.153958 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.154453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.154950 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.155445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.155949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.156445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.156944 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.157444 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.157944 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.158439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.158935 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.159448 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.159942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.160435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.160924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.161437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.161927 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.162425 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.162916 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.163425 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.163917 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.164419 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.164924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.165426 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.165921 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.166421 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.166936 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.167428 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.167922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.168433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.168939 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.169433 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.169924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.170440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.170942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.171435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.171924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.172434 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.172942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.173436 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.173945 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.174441 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.174937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.175435 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.175952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.176449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.176946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.177449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.177948 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.178443 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.178939 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.179445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.179945 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.180444 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.180951 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.181455 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.181952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.182448 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.182959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.183452 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.183948 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.184450 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.184960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.185454 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.185953 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.186457 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.186955 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.187452 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.187952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.188456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.188955 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.189446 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.189958 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.190453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.190949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.191444 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.191949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.192443 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.192943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.193449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.193953 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.194447 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.194943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.195455 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.195952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.196445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.196952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.197441 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.197937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.198440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.198944 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.199440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.199942 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.205033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.205549 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.206048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.206542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.207034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.218843 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.219368 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.219876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.220392 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.220894 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.221406 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.221906 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.222415 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.222913 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.223418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.223919 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.224428 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.224926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.225432 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.225933 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.226431 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.226937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.227441 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.227944 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.228454 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.228951 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.229446 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.229951 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.230452 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.230949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.231445 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.231941 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.232436 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.232937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.233432 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.233924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.234422 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.234917 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.235419 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.235909 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.236423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.236921 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.237421 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.237911 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.238409 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.238900 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.239397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.239887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.240390 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.240886 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.241394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.241891 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.242391 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.242887 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.243386 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.243877 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.244373 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.244862 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.245359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.245847 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.246343 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.246833 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.247326 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.247811 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.248306 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.248796 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.249290 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.249781 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.250278 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.250784 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.251275 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.251760 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.252269 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.252761 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.253255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.253743 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.254237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.254728 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.255219 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.255710 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.256208 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.256715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.257215 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.257702 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.258196 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.258685 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.259176 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.259675 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.260175 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.260668 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.261158 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.261651 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.262149 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.262641 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.263141 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.263633 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.264137 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.264631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.265136 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.265632 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.266134 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.266625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.267123 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.267613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.268113 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.268606 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.269102 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.269590 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.270092 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.270594 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.271091 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.271584 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.272098 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.272595 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.273094 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.273596 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.274097 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.274586 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.275087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.275587 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.276086 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.276581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.277094 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.277588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.278091 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.278581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.279088 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.279579 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.280081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.280578 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.281082 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.281569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.282068 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.282569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.283068 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.283561 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.284062 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.284566 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.285069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.285560 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.286077 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.286576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.287081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.287575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.288087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.288580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.289080 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.289583 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.290086 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.290577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.291080 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.291586 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.292091 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.292593 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.293099 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.293592 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.294095 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.294586 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.295092 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.295586 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.296081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.296580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.297084 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.297576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.298085 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.298585 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.299086 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.299579 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.300083 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.300585 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.301085 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.301575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.302085 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.302580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.303080 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.303571 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.304083 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.304577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.305075 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.305579 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.306081 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.306570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.307069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.307575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.308083 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.308576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.309093 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.309588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.310090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.310580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.311087 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.311578 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.312078 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.312573 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.313075 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.313574 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.314074 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.314579 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.315077 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.315568 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
  - 1_0_bwd_module::TEGroupedMLP:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts
    - inputs: <32704x7168xbf16>{7168, 1}, None:
    - 1_0_bwd_module::Fp8Unpadding:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.fp8_unpadding
      - inputs: ['<32704x7168xbf16>{7168, 1}'], {}
    - 1_0_bwd_module::TERowParallelGroupedLinear:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.linear_fc2
      - inputs: <32704x7168xbf16>{7168, 1}, None:
      - name: api::_GroupedLinear
        inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f5331d3e470>, <32704x7168xbf16>{7168, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
        2025-10-09 13:08:37.319816 %681140:0:0% te::split_quantize(<32704x7168xbf16>{7168, 1}, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:37.325871 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        2025-10-09 13:08:37.334357 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<2048x8080xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8176xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8240xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<7168x2048xbf16>{2048, 1}+44040192, <7168x2048xbf16>{2048, 1}+29360128, <7168x2048xbf16>{2048, 1}+14680064, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
    - name: api::WeightedSwiGLUFunction
      inputs: <torch.autograd.function.WeightedSwiGLUFunctionBackward object at 0x7f5331d3dae0>, <32704x2048xbf16>{2048, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
      2025-10-09 13:08:37.336525 %681140:0:0% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x1xf32>{1, 1}) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.336973 %681140:0:0% aten::split(self:<32704x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32704x2048xbf16>{4096, 1}, <32704x2048xbf16>{4096, 1}+2048
      2025-10-09 13:08:37.341060 %681140:0:0% aten::sigmoid(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.341946 %681140:0:0% aten::mul(self:<32704x2048xf32>{2048, 1}, other:<32704x2048xbf16>{2048, 1}) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.342456 %681140:0:0% aten::sigmoid(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.343084 %681140:0:0% aten::rsub(self:<32704x2048xbf16>{2048, 1}, other:1:int) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.343666 %681140:0:0% aten::mul(self:<32704x2048xbf16>{4096, 1}, other:<32704x2048xbf16>{2048, 1}) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.344107 %681140:0:0% aten::add(self:<32704x2048xbf16>{2048, 1}, other:1:int) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.344820 %681140:0:0% aten::mul(self:<32704x2048xf32>{2048, 1}, other:<32704x2048xbf16>{2048, 1}) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.345533 %681140:0:0% aten::mul(self:<32704x2048xf32>{2048, 1}, other:<32704x2048xbf16>{4096, 1}+2048) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.346037 %681140:0:0% aten::silu(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.346721 %681140:0:0% aten::mul(self:<32704x2048xf32>{2048, 1}, other:<32704x2048xbf16>{2048, 1}) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.347545 %681140:None:0% aten::cat(tensors:list{<32704x2048xf32>{2048, 1}, <32704x2048xf32>{2048, 1}}, dim:-1:int) -> <32704x4096xf32>{4096, 1}
      2025-10-09 13:08:37.347852 %681140:0:0% aten::split(self:<32704x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32704x2048xbf16>{4096, 1}, <32704x2048xbf16>{4096, 1}+2048
      2025-10-09 13:08:37.348357 %681140:0:0% aten::silu(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.348854 %681140:0:0% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x2048xbf16>{4096, 1}+2048) -> <32704x2048xbf16>{2048, 1}
      2025-10-09 13:08:37.349749 %681140:0:0% aten::_to_copy(self:<32704x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.350536 %681140:0:0% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x2048xf32>{2048, 1}) -> <32704x2048xf32>{2048, 1}
      2025-10-09 13:08:37.351028 %681140:0:0% aten::sum(self:<32704x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <32704x1xf32>{1, 1}
      2025-10-09 13:08:37.351676 %681140:0:0% aten::_to_copy(self:<32704x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <32704x4096xbf16>{4096, 1}
    - 1_0_bwd_module::TEColumnParallelGroupedLinear:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.linear_fc1
      - inputs: <32704x4096xbf16>{4096, 1}, None:
      - name: api::_GroupedLinear
        inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f5331d3d9d0>, <32704x4096xbf16>{4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
        2025-10-09 13:08:37.353690 %681140:0:0% te::split_quantize(<32704x4096xbf16>{4096, 1}, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<4096x8080xu8>, <64x4096xf32>, <8080x4096xu8>, <32x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8176xu8>, <64x4096xf32>, <8176x4096xu8>, <32x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8240xu8>, <65x4096xf32>, <8240x4096xu8>, <32x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:37.362906 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8080xu8>, <64x4096xf32>, <8080x4096xu8>, <32x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8176xu8>, <64x4096xf32>, <8176x4096xu8>, <32x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8240xu8>, <65x4096xf32>, <8240x4096xu8>, <32x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        2025-10-09 13:08:37.372100 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<7168x8080xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8240xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8080xu8>, <64x4096xf32>, <8080x4096xu8>, <32x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8176xu8>, <64x4096xf32>, <8176x4096xu8>, <32x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8240xu8>, <65x4096xf32>, <8240x4096xu8>, <32x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<4096x7168xbf16>{7168, 1}+146800640, <4096x7168xbf16>{7168, 1}+117440512, <4096x7168xbf16>{7168, 1}+88080384, <4096x7168xbf16>{7168, 1}+58720256}, DType_kBFloat16:DType, list{8080:int, 8176:int, 8240:int, 8208:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
    - 1_0_bwd_module::Fp8Padding:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.fp8_padding
      - inputs: <32704x1xf32>{1, 1}, None:
    2025-10-09 13:08:37.373682 %681140:0:0% aten::squeeze(self:<32704x1xf32>{1, 1}, dim:-1:int) -> <32704xf32>{1}
    - 1_0_bwd_module::Fp8Padding:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.experts.fp8_padding
      - inputs: <32704x7168xbf16>{7168, 1}, None:
    2025-10-09 13:08:37.374323 %681140:None:0% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
  2025-10-09 13:08:37.375994 %681140:None:0% aten::zeros(size:list{15713:int, 9:int}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <15713x9xi32>{9, 1}
  - name: api::_moe_permute_mask_map
    inputs: <torch.autograd.function._moe_permute_mask_mapBackward object at 0x7f5331d3cc00>, <32704x7168xbf16>{7168, 1}, <15713x9xi32>{9, 1}, <32704xf32>{1}
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
    2025-10-09 13:08:37.381439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.383428 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.385395 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.387355 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.389313 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.411684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.413686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.415667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.417642 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.419630 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.421612 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.423583 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.425560 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.427528 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.429501 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.431464 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.433439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.435406 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.437371 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.439336 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.441301 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.443264 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.445227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.447193 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.449159 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.451122 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.453096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.455070 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.457071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.459046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.461027 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.463001 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.464984 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.466973 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.468949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.470917 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.472901 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.474865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.476841 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.478803 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.480780 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.482751 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.484717 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.486680 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.488663 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.490629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.492595 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.494572 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.496533 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.498497 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.500460 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.502449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.504418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.506378 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.508360 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.514959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.516153 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.517328 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.518498 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.519661 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.539855 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.541073 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.542259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.543444 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.544629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.545814 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.546991 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.548167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.549345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.550532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.551715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.552894 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.554086 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.555266 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.556441 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.557614 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.558783 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.559970 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.561154 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.562328 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.563502 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.564673 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.565875 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.567064 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.568240 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.569412 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.570588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.571765 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.572956 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.574139 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.575311 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.576490 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.577683 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.578851 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.580027 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.581196 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.582379 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.583548 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.584721 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.585902 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.587096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.588275 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.589449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.590630 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.591801 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.592986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.594167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.595351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.596529 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.597703 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.598888 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.600075 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.601256 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.602440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.603611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.604784 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.605963 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.607155 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.608333 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.609507 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.610693 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.611860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.613042 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.614218 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.615406 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.616581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.617754 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.618924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.620126 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.621295 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.622474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.623654 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.624823 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.625998 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.627174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.628363 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.629542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.630715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.631900 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.633085 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.634264 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.635437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.636629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.637804 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.642633 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.643439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.644229 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.645009 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.645785 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.662691 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.663503 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.664301 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.665102 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.665891 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.666688 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.667480 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.668273 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.669068 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.669859 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.670669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.671466 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.672250 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.673040 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.673826 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.674613 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.675398 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.676194 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.676986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.677774 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.678561 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.679359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.680154 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.680947 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.681735 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.682536 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.683325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.684117 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.684932 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.685719 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.686506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.687288 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.688082 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.688865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.689655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.690452 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.691237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.692025 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.692816 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.693599 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.694381 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.695165 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.695965 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.696746 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.697531 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.698315 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.699110 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.699890 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.700677 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.701471 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.702255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.703048 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.703835 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.704634 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.705416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.706201 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.706994 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.707780 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.708564 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.709344 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.710139 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.710924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.711722 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.712520 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.713313 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.714101 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.714883 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.715684 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.716469 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.717261 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.718065 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.718848 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.719636 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.720425 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.721218 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.722004 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.722783 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.723569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.724371 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.725156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.725941 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.726737 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.727527 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.728317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.729107 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.729903 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.730695 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.731481 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.732277 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.733072 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.733852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.734637 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.735426 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.736214 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.736998 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.737788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.738570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.739359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.740150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.740946 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.741727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.742511 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.743305 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.744100 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.744888 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.745679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.746475 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.747258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.748044 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.748835 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.749633 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.750418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.751202 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.751999 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.752783 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.753570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.754365 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.755166 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.755954 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.756737 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.757530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.758316 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.759103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.759895 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.760701 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.765325 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.765962 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.766581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.767192 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.767799 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.782038 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.782683 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.783309 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.783936 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.784555 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.785173 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.785787 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.786403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.787034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.787649 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.788266 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.788878 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.789505 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.790131 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.790750 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.791369 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.791986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.792604 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.793220 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.793830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.794447 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.795068 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.795689 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.796309 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.796926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.797545 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.798157 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.798767 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.799384 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.799996 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.800607 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.801230 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.801842 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.802453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.803071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.803683 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.804304 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.804913 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.805530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.806150 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.806783 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.807400 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.808017 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.808629 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.809241 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.809861 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.810471 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.811095 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.811709 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.812340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.812960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.813569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.814181 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.814801 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.815413 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.816022 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.816639 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.817258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.817868 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.818484 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.819111 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.819718 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.820329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.820954 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.821570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.822182 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.822788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.823412 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.824024 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.824633 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.825243 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.825860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.826474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.827090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.827716 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.828337 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.828949 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.829558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.830179 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.830787 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.831396 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.832015 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.832625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.833233 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.833840 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.834464 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.835079 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.835689 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.836307 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.836918 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.837536 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.838163 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.838784 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.839402 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.840012 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.840637 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.841249 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.841859 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.842474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.843100 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.843715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.844336 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.844950 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.845570 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.846180 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.846796 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.847423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.848036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.848647 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.849259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.849876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.850495 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.851107 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.851726 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.852343 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.852960 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.853569 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.854191 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.854797 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.855403 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.856028 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.856646 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.857260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.857871 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.858497 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.859116 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.859729 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.860351 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.860968 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.861577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.862187 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.862807 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.863416 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.864029 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.864642 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.865271 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.865876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.866493 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.867122 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.867730 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.868340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.868958 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.869576 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.870188 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.870802 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.871423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.872034 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.872643 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.873255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.873875 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.874492 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.875104 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.875722 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.876332 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.876945 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.877554 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.878175 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.878790 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.879404 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.883572 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.884143 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.884686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.885232 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.885763 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.898580 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.899162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.899715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.900271 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.900824 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.901375 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.901919 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.902474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.903019 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.903565 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.904120 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.904663 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.905208 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.905756 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.906299 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.906839 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.907386 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.907926 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.908474 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.909016 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.909554 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.910102 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.910640 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.911184 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.911727 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.912275 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.912813 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.913356 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.913893 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.914438 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.914981 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.915518 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.916063 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.916601 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.917147 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.917687 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.918227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.918766 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.919307 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.919845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.920385 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.920937 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.921480 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.922037 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.922574 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.923126 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.923665 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.924209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.924745 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.925286 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.925830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.926374 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.926914 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.927463 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.928016 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.928558 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.929104 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.929641 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.930210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.930759 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.931308 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.931856 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.932412 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.932957 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.933493 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.934046 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.934588 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.935135 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.935673 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.936232 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.936777 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.937319 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.937860 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.938418 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.938964 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.939506 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.940065 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.940609 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.941154 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.941704 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.942245 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.942785 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.943326 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.943876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.944422 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.944967 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.945515 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.946059 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.946596 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.947136 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.947686 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.948232 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.948771 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.949322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.949861 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.950410 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.950965 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.951507 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.952051 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.952590 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.953142 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.953679 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.954219 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.954768 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.955307 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.955845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.956382 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.956933 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.957475 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.958017 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.958568 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.959115 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.959648 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.960187 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.960732 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.961273 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.961807 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.962360 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.962898 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.963448 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.963998 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.964540 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.965090 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.965628 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.966175 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.966711 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.967260 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.967809 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.968350 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.968893 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.969439 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.969986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.970522 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.971070 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.971615 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.972158 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.972698 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.973247 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.973786 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.974330 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.974867 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.975421 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.975967 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.976505 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.977058 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.977592 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.978134 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.978678 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.979230 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.979767 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.980308 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.980855 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.981394 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.981939 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.982488 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.983033 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.983575 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.984119 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.984671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.985211 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.985749 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.986302 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.986838 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.987378 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.987920 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.988471 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.989011 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.989547 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.990105 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.990641 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.991186 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.991724 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.992278 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.992821 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.993360 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.993911 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.994456 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.994991 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:37.995542 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.000076 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.000625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.001160 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.001683 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.002209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.014751 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.015315 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.015852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.016393 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.016936 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.017471 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.018005 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.018540 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.019082 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.019609 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.020148 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.020675 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.021206 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.021733 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.022267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.022794 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.023326 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.023851 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.024379 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.024905 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.025437 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.025972 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.026501 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.027031 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.027563 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.028098 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.028625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.029156 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.029680 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.030206 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.030731 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.031258 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.031789 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.032317 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.032850 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.033376 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.033903 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.034432 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.034959 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.035482 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.036007 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.036532 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.037062 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.037587 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.038118 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.038647 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.039173 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.039697 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.040228 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.040756 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.041286 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.041816 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.042344 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.042866 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.043397 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.043922 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.044453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.044984 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.045509 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.046036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.046567 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.047107 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.047635 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.048161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.048690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.049217 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.049737 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.050263 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.050788 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.051318 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.051844 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.052374 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.052910 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.053442 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.053995 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.054525 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.055059 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.055584 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.056128 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.056649 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.057174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.057700 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.058237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.058758 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.059284 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.059819 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.060345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.060873 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.061406 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.061943 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.062476 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.063005 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.063530 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.064077 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.064602 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.065136 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.065671 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.066198 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.066720 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.067247 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.067781 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.068313 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.068843 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.069380 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.069908 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.070440 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.070969 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.071509 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.072036 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.072563 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.073103 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.073627 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.074161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.074695 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.075237 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.075766 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.076293 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.076832 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.077357 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.077880 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.078422 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.078952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.079479 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.080010 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.080546 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.081080 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.081603 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.082144 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.082669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.083194 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.083715 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.084252 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.084776 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.085301 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.085834 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.086360 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.086883 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.087423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.087948 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.088472 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.089001 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.089543 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.090071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.090592 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.091137 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.091660 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.092185 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.092706 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.093241 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.093765 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.094289 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.094822 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.095359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.095891 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.096429 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.096950 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.097471 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.097998 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.098540 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.099072 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.099599 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.100143 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.100667 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.101192 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.101713 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.102259 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.102780 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.103310 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.103845 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.104382 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.104907 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.105447 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.105978 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.106502 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.107026 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.107564 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.108095 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.108625 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.109162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.109687 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.110213 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.110734 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.111275 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.115767 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.116306 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.116822 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.117332 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.117840 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.130136 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.130677 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.131209 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.131732 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.132255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.132778 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.133292 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.133806 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.134333 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.134841 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.135357 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.135868 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.136388 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.136905 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.137423 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.137938 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.138449 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.138968 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.139480 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.139993 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.140500 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.141012 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.141527 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.142039 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.142551 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.143067 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.143577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.144093 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.144600 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.145114 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.145623 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.146141 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.146652 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.147162 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.147672 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.148183 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.148690 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.149210 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.149722 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.150242 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.150753 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.151267 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.151773 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.152287 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.152800 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.153309 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.153819 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.154329 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.154834 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.155345 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.155852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.156360 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.156869 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.157385 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.157894 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.158407 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.158918 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.159438 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.159952 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.160461 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.160987 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.161496 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.162005 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.162509 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.163021 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.163540 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.164064 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.164585 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.165097 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.165602 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.166126 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.166631 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.167148 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.167655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.168174 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.168680 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.169189 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.169694 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.170212 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.170717 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.171227 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.171742 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.172252 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.172763 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.173283 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.173805 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.174315 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.174832 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.175348 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.175852 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.176359 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.176863 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.177386 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.177898 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.178412 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.178925 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.179453 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.179969 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.180485 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.180992 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.181501 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.182013 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.182547 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.183069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.183578 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.184108 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.184618 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.185130 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.185638 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.186161 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.186664 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.187179 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.187694 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.188212 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.188721 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.189240 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.189746 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.190254 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.190759 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.191280 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.191783 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.192296 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.192812 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.193322 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.193830 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.194340 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.194858 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.195371 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.195876 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.196402 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.196907 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.197420 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.197933 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.198451 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.198964 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.199478 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.200003 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.200516 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.201031 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.201551 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.202069 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.202577 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.203094 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.203611 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.204122 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.204628 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.205146 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.205655 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.206167 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.206675 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.207204 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.207712 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.208222 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.208740 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.209255 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.209764 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.210283 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.210790 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.211303 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.211811 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.212330 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.212838 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.213348 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.213865 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.214383 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.214893 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.215408 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.215924 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.216442 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.216958 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.217476 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.217986 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.218494 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.219011 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.219520 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.220032 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.220547 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.221071 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.221581 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.222096 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.222615 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.223129 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.223638 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.224151 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.224669 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.225182 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.225687 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.226206 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
    2025-10-09 13:08:38.226712 %681140:0:0% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
  2025-10-09 13:08:38.228937 %681140:None:0% aten::zeros(size:list{15713:int, 4:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <15713x4xbool>{4, 1}
  - name: api::IndicesToMultihot
    inputs: <torch.autograd.function.IndicesToMultihotBackward object at 0x7f5331d3c7c0>, <15713x4xbool>{4, 1}, <15713x4xf32>{4, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_indices_converter.py
  - 1_0_bwd_module::SharedExpertMLP:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.shared_experts
    - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
    - 1_0_bwd_module::TERowParallelLinear:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.shared_experts.linear_fc2
      - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
      - name: api::_Linear
        inputs: <torch.autograd.function._LinearBackward object at 0x7f5331d3c380>, <4096x1x7168xbf16>{7168, 7168, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
        - name: api::_QuantizeFunc
          inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
          2025-10-09 13:08:38.234958 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:38.236818 %681140:0:0% te::generic_gemm(tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x2048xbf16>{2048, 1}, None:, None:, None:
        2025-10-09 13:08:38.238378 %681140:0:0% te::generic_gemm(tuple{<2048x4096x1xu8>, <32x2048xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x2048xbf16>{2048, 1}+926686208, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x2048xbf16>{2048, 1}+926686208, None:, None:, None:
    - name: api::SwiGLUFunction
      inputs: <torch.autograd.function.SwiGLUFunctionBackward object at 0x7f5331d3c050>, <4096x2048xbf16>{2048, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
      2025-10-09 13:08:38.239893 %681140:0:0% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048
      2025-10-09 13:08:38.240208 %681140:0:0% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.240495 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.240723 %681140:0:0% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.241057 %681140:0:0% aten::rsub(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.241359 %681140:0:0% aten::mul(self:<4096x2048xbf16>{4096, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.241684 %681140:0:0% aten::add(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.241904 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.242156 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.242388 %681140:0:0% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.242598 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.243010 %681140:None:0% aten::cat(tensors:list{<4096x2048xbf16>{2048, 1}, <4096x2048xbf16>{2048, 1}}, dim:-1:int) -> <4096x4096xbf16>{4096, 1}
    - 1_0_bwd_module::TEColumnParallelLinear:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.shared_experts.linear_fc1
      - inputs: <4096x1x4096xbf16>{4096, 4096, 1}, None:
      - name: api::_Linear
        inputs: <torch.autograd.function._LinearBackward object at 0x7f5331f1bac0>, <4096x1x4096xbf16>{4096, 4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
        - name: api::_QuantizeFunc
          inputs: None:, <4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
          2025-10-09 13:08:38.244007 %681140:0:0% te::quantize(<4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}
        - name: api::_QuantizeFunc
          inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <False, True, True>:Float8BlockQuantizer
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
          2025-10-09 13:08:38.244659 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:38.246738 %681140:0:0% te::generic_gemm(tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
        2025-10-09 13:08:38.248695 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <4096x7168xbf16>{7168, 1}+941366272, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}+941366272, None:, None:, None:
  2025-10-09 13:08:38.250217 %681140:None:0% aten::zeros(size:list{15713:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <15713x8xi64>{8, 1}
  2025-10-09 13:08:38.250523 %681140:None:0% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
  - name: api::FusedDispatch
    inputs: <torch.autograd.function.FusedDispatchBackward object at 0x7f5331f1b680>, <15713x7168xbf16>{7168, 1}, <15713x8xi64>{8, 1}, <15713x8xf32>{8, 1}, <4xi64>{1}, None:
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
    2025-10-09 13:08:38.252042 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.252235 %681140:0:270142432% aten::record_stream(self:<15713x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.252410 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.252580 %681140:0:270142432% aten::record_stream(self:<15713xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.252754 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.252924 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.253099 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.253263 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.253428 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.253593 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.253757 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.253921 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.254096 %681140:0:270142432% aten::record_stream(self:<15713x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.254263 %681140:0:270142432% aten::record_stream(self:<15713x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.254426 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.254590 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
  2025-10-09 13:08:38.255238 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xf32>{32, 1}
  2025-10-09 13:08:38.259608 %681140:0:0% aten::scatter(self:<4096x32xf32>{32, 1}, dim:-1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
  2025-10-09 13:08:38.260085 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
  2025-10-09 13:08:38.260518 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xbool>{32, 1}
  - 1_0_bwd_module::TopKRouter:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.mlp.router
    - inputs: <4096x32xf32>{32, 1}, None:
    - name: api::MoEAuxLossAutoScaler
      inputs: <torch.autograd.function.MoEAuxLossAutoScalerBackward object at 0x7f5331f1b460>, <4096x32xf32>{32, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
      2025-10-09 13:08:38.261011 %681140:0:0% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
    2025-10-09 13:08:38.261344 %681140:0:0% aten::sum(self:<1xf32>{1}) -> <1xf32>{1}
    2025-10-09 13:08:38.261598 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
    - name: api::FusedAuxLoss
      inputs: <torch.autograd.function.FusedAuxLossBackward object at 0x7f5331f1b240>, <1xf32>{1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
      2025-10-09 13:08:38.262075 %681140:None:0% te::fused_moe_aux_loss_bwd(Const_buf:<1xf32>{1}, tokens_per_expert:<32xi64>{1}, num_rows:4096:int, num_cols:32:int, grad_aux_loss:<1xf32>{1}) -> <4096x32xf32>{32, 1}
    2025-10-09 13:08:38.262656 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xbool>{32, 1}
    - name: api::FusedComputeScoresForMoEAuxLoss
      inputs: <torch.autograd.function.FusedComputeScoresForMoEAuxLossBackward object at 0x7f5331f1af10>, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
      2025-10-09 13:08:38.263217 %681140:None:0% te::fused_score_for_moe_aux_loss_bwd(num_tokens:4096:int, num_experts:32:int, intermediate_output:<4096x32xf32>{32, 1}, grad_scores:<4096x32xf32>{32, 1}, topk:8:int, score_function:sigmoid:str) -> <4096x32xf32>{32, 1}
    2025-10-09 13:08:38.263740 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xbool>{32, 1}
    - name: api::FusedTopkScoreFunction
      inputs: <torch.autograd.function.FusedTopkScoreFunctionBackward object at 0x7f5331f1acf0>, <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
      2025-10-09 13:08:38.264368 %681140:0:0% te::fused_topk_with_score_function_bwd(4096:int, 32:int, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}, <4096x32xf32>{32, 1}, 8:int, True:bool, 2_5:float, sigmoid:str) -> <4096x32xf32>{32, 1}
    2025-10-09 13:08:38.264907 %681140:0:0% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
    - name: api::RandomSTE
      inputs: <torch.autograd.function.RandomSTEBackward object at 0x7f5331f1aad0>, <4096x1x32xf32>{32, 32, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
    - name: api::RouterGatingLinearFunction
      inputs: <torch.autograd.function.RouterGatingLinearFunctionBackward object at 0x7f5331f1a9c0>, <4096x1x32xf32>{32, 32, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
      2025-10-09 13:08:38.265668 %681140:0:0% aten::_to_copy(self:<32x7168xbf16>{7168, 1}+970726400, dtype:torch_float32:dtype) -> <32x7168xf32>{7168, 1}
      2025-10-09 13:08:38.303755 %681140:0:0% te::generic_gemm(<32x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xf32>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.304627 %681140:0:0% aten::_to_copy(self:<4096x7168xbf16>{7168, 1}, dtype:torch_float32:dtype) -> <4096x7168xf32>{7168, 1}
      2025-10-09 13:08:38.305774 %681140:0:0% te::generic_gemm(<4096x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, True:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32x7168xf32>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.306269 %681140:0:0% aten::_to_copy(self:<4096x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <4096x7168xbf16>{7168, 1}
      2025-10-09 13:08:38.306683 %681140:0:0% aten::_to_copy(self:<32x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <32x7168xbf16>{7168, 1}
    2025-10-09 13:08:38.307378 %681140:0:270202352% aten::add_(self:<32x7168xbf16>{7168, 1}+970726400, other:<32x7168xbf16>{7168, 1}) -> <32x7168xbf16>{7168, 1}+970726400
  2025-10-09 13:08:38.307873 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.mtp.layers.0.transformer_layer.pre_mlp_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  2025-10-09 13:08:38.308888 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+970955776, other:<7168xbf16>{1}) -> <7168xbf16>{1}+970955776
2025-10-09 13:08:38.309328 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::MLASelfAttention:
  - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - 1_0_bwd_module::TERowParallelLinear:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_proj
    - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5331f19d00>, <4096x1x7168xbf16>{7168, 7168, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.310276 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
      - name: api::_QuantizeFunc
        inputs: None:, <4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.310985 %681140:0:0% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.315727 %681140:0:0% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x16384xbf16>{16384, 1}, None:, None:, None:
      2025-10-09 13:08:38.320281 %681140:0:0% te::generic_gemm(tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x16384xbf16>{16384, 1}+1040629760, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x16384xbf16>{16384, 1}+1040629760, None:, None:, None:
  - 1_0_bwd_module::TEDotProductAttention:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.core_attention
    - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
    - 1_0_bwd_module::UnfusedDotProductAttention:
      - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.core_attention.unfused_attention
      - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
      2025-10-09 13:08:38.321858 %681140:0:0% aten::permute(self:<4096x1x128x128xbf16>{16384, 16384, 128, 1}, dims:list{1:int, 2:int, 0:int, 3:int}) -> <1x128x4096x128xbf16>{16384, 128, 16384, 1}
      2025-10-09 13:08:38.322217 %681140:0:0% aten::transpose(self:<128x4096x4096xbf16>{16777216, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x4096xbf16>{16777216, 1, 4096}
      2025-10-09 13:08:38.329361 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 1, 4096}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
      2025-10-09 13:08:38.329658 %681140:0:0% aten::transpose(self:<128x4096x128xbf16>{128, 16384, 1}, dim0:1:int, dim1:2:int) -> <128x128x4096xbf16>{128, 1, 16384}
      2025-10-09 13:08:38.334885 %681140:0:0% aten::bmm(self:<128x4096x128xbf16>{128, 16384, 1}, mat2:<128x128x4096xbf16>{128, 1, 16384}) -> <128x4096x4096xbf16>{16777216, 4096, 1}
      2025-10-09 13:08:38.335210 %681140:0:0% aten::transpose(self:<128x4096x128xbf16>{524288, 128, 1}, dim0:0:int, dim1:1:int) -> <4096x128x128xbf16>{128, 524288, 1}
      - 1_0_bwd_module::Dropout:
        - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.core_attention.unfused_attention.attention_dropout
        - inputs: ['<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}'], {}
      - name: api::ScaledMaskedSoftmax
        inputs: <torch.autograd.function.ScaledMaskedSoftmaxBackward object at 0x7f5331f19370>, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/attention/dot_product_attention/softmax.py
        2025-10-09 13:08:38.342962 %681140:0:0% te::scaled_masked_softmax_backward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
      2025-10-09 13:08:38.343443 %681140:0:0% aten::transpose(self:<128x192x4096xbf16>{192, 1, 24576}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{192, 24576, 1}
      2025-10-09 13:08:38.352744 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x192xbf16>{192, 24576, 1}) -> <128x4096x192xbf16>{786432, 192, 1}
      2025-10-09 13:08:38.353293 %681140:0:0% aten::mul(self:<128x4096x192xbf16>{786432, 192, 1}, other:0_1352337788608801:float) -> <128x4096x192xbf16>{786432, 192, 1}
      2025-10-09 13:08:38.353533 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
      2025-10-09 13:08:38.362974 %681140:0:0% aten::bmm(self:<128x192x4096xbf16>{192, 1, 24576}, mat2:<128x4096x4096xbf16>{16777216, 4096, 1}) -> <128x192x4096xbf16>{786432, 4096, 1}
      2025-10-09 13:08:38.363415 %681140:0:0% aten::mul(self:<128x192x4096xbf16>{786432, 4096, 1}, other:0_1352337788608801:float) -> <128x192x4096xbf16>{786432, 4096, 1}
      2025-10-09 13:08:38.363729 %681140:0:0% aten::transpose(self:<128x192x4096xbf16>{786432, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{786432, 1, 4096}
      2025-10-09 13:08:38.364018 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{786432, 1, 4096}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{1, 786432, 4096}
      2025-10-09 13:08:38.364291 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{786432, 192, 1}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{192, 786432, 1}
    2025-10-09 13:08:38.365042 %681140:None:0% aten::zeros(size:list{1:int, 1:int, 4096:int, 4096:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
  2025-10-09 13:08:38.365566 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, dim:3:int, start:0:int, end:128:int) -> <4096x1x128x128xbf16>{1, 100663296, 786432, 4096}
  2025-10-09 13:08:38.365886 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, dim:3:int, start:128:int, end:192:int) -> <4096x1x128x64xbf16>{1, 100663296, 786432, 4096}+524288
  2025-10-09 13:08:38.366595 %681140:0:0% aten::sum(self:<4096x1x128x64xbf16>{1, 100663296, 786432, 4096}+524288, dim:list{2:int}, keepdim:True:bool) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.366947 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, dim:3:int, start:0:int, end:128:int) -> <4096x1x128x128xbf16>{192, 100663296, 786432, 1}
  2025-10-09 13:08:38.367265 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, dim:3:int, start:128:int, end:192:int) -> <4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128
  2025-10-09 13:08:38.367593 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:64:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.367898 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:64:int, end:64:int) -> <4096x1x1x0xbf16>{64, 64, 64, 1}+64
  2025-10-09 13:08:38.368157 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.368484 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.368792 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}+32
  2025-10-09 13:08:38.369050 %681140:0:0% aten::neg(self:<4096x1x1x32xbf16>{64, 64, 64, 1}) -> <4096x1x1x32xbf16>{32, 32, 32, 1}
  2025-10-09 13:08:38.369400 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{64, 64, 64, 1}+32, <4096x1x1x32xbf16>{32, 32, 32, 1}}, dim:3:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.369633 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.369887 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.370177 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.370434 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}+32
  2025-10-09 13:08:38.370987 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x32xbf16>{64, 64, 64, 1}+32, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.371493 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x32xbf16>{64, 64, 64, 1}, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.371745 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.372227 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x0xbf16>{64, 64, 64, 1}+64, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:64:int, end:9223372036854775807:int, step:1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.372490 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.372825 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, dim:3:int, start:0:int, end:64:int) -> <4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128
  2025-10-09 13:08:38.373152 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, dim:3:int, start:64:int, end:64:int) -> <4096x1x128x0xbf16>{192, 100663296, 786432, 1}+192
  2025-10-09 13:08:38.373598 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.373954 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x128x32xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.374265 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x128x32xbf16>{64, 33554432, 262144, 1}+32
  2025-10-09 13:08:38.374567 %681140:0:0% aten::neg(self:<4096x1x128x32xbf16>{64, 33554432, 262144, 1}) -> <4096x1x128x32xbf16>{32, 16777216, 131072, 1}
  2025-10-09 13:08:38.375129 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{64, 33554432, 262144, 1}+32, <4096x1x128x32xbf16>{32, 16777216, 131072, 1}}, dim:3:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.375508 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.375995 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}) -> <4096x1x128x64xbf16>{8192, 33554432, 64, 1}
  2025-10-09 13:08:38.376338 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{8192, 33554432, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x128x32xbf16>{8192, 33554432, 64, 1}
  2025-10-09 13:08:38.376649 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{8192, 33554432, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x128x32xbf16>{8192, 33554432, 64, 1}+32
  2025-10-09 13:08:38.377259 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x32xbf16>{8192, 33554432, 64, 1}+32, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.377852 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x32xbf16>{8192, 33554432, 64, 1}, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.378171 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.378674 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x0xbf16>{192, 100663296, 786432, 1}+192, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:64:int, end:9223372036854775807:int, step:1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.379001 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.380254 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{1, 100663296, 786432, 4096}, <4096x1x128x128xbf16>{128, 67108864, 524288, 1}}, dim:3:int) -> <4096x1x128x256xbf16>{32768, 32768, 256, 1}
  2025-10-09 13:08:38.381314 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{192, 100663296, 786432, 1}, <4096x1x128x64xbf16>{8192, 8192, 64, 1}}, dim:3:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
  2025-10-09 13:08:38.381593 %681140:0:0% aten::squeeze(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:-2:int) -> <4096x1x64xbf16>{64, 64, 1}
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_kv_up_proj
    - inputs: <4096x1x32768xbf16>{32768, 32768, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5331f186b0>, <4096x1x32768xbf16>{32768, 32768, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.382813 %681140:0:0% te::quantize(<4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.386897 %681140:0:0% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x512xbf16>{512, 1}, None:, None:, None:
      2025-10-09 13:08:38.388550 %681140:0:0% te::generic_gemm(tuple{<512x4096xu8>, <32x512xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <32768x512xbf16>{512, 1}+970962944, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32768x512xbf16>{512, 1}+970962944, None:, None:, None:
      2025-10-09 13:08:38.389139 %681140:0:0% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+987740160, 0:int, False:bool) -> <4096x512xbf16>{512, 1}, <512xbf16>{1}
    2025-10-09 13:08:38.390480 %681140:0:270202352% aten::add_(self:<512xbf16>{1}+987740160, other:<512xbf16>{1}) -> <512xbf16>{1}+987740160
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_q_up_proj
    - inputs: <4096x1x24576xbf16>{24576, 24576, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5331f18270>, <4096x1x24576xbf16>{24576, 24576, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.391722 %681140:0:0% te::quantize(<4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.394084 %681140:0:0% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1536xbf16>{1536, 1}, None:, None:, None:
      2025-10-09 13:08:38.396279 %681140:0:0% te::generic_gemm(tuple{<1536x4096xu8>, <32x1536xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <24576x1536xbf16>{1536, 1}+991869440, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <24576x1536xbf16>{1536, 1}+991869440, None:, None:, None:
      2025-10-09 13:08:38.396850 %681140:0:0% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1029618176, 0:int, False:bool) -> <4096x1536xbf16>{1536, 1}, <1536xbf16>{1}
    2025-10-09 13:08:38.398154 %681140:0:270202352% aten::add_(self:<1536xbf16>{1}+1029618176, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1029618176
  2025-10-09 13:08:38.398701 %681140:None:0% aten::cat(tensors:list{<4096x1x512xbf16>{512, 512, 1}, <4096x1x64xbf16>{64, 64, 1}}, dim:2:int) -> <4096x1x576xbf16>{576, 576, 1}
  - 1_0_bwd_module::TELinear:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_kv_down_proj
    - inputs: <4096x1x576xbf16>{576, 576, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5331eb7ce0>, <4096x1x576xbf16>{576, 576, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.399506 %681140:0:0% te::quantize(<4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.400893 %681140:0:0% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.404312 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <576x7168xbf16>{7168, 1}+987740672, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <576x7168xbf16>{7168, 1}+987740672, None:, None:, None:
  - 1_0_bwd_module::TELinear:
    - param_name: deepseek_v3.mtp.layers.0.transformer_layer.self_attention.linear_q_down_proj
    - inputs: <4096x1x1536xbf16>{1536, 1536, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5331eb78a0>, <4096x1x1536xbf16>{1536, 1536, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.406066 %681140:0:0% te::quantize(<4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.407644 %681140:0:0% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.409068 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <1536x7168xbf16>{7168, 1}+1029619712, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <1536x7168xbf16>{7168, 1}+1029619712, None:, None:, None:
  2025-10-09 13:08:38.410358 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.mtp.layers.0.transformer_layer.input_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  2025-10-09 13:08:38.411355 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1158070272, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1158070272
2025-10-09 13:08:38.411775 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- name: api::_GatherFromModelParallelRegion
  inputs: <torch.autograd.function._GatherFromModelParallelRegionBackward object at 0x7f5331eb7020>, <4096x1x7168xbf16>{7168, 7168, 1}
  file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
- 1_0_bwd_module::TEColumnParallelLinear:
  - param_name: deepseek_v3.mtp.layers.0.eh_proj
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - name: api::_Linear
    inputs: <torch.autograd.function._LinearBackward object at 0x7f5331eb6cf0>, <4096x1x7168xbf16>{7168, 7168, 1}
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
    - name: api::_QuantizeFunc
      inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
      2025-10-09 13:08:38.412753 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
    2025-10-09 13:08:38.417005 %681140:0:0% te::generic_gemm(tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x14336xbf16>{14336, 1}, None:, None:, None:
    2025-10-09 13:08:38.421099 %681140:0:0% te::generic_gemm(tuple{<14336x4096x1xu8>, <32x14336xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x14336xbf16>{14336, 1}+1158077440, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x14336xbf16>{14336, 1}+1158077440, None:, None:, None:
2025-10-09 13:08:38.422489 %681140:0:0% aten::slice(self:<4096x1x14336xbf16>{14336, 14336, 1}, dim:2:int, start:0:int, end:7168:int) -> <4096x1x7168xbf16>{14336, 14336, 1}
2025-10-09 13:08:38.422820 %681140:0:0% aten::slice(self:<4096x1x14336xbf16>{14336, 14336, 1}, dim:2:int, start:7168:int, end:14336:int) -> <4096x1x7168xbf16>{14336, 14336, 1}+7168
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.mtp.layers.0.hnorm
  - inputs: ['<4096x1x7168xbf16>{14336, 14336, 1}+7168'], {}
  2025-10-09 13:08:38.423394 %681140:0:0% aten::clone(self:<4096x1x7168xbf16>{14336, 14336, 1}+7168, memory_format:torch_contiguous_format:memory_format) -> <4096x1x7168xbf16>{7168, 7168, 1}
  2025-10-09 13:08:38.424116 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1260837888, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1260837888
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.mtp.layers.0.enorm
  - inputs: ['<4096x1x7168xbf16>{14336, 14336, 1}'], {}
  2025-10-09 13:08:38.424753 %681140:0:0% aten::clone(self:<4096x1x7168xbf16>{14336, 14336, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x7168xbf16>{7168, 7168, 1}
  2025-10-09 13:08:38.425456 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1260845056, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1260845056
2025-10-09 13:08:38.425891 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::LanguageModelEmbedding:
  - param_name: deepseek_v3.embedding
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
- 1_0_bwd_module::Dropout:
  - param_name: deepseek_v3.embedding.embedding_dropout
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
2025-10-09 13:08:38.426495 %681140:0:0% aten::transpose(self:<4096x1x7168xbf16>{7168, 7168, 1}, dim0:0:int, dim1:1:int) -> <1x4096x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::VocabParallelEmbedding:
  - param_name: deepseek_v3.embedding.word_embeddings
  - inputs: ['<1x4096x7168xbf16>{7168, 7168, 1}'], {}
- name: api::_ReduceFromModelParallelRegion
  inputs: <torch.autograd.function._ReduceFromModelParallelRegionBackward object at 0x7f5331eb56a0>, <1x4096x7168xbf16>{7168, 7168, 1}
  file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
2025-10-09 13:08:38.440346 %681140:0:0% aten::embedding_dense_backward(grad_output:<1x4096x7168xbf16>{7168, 7168, 1}, indices:<1x4096xi64>{4096, 1}, num_weights:129280:int, padding_idx:-1:int, scale_grad_by_freq:False:bool) -> <129280x7168xbf16>{7168, 1}
2025-10-09 13:08:38.440875 %681140:None:0% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::TransformerBlock:
  - param_name: deepseek_v3.decoder
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.decoder.final_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  2025-10-09 13:08:38.441949 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1260852224, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1260852224
- 1_0_bwd_module::TransformerLayer:
  - param_name: deepseek_v3.decoder.layers.1
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
- 1_0_bwd_module::MoELayer:
  - param_name: deepseek_v3.decoder.layers.1.mlp
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - name: api::FusedCombine
    inputs: <torch.autograd.function.FusedCombineBackward object at 0x7f5331eb4e20>, <4096x7168xbf16>{7168, 1}, None:
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
    2025-10-09 13:08:38.449514 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.449720 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.449896 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.450075 %681140:0:270142432% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.450246 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.450411 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.450576 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.450742 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.450907 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.451085 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.451252 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.451417 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.451580 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.451743 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.451905 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.452088 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.452254 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.452414 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.452572 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.452731 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
  - name: api::_moe_unpermute_mask_map
    inputs: <torch.autograd.function._moe_unpermute_mask_mapBackward object at 0x7f5330843ce0>, <15712x7168xbf16>{7168, 1}
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
  - 1_0_bwd_module::TEGroupedMLP:
    - param_name: deepseek_v3.decoder.layers.1.mlp.experts
    - inputs: <32736x7168xbf16>{7168, 1}, None:
    - 1_0_bwd_module::Fp8Unpadding:
      - param_name: deepseek_v3.decoder.layers.1.mlp.experts.fp8_unpadding
      - inputs: ['<32736x7168xbf16>{7168, 1}'], {}
    - 1_0_bwd_module::TERowParallelGroupedLinear:
      - param_name: deepseek_v3.decoder.layers.1.mlp.experts.linear_fc2
      - inputs: <32736x7168xbf16>{7168, 1}, None:
      - name: api::_GroupedLinear
        inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f5330843240>, <32736x7168xbf16>{7168, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
        2025-10-09 13:08:38.456020 %681140:0:0% te::split_quantize(<32736x7168xbf16>{7168, 1}, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:38.461951 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32736x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        2025-10-09 13:08:38.467738 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<2048x8288xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8080xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8080xu8>, <64x7168xf32>, <8080x7168xu8>, <56x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<7168x2048xbf16>{2048, 1}+220200960, <7168x2048xbf16>{2048, 1}+205520896, <7168x2048xbf16>{2048, 1}+190840832, <7168x2048xbf16>{2048, 1}+176160768}, DType_kBFloat16:DType, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
    - name: api::WeightedSwiGLUFunction
      inputs: <torch.autograd.function.WeightedSwiGLUFunctionBackward object at 0x7f53308428b0>, <32736x2048xbf16>{2048, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
      2025-10-09 13:08:38.469609 %681140:0:0% aten::mul(self:<32736x2048xbf16>{2048, 1}, other:<32736x1xf32>{1, 1}) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.469923 %681140:0:0% aten::split(self:<32736x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32736x2048xbf16>{4096, 1}, <32736x2048xbf16>{4096, 1}+2048
      2025-10-09 13:08:38.470490 %681140:0:0% aten::sigmoid(self:<32736x2048xbf16>{4096, 1}) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.471248 %681140:0:0% aten::mul(self:<32736x2048xf32>{2048, 1}, other:<32736x2048xbf16>{2048, 1}) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.471743 %681140:0:0% aten::sigmoid(self:<32736x2048xbf16>{4096, 1}) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.472162 %681140:0:0% aten::rsub(self:<32736x2048xbf16>{2048, 1}, other:1:int) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.472723 %681140:0:0% aten::mul(self:<32736x2048xbf16>{4096, 1}, other:<32736x2048xbf16>{2048, 1}) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.473124 %681140:0:0% aten::add(self:<32736x2048xbf16>{2048, 1}, other:1:int) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.473827 %681140:0:0% aten::mul(self:<32736x2048xf32>{2048, 1}, other:<32736x2048xbf16>{2048, 1}) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.474541 %681140:0:0% aten::mul(self:<32736x2048xf32>{2048, 1}, other:<32736x2048xbf16>{4096, 1}+2048) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.475049 %681140:0:0% aten::silu(self:<32736x2048xbf16>{4096, 1}) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.475735 %681140:0:0% aten::mul(self:<32736x2048xf32>{2048, 1}, other:<32736x2048xbf16>{2048, 1}) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.476391 %681140:None:0% aten::cat(tensors:list{<32736x2048xf32>{2048, 1}, <32736x2048xf32>{2048, 1}}, dim:-1:int) -> <32736x4096xf32>{4096, 1}
      2025-10-09 13:08:38.476694 %681140:0:0% aten::split(self:<32736x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32736x2048xbf16>{4096, 1}, <32736x2048xbf16>{4096, 1}+2048
      2025-10-09 13:08:38.477187 %681140:0:0% aten::silu(self:<32736x2048xbf16>{4096, 1}) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.477679 %681140:0:0% aten::mul(self:<32736x2048xbf16>{2048, 1}, other:<32736x2048xbf16>{4096, 1}+2048) -> <32736x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.478546 %681140:0:0% aten::_to_copy(self:<32736x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.479324 %681140:0:0% aten::mul(self:<32736x2048xbf16>{2048, 1}, other:<32736x2048xf32>{2048, 1}) -> <32736x2048xf32>{2048, 1}
      2025-10-09 13:08:38.479810 %681140:0:0% aten::sum(self:<32736x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <32736x1xf32>{1, 1}
      2025-10-09 13:08:38.480444 %681140:0:0% aten::_to_copy(self:<32736x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <32736x4096xbf16>{4096, 1}
    - 1_0_bwd_module::TEColumnParallelGroupedLinear:
      - param_name: deepseek_v3.decoder.layers.1.mlp.experts.linear_fc1
      - inputs: <32736x4096xbf16>{4096, 1}, None:
      - name: api::_GroupedLinear
        inputs: <torch.autograd.function._GroupedLinearBackward object at 0x7f53308427a0>, <32736x4096xbf16>{4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/grouped_linear.py
        2025-10-09 13:08:38.482438 %681140:0:0% te::split_quantize(<32736x4096xbf16>{4096, 1}, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<4096x8288xu8>, <65x4096xf32>, <8288x4096xu8>, <32x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8080xu8>, <64x4096xf32>, <8080x4096xu8>, <32x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:38.491615 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8288xu8>, <65x4096xf32>, <8288x4096xu8>, <32x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8080xu8>, <64x4096xf32>, <8080x4096xu8>, <32x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32736x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        2025-10-09 13:08:38.500700 %681140:None:0% te::te_general_grouped_gemm(list{tuple{<7168x8288xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8080xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8288xu8>, <65x4096xf32>, <8288x4096xu8>, <32x8288xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8080xu8>, <64x4096xf32>, <8080x4096xu8>, <32x8080xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<4096x7168xbf16>{7168, 1}+322961408, <4096x7168xbf16>{7168, 1}+293601280, <4096x7168xbf16>{7168, 1}+264241152, <4096x7168xbf16>{7168, 1}+234881024}, DType_kBFloat16:DType, list{8288:int, 8208:int, 8080:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
    - 1_0_bwd_module::Fp8Padding:
      - param_name: deepseek_v3.decoder.layers.1.mlp.experts.fp8_padding
      - inputs: <32736x1xf32>{1, 1}, None:
    2025-10-09 13:08:38.502244 %681140:0:0% aten::squeeze(self:<32736x1xf32>{1, 1}, dim:-1:int) -> <32736xf32>{1}
    - 1_0_bwd_module::Fp8Padding:
      - param_name: deepseek_v3.decoder.layers.1.mlp.experts.fp8_padding
      - inputs: <32736x7168xbf16>{7168, 1}, None:
    2025-10-09 13:08:38.502836 %681140:None:0% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
  2025-10-09 13:08:38.503425 %681140:None:0% aten::zeros(size:list{15712:int, 9:int}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <15712x9xi32>{9, 1}
  - name: api::_moe_permute_mask_map
    inputs: <torch.autograd.function._moe_permute_mask_mapBackward object at 0x7f53308408d0>, <32736x7168xbf16>{7168, 1}, <15712x9xi32>{9, 1}, <32736xf32>{1}
    file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/permutation.py
  2025-10-09 13:08:38.504524 %681140:None:0% aten::zeros(size:list{15712:int, 4:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <15712x4xbool>{4, 1}
  - name: api::IndicesToMultihot
    inputs: <torch.autograd.function.IndicesToMultihotBackward object at 0x7f53307c7ce0>, <15712x4xbool>{4, 1}, <15712x4xf32>{4, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_indices_converter.py
  - 1_0_bwd_module::SharedExpertMLP:
    - param_name: deepseek_v3.decoder.layers.1.mlp.shared_experts
    - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
    - 1_0_bwd_module::TERowParallelLinear:
      - param_name: deepseek_v3.decoder.layers.1.mlp.shared_experts.linear_fc2
      - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
      - name: api::_Linear
        inputs: <torch.autograd.function._LinearBackward object at 0x7f53307c78a0>, <4096x1x7168xbf16>{7168, 7168, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
        - name: api::_QuantizeFunc
          inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
          2025-10-09 13:08:38.505784 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:38.507305 %681140:0:0% te::generic_gemm(tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x2048xbf16>{2048, 1}, None:, None:, None:
        2025-10-09 13:08:38.508764 %681140:0:0% te::generic_gemm(tuple{<2048x4096x1xu8>, <32x2048xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x2048xbf16>{2048, 1}+1260859392, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x2048xbf16>{2048, 1}+1260859392, None:, None:, None:
    - name: api::SwiGLUFunction
      inputs: <torch.autograd.function.SwiGLUFunctionBackward object at 0x7f53307c7570>, <4096x2048xbf16>{2048, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
      2025-10-09 13:08:38.510196 %681140:0:0% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048
      2025-10-09 13:08:38.510450 %681140:0:0% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.510672 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.510892 %681140:0:0% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.511185 %681140:0:0% aten::rsub(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.511427 %681140:0:0% aten::mul(self:<4096x2048xbf16>{4096, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.511700 %681140:0:0% aten::add(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.511913 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.512157 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.512378 %681140:0:0% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.512583 %681140:0:0% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
      2025-10-09 13:08:38.512890 %681140:None:0% aten::cat(tensors:list{<4096x2048xbf16>{2048, 1}, <4096x2048xbf16>{2048, 1}}, dim:-1:int) -> <4096x4096xbf16>{4096, 1}
    - 1_0_bwd_module::TEColumnParallelLinear:
      - param_name: deepseek_v3.decoder.layers.1.mlp.shared_experts.linear_fc1
      - inputs: <4096x1x4096xbf16>{4096, 4096, 1}, None:
      - name: api::_Linear
        inputs: <torch.autograd.function._LinearBackward object at 0x7f53307c7240>, <4096x1x4096xbf16>{4096, 4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
        - name: api::_QuantizeFunc
          inputs: None:, <4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
          2025-10-09 13:08:38.513824 %681140:0:0% te::quantize(<4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}
        - name: api::_QuantizeFunc
          inputs: None:, <4096x1x7168xbf16>{7168, 7168, 1}, <False, True, True>:Float8BlockQuantizer
          file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
          2025-10-09 13:08:38.514418 %681140:0:0% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}
        2025-10-09 13:08:38.516307 %681140:0:0% te::generic_gemm(tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
        2025-10-09 13:08:38.518184 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <4096x7168xbf16>{7168, 1}+1275539456, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}+1275539456, None:, None:, None:
  2025-10-09 13:08:38.519672 %681140:None:0% aten::zeros(size:list{15712:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <15712x8xi64>{8, 1}
  2025-10-09 13:08:38.519982 %681140:None:0% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
  - name: api::FusedDispatch
    inputs: <torch.autograd.function.FusedDispatchBackward object at 0x7f53307c69c0>, <15712x7168xbf16>{7168, 1}, <15712x8xi64>{8, 1}, <15712x8xf32>{8, 1}, <4xi64>{1}, None:
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/fused_a2a.py
    2025-10-09 13:08:38.523523 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.523714 %681140:0:270142432% aten::record_stream(self:<15712x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.523889 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.524069 %681140:0:270142432% aten::record_stream(self:<15712xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.524240 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.524407 %681140:0:270142432% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.524574 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.524741 %681140:0:270142432% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.524905 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.525086 %681140:0:270142432% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.525252 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.525419 %681140:0:270142432% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.525587 %681140:0:270142432% aten::record_stream(self:<15712x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.525751 %681140:0:270142432% aten::record_stream(self:<15712x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
    2025-10-09 13:08:38.525918 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=9:Stream) -> None:
    2025-10-09 13:08:38.526097 %681140:0:270142432% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=0,_stream_id=0:Stream) -> None:
  2025-10-09 13:08:38.526687 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xf32>{32, 1}
  2025-10-09 13:08:38.527025 %681140:0:0% aten::scatter(self:<4096x32xf32>{32, 1}, dim:-1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
  2025-10-09 13:08:38.527429 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
  2025-10-09 13:08:38.527792 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xbool>{32, 1}
  - 1_0_bwd_module::TopKRouter:
    - param_name: deepseek_v3.decoder.layers.1.mlp.router
    - inputs: <4096x32xf32>{32, 1}, None:
    - name: api::MoEAuxLossAutoScaler
      inputs: <torch.autograd.function.MoEAuxLossAutoScalerBackward object at 0x7f53307c67a0>, <4096x32xf32>{32, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
      2025-10-09 13:08:38.528265 %681140:0:0% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
    2025-10-09 13:08:38.528583 %681140:0:0% aten::sum(self:<1xf32>{1}) -> <1xf32>{1}
    2025-10-09 13:08:38.528834 %681140:0:0% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
    - name: api::FusedAuxLoss
      inputs: <torch.autograd.function.FusedAuxLossBackward object at 0x7f53307c6580>, <1xf32>{1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
      2025-10-09 13:08:38.529197 %681140:None:0% te::fused_moe_aux_loss_bwd(Const_buf:<1xf32>{1}, tokens_per_expert:<32xi64>{1}, num_rows:4096:int, num_cols:32:int, grad_aux_loss:<1xf32>{1}) -> <4096x32xf32>{32, 1}
    2025-10-09 13:08:38.529741 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xbool>{32, 1}
    - name: api::FusedComputeScoresForMoEAuxLoss
      inputs: <torch.autograd.function.FusedComputeScoresForMoEAuxLossBackward object at 0x7f53307c6250>, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
      2025-10-09 13:08:38.530177 %681140:None:0% te::fused_score_for_moe_aux_loss_bwd(num_tokens:4096:int, num_experts:32:int, intermediate_output:<4096x32xf32>{32, 1}, grad_scores:<4096x32xf32>{32, 1}, topk:8:int, score_function:sigmoid:str) -> <4096x32xf32>{32, 1}
    2025-10-09 13:08:38.530678 %681140:None:0% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <4096x32xbool>{32, 1}
    - name: api::FusedTopkScoreFunction
      inputs: <torch.autograd.function.FusedTopkScoreFunctionBackward object at 0x7f53307c6030>, <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/router.py
      2025-10-09 13:08:38.531157 %681140:0:0% te::fused_topk_with_score_function_bwd(4096:int, 32:int, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}, <4096x32xf32>{32, 1}, 8:int, True:bool, 2_5:float, sigmoid:str) -> <4096x32xf32>{32, 1}
    2025-10-09 13:08:38.531626 %681140:0:0% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
    - name: api::RandomSTE
      inputs: <torch.autograd.function.RandomSTEBackward object at 0x7f53307c5e10>, <4096x1x32xf32>{32, 32, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
    - name: api::RouterGatingLinearFunction
      inputs: <torch.autograd.function.RouterGatingLinearFunctionBackward object at 0x7f53307c5d00>, <4096x1x32xf32>{32, 32, 1}
      file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/transformer/moe/moe_utils.py
      2025-10-09 13:08:38.532364 %681140:0:0% aten::_to_copy(self:<32x7168xbf16>{7168, 1}+1304899584, dtype:torch_float32:dtype) -> <32x7168xf32>{7168, 1}
      2025-10-09 13:08:38.533209 %681140:0:0% te::generic_gemm(<32x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xf32>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.533756 %681140:0:0% aten::_to_copy(self:<4096x7168xbf16>{7168, 1}, dtype:torch_float32:dtype) -> <4096x7168xf32>{7168, 1}
      2025-10-09 13:08:38.534526 %681140:0:0% te::generic_gemm(<4096x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, True:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32x7168xf32>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.534942 %681140:0:0% aten::_to_copy(self:<4096x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <4096x7168xbf16>{7168, 1}
      2025-10-09 13:08:38.535302 %681140:0:0% aten::_to_copy(self:<32x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <32x7168xbf16>{7168, 1}
    2025-10-09 13:08:38.535849 %681140:0:270202352% aten::add_(self:<32x7168xbf16>{7168, 1}+1304899584, other:<32x7168xbf16>{7168, 1}) -> <32x7168xbf16>{7168, 1}+1304899584
  2025-10-09 13:08:38.536317 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.decoder.layers.1.pre_mlp_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  2025-10-09 13:08:38.537295 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1305128960, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1305128960
2025-10-09 13:08:38.537709 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::MLASelfAttention:
  - param_name: deepseek_v3.decoder.layers.1.self_attention
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - 1_0_bwd_module::TERowParallelLinear:
    - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_proj
    - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f53307c5150>, <4096x1x7168xbf16>{7168, 7168, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.538614 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
      - name: api::_QuantizeFunc
        inputs: None:, <4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.539278 %681140:0:0% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.543747 %681140:0:0% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x16384xbf16>{16384, 1}, None:, None:, None:
      2025-10-09 13:08:38.548225 %681140:0:0% te::generic_gemm(tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x16384xbf16>{16384, 1}+1374802944, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x16384xbf16>{16384, 1}+1374802944, None:, None:, None:
  - 1_0_bwd_module::TEDotProductAttention:
    - param_name: deepseek_v3.decoder.layers.1.self_attention.core_attention
    - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
    - 1_0_bwd_module::UnfusedDotProductAttention:
      - param_name: deepseek_v3.decoder.layers.1.self_attention.core_attention.unfused_attention
      - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
      2025-10-09 13:08:38.549699 %681140:0:0% aten::permute(self:<4096x1x128x128xbf16>{16384, 16384, 128, 1}, dims:list{1:int, 2:int, 0:int, 3:int}) -> <1x128x4096x128xbf16>{16384, 128, 16384, 1}
      2025-10-09 13:08:38.550001 %681140:0:0% aten::transpose(self:<128x4096x4096xbf16>{16777216, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x4096xbf16>{16777216, 1, 4096}
      2025-10-09 13:08:38.554273 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 1, 4096}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
      2025-10-09 13:08:38.554501 %681140:0:0% aten::transpose(self:<128x4096x128xbf16>{128, 16384, 1}, dim0:1:int, dim1:2:int) -> <128x128x4096xbf16>{128, 1, 16384}
      2025-10-09 13:08:38.558632 %681140:0:0% aten::bmm(self:<128x4096x128xbf16>{128, 16384, 1}, mat2:<128x128x4096xbf16>{128, 1, 16384}) -> <128x4096x4096xbf16>{16777216, 4096, 1}
      2025-10-09 13:08:38.558888 %681140:0:0% aten::transpose(self:<128x4096x128xbf16>{524288, 128, 1}, dim0:0:int, dim1:1:int) -> <4096x128x128xbf16>{128, 524288, 1}
      - 1_0_bwd_module::Dropout:
        - param_name: deepseek_v3.decoder.layers.1.self_attention.core_attention.unfused_attention.attention_dropout
        - inputs: ['<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}'], {}
      - name: api::ScaledMaskedSoftmax
        inputs: <torch.autograd.function.ScaledMaskedSoftmaxBackward object at 0x7f53307c47c0>, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/attention/dot_product_attention/softmax.py
        2025-10-09 13:08:38.566382 %681140:0:0% te::scaled_masked_softmax_backward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
      2025-10-09 13:08:38.566818 %681140:0:0% aten::transpose(self:<128x192x4096xbf16>{192, 1, 24576}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{192, 24576, 1}
      2025-10-09 13:08:38.573049 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x192xbf16>{192, 24576, 1}) -> <128x4096x192xbf16>{786432, 192, 1}
      2025-10-09 13:08:38.573429 %681140:0:0% aten::mul(self:<128x4096x192xbf16>{786432, 192, 1}, other:0_1352337788608801:float) -> <128x4096x192xbf16>{786432, 192, 1}
      2025-10-09 13:08:38.573645 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
      2025-10-09 13:08:38.579769 %681140:0:0% aten::bmm(self:<128x192x4096xbf16>{192, 1, 24576}, mat2:<128x4096x4096xbf16>{16777216, 4096, 1}) -> <128x192x4096xbf16>{786432, 4096, 1}
      2025-10-09 13:08:38.580132 %681140:0:0% aten::mul(self:<128x192x4096xbf16>{786432, 4096, 1}, other:0_1352337788608801:float) -> <128x192x4096xbf16>{786432, 4096, 1}
      2025-10-09 13:08:38.580383 %681140:0:0% aten::transpose(self:<128x192x4096xbf16>{786432, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{786432, 1, 4096}
      2025-10-09 13:08:38.580604 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{786432, 1, 4096}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{1, 786432, 4096}
      2025-10-09 13:08:38.580820 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{786432, 192, 1}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{192, 786432, 1}
    2025-10-09 13:08:38.581482 %681140:None:0% aten::zeros(size:list{1:int, 1:int, 4096:int, 4096:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
  2025-10-09 13:08:38.581948 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, dim:3:int, start:0:int, end:128:int) -> <4096x1x128x128xbf16>{1, 100663296, 786432, 4096}
  2025-10-09 13:08:38.582214 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, dim:3:int, start:128:int, end:192:int) -> <4096x1x128x64xbf16>{1, 100663296, 786432, 4096}+524288
  2025-10-09 13:08:38.582797 %681140:0:0% aten::sum(self:<4096x1x128x64xbf16>{1, 100663296, 786432, 4096}+524288, dim:list{2:int}, keepdim:True:bool) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.583097 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, dim:3:int, start:0:int, end:128:int) -> <4096x1x128x128xbf16>{192, 100663296, 786432, 1}
  2025-10-09 13:08:38.583355 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, dim:3:int, start:128:int, end:192:int) -> <4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128
  2025-10-09 13:08:38.583633 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:64:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.583885 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:64:int, end:64:int) -> <4096x1x1x0xbf16>{64, 64, 64, 1}+64
  2025-10-09 13:08:38.584130 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.584408 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.584661 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}+32
  2025-10-09 13:08:38.584869 %681140:0:0% aten::neg(self:<4096x1x1x32xbf16>{64, 64, 64, 1}) -> <4096x1x1x32xbf16>{32, 32, 32, 1}
  2025-10-09 13:08:38.585168 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{64, 64, 64, 1}+32, <4096x1x1x32xbf16>{32, 32, 32, 1}}, dim:3:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.585387 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.585634 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.585911 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.586174 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}+32
  2025-10-09 13:08:38.586623 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x32xbf16>{64, 64, 64, 1}+32, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.587056 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x32xbf16>{64, 64, 64, 1}, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.587303 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.587708 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x0xbf16>{64, 64, 64, 1}+64, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:64:int, end:9223372036854775807:int, step:1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.587968 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.588257 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, dim:3:int, start:0:int, end:64:int) -> <4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128
  2025-10-09 13:08:38.588512 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, dim:3:int, start:64:int, end:64:int) -> <4096x1x128x0xbf16>{192, 100663296, 786432, 1}+192
  2025-10-09 13:08:38.588890 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.589178 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x128x32xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.589433 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x128x32xbf16>{64, 33554432, 262144, 1}+32
  2025-10-09 13:08:38.589691 %681140:0:0% aten::neg(self:<4096x1x128x32xbf16>{64, 33554432, 262144, 1}) -> <4096x1x128x32xbf16>{32, 16777216, 131072, 1}
  2025-10-09 13:08:38.590190 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{64, 33554432, 262144, 1}+32, <4096x1x128x32xbf16>{32, 16777216, 131072, 1}}, dim:3:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.590565 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.590974 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}) -> <4096x1x128x64xbf16>{8192, 33554432, 64, 1}
  2025-10-09 13:08:38.591257 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{8192, 33554432, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x128x32xbf16>{8192, 33554432, 64, 1}
  2025-10-09 13:08:38.591511 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{8192, 33554432, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x128x32xbf16>{8192, 33554432, 64, 1}+32
  2025-10-09 13:08:38.592036 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x32xbf16>{8192, 33554432, 64, 1}+32, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.592560 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x32xbf16>{8192, 33554432, 64, 1}, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.592873 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.593306 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x0xbf16>{192, 100663296, 786432, 1}+192, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:64:int, end:9223372036854775807:int, step:1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.593617 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.594813 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{1, 100663296, 786432, 4096}, <4096x1x128x128xbf16>{128, 67108864, 524288, 1}}, dim:3:int) -> <4096x1x128x256xbf16>{32768, 32768, 256, 1}
  2025-10-09 13:08:38.595818 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{192, 100663296, 786432, 1}, <4096x1x128x64xbf16>{8192, 8192, 64, 1}}, dim:3:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
  2025-10-09 13:08:38.596050 %681140:0:0% aten::squeeze(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:-2:int) -> <4096x1x64xbf16>{64, 64, 1}
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_kv_up_proj
    - inputs: <4096x1x32768xbf16>{32768, 32768, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5330943ce0>, <4096x1x32768xbf16>{32768, 32768, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.597174 %681140:0:0% te::quantize(<4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.598823 %681140:0:0% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x512xbf16>{512, 1}, None:, None:, None:
      2025-10-09 13:08:38.600412 %681140:0:0% te::generic_gemm(tuple{<512x4096xu8>, <32x512xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <32768x512xbf16>{512, 1}+1305136128, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32768x512xbf16>{512, 1}+1305136128, None:, None:, None:
      2025-10-09 13:08:38.600912 %681140:0:0% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+1321913344, 0:int, False:bool) -> <4096x512xbf16>{512, 1}, <512xbf16>{1}
    2025-10-09 13:08:38.602222 %681140:0:270202352% aten::add_(self:<512xbf16>{1}+1321913344, other:<512xbf16>{1}) -> <512xbf16>{1}+1321913344
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_q_up_proj
    - inputs: <4096x1x24576xbf16>{24576, 24576, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f53309438a0>, <4096x1x24576xbf16>{24576, 24576, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.603415 %681140:0:0% te::quantize(<4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.605578 %681140:0:0% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1536xbf16>{1536, 1}, None:, None:, None:
      2025-10-09 13:08:38.607716 %681140:0:0% te::generic_gemm(tuple{<1536x4096xu8>, <32x1536xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <24576x1536xbf16>{1536, 1}+1326042624, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <24576x1536xbf16>{1536, 1}+1326042624, None:, None:, None:
      2025-10-09 13:08:38.608252 %681140:0:0% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1363791360, 0:int, False:bool) -> <4096x1536xbf16>{1536, 1}, <1536xbf16>{1}
    2025-10-09 13:08:38.609507 %681140:0:270202352% aten::add_(self:<1536xbf16>{1}+1363791360, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1363791360
  2025-10-09 13:08:38.610006 %681140:None:0% aten::cat(tensors:list{<4096x1x512xbf16>{512, 512, 1}, <4096x1x64xbf16>{64, 64, 1}}, dim:2:int) -> <4096x1x576xbf16>{576, 576, 1}
  - 1_0_bwd_module::TELinear:
    - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_kv_down_proj
    - inputs: <4096x1x576xbf16>{576, 576, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5330943350>, <4096x1x576xbf16>{576, 576, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.610737 %681140:0:0% te::quantize(<4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.611914 %681140:0:0% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.613044 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <576x7168xbf16>{7168, 1}+1321913856, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <576x7168xbf16>{7168, 1}+1321913856, None:, None:, None:
  - 1_0_bwd_module::TELinear:
    - param_name: deepseek_v3.decoder.layers.1.self_attention.linear_q_down_proj
    - inputs: <4096x1x1536xbf16>{1536, 1536, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5330942f10>, <4096x1x1536xbf16>{1536, 1536, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.614690 %681140:0:0% te::quantize(<4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.616056 %681140:0:0% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.617376 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <1536x7168xbf16>{7168, 1}+1363792896, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <1536x7168xbf16>{7168, 1}+1363792896, None:, None:, None:
  2025-10-09 13:08:38.618630 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.decoder.layers.1.input_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  2025-10-09 13:08:38.619607 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1492243456, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1492243456
2025-10-09 13:08:38.620037 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::TransformerLayer:
  - param_name: deepseek_v3.decoder.layers.0
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
- 1_0_bwd_module::MLP:
  - param_name: deepseek_v3.decoder.layers.0.mlp
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - 1_0_bwd_module::TERowParallelLinear:
    - param_name: deepseek_v3.decoder.layers.0.mlp.linear_fc2
    - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5330942360>, <4096x1x7168xbf16>{7168, 7168, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.621096 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.626275 %681140:0:0% te::generic_gemm(tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x18432xbf16>{18432, 1}, None:, None:, None:
      2025-10-09 13:08:38.631257 %681140:0:0% te::generic_gemm(tuple{<18432x4096x1xu8>, <32x18432xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x18432xbf16>{18432, 1}+1492250624, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x18432xbf16>{18432, 1}+1492250624, None:, None:, None:
  - name: api::SwiGLUFunction
    inputs: <torch.autograd.function.SwiGLUFunctionBackward object at 0x7f5330942030>, <4096x18432xbf16>{18432, 1}
    file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/fusions/fused_bias_swiglu.py
    2025-10-09 13:08:38.632765 %681140:0:0% aten::split(self:<4096x36864xbf16>{36864, 1}, split_size:18432:int, dim:-1:int) -> <4096x18432xbf16>{36864, 1}, <4096x18432xbf16>{36864, 1}+18432
    2025-10-09 13:08:38.633391 %681140:0:0% aten::sigmoid(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.633794 %681140:0:0% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.634331 %681140:0:0% aten::sigmoid(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.634747 %681140:0:0% aten::rsub(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.635349 %681140:0:0% aten::mul(self:<4096x18432xbf16>{36864, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.635750 %681140:0:0% aten::add(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.636092 %681140:0:0% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.636625 %681140:0:0% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{36864, 1}+18432) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.637155 %681140:0:0% aten::silu(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.637475 %681140:0:0% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
    2025-10-09 13:08:38.638045 %681140:None:0% aten::cat(tensors:list{<4096x18432xbf16>{18432, 1}, <4096x18432xbf16>{18432, 1}}, dim:-1:int) -> <4096x36864xbf16>{36864, 1}
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.decoder.layers.0.mlp.linear_fc1
    - inputs: <4096x1x36864xbf16>{36864, 36864, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5330941d00>, <4096x1x36864xbf16>{36864, 36864, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x36864xbf16>{36864, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.639353 %681140:0:0% te::quantize(<4096x36864xbf16>{36864, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<36864x4096xu8>, <32x36864xf32>, <4096x36864xu8>, <288x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.648326 %681140:0:0% te::generic_gemm(tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<36864x4096xu8>, <32x36864xf32>, <4096x36864xu8>, <288x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.657188 %681140:0:0% te::generic_gemm(tuple{<7168x4096xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<36864x4096xu8>, <32x36864xf32>, <4096x36864xu8>, <288x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <36864x7168xbf16>{7168, 1}+1624371200, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <36864x7168xbf16>{7168, 1}+1624371200, None:, None:, None:
      2025-10-09 13:08:38.657902 %681140:0:0% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1888612352, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
    2025-10-09 13:08:38.659240 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+1888612352, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1888612352
2025-10-09 13:08:38.659869 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::MLASelfAttention:
  - param_name: deepseek_v3.decoder.layers.0.self_attention
  - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
  - 1_0_bwd_module::TERowParallelLinear:
    - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_proj
    - inputs: <4096x1x7168xbf16>{7168, 7168, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f5330941590>, <4096x1x7168xbf16>{7168, 7168, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.660774 %681140:0:0% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
      - name: api::_QuantizeFunc
        inputs: None:, <4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.661433 %681140:0:0% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.665908 %681140:0:0% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x16384xbf16>{16384, 1}, None:, None:, None:
      2025-10-09 13:08:38.670388 %681140:0:0% te::generic_gemm(tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x16384xbf16>{16384, 1}+1958286336, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x16384xbf16>{16384, 1}+1958286336, None:, None:, None:
  - 1_0_bwd_module::TEDotProductAttention:
    - param_name: deepseek_v3.decoder.layers.0.self_attention.core_attention
    - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
    - 1_0_bwd_module::UnfusedDotProductAttention:
      - param_name: deepseek_v3.decoder.layers.0.self_attention.core_attention.unfused_attention
      - inputs: ['<4096x1x16384xbf16>{16384, 16384, 1}'], {}
      2025-10-09 13:08:38.671856 %681140:0:0% aten::permute(self:<4096x1x128x128xbf16>{16384, 16384, 128, 1}, dims:list{1:int, 2:int, 0:int, 3:int}) -> <1x128x4096x128xbf16>{16384, 128, 16384, 1}
      2025-10-09 13:08:38.672158 %681140:0:0% aten::transpose(self:<128x4096x4096xbf16>{16777216, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x4096xbf16>{16777216, 1, 4096}
      2025-10-09 13:08:38.676426 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 1, 4096}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
      2025-10-09 13:08:38.676665 %681140:0:0% aten::transpose(self:<128x4096x128xbf16>{128, 16384, 1}, dim0:1:int, dim1:2:int) -> <128x128x4096xbf16>{128, 1, 16384}
      2025-10-09 13:08:38.680798 %681140:0:0% aten::bmm(self:<128x4096x128xbf16>{128, 16384, 1}, mat2:<128x128x4096xbf16>{128, 1, 16384}) -> <128x4096x4096xbf16>{16777216, 4096, 1}
      2025-10-09 13:08:38.681062 %681140:0:0% aten::transpose(self:<128x4096x128xbf16>{524288, 128, 1}, dim0:0:int, dim1:1:int) -> <4096x128x128xbf16>{128, 524288, 1}
      - 1_0_bwd_module::Dropout:
        - param_name: deepseek_v3.decoder.layers.0.self_attention.core_attention.unfused_attention.attention_dropout
        - inputs: ['<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}'], {}
      - name: api::ScaledMaskedSoftmax
        inputs: <torch.autograd.function.ScaledMaskedSoftmaxBackward object at 0x7f5330940c00>, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/attention/dot_product_attention/softmax.py
        2025-10-09 13:08:38.688604 %681140:0:0% te::scaled_masked_softmax_backward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
      2025-10-09 13:08:38.689040 %681140:0:0% aten::transpose(self:<128x192x4096xbf16>{192, 1, 24576}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{192, 24576, 1}
      2025-10-09 13:08:38.695260 %681140:0:0% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x192xbf16>{192, 24576, 1}) -> <128x4096x192xbf16>{786432, 192, 1}
      2025-10-09 13:08:38.695639 %681140:0:0% aten::mul(self:<128x4096x192xbf16>{786432, 192, 1}, other:0_1352337788608801:float) -> <128x4096x192xbf16>{786432, 192, 1}
      2025-10-09 13:08:38.695858 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
      2025-10-09 13:08:38.701998 %681140:0:0% aten::bmm(self:<128x192x4096xbf16>{192, 1, 24576}, mat2:<128x4096x4096xbf16>{16777216, 4096, 1}) -> <128x192x4096xbf16>{786432, 4096, 1}
      2025-10-09 13:08:38.702349 %681140:0:0% aten::mul(self:<128x192x4096xbf16>{786432, 4096, 1}, other:0_1352337788608801:float) -> <128x192x4096xbf16>{786432, 4096, 1}
      2025-10-09 13:08:38.702596 %681140:0:0% aten::transpose(self:<128x192x4096xbf16>{786432, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{786432, 1, 4096}
      2025-10-09 13:08:38.702816 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{786432, 1, 4096}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{1, 786432, 4096}
      2025-10-09 13:08:38.703045 %681140:0:0% aten::transpose(self:<128x4096x192xbf16>{786432, 192, 1}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{192, 786432, 1}
    2025-10-09 13:08:38.703708 %681140:None:0% aten::zeros(size:list{1:int, 1:int, 4096:int, 4096:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
  2025-10-09 13:08:38.704170 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, dim:3:int, start:0:int, end:128:int) -> <4096x1x128x128xbf16>{1, 100663296, 786432, 4096}
  2025-10-09 13:08:38.704435 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, dim:3:int, start:128:int, end:192:int) -> <4096x1x128x64xbf16>{1, 100663296, 786432, 4096}+524288
  2025-10-09 13:08:38.705028 %681140:0:0% aten::sum(self:<4096x1x128x64xbf16>{1, 100663296, 786432, 4096}+524288, dim:list{2:int}, keepdim:True:bool) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.705315 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, dim:3:int, start:0:int, end:128:int) -> <4096x1x128x128xbf16>{192, 100663296, 786432, 1}
  2025-10-09 13:08:38.705573 %681140:0:0% aten::slice(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, dim:3:int, start:128:int, end:192:int) -> <4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128
  2025-10-09 13:08:38.705846 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:64:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.706109 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:64:int, end:64:int) -> <4096x1x1x0xbf16>{64, 64, 64, 1}+64
  2025-10-09 13:08:38.706338 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.706611 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.706860 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}+32
  2025-10-09 13:08:38.707085 %681140:0:0% aten::neg(self:<4096x1x1x32xbf16>{64, 64, 64, 1}) -> <4096x1x1x32xbf16>{32, 32, 32, 1}
  2025-10-09 13:08:38.707376 %681140:None:0% aten::cat(tensors:list{<4096x1x1x32xbf16>{64, 64, 64, 1}+32, <4096x1x1x32xbf16>{32, 32, 32, 1}}, dim:3:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.707594 %681140:0:0% aten::mul(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.707837 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.708126 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.708389 %681140:0:0% aten::slice(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x1x32xbf16>{64, 64, 64, 1}+32
  2025-10-09 13:08:38.708841 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x32xbf16>{64, 64, 64, 1}+32, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.709291 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x32xbf16>{64, 64, 64, 1}, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.709541 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.709957 %681140:0:0% aten::slice_backward(grad_output:<4096x1x1x0xbf16>{64, 64, 64, 1}+64, input_sizes:list{4096:int, 1:int, 1:int, 64:int}, dim:3:int, start:64:int, end:9223372036854775807:int, step:1:int) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.710213 %681140:0:0% aten::add(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
  2025-10-09 13:08:38.710491 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, dim:3:int, start:0:int, end:64:int) -> <4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128
  2025-10-09 13:08:38.710747 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, dim:3:int, start:64:int, end:64:int) -> <4096x1x128x0xbf16>{192, 100663296, 786432, 1}+192
  2025-10-09 13:08:38.711133 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.711411 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x128x32xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.711664 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x128x32xbf16>{64, 33554432, 262144, 1}+32
  2025-10-09 13:08:38.711923 %681140:0:0% aten::neg(self:<4096x1x128x32xbf16>{64, 33554432, 262144, 1}) -> <4096x1x128x32xbf16>{32, 16777216, 131072, 1}
  2025-10-09 13:08:38.712438 %681140:None:0% aten::cat(tensors:list{<4096x1x128x32xbf16>{64, 33554432, 262144, 1}+32, <4096x1x128x32xbf16>{32, 16777216, 131072, 1}}, dim:3:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.712814 %681140:0:0% aten::mul(self:<4096x1x128x64xbf16>{192, 100663296, 786432, 1}+128, other:<4096x1x1x64xbf16>{64, 64, 64, 1}) -> <4096x1x128x64xbf16>{64, 33554432, 262144, 1}
  2025-10-09 13:08:38.713227 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{64, 33554432, 262144, 1}) -> <4096x1x128x64xbf16>{8192, 33554432, 64, 1}
  2025-10-09 13:08:38.713512 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{8192, 33554432, 64, 1}, dim:3:int, start:0:int, end:32:int) -> <4096x1x128x32xbf16>{8192, 33554432, 64, 1}
  2025-10-09 13:08:38.713765 %681140:0:0% aten::slice(self:<4096x1x128x64xbf16>{8192, 33554432, 64, 1}, dim:3:int, start:32:int, end:64:int) -> <4096x1x128x32xbf16>{8192, 33554432, 64, 1}+32
  2025-10-09 13:08:38.714296 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x32xbf16>{8192, 33554432, 64, 1}+32, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:1:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.714812 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x32xbf16>{8192, 33554432, 64, 1}, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:0:int, end:9223372036854775807:int, step:2:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.715124 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.715550 %681140:0:0% aten::slice_backward(grad_output:<4096x1x128x0xbf16>{192, 100663296, 786432, 1}+192, input_sizes:list{4096:int, 1:int, 128:int, 64:int}, dim:3:int, start:64:int, end:9223372036854775807:int, step:1:int) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.715861 %681140:0:0% aten::add(self:<4096x1x128x64xbf16>{8192, 8192, 64, 1}, other:<4096x1x128x64xbf16>{8192, 8192, 64, 1}) -> <4096x1x128x64xbf16>{8192, 8192, 64, 1}
  2025-10-09 13:08:38.717062 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{1, 100663296, 786432, 4096}, <4096x1x128x128xbf16>{128, 67108864, 524288, 1}}, dim:3:int) -> <4096x1x128x256xbf16>{32768, 32768, 256, 1}
  2025-10-09 13:08:38.718070 %681140:None:0% aten::cat(tensors:list{<4096x1x128x128xbf16>{192, 100663296, 786432, 1}, <4096x1x128x64xbf16>{8192, 8192, 64, 1}}, dim:3:int) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
  2025-10-09 13:08:38.718298 %681140:0:0% aten::squeeze(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:-2:int) -> <4096x1x64xbf16>{64, 64, 1}
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_kv_up_proj
    - inputs: <4096x1x32768xbf16>{32768, 32768, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f5330940050>, <4096x1x32768xbf16>{32768, 32768, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.719425 %681140:0:0% te::quantize(<4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.721075 %681140:0:0% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x512xbf16>{512, 1}, None:, None:, None:
      2025-10-09 13:08:38.722623 %681140:0:0% te::generic_gemm(tuple{<512x4096xu8>, <32x512xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <32768x512xbf16>{512, 1}+1888619520, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32768x512xbf16>{512, 1}+1888619520, None:, None:, None:
      2025-10-09 13:08:38.723141 %681140:0:0% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+1905396736, 0:int, False:bool) -> <4096x512xbf16>{512, 1}, <512xbf16>{1}
    2025-10-09 13:08:38.724460 %681140:0:270202352% aten::add_(self:<512xbf16>{1}+1905396736, other:<512xbf16>{1}) -> <512xbf16>{1}+1905396736
  - 1_0_bwd_module::TELayerNormColumnParallelLinear:
    - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_q_up_proj
    - inputs: <4096x1x24576xbf16>{24576, 24576, 1}, None:
    - name: api::_LayerNormLinear
      inputs: <torch.autograd.function._LayerNormLinearBackward object at 0x7f53308c3bd0>, <4096x1x24576xbf16>{24576, 24576, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.725637 %681140:0:0% te::quantize(<4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.727809 %681140:0:0% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1536xbf16>{1536, 1}, None:, None:, None:
      2025-10-09 13:08:38.729938 %681140:0:0% te::generic_gemm(tuple{<1536x4096xu8>, <32x1536xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <24576x1536xbf16>{1536, 1}+1909526016, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <24576x1536xbf16>{1536, 1}+1909526016, None:, None:, None:
      2025-10-09 13:08:38.730465 %681140:0:0% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1947274752, 0:int, False:bool) -> <4096x1536xbf16>{1536, 1}, <1536xbf16>{1}
    2025-10-09 13:08:38.731719 %681140:0:270202352% aten::add_(self:<1536xbf16>{1}+1947274752, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1947274752
  2025-10-09 13:08:38.732223 %681140:None:0% aten::cat(tensors:list{<4096x1x512xbf16>{512, 512, 1}, <4096x1x64xbf16>{64, 64, 1}}, dim:2:int) -> <4096x1x576xbf16>{576, 576, 1}
  - 1_0_bwd_module::TELinear:
    - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_kv_down_proj
    - inputs: <4096x1x576xbf16>{576, 576, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f53308c3680>, <4096x1x576xbf16>{576, 576, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.732957 %681140:0:0% te::quantize(<4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.734128 %681140:0:0% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.735255 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <576x7168xbf16>{7168, 1}+1905397248, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <576x7168xbf16>{7168, 1}+1905397248, None:, None:, None:
  - 1_0_bwd_module::TELinear:
    - param_name: deepseek_v3.decoder.layers.0.self_attention.linear_q_down_proj
    - inputs: <4096x1x1536xbf16>{1536, 1536, 1}, None:
    - name: api::_Linear
      inputs: <torch.autograd.function._LinearBackward object at 0x7f53308c3130>, <4096x1x1536xbf16>{1536, 1536, 1}
      file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/module/linear.py
      - name: api::_QuantizeFunc
        inputs: None:, <4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer
        file: /usr/local/lib/python3.12/dist-packages/transformer_engine/pytorch/tensor/quantized_tensor.py
        2025-10-09 13:08:38.736915 %681140:0:0% te::quantize(<4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}
      2025-10-09 13:08:38.738276 %681140:0:0% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
      2025-10-09 13:08:38.739608 %681140:0:0% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <1536x7168xbf16>{7168, 1}+1947276288, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <1536x7168xbf16>{7168, 1}+1947276288, None:, None:, None:
  2025-10-09 13:08:38.740890 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::RMSNorm:
  - param_name: deepseek_v3.decoder.layers.0.input_layernorm
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
  2025-10-09 13:08:38.741872 %681140:0:270202352% aten::add_(self:<7168xbf16>{1}+2075726848, other:<7168xbf16>{1}) -> <7168xbf16>{1}+2075726848
2025-10-09 13:08:38.742297 %681140:0:0% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::LanguageModelEmbedding:
  - param_name: deepseek_v3.embedding
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
- 1_0_bwd_module::Dropout:
  - param_name: deepseek_v3.embedding.embedding_dropout
  - inputs: ['<4096x1x7168xbf16>{7168, 7168, 1}'], {}
2025-10-09 13:08:38.742828 %681140:0:0% aten::transpose(self:<4096x1x7168xbf16>{7168, 7168, 1}, dim0:0:int, dim1:1:int) -> <1x4096x7168xbf16>{7168, 7168, 1}
- 1_0_bwd_module::VocabParallelEmbedding:
  - param_name: deepseek_v3.embedding.word_embeddings
  - inputs: ['<1x4096x7168xbf16>{7168, 7168, 1}'], {}
- name: api::_ReduceFromModelParallelRegion
  inputs: <torch.autograd.function._ReduceFromModelParallelRegionBackward object at 0x7f5348b1c6b0>, <1x4096x7168xbf16>{7168, 7168, 1}
  file: /data/data/lm_0917/Megatron-LM_0917/megatron/core/tensor_parallel/mappings.py
2025-10-09 13:08:38.744816 %681140:0:0% aten::embedding_dense_backward(grad_output:<1x4096x7168xbf16>{7168, 7168, 1}, indices:<1x4096xi64>{4096, 1}, num_weights:129280:int, padding_idx:-1:int, scale_grad_by_freq:False:bool) -> <129280x7168xbf16>{7168, 1}
2025-10-09 13:08:38.746647 %681140:0:270202352% aten::add(self:<129280x7168xbf16>{7168, 1}, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
2025-10-09 13:08:38.748569 %681140:0:270202352% aten::add_(self:<129280x7168xbf16>{7168, 1}+2075734016, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}+2075734016
2025-10-09 13:08:38.766789 %681140:0:0% aten::linalg_vector_norm(self:<3002413056xbf16>{1}) -> <1xbf16>{1}
2025-10-09 13:08:38.771292 %681140:0:0% aten::mul_(self:<3002413056xbf16>{1}, other:0_125:float) -> <3002413056xbf16>{1}
2025-10-09 13:08:38.771739 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:0:int, end:375301632:int) -> <375301632xbf16>{1}
2025-10-09 13:08:38.772080 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:375301632:int, end:750603264:int) -> <375301632xbf16>{1}+375301632
2025-10-09 13:08:38.772395 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:750603264:int, end:1125904896:int) -> <375301632xbf16>{1}+750603264
2025-10-09 13:08:38.772707 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:1125904896:int, end:1501206528:int) -> <375301632xbf16>{1}+1125904896
2025-10-09 13:08:38.773023 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:1501206528:int, end:1876508160:int) -> <375301632xbf16>{1}+1501206528
2025-10-09 13:08:38.773326 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:1876508160:int, end:2251809792:int) -> <375301632xbf16>{1}+1876508160
2025-10-09 13:08:38.773626 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:2251809792:int, end:2627111424:int) -> <375301632xbf16>{1}+2251809792
2025-10-09 13:08:38.773936 %681140:0:0% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:2627111424:int, end:3002413056:int) -> <375301632xbf16>{1}+2627111424
2025-10-09 13:08:39.476673 %681140:None:0% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<375301632xbf16>{1}}, inputs:list{<3002413056xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
2025-10-09 13:08:39.477646 %681140:0:0% aten::linalg_vector_norm(self:<352321536xbf16>{1}) -> <1xbf16>{1}
2025-10-09 13:08:39.478563 %681140:0:0% aten::mul_(self:<352321536xbf16>{1}, other:0_125:float) -> <352321536xbf16>{1}
2025-10-09 13:08:39.478953 %681140:0:0% aten::slice(self:<352321536xbf16>{1}, dim:0:int, start:0:int, end:352321536:int) -> <352321536xbf16>{1}
2025-10-09 13:08:39.479610 %681140:None:0% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<352321536xbf16>{1}}, inputs:list{<352321536xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
2025-10-09 13:08:39.480446 %681140:None:0% aten::stack(tensors:list{<32xbf16>{1}, <32xbf16>{1}}) -> <2x32xbf16>{32, 1}
2025-10-09 13:08:39.480762 %681140:None:0% aten::stack(tensors:list{<32xf32>{1}, <32xf32>{1}}) -> <2x32xf32>{32, 1}
2025-10-09 13:08:40.331811 %681140:None:0% c10d::allreduce_(tensors:list{<2x32xbf16>{32, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<2x32xbf16>{32, 1}}, Work:distributed
2025-10-09 13:08:40.332416 %681140:0:0% aten::sum(self:<2x32xbf16>{32, 1}, dim:list{-1:int}, keepdim:True:bool) -> <2x1xbf16>{1, 1}
2025-10-09 13:08:40.333086 %681140:0:0% aten::div(self:<2x1xbf16>{1, 1}, other:32:int) -> <2x1xbf16>{1, 1}
2025-10-09 13:08:40.333432 %681140:0:0% aten::sub(self:<2x1xbf16>{1, 1}, other:<2x32xbf16>{32, 1}) -> <2x32xbf16>{32, 1}
2025-10-09 13:08:40.333769 %681140:0:0% aten::sign(self:<2x32xbf16>{32, 1}) -> <2x32xbf16>{32, 1}
2025-10-09 13:08:40.334073 %681140:0:0% aten::mul(self:<2x32xbf16>{32, 1}, other:0_001:float) -> <2x32xbf16>{32, 1}
2025-10-09 13:08:40.334484 %681140:0:0% aten::add(self:<2x32xf32>{32, 1}, other:<2x32xbf16>{32, 1}) -> <2x32xf32>{32, 1}
2025-10-09 13:08:40.334789 %681140:0:0% aten::unbind(self:<2x32xf32>{32, 1}) -> <32xf32>{1}, <32xf32>{1}+32
2025-10-09 13:08:40.335030 %681140:0:0% aten::zero_(self:<32xbf16>{1}) -> <32xbf16>{1}
2025-10-09 13:08:40.335340 %681140:0:0% aten::copy_(self:<32xf32>{1}, src:<32xf32>{1}) -> <32xf32>{1}
2025-10-09 13:08:40.335519 %681140:0:0% aten::zero_(self:<32xbf16>{1}) -> <32xbf16>{1}
2025-10-09 13:08:40.335798 %681140:0:0% aten::copy_(self:<32xf32>{1}, src:<32xf32>{1}+32) -> <32xf32>{1}
2025-10-09 13:08:40.336953 %681140:None:0% c10d::allreduce_(tensors:list{<1xi32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<1xi32>{1}}, Work:distributed
2025-10-09 13:08:40.337485 %681140:0:0% aten::slice(self:<926679040xbf16>{1}, dim:0:int, start:0:int, end:375301632:int) -> <375301632xbf16>{1}
2025-10-09 13:08:40.337863 %681140:0:0% aten::slice(self:<14680064xbf16>{1}, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}
2025-10-09 13:08:40.338216 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+14680064, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+14680064
2025-10-09 13:08:40.338552 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+29360128, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+29360128
2025-10-09 13:08:40.338890 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+44040192, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+44040192
2025-10-09 13:08:40.339226 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+58720256, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+58720256
2025-10-09 13:08:40.339560 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+88080384, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+88080384
2025-10-09 13:08:40.339883 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+117440512, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+117440512
2025-10-09 13:08:40.340215 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+146800640, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+146800640
2025-10-09 13:08:40.340541 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+176160768, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+176160768
2025-10-09 13:08:40.340864 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+190840832, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+190840832
2025-10-09 13:08:40.341191 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+205520896, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+205520896
2025-10-09 13:08:40.341519 %681140:0:0% aten::slice(self:<14680064xbf16>{1}+220200960, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+220200960
2025-10-09 13:08:40.341838 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+234881024, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+234881024
2025-10-09 13:08:40.342168 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+264241152, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+264241152
2025-10-09 13:08:40.342489 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+293601280, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+293601280
2025-10-09 13:08:40.342817 %681140:0:0% aten::slice(self:<29360128xbf16>{1}+322961408, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+322961408
2025-10-09 13:08:40.343433 %681140:None:0% aten::zeros(size:list{320:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_0:device) -> <320xf32>{1}
2025-10-09 13:08:40.345837 %681140:0:0% aten::pow(self:<1xf32>{1}, exponent:2_0:float) -> <1xf32>{1}
2025-10-09 13:08:41.113858 %681140:None:0% c10d::allreduce_(tensors:list{<1xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<1xf32>{1}}, Work:distributed