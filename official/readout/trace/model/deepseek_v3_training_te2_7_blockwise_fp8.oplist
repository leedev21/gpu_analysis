================================================== trace ops ==================================================
te::get_cudnn_version 1
        %te.2_7_0:3% te::get_cudnn_version() -> 91001:int
aten::embedding 1
        %torch.2_8_0:3% aten::embedding(weight:<129280x7168xbf16>{7168, 1}+2075734016, indices:<1x4096xi64>{4096, 1}) -> <1x4096x7168xbf16>{29360128, 7168, 1}
aten::transpose 23
        %torch.2_8_0:3% aten::transpose(self:<1x4096x7168xbf16>{29360128, 7168, 1}, dim0:0:int, dim1:1:int) -> <4096x1x7168xbf16>{7168, 29360128, 1}
        %torch.2_8_0:3% aten::transpose(self:<4096x128x192xbf16>{24576, 192, 1}, dim0:0:int, dim1:1:int) -> <128x4096x192xbf16>{192, 24576, 1}
        %torch.2_8_0:3% aten::transpose(self:<128x4096x192xbf16>{192, 24576, 1}, dim0:1:int, dim1:2:int) -> <128x192x4096xbf16>{192, 1, 24576}
        %torch.2_8_0:3% aten::transpose(self:<4096x128x128xbf16>{16384, 128, 1}, dim0:0:int, dim1:1:int) -> <128x4096x128xbf16>{128, 16384, 1}
        %torch.2_8_0:3% aten::transpose(self:<15667x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x15667xbool>{1, 4}
        %torch.2_8_0:3% aten::transpose(self:<4x15667xi32>{15667, 1}, dim0:0:int, dim1:1:int) -> <15667x4xi32>{1, 15667}
        %torch.2_8_0:3% aten::transpose(self:<15699x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x15699xbool>{1, 4}
        %torch.2_8_0:3% aten::transpose(self:<4x15699xi32>{15699, 1}, dim0:0:int, dim1:1:int) -> <15699x4xi32>{1, 15699}
        %torch.2_8_0:3% aten::transpose(self:<1x4096xi64>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:3% aten::transpose(self:<4096x1xf32>{1, 1}, dim0:0:int, dim1:1:int) -> <1x4096xf32>{1, 1}
        %torch.2_8_0:3% aten::transpose(self:<1x4096xf32>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xf32>{1, 4096}
        %torch.2_8_0:3% aten::transpose(self:<128x4096x4096xbf16>{16777216, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x4096xbf16>{16777216, 1, 4096}
        %torch.2_8_0:3% aten::transpose(self:<128x4096x128xbf16>{128, 16384, 1}, dim0:1:int, dim1:2:int) -> <128x128x4096xbf16>{128, 1, 16384}
        %torch.2_8_0:3% aten::transpose(self:<128x4096x128xbf16>{524288, 128, 1}, dim0:0:int, dim1:1:int) -> <4096x128x128xbf16>{128, 524288, 1}
        %torch.2_8_0:3% aten::transpose(self:<128x192x4096xbf16>{192, 1, 24576}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{192, 24576, 1}
        %torch.2_8_0:3% aten::transpose(self:<128x192x4096xbf16>{786432, 4096, 1}, dim0:1:int, dim1:2:int) -> <128x4096x192xbf16>{786432, 1, 4096}
        %torch.2_8_0:3% aten::transpose(self:<128x4096x192xbf16>{786432, 1, 4096}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{1, 786432, 4096}
        %torch.2_8_0:3% aten::transpose(self:<128x4096x192xbf16>{786432, 192, 1}, dim0:0:int, dim1:1:int) -> <4096x128x192xbf16>{192, 786432, 1}
        %torch.2_8_0:3% aten::transpose(self:<4096x1x7168xbf16>{7168, 7168, 1}, dim0:0:int, dim1:1:int) -> <1x4096x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::transpose(self:<15957x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x15957xbool>{1, 4}
        %torch.2_8_0:3% aten::transpose(self:<4x15957xi32>{15957, 1}, dim0:0:int, dim1:1:int) -> <15957x4xi32>{1, 15957}
        %torch.2_8_0:3% aten::transpose(self:<15824x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x15824xbool>{1, 4}
        %torch.2_8_0:3% aten::transpose(self:<4x15824xi32>{15824, 1}, dim0:0:int, dim1:1:int) -> <15824x4xi32>{1, 15824}
te::rmsnorm_fwd 16
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+2075726848, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1947274752, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+1905396736, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1888612352, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1492243456, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1363791360, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+1321913344, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1305128960, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1260852224, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1260845056, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1260837888, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1158070272, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1029618176, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+987740160, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+970955776, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
        %te.2_7_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+926679040, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}
aten::cos 1
        %torch.2_8_0:3% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
aten::mul 46
        %torch.2_8_0:3% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{36864, 1}+18432) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4xf32>{1}, other:16:int) -> <4xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<32720x2048xbf16>{2048, 1}, other:<32720x2048xbf16>{4096, 1}+2048) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32720x2048xbf16>{2048, 1}, other:<32720x1xf32>{1, 1}) -> <32720x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x2048xbf16>{4096, 1}+2048) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x1xf32>{1, 1}) -> <32704x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<1x4096xf32>{4096, 1}, other:<1x4096xf32>{1, 1}) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::mul(self:<1x4096xf32>{4096, 1}, other:0_1:float) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::mul(self:<4096xf32>{1}, other:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<4096xf32>{0}, other:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1x4096xf32>{4096, 1}, other:<1xf32>{1}) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::mul(self:<1x4096xf32>{4096, 1}, other:<1x4096xf32>{4096, 1}) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::mul(self:<32704x2048xf32>{2048, 1}, other:<32704x2048xbf16>{2048, 1}) -> <32704x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32704x2048xbf16>{4096, 1}, other:<32704x2048xbf16>{2048, 1}) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32704x2048xf32>{2048, 1}, other:<32704x2048xbf16>{4096, 1}+2048) -> <32704x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32704x2048xbf16>{2048, 1}, other:<32704x2048xf32>{2048, 1}) -> <32704x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x2048xbf16>{4096, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<128x4096x192xbf16>{786432, 192, 1}, other:0_1352337788608801:float) -> <128x4096x192xbf16>{786432, 192, 1}
        %torch.2_8_0:3% aten::mul(self:<128x192x4096xbf16>{786432, 4096, 1}, other:0_1352337788608801:float) -> <128x192x4096xbf16>{786432, 4096, 1}
        %torch.2_8_0:3% aten::mul(self:<32720x2048xf32>{2048, 1}, other:<32720x2048xbf16>{2048, 1}) -> <32720x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32720x2048xbf16>{4096, 1}, other:<32720x2048xbf16>{2048, 1}) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32720x2048xf32>{2048, 1}, other:<32720x2048xbf16>{4096, 1}+2048) -> <32720x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32720x2048xbf16>{2048, 1}, other:<32720x2048xf32>{2048, 1}) -> <32720x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x18432xbf16>{36864, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::mul(self:<2x32xbf16>{32, 1}, other:0_001:float) -> <2x32xbf16>{32, 1}
        %torch.2_8_0:3% aten::mul(self:<3xf32>{1}, other:1_0:float) -> <3xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<2xf32>{1}, other:1_0:float) -> <2xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1xf32>{1}, other:1_0:float) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<33200x2048xbf16>{2048, 1}, other:<33200x2048xbf16>{4096, 1}+2048) -> <33200x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<33200x2048xbf16>{2048, 1}, other:<33200x1xf32>{1, 1}) -> <33200x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32832x2048xbf16>{2048, 1}, other:<32832x2048xbf16>{4096, 1}+2048) -> <32832x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32832x2048xbf16>{2048, 1}, other:<32832x1xf32>{1, 1}) -> <32832x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32832x2048xf32>{2048, 1}, other:<32832x2048xbf16>{2048, 1}) -> <32832x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32832x2048xbf16>{4096, 1}, other:<32832x2048xbf16>{2048, 1}) -> <32832x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32832x2048xf32>{2048, 1}, other:<32832x2048xbf16>{4096, 1}+2048) -> <32832x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<32832x2048xbf16>{2048, 1}, other:<32832x2048xf32>{2048, 1}) -> <32832x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<33200x2048xf32>{2048, 1}, other:<33200x2048xbf16>{2048, 1}) -> <33200x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<33200x2048xbf16>{4096, 1}, other:<33200x2048xbf16>{2048, 1}) -> <33200x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<33200x2048xf32>{2048, 1}, other:<33200x2048xbf16>{4096, 1}+2048) -> <33200x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<33200x2048xbf16>{2048, 1}, other:<33200x2048xf32>{2048, 1}) -> <33200x2048xf32>{2048, 1}
aten::_to_copy 77
        %torch.2_8_0:3% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32xbf16>{1}, dtype:torch_float32:dtype) -> <32xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1xi64>{1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <1xi64>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+5, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+10, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+15, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+20, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+25, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+30, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+35, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+108, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+216, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+324, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+432, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+540, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+648, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+756, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<4xf32>{1}, dtype:torch_int64:dtype) -> <4xi64>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<4x15667xbool>{15667, 1}, dtype:torch_int32:dtype) -> <4x15667xi32>{15667, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<0xf32>{1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <0xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<32720x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4x15699xbool>{15699, 1}, dtype:torch_int32:dtype) -> <4x15699xi32>{15699, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32704x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32704x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <32704x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32704x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <32704x4096xbf16>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32x7168xbf16>{7168, 1}+970726400, dtype:torch_float32:dtype) -> <32x7168xf32>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x7168xbf16>{7168, 1}, dtype:torch_float32:dtype) -> <4096x7168xf32>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <4096x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <32x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32720x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <32720x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32720x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <32720x4096xbf16>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32x7168xbf16>{7168, 1}+1304899584, dtype:torch_float32:dtype) -> <32x7168xf32>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+1260859392, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+1275539456, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<229376xbf16>{1}+1304899584, dtype:torch_float32:dtype) -> <229376xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<16777216xbf16>{1}+1305136128, dtype:torch_float32:dtype) -> <16777216xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<4128768xbf16>{1}+1321913856, dtype:torch_float32:dtype) -> <4128768xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<37748736xbf16>{1}+1326042624, dtype:torch_float32:dtype) -> <37748736xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<11010048xbf16>{1}+1363792896, dtype:torch_float32:dtype) -> <11010048xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<117440512xbf16>{1}+1374802944, dtype:torch_float32:dtype) -> <117440512xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<8955904xbf16>{1}+1492250624, dtype:torch_float32:dtype) -> <8955904xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1158070272, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1260837888, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1260845056, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1260852224, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1305128960, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<512xbf16>{1}+1321913344, dtype:torch_float32:dtype) -> <512xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1536xbf16>{1}+1363791360, dtype:torch_float32:dtype) -> <1536xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1492243456, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+44040192, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+58720256, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+88080384, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+117440512, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+146800640, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+176160768, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+190840832, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+205520896, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+220200960, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+234881024, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+264241152, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+293601280, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+322961408, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1x4096xi64>{4096, 1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device, non_blocking:True:bool) -> <1x4096xi64>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<1x4096xf32>{4096, 1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device, non_blocking:True:bool) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device, non_blocking:True:bool) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4x15957xbool>{15957, 1}, dtype:torch_int32:dtype) -> <4x15957xi32>{15957, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<33200x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <33200x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4x15824xbool>{15824, 1}, dtype:torch_int32:dtype) -> <4x15824xi32>{15824, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32832x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <32832x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32832x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <32832x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32832x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <32832x4096xbf16>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<33200x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <33200x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<33200x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <33200x4096xbf16>{4096, 1}
aten::sin 1
        %torch.2_8_0:3% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
te::quantize 91
        %te.2_7_0:3% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1947276288, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<576x7168xbf16>{7168, 1}+1905397248, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+1909526016, <True, True, False>:Float8BlockQuantizer) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<32768x512xbf16>{512, 1}+1888619520, <True, True, False>:Float8BlockQuantizer) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1958286336, <True, True, False>:Float8BlockQuantizer) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<36864x7168xbf16>{7168, 1}+1624371200, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x18432xbf16>{18432, 18432, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<18432x4096x1xu8>, <32x18432xf32>, <4096x1x18432xu8>, <144x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x18432xbf16>{18432, 1}+1492250624, <True, True, False>:Float8BlockQuantizer) -> tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1363792896, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<576x7168xbf16>{7168, 1}+1321913856, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+1326042624, <True, True, False>:Float8BlockQuantizer) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<32768x512xbf16>{512, 1}+1305136128, <True, True, False>:Float8BlockQuantizer) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1374802944, <True, True, False>:Float8BlockQuantizer) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, False, True>:Float8BlockQuantizer) -> tuple{<4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+1275539456, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<2048x4096x1xu8>, <32x2048xf32>, <4096x1x2048xu8>, <16x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+1260859392, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+322961408, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+293601280, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+264241152, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+234881024, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+220200960, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+205520896, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+190840832, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+176160768, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x14336xbf16>{14336, 14336, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<14336x4096x1xu8>, <32x14336xf32>, <4096x1x14336xu8>, <112x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x14336xbf16>{14336, 1}+1158077440, <True, True, False>:Float8BlockQuantizer) -> tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, False>:Float8BlockQuantizer) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, False>:Float8BlockQuantizer) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, False>:Float8BlockQuantizer) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, False>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, False>:Float8BlockQuantizer) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <False, True, True>:Float8BlockQuantizer) -> tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x36864xbf16>{36864, 1}, <True, True, True>:Float8BlockQuantizer) -> tuple{<36864x4096xu8>, <32x36864xf32>, <4096x36864xu8>, <288x4096xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1947276288, <True, True, False>:Float8BlockQuantizer, tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<576x7168xbf16>{7168, 1}+1905397248, <True, True, False>:Float8BlockQuantizer, tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+1909526016, <True, True, False>:Float8BlockQuantizer, tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}        %te.2_7_0:3% te::quantize(<32768x512xbf16>{512, 1}+1888619520, <True, True, False>:Float8BlockQuantizer, tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1958286336, <True, True, False>:Float8BlockQuantizer, tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<36864x7168xbf16>{7168, 1}+1624371200, <True, True, False>:Float8BlockQuantizer, tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}        %te.2_7_0:3% te::quantize(<7168x18432xbf16>{18432, 1}+1492250624, <True, True, False>:Float8BlockQuantizer, tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1363792896, <True, True, False>:Float8BlockQuantizer, tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<576x7168xbf16>{7168, 1}+1321913856, <True, True, False>:Float8BlockQuantizer, tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+1326042624, <True, True, False>:Float8BlockQuantizer, tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}        %te.2_7_0:3% te::quantize(<32768x512xbf16>{512, 1}+1305136128, <True, True, False>:Float8BlockQuantizer, tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1374802944, <True, True, False>:Float8BlockQuantizer, tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+1275539456, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+1260859392, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+322961408, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+293601280, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+264241152, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+234881024, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+220200960, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+205520896, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+190840832, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+176160768, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x14336xbf16>{14336, 1}+1158077440, <True, True, False>:Float8BlockQuantizer, tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, False>:Float8BlockQuantizer, tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, False>:Float8BlockQuantizer, tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, False>:Float8BlockQuantizer, tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, False>:Float8BlockQuantizer, tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, False>:Float8BlockQuantizer, tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, False>:Float8BlockQuantizer, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, False>:Float8BlockQuantizer, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, None:) -> tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}
te::generic_gemm 46
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x1536xbf16>{1536, 1536, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096x1xu8>, <32x7168xf32>, <4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x576xbf16>{576, 576, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x24576xbf16>{24576, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<512x4096xu8>, <32x512xf32>, <4096x512xu8>, <4x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32768xbf16>{32768, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x16384xu8>, <128x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x36864xbf16>{36864, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<18432x4096x1xu8>, <32x18432xf32>, <4096x1x18432xu8>, <144x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(<32x7168xbf16>{7168, 1}+1304899584, True:bool, <4096x7168xbf16>{7168, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32xf32>{32, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<4096x1x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x4096xbf16>{4096, 4096, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<2048x4096x1xu8>, <32x2048xf32>, <4096x1x2048xu8>, <16x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}, True:bool, tuple{<14336x4096x1xu8>, <32x14336xf32>, <4096x1x14336xu8>, <112x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(<32x7168xbf16>{7168, 1}+970726400, True:bool, <4096x7168xbf16>{7168, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x32xf32>{32, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x2048xbf16>{2048, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<2048x4096x1xu8>, <32x2048xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x2048xbf16>{2048, 1}+926686208, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x2048xbf16>{2048, 1}+926686208, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <4096x7168xbf16>{7168, 1}+941366272, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}+941366272, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(<32x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xf32>{7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(<4096x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, True:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32x7168xf32>{7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<16384x7168xu8>, <128x56xf32>, <7168x16384xu8>, <56x128xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x16384xbf16>{16384, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x16384xbf16>{16384, 1}+1040629760, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x16384xbf16>{16384, 1}+1040629760, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<512x32768xu8>, <4x256xf32>, <32768x512xu8>, <256x4xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x512xbf16>{512, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<512x4096xu8>, <32x512xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <32768x512xbf16>{512, 1}+970962944, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32768x512xbf16>{512, 1}+970962944, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<1536x24576xu8>, <12x192xf32>, <24576x1536xu8>, <192x12xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x1536xbf16>{1536, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<1536x4096xu8>, <32x1536xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <24576x1536xbf16>{1536, 1}+991869440, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <24576x1536xbf16>{1536, 1}+991869440, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x576xu8>, <56x8xf32>, <576x7168xu8>, <5x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <576x7168xbf16>{7168, 1}+987740672, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <576x7168xbf16>{7168, 1}+987740672, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x1536xu8>, <56x12xf32>, <1536x7168xu8>, <12x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <1536x7168xbf16>{7168, 1}+1029619712, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <1536x7168xbf16>{7168, 1}+1029619712, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<14336x7168xu8>, <112x56xf32>, <7168x14336xu8>, <56x112xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x14336xbf16>{14336, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<14336x4096x1xu8>, <32x14336xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x14336xbf16>{14336, 1}+1158077440, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x14336xbf16>{14336, 1}+1158077440, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<2048x4096x1xu8>, <32x2048xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x2048xbf16>{2048, 1}+1260859392, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x2048xbf16>{2048, 1}+1260859392, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<4096x4096xu8>, <32x4096xf32>, <4096x4096xu8>, <32x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <4096x7168xbf16>{7168, 1}+1275539456, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}+1275539456, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x16384xbf16>{16384, 1}+1374802944, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x16384xbf16>{16384, 1}+1374802944, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<512x4096xu8>, <32x512xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <32768x512xbf16>{512, 1}+1305136128, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32768x512xbf16>{512, 1}+1305136128, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<1536x4096xu8>, <32x1536xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <24576x1536xbf16>{1536, 1}+1326042624, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <24576x1536xbf16>{1536, 1}+1326042624, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <576x7168xbf16>{7168, 1}+1321913856, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <576x7168xbf16>{7168, 1}+1321913856, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <1536x7168xbf16>{7168, 1}+1363792896, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <1536x7168xbf16>{7168, 1}+1363792896, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<18432x7168xu8>, <144x56xf32>, <7168x18432xu8>, <56x144xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x18432xbf16>{18432, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<18432x4096x1xu8>, <32x18432xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x18432xbf16>{18432, 1}+1492250624, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x18432xbf16>{18432, 1}+1492250624, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x36864xu8>, <56x288xf32>, <36864x7168xu8>, <288x56xf32>, 2D:Mode, GEMM_READY:Format}, False:bool, tuple{<36864x4096xu8>, <32x36864xf32>, <4096x36864xu8>, <288x4096xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <4096x7168xbf16>{7168, 1}, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<36864x4096xu8>, <32x36864xf32>, <4096x36864xu8>, <288x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <36864x7168xbf16>{7168, 1}+1624371200, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <36864x7168xbf16>{7168, 1}+1624371200, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<16384x4096x1xu8>, <32x16384xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<7168x4096xu8>, <32x7168xf32>, <4096x7168xu8>, <56x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <7168x16384xbf16>{16384, 1}+1958286336, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <7168x16384xbf16>{16384, 1}+1958286336, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<512x4096xu8>, <32x512xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<32768x4096xu8>, <32x32768xf32>, <4096x32768xu8>, <256x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <32768x512xbf16>{512, 1}+1888619520, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <32768x512xbf16>{512, 1}+1888619520, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<1536x4096xu8>, <32x1536xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<24576x4096xu8>, <32x24576xf32>, <4096x24576xu8>, <192x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <24576x1536xbf16>{1536, 1}+1909526016, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <24576x1536xbf16>{1536, 1}+1909526016, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<576x4096xu8>, <32x576xf32>, <4096x576xu8>, <5x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <576x7168xbf16>{7168, 1}+1905397248, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <576x7168xbf16>{7168, 1}+1905397248, None:, None:, None:
        %te.2_7_0:3% te::generic_gemm(tuple{<7168x4096x1xu8>, <32x7168xf32>, 1D:Mode, GEMM_READY:Format}, False:bool, tuple{<1536x4096xu8>, <32x1536xf32>, <4096x1536xu8>, <12x4096xf32>, 1D:Mode, GEMM_READY:Format}, True:bool, <1536x7168xbf16>{7168, 1}+1947276288, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool, alpha:1_0:float, beta:0_0:float) -> <1536x7168xbf16>{7168, 1}+1947276288, None:, None:, None:
aten::split_with_sizes 1
        %torch.2_8_0:3% aten::split_with_sizes(self:<4096x1x576xbf16>{576, 576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> <4096x1x512xbf16>{576, 576, 1}, <4096x1x64xbf16>{576, 576, 1}+512
aten::clone 14
        %torch.2_8_0:3% aten::clone(self:<4096x1x512xbf16>{576, 512, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x512xbf16>{512, 512, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x128x192xbf16>{24576, 192, 1}) -> <4096x128x192xbf16>{24576, 192, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x1x128x128xbf16>{128, 67108864, 524288, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x128xbf16>{16384, 16384, 128, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x1x32xf32>{32, 32, 1}) -> <4096x1x32xf32>{32, 32, 1}
        %torch.2_8_0:3% aten::clone(self:<4x15667xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x15667xbool>{15667, 1}
        %torch.2_8_0:3% aten::clone(self:<4x15699xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x15699xbool>{15699, 1}
        %torch.2_8_0:3% aten::clone(self:<1x4096xi64>{4096, 1}) -> <1x4096xi64>{4096, 1}
        %torch.2_8_0:3% aten::clone(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::clone(self:<4096x1x128x192xbf16>{1, 100663296, 786432, 4096}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x1x128x192xbf16>{192, 100663296, 786432, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x128x192xbf16>{24576, 24576, 192, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x1x7168xbf16>{14336, 14336, 1}+7168, memory_format:torch_contiguous_format:memory_format) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x1x7168xbf16>{14336, 14336, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::clone(self:<4x15957xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x15957xbool>{15957, 1}
        %torch.2_8_0:3% aten::clone(self:<4x15824xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x15824xbool>{15824, 1}
aten::unsqueeze 13
        %torch.2_8_0:3% aten::unsqueeze(self:<4096x1x64xbf16>{576, 576, 1}+512, dim:-2:int) -> <4096x1x1x64xbf16>{576, 576, 64, 1}+512
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+1
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+2
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+3
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+4
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+5
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+6
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+7
        %torch.2_8_0:3% aten::unsqueeze(self:<32720xf32>{1}, dim:-1:int) -> <32720x1xf32>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<32704xf32>{1}, dim:-1:int) -> <32704x1xf32>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<33200xf32>{1}, dim:-1:int) -> <33200x1xf32>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<32832xf32>{1}, dim:-1:int) -> <32832x1xf32>{1, 1}
aten::copy_ 47
        %torch.2_8_0:3% aten::copy_(self:<4096x128x192xbf16>{24576, 192, 1}, src:<4096x128x192xbf16>{24576, 192, 1}) -> <4096x128x192xbf16>{24576, 192, 1}
        %torch.2_8_0:3% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<32xf32>{1}, src:<32xf32>{1}) -> <32xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<32xf32>{1}, src:<32xf32>{1}+32) -> <32xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<32165376xf32>{1}, src:<32165376xbf16>{1}+1125904896, non_blocking:True:bool) -> <32165376xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<102760448xf32>{1}, src:<102760448xbf16>{1}+1158077440, non_blocking:True:bool) -> <102760448xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+1260859392, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+1260859392
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+1275539456, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+1275539456
        %torch.2_8_0:3% aten::copy_(self:<229376xbf16>{1}+1304899584, src:<229376xf32>{1}) -> <229376xbf16>{1}+1304899584
        %torch.2_8_0:3% aten::copy_(self:<16777216xbf16>{1}+1305136128, src:<16777216xf32>{1}) -> <16777216xbf16>{1}+1305136128
        %torch.2_8_0:3% aten::copy_(self:<4128768xbf16>{1}+1321913856, src:<4128768xf32>{1}) -> <4128768xbf16>{1}+1321913856
        %torch.2_8_0:3% aten::copy_(self:<37748736xbf16>{1}+1326042624, src:<37748736xf32>{1}) -> <37748736xbf16>{1}+1326042624
        %torch.2_8_0:3% aten::copy_(self:<11010048xbf16>{1}+1363792896, src:<11010048xf32>{1}) -> <11010048xbf16>{1}+1363792896
        %torch.2_8_0:3% aten::copy_(self:<117440512xbf16>{1}+1374802944, src:<117440512xf32>{1}) -> <117440512xbf16>{1}+1374802944
        %torch.2_8_0:3% aten::copy_(self:<8955904xbf16>{1}+1492250624, src:<8955904xf32>{1}) -> <8955904xbf16>{1}+1492250624
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1158070272, src:<7168xf32>{1}) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1260837888, src:<7168xf32>{1}) -> <7168xbf16>{1}+1260837888
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1260845056, src:<7168xf32>{1}) -> <7168xbf16>{1}+1260845056
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1260852224, src:<7168xf32>{1}) -> <7168xbf16>{1}+1260852224
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1305128960, src:<7168xf32>{1}) -> <7168xbf16>{1}+1305128960
        %torch.2_8_0:3% aten::copy_(self:<512xbf16>{1}+1321913344, src:<512xf32>{1}) -> <512xbf16>{1}+1321913344
        %torch.2_8_0:3% aten::copy_(self:<1536xbf16>{1}+1363791360, src:<1536xf32>{1}) -> <1536xbf16>{1}+1363791360
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1492243456, src:<7168xf32>{1}) -> <7168xbf16>{1}+1492243456
        %torch.2_8_0:3% aten::copy_(self:<32165376xbf16>{1}+1125904896, src:<32165376xf32>{1}, non_blocking:True:bool) -> <32165376xbf16>{1}+1125904896
        %torch.2_8_0:3% aten::copy_(self:<102760448xbf16>{1}+1158077440, src:<102760448xf32>{1}, non_blocking:True:bool) -> <102760448xbf16>{1}+1158077440
        %torch.2_8_0:3% aten::copy_(self:<14680064xf32>{1}, src:<14680064xbf16>{1}, non_blocking:True:bool) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xf32>{1}, src:<14680064xbf16>{1}+14680064, non_blocking:True:bool) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xf32>{1}, src:<14680064xbf16>{1}+29360128, non_blocking:True:bool) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+44040192, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+58720256, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+88080384, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+117440512, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+146800640, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+146800640
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+176160768, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+176160768
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+190840832, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+190840832
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+205520896, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+205520896
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+220200960, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+220200960
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+234881024, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+234881024
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+264241152, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+264241152
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+293601280, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+293601280
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+322961408, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+322961408
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}, src:<14680064xf32>{1}, non_blocking:True:bool) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+14680064, src:<14680064xf32>{1}, non_blocking:True:bool) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+29360128, src:<14680064xf32>{1}, non_blocking:True:bool) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:3% aten::copy_(self:<6004826112xu8>{1}, src:<6004826112xu8>{1}) -> <6004826112xu8>{1}
        %torch.2_8_0:3% aten::copy_(self:<128xu8>{1}, src:<128xu8>{1}) -> <128xu8>{1}
        %torch.2_8_0:3% aten::copy_(self:<704643072xu8>{1}, src:<704643072xu8>{1}) -> <704643072xu8>{1}
aten::zero_ 19
        %torch.2_8_0:3% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
        %torch.2_8_0:3% aten::zero_(self:<32xbf16>{1}) -> <32xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<14680064xf32>{1}) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<29360128xf32>{1}) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<229376xf32>{1}) -> <229376xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<16777216xf32>{1}) -> <16777216xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<4128768xf32>{1}) -> <4128768xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<37748736xf32>{1}) -> <37748736xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<11010048xf32>{1}) -> <11010048xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<117440512xf32>{1}) -> <117440512xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<8955904xf32>{1}) -> <8955904xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<7168xf32>{1}) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<512xf32>{1}) -> <512xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<1536xf32>{1}) -> <1536xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<3xf32>{1}) -> <3xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<2xf32>{1}) -> <2xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<3002413056xbf16>{1}) -> <3002413056xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<352321536xbf16>{1}) -> <352321536xbf16>{1}
aten::arange 2
        %torch.2_8_0:3% aten::arange(start:0:int, end:8192:int, step:4096:int, dtype:torch_int32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <2xi32>{1}
        %torch.2_8_0:3% aten::arange(end:4096:int, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <4096xi32>{1}
te::get_fused_attn_backend 1
        %te.2_7_0:3% te::get_fused_attn_backend(True:bool, DType_kBFloat16:DType, DType_kBFloat16:DType, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, 0_0:float, 128:int, 128:int, 4096:int, 4096:int, 192:int, 128:int, -1:int, 0:int) -> NVTE_Fused_Attn_Backend_NVTE_No_Backend:NVTE_Fused_Attn_Backend
aten::sub 3
        %torch.2_8_0:3% aten::sub(self:<1x1x4096x1xi32>{4096, 4096, 1, 1}, other:<1x1x1x4096xi32>{4096, 4096, 4096, 1}) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::sub(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:4096:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::sub(self:<2x1xbf16>{1, 1}, other:<2x32xbf16>{32, 1}) -> <2x32xbf16>{32, 1}
aten::add 12
        %torch.2_8_0:3% aten::add(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::add(self:<129280x7168xbf16>{7168, 1}, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::add(self:<32704x2048xbf16>{2048, 1}, other:1:int) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::add(self:<32720x2048xbf16>{2048, 1}, other:1:int) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::add(self:<2x32xf32>{32, 1}, other:<2x32xbf16>{32, 1}) -> <2x32xf32>{32, 1}
        %torch.2_8_0:3% aten::add(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::add(self:<32832x2048xbf16>{2048, 1}, other:1:int) -> <32832x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add(self:<33200x2048xbf16>{2048, 1}, other:1:int) -> <33200x2048xbf16>{2048, 1}
aten::le 5
        %torch.2_8_0:3% aten::le(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::le(self:<4xi64>{1}, other:15667:int) -> <4xbool>{1}
        %torch.2_8_0:3% aten::le(self:<4xi64>{1}, other:15699:int) -> <4xbool>{1}
        %torch.2_8_0:3% aten::le(self:<4xi64>{1}, other:15957:int) -> <4xbool>{1}
        %torch.2_8_0:3% aten::le(self:<4xi64>{1}, other:15824:int) -> <4xbool>{1}
aten::lt 1
        %torch.2_8_0:3% aten::lt(self:<1x1x4096x4096xi32>{16777216, 16777216, 4096, 1}, other:0:int) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
aten::bitwise_not 1
        %torch.2_8_0:3% aten::bitwise_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
aten::bitwise_and 1
        %torch.2_8_0:3% aten::bitwise_and(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
aten::logical_not 1
        %torch.2_8_0:3% aten::logical_not(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
aten::logical_or 1
        %torch.2_8_0:3% aten::logical_or(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, other:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
aten::baddbmm 1
        %torch.2_8_0:3% aten::baddbmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, batch1:<128x4096x192xbf16>{192, 24576, 1}, batch2:<128x192x4096xbf16>{192, 1, 24576}, beta:0_0:float, alpha:0_1352337788608801:float) -> <128x4096x4096xbf16>{16777216, 4096, 1}
te::scaled_masked_softmax_forward 1
        %te.2_7_0:3% te::scaled_masked_softmax_forward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
aten::eq 1
        %torch.2_8_0:3% aten::eq(self:<5056xu8>{1}, other:<5056xu8>{1}) -> <5056xbool>{1}
aten::all 2
        %torch.2_8_0:3% aten::all(self:<5056xbool>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::all(self:<4xbool>{1}) -> <1xbool>{1}
aten::bmm 5
        %torch.2_8_0:3% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
        %torch.2_8_0:3% aten::bmm(self:<128x4096x4096xbf16>{16777216, 1, 4096}, mat2:<128x4096x128xbf16>{128, 16384, 1}) -> <128x4096x128xbf16>{524288, 128, 1}
        %torch.2_8_0:3% aten::bmm(self:<128x4096x128xbf16>{128, 16384, 1}, mat2:<128x128x4096xbf16>{128, 1, 16384}) -> <128x4096x4096xbf16>{16777216, 4096, 1}
        %torch.2_8_0:3% aten::bmm(self:<128x4096x4096xbf16>{16777216, 4096, 1}, mat2:<128x4096x192xbf16>{192, 24576, 1}) -> <128x4096x192xbf16>{786432, 192, 1}
        %torch.2_8_0:3% aten::bmm(self:<128x192x4096xbf16>{192, 1, 24576}, mat2:<128x4096x4096xbf16>{16777216, 4096, 1}) -> <128x192x4096xbf16>{786432, 4096, 1}
aten::permute 2
        %torch.2_8_0:3% aten::permute(self:<1x128x4096x128xbf16>{67108864, 524288, 128, 1}, dims:list{2:int, 0:int, 1:int, 3:int}) -> <4096x1x128x128xbf16>{128, 67108864, 524288, 1}
        %torch.2_8_0:3% aten::permute(self:<4096x1x128x128xbf16>{16384, 16384, 128, 1}, dims:list{1:int, 2:int, 0:int, 3:int}) -> <1x128x4096x128xbf16>{16384, 128, 16384, 1}
aten::split 8
        %torch.2_8_0:3% aten::split(self:<4096x36864xbf16>{36864, 1}, split_size:18432:int, dim:-1:int) -> <4096x18432xbf16>{36864, 1}, <4096x18432xbf16>{36864, 1}+18432
        %torch.2_8_0:3% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048
        %torch.2_8_0:3% aten::split(self:<32720x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32720x2048xbf16>{4096, 1}, <32720x2048xbf16>{4096, 1}+2048
        %torch.2_8_0:3% aten::split(self:<4096x1x7168xbf16>{7168, 7168, 1}, split_size:4096:int) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::split(self:<32704x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32704x2048xbf16>{4096, 1}, <32704x2048xbf16>{4096, 1}+2048
        %torch.2_8_0:3% aten::split(self:<8192x1x7168xbf16>{7168, 7168, 1}, split_size:4096:int) -> <4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}+29360128
        %torch.2_8_0:3% aten::split(self:<33200x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <33200x2048xbf16>{4096, 1}, <33200x2048xbf16>{4096, 1}+2048
        %torch.2_8_0:3% aten::split(self:<32832x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> <32832x2048xbf16>{4096, 1}, <32832x2048xbf16>{4096, 1}+2048
aten::silu 6
        %torch.2_8_0:3% aten::silu(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::silu(self:<32720x2048xbf16>{4096, 1}) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::silu(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::silu(self:<33200x2048xbf16>{4096, 1}) -> <33200x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::silu(self:<32832x2048xbf16>{4096, 1}) -> <32832x2048xbf16>{2048, 1}
aten::normal_ 4
        %torch.2_8_0:3% aten::normal_(self:<4096x1x32xf32>{32, 32, 1}, generator:%252:tuple{seed=45:i32, offset=None:NoneType}) -> <4096x1x32xf32>{32, 32, 1}
        %torch.2_8_0:3% aten::normal_(self:<4096x1x32xf32>{32, 32, 1}, generator:%611:tuple{seed=45:i32, offset=None:NoneType}) -> <4096x1x32xf32>{32, 32, 1}
        %torch.2_8_0:3% aten::normal_(self:<4096x1x32xf32>{32, 32, 1}, generator:%1512:tuple{seed=45:i32, offset=None:NoneType}) -> <4096x1x32xf32>{32, 32, 1}
        %torch.2_8_0:3% aten::normal_(self:<4096x1x32xf32>{32, 32, 1}, generator:%1665:tuple{seed=45:i32, offset=None:NoneType}) -> <4096x1x32xf32>{32, 32, 1}
te::fused_topk_with_score_function_fwd 1
        %te.2_7_0:3% te::fused_topk_with_score_function_fwd(<4096x32xf32>{32, 1}, 8:int, True:bool, 2:int, 1:int, 2_5:float, sigmoid:str, <32xf32>{1}) -> <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
te::fused_score_for_moe_aux_loss_fwd 1
        %te.2_7_0:3% te::fused_score_for_moe_aux_loss_fwd(logits:<4096x32xf32>{32, 1}, topk:8:int, score_function:sigmoid:str) -> <4096x32xf32>{32, 1}, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}
aten::sum 13
        %torch.2_8_0:3% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<4xi64>{1}) -> <1xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<1x4096xi64>{4096, 1}) -> <1xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<1x4096xf32>{4096, 1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<4096xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<32704x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <32704x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<32720x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <32720x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<2x32xbf16>{32, 1}, dim:list{-1:int}, keepdim:True:bool) -> <2x1xbf16>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<3xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<2xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<32832x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <32832x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<33200x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <33200x1xf32>{1, 1}
te::fused_moe_aux_loss_fwd 1
        %te.2_7_0:3% te::fused_moe_aux_loss_fwd(probs:<4096x32xf32>{32, 1}, tokens_per_expert:<32xi64>{1}, total_num_tokens:4096:int, num_experts:32:int, num_rows:4096:int, num_cols:32:int, topk:8:int, coeff:0_0001:float) -> <1xf32>{1}, <1xf32>{1}
aten::div 8
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:0_0001:float) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<4xi64>{1}, other:16:int) -> <4xf32>{1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<1x4096xf32>{4096, 1}, other:<1xf32>{1}) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::div(self:<2xf32>{1}, other:8:int) -> <2xf32>{1}
        %torch.2_8_0:3% aten::div(self:<2x1xbf16>{1, 1}, other:32:int) -> <2x1xbf16>{1, 1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:2:int) -> <1xf32>{1}
aten::zeros 27
        %torch.2_8_0:3% aten::zeros(size:list{3:int}, device:cuda_3:device, pin_memory:False:bool) -> <3xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{8:int}, dtype:torch_int64:dtype, device:cuda:device, pin_memory:False:bool) -> <8xi64>{1}
        %torch.2_8_0:3% aten::zeros(size:list{4096:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{12288:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <12288xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{1:int}, device:cuda_3:device, pin_memory:False:bool) -> <1xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{1:int, 4096:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <1x4096xi64>{4096, 1}
        %torch.2_8_0:3% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
        %torch.2_8_0:3% aten::zeros(size:list{15699:int, 9:int}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15699x9xi32>{9, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15699:int, 4:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15699x4xbool>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15699:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15699x8xi64>{8, 1}
        %torch.2_8_0:3% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:3% aten::zeros(size:list{1:int, 1:int, 4096:int, 4096:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15667:int, 9:int}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15667x9xi32>{9, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15667:int, 4:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15667x4xbool>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15667:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15667x8xi64>{8, 1}
        %torch.2_8_0:3% aten::zeros(size:list{320:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <320xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{}, dtype:torch_float32:dtype, device:cpu:device, pin_memory:False:bool) -> <1xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{2:int}, device:cuda:device, pin_memory:False:bool) -> <2xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{8:int, 24:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <8x24xf32>{24, 1}
        %torch.2_8_0:3% aten::zeros(size:list{}, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <1xi32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{15824:int, 9:int}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15824x9xi32>{9, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15824:int, 4:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15824x4xbool>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15824:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15824x8xi64>{8, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15957:int, 9:int}, dtype:torch_int32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15957x9xi32>{9, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15957:int, 4:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15957x4xbool>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{15957:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <15957x8xi64>{8, 1}
aten::add_ 22
        %torch.2_8_0:3% aten::add_(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::add_(self:<32xbf16>{1}, other:<32xi64>{1}) -> <32xbf16>{1}
        %torch.2_8_0:3% aten::add_(self:<1xi32>{1}, other:<1xi32>{1}) -> <1xi32>{1}
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+926679040, other:<7168xbf16>{1}) -> <7168xbf16>{1}+926679040
        %torch.2_8_0:3% aten::add_(self:<32x7168xbf16>{7168, 1}+970726400, other:<32x7168xbf16>{7168, 1}) -> <32x7168xbf16>{7168, 1}+970726400
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+970955776, other:<7168xbf16>{1}) -> <7168xbf16>{1}+970955776
        %torch.2_8_0:3% aten::add_(self:<512xbf16>{1}+987740160, other:<512xbf16>{1}) -> <512xbf16>{1}+987740160
        %torch.2_8_0:3% aten::add_(self:<1536xbf16>{1}+1029618176, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1029618176
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1158070272, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1260837888, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1260837888
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1260845056, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1260845056
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1260852224, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1260852224
        %torch.2_8_0:3% aten::add_(self:<32x7168xbf16>{7168, 1}+1304899584, other:<32x7168xbf16>{7168, 1}) -> <32x7168xbf16>{7168, 1}+1304899584
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1305128960, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1305128960
        %torch.2_8_0:3% aten::add_(self:<512xbf16>{1}+1321913344, other:<512xbf16>{1}) -> <512xbf16>{1}+1321913344
        %torch.2_8_0:3% aten::add_(self:<1536xbf16>{1}+1363791360, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1363791360
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1492243456, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1492243456
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1888612352, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1888612352
        %torch.2_8_0:3% aten::add_(self:<512xbf16>{1}+1905396736, other:<512xbf16>{1}) -> <512xbf16>{1}+1905396736
        %torch.2_8_0:3% aten::add_(self:<1536xbf16>{1}+1947274752, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1947274752
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+2075726848, other:<7168xbf16>{1}) -> <7168xbf16>{1}+2075726848
        %torch.2_8_0:3% aten::add_(self:<129280x7168xbf16>{7168, 1}+2075734016, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}+2075734016
aten::topk 1
        %torch.2_8_0:3% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int) -> <4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}
c10d::allgather_ 3
        %torch.2_8_0:3% c10d::allgather_(output_tensors:list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, input_tensors:list{<1xi64>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, Work:distributed
        %torch.2_8_0:3% c10d::allgather_(output_tensors:list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, input_tensors:list{<5xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, Work:distributed
        %torch.2_8_0:3% c10d::allgather_(output_tensors:list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, input_tensors:list{<108xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, Work:distributed
aten::gt 31
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+1, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+2, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+3, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+4, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+5, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+6, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+7, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+1, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+2, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+3, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+4, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+5, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+6, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+7, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+8, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+9, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+10, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+11, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+12, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+13, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+14, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+15, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+16, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+17, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+18, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+19, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+20, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+21, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+22, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+23, other:0_0:float) -> <8xbool>{1}
aten::resize_ 2
        %torch.2_8_0:3% aten::resize_(self:<5xu8>{1}, size:list{5:int}) -> <5xu8>{1}
        %torch.2_8_0:3% aten::resize_(self:<108xu8>{1}, size:list{108:int}) -> <108xu8>{1}
aten::slice 66
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:0:int, end:5:int) -> <5xu8>{1}
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:5:int, end:10:int) -> <5xu8>{1}+5
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:10:int, end:15:int) -> <5xu8>{1}+10
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:15:int, end:20:int) -> <5xu8>{1}+15
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:20:int, end:25:int) -> <5xu8>{1}+20
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:25:int, end:30:int) -> <5xu8>{1}+25
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:30:int, end:35:int) -> <5xu8>{1}+30
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:35:int, end:40:int) -> <5xu8>{1}+35
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:0:int, end:108:int) -> <108xu8>{1}
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:108:int, end:216:int) -> <108xu8>{1}+108
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:216:int, end:324:int) -> <108xu8>{1}+216
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:324:int, end:432:int) -> <108xu8>{1}+324
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:432:int, end:540:int) -> <108xu8>{1}+432
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:540:int, end:648:int) -> <108xu8>{1}+540
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:648:int, end:756:int) -> <108xu8>{1}+648
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:756:int, end:864:int) -> <108xu8>{1}+756
        %torch.2_8_0:3% aten::slice(self:<8192x1x7168xbf16>{7168, 7168, 1}, dim:0:int, start:0:int, end:4096:int) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::slice(self:<8192x1x7168xbf16>{7168, 7168, 1}, dim:0:int, start:4096:int, end:8192:int) -> <4096x1x7168xbf16>{7168, 7168, 1}+29360128
        %torch.2_8_0:3% aten::slice(self:<4096x1x14336xbf16>{14336, 14336, 1}, dim:2:int, start:0:int, end:7168:int) -> <4096x1x7168xbf16>{14336, 14336, 1}
        %torch.2_8_0:3% aten::slice(self:<4096x1x14336xbf16>{14336, 14336, 1}, dim:2:int, start:7168:int, end:14336:int) -> <4096x1x7168xbf16>{14336, 14336, 1}+7168
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:0:int, end:375301632:int) -> <375301632xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:375301632:int, end:750603264:int) -> <375301632xbf16>{1}+375301632
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:750603264:int, end:1125904896:int) -> <375301632xbf16>{1}+750603264
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:1125904896:int, end:1501206528:int) -> <375301632xbf16>{1}+1125904896
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:1501206528:int, end:1876508160:int) -> <375301632xbf16>{1}+1501206528
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:1876508160:int, end:2251809792:int) -> <375301632xbf16>{1}+1876508160
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:2251809792:int, end:2627111424:int) -> <375301632xbf16>{1}+2251809792
        %torch.2_8_0:3% aten::slice(self:<3002413056xbf16>{1}, dim:0:int, start:2627111424:int, end:3002413056:int) -> <375301632xbf16>{1}+2627111424
        %torch.2_8_0:3% aten::slice(self:<352321536xbf16>{1}, dim:0:int, start:0:int, end:352321536:int) -> <352321536xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<117440512xbf16>{1}+1040629760, dim:0:int, start:85275136:int, end:117440512:int) -> <32165376xbf16>{1}+1125904896
        %torch.2_8_0:3% aten::slice(self:<102760448xbf16>{1}+1158077440, dim:0:int, start:0:int, end:102760448:int) -> <102760448xbf16>{1}+1158077440
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+1260859392, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+1260859392
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+1275539456, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+1275539456
        %torch.2_8_0:3% aten::slice(self:<229376xbf16>{1}+1304899584, dim:0:int, start:0:int, end:229376:int) -> <229376xbf16>{1}+1304899584
        %torch.2_8_0:3% aten::slice(self:<16777216xbf16>{1}+1305136128, dim:0:int, start:0:int, end:16777216:int) -> <16777216xbf16>{1}+1305136128
        %torch.2_8_0:3% aten::slice(self:<4128768xbf16>{1}+1321913856, dim:0:int, start:0:int, end:4128768:int) -> <4128768xbf16>{1}+1321913856
        %torch.2_8_0:3% aten::slice(self:<37748736xbf16>{1}+1326042624, dim:0:int, start:0:int, end:37748736:int) -> <37748736xbf16>{1}+1326042624
        %torch.2_8_0:3% aten::slice(self:<11010048xbf16>{1}+1363792896, dim:0:int, start:0:int, end:11010048:int) -> <11010048xbf16>{1}+1363792896
        %torch.2_8_0:3% aten::slice(self:<117440512xbf16>{1}+1374802944, dim:0:int, start:0:int, end:117440512:int) -> <117440512xbf16>{1}+1374802944
        %torch.2_8_0:3% aten::slice(self:<132120576xbf16>{1}+1492250624, dim:0:int, start:0:int, end:8955904:int) -> <8955904xbf16>{1}+1492250624
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1158070272, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1260837888, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1260837888
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1260845056, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1260845056
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1260852224, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1260852224
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1305128960, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1305128960
        %torch.2_8_0:3% aten::slice(self:<512xbf16>{1}+1321913344, dim:0:int, start:0:int, end:512:int) -> <512xbf16>{1}+1321913344
        %torch.2_8_0:3% aten::slice(self:<1536xbf16>{1}+1363791360, dim:0:int, start:0:int, end:1536:int) -> <1536xbf16>{1}+1363791360
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1492243456, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1492243456
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+14680064, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+29360128, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+44040192, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+58720256, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+88080384, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+117440512, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+146800640, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+146800640
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+176160768, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+176160768
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+190840832, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+190840832
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+205520896, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+205520896
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+220200960, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+220200960
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+234881024, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+234881024
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+264241152, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+264241152
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+293601280, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+293601280
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+322961408, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+322961408
        %torch.2_8_0:3% aten::slice(self:<24xf32>{1}+72, dim:0:int, start:0:int, end:9223372036854775807:int) -> <24xf32>{1}+72
        %torch.2_8_0:3% aten::slice(self:<8x24xf32>{24, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x24xf32>{24, 1}
aten::record_stream 50
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15667x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15699x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15957x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<15824x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
aten::ceil 1
        %torch.2_8_0:3% aten::ceil(self:<4xf32>{1}) -> <4xf32>{1}
te::split_quantize 16
        %te.2_7_0:3% te::split_quantize(<32720x7168xbf16>{7168, 1}, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<7168x8112xu8>, <64x7168xf32>, <8112x7168xu8>, <56x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32720x2048xbf16>{2048, 1}, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<2048x8112xu8>, <64x2048xf32>, <8112x2048xu8>, <16x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, <8224x2048xu8>, <16x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, <8160x2048xu8>, <16x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, <8224x2048xu8>, <16x8224xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32704x7168xbf16>{7168, 1}, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8144xu8>, <64x7168xf32>, <8144x7168xu8>, <56x8144xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32704x2048xbf16>{2048, 1}, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<2048x8192xu8>, <64x2048xf32>, <8192x2048xu8>, <16x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8176xu8>, <64x2048xf32>, <8176x2048xu8>, <16x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8192xu8>, <64x2048xf32>, <8192x2048xu8>, <16x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8144xu8>, <64x2048xf32>, <8144x2048xu8>, <16x8144xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32704x7168xbf16>{7168, 1}, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8144xu8>, <64x7168xf32>, <8144x7168xu8>, <56x8144xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32704x4096xbf16>{4096, 1}, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<4096x8192xu8>, <64x4096xf32>, <8192x4096xu8>, <32x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8176xu8>, <64x4096xf32>, <8176x4096xu8>, <32x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8192xu8>, <64x4096xf32>, <8192x4096xu8>, <32x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8144xu8>, <64x4096xf32>, <8144x4096xu8>, <32x8144xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32720x7168xbf16>{7168, 1}, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<7168x8112xu8>, <64x7168xf32>, <8112x7168xu8>, <56x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32720x4096xbf16>{4096, 1}, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<4096x8112xu8>, <64x4096xf32>, <8112x4096xu8>, <32x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<33200x7168xbf16>{7168, 1}, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<7168x8336xu8>, <66x7168xf32>, <8336x7168xu8>, <56x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8272xu8>, <65x7168xf32>, <8272x7168xu8>, <56x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8304xu8>, <65x7168xf32>, <8304x7168xu8>, <56x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<33200x2048xbf16>{2048, 1}, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<2048x8336xu8>, <66x2048xf32>, <8336x2048xu8>, <16x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8272xu8>, <65x2048xf32>, <8272x2048xu8>, <16x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8304xu8>, <65x2048xf32>, <8304x2048xu8>, <16x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8288xu8>, <65x2048xf32>, <8288x2048xu8>, <16x8288xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32832x7168xbf16>{7168, 1}, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32832x2048xbf16>{2048, 1}, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> tuple{<2048x8240xu8>, <65x2048xf32>, <8240x2048xu8>, <16x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, <8208x2048xu8>, <16x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, <8224x2048xu8>, <16x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, <8160x2048xu8>, <16x8160xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32832x7168xbf16>{7168, 1}, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<32832x4096xbf16>{4096, 1}, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<4096x8240xu8>, <65x4096xf32>, <8240x4096xu8>, <32x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<33200x7168xbf16>{7168, 1}, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<7168x8336xu8>, <66x7168xf32>, <8336x7168xu8>, <56x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8272xu8>, <65x7168xf32>, <8272x7168xu8>, <56x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8304xu8>, <65x7168xf32>, <8304x7168xu8>, <56x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}
        %te.2_7_0:3% te::split_quantize(<33200x4096xbf16>{4096, 1}, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> tuple{<4096x8336xu8>, <66x4096xf32>, <8336x4096xu8>, <32x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8272xu8>, <65x4096xf32>, <8272x4096xu8>, <32x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8304xu8>, <65x4096xf32>, <8304x4096xu8>, <32x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8288xu8>, <65x4096xf32>, <8288x4096xu8>, <32x8288xf32>, 1D:Mode, GEMM_READY:Format}
te::get_num_cublas_streams 1
        %te.2_7_0:3% te::get_num_cublas_streams() -> 4:int
te::te_general_grouped_gemm 24
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<7168x8112xu8>, <64x7168xf32>, <8112x7168xu8>, <56x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32720x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<2048x8112xu8>, <64x2048xf32>, <8112x2048xu8>, <16x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, <8224x2048xu8>, <16x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, <8160x2048xu8>, <16x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, <8224x2048xu8>, <16x8224xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32720x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8144xu8>, <64x7168xf32>, <8144x7168xu8>, <56x8144xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<2048x8192xu8>, <64x2048xf32>, <8192x2048xu8>, <16x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8176xu8>, <64x2048xf32>, <8176x2048xu8>, <16x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8192xu8>, <64x2048xf32>, <8192x2048xu8>, <16x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8144xu8>, <64x2048xf32>, <8144x2048xu8>, <16x8144xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8144xu8>, <64x7168xf32>, <8144x7168xu8>, <56x8144xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x8192xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8176xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8192xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8144xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, <8176x7168xu8>, <56x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8192xu8>, <64x7168xf32>, <8192x7168xu8>, <56x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8144xu8>, <64x7168xf32>, <8144x7168xu8>, <56x8144xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<7168x2048xbf16>{2048, 1}+44040192, <7168x2048xbf16>{2048, 1}+29360128, <7168x2048xbf16>{2048, 1}+14680064, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8192xu8>, <64x4096xf32>, <8192x4096xu8>, <32x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8176xu8>, <64x4096xf32>, <8176x4096xu8>, <32x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8192xu8>, <64x4096xf32>, <8192x4096xu8>, <32x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8144xu8>, <64x4096xf32>, <8144x4096xu8>, <32x8144xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32704x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x8192xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8176xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8192xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8144xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8192xu8>, <64x4096xf32>, <8192x4096xu8>, <32x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8176xu8>, <64x4096xf32>, <8176x4096xu8>, <32x8176xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8192xu8>, <64x4096xf32>, <8192x4096xu8>, <32x8192xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8144xu8>, <64x4096xf32>, <8144x4096xu8>, <32x8144xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<4096x7168xbf16>{7168, 1}+146800640, <4096x7168xbf16>{7168, 1}+117440512, <4096x7168xbf16>{7168, 1}+88080384, <4096x7168xbf16>{7168, 1}+58720256}, DType_kBFloat16:DType, list{8192:int, 8176:int, 8192:int, 8144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8112xu8>, <64x7168xf32>, <8112x7168xu8>, <56x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32720x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x8112xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8112xu8>, <64x7168xf32>, <8112x7168xu8>, <56x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<7168x2048xbf16>{2048, 1}+220200960, <7168x2048xbf16>{2048, 1}+205520896, <7168x2048xbf16>{2048, 1}+190840832, <7168x2048xbf16>{2048, 1}+176160768}, DType_kBFloat16:DType, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8112xu8>, <64x4096xf32>, <8112x4096xu8>, <32x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32720x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x8112xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8112xu8>, <64x4096xf32>, <8112x4096xu8>, <32x8112xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<4096x7168xbf16>{7168, 1}+322961408, <4096x7168xbf16>{7168, 1}+293601280, <4096x7168xbf16>{7168, 1}+264241152, <4096x7168xbf16>{7168, 1}+234881024}, DType_kBFloat16:DType, list{8112:int, 8224:int, 8160:int, 8224:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<7168x8336xu8>, <66x7168xf32>, <8336x7168xu8>, <56x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8272xu8>, <65x7168xf32>, <8272x7168xu8>, <56x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8304xu8>, <65x7168xf32>, <8304x7168xu8>, <56x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<33200x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<2048x8336xu8>, <66x2048xf32>, <8336x2048xu8>, <16x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8272xu8>, <65x2048xf32>, <8272x2048xu8>, <16x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8304xu8>, <65x2048xf32>, <8304x2048xu8>, <16x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8288xu8>, <65x2048xf32>, <8288x2048xu8>, <16x8288xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<33200x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32832x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, True:bool, list{tuple{<2048x8240xu8>, <65x2048xf32>, <8240x2048xu8>, <16x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, <8208x2048xu8>, <16x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, <8224x2048xu8>, <16x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, <8160x2048xu8>, <16x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32832x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32832x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x8240xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8208xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8224xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8160xu8>, <64x2048xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8240xu8>, <65x7168xf32>, <8240x7168xu8>, <56x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, <8208x7168xu8>, <56x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, <8224x7168xu8>, <56x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, <8160x7168xu8>, <56x8160xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<7168x2048xbf16>{2048, 1}+44040192, <7168x2048xbf16>{2048, 1}+29360128, <7168x2048xbf16>{2048, 1}+14680064, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8240xu8>, <65x4096xf32>, <8240x4096xu8>, <32x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<32832x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x8240xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8208xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8224xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8160xu8>, <64x7168xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8240xu8>, <65x4096xf32>, <8240x4096xu8>, <32x8240xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8208xu8>, <65x4096xf32>, <8208x4096xu8>, <32x8208xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8224xu8>, <65x4096xf32>, <8224x4096xu8>, <32x8224xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8160xu8>, <64x4096xf32>, <8160x4096xu8>, <32x8160xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<4096x7168xbf16>{7168, 1}+146800640, <4096x7168xbf16>{7168, 1}+117440512, <4096x7168xbf16>{7168, 1}+88080384, <4096x7168xbf16>{7168, 1}+58720256}, DType_kBFloat16:DType, list{8240:int, 8208:int, 8224:int, 8160:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<2048x7168xu8>, <16x56xf32>, <7168x2048xu8>, <56x16xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8336xu8>, <66x7168xf32>, <8336x7168xu8>, <56x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8272xu8>, <65x7168xf32>, <8272x7168xu8>, <56x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8304xu8>, <65x7168xf32>, <8304x7168xu8>, <56x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<33200x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<2048x8336xu8>, <66x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8272xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8304xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<2048x8288xu8>, <65x2048xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<7168x8336xu8>, <66x7168xf32>, <8336x7168xu8>, <56x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8272xu8>, <65x7168xf32>, <8272x7168xu8>, <56x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8304xu8>, <65x7168xf32>, <8304x7168xu8>, <56x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8288xu8>, <65x7168xf32>, <8288x7168xu8>, <56x8288xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<7168x2048xbf16>{2048, 1}+220200960, <7168x2048xbf16>{2048, 1}+205520896, <7168x2048xbf16>{2048, 1}+190840832, <7168x2048xbf16>{2048, 1}+176160768}, DType_kBFloat16:DType, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}, tuple{<7168x4096xu8>, <56x32xf32>, <4096x7168xu8>, <32x56xf32>, 2D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8336xu8>, <66x4096xf32>, <8336x4096xu8>, <32x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8272xu8>, <65x4096xf32>, <8272x4096xu8>, <32x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8304xu8>, <65x4096xf32>, <8304x4096xu8>, <32x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8288xu8>, <65x4096xf32>, <8288x4096xu8>, <32x8288xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{<33200x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
        %te.2_7_0:3% te::te_general_grouped_gemm(list{tuple{<7168x8336xu8>, <66x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8272xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8304xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<7168x8288xu8>, <65x7168xf32>, 1D:Mode, GEMM_READY:Format}}, False:bool, list{tuple{<4096x8336xu8>, <66x4096xf32>, <8336x4096xu8>, <32x8336xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8272xu8>, <65x4096xf32>, <8272x4096xu8>, <32x8272xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8304xu8>, <65x4096xf32>, <8304x4096xu8>, <32x8304xf32>, 1D:Mode, GEMM_READY:Format}, tuple{<4096x8288xu8>, <65x4096xf32>, <8288x4096xu8>, <32x8288xf32>, 1D:Mode, GEMM_READY:Format}}, True:bool, list{<4096x7168xbf16>{7168, 1}+322961408, <4096x7168xbf16>{7168, 1}+293601280, <4096x7168xbf16>{7168, 1}+264241152, <4096x7168xbf16>{7168, 1}+234881024}, DType_kBFloat16:DType, list{8336:int, 8272:int, 8304:int, 8288:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> <0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}
aten::roll 2
        %torch.2_8_0:3% aten::roll(self:<1x4096xi64>{4096, 1}, shifts:list{-1:int}, dims:list{-1:int}) -> <1x4096xi64>{4096, 1}
        %torch.2_8_0:3% aten::roll(self:<1x4096xf32>{4096, 1}, shifts:list{-1:int}, dims:list{-1:int}) -> <1x4096xf32>{4096, 1}
aten::fill_ 2
        %torch.2_8_0:3% aten::fill_(self:<1xi64>{4096}+4095, value:0:int) -> <1xi64>{4096}+4095
        %torch.2_8_0:3% aten::fill_(self:<1xf32>{4096}+4095, value:0:int) -> <1xf32>{4096}+4095
aten::cat 12
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}}, dim:-1:int) -> <4096x1x14336xbf16>{14336, 14336, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}, <4096x1x7168xbf16>{7168, 7168, 1}}) -> <8192x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
        %torch.2_8_0:3% aten::cat(tensors:list{<32704x2048xf32>{2048, 1}, <32704x2048xf32>{2048, 1}}, dim:-1:int) -> <32704x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x2048xbf16>{2048, 1}, <4096x2048xbf16>{2048, 1}}, dim:-1:int) -> <4096x4096xbf16>{4096, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x1x512xbf16>{512, 512, 1}, <4096x1x64xbf16>{64, 64, 1}}, dim:2:int) -> <4096x1x576xbf16>{576, 576, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x1x7168xbf16>{7168, 7168, 1}}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<32720x2048xf32>{2048, 1}, <32720x2048xf32>{2048, 1}}, dim:-1:int) -> <32720x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x18432xbf16>{18432, 1}, <4096x18432xbf16>{18432, 1}}, dim:-1:int) -> <4096x36864xbf16>{36864, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<1xf32>{1}}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::cat(tensors:list{<32832x2048xf32>{2048, 1}, <32832x2048xf32>{2048, 1}}, dim:-1:int) -> <32832x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<33200x2048xf32>{2048, 1}, <33200x2048xf32>{2048, 1}}, dim:-1:int) -> <33200x4096xf32>{4096, 1}
aten::mm 3
        %torch.2_8_0:3% aten::mm(self:<4096x7168xbf16>{7168, 1}+29360128, mat2:<7168x129280xbf16>{1, 7168}) -> <4096x129280xbf16>{129280, 1}
        %torch.2_8_0:3% aten::mm(self:<4096x7168xbf16>{7168, 1}, mat2:<7168x129280xbf16>{1, 7168}) -> <4096x129280xbf16>{129280, 1}
        %torch.2_8_0:3% aten::mm(self:<4096x129280xbf16>{129280, 1}, mat2:<129280x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}
aten::stack 3
        %torch.2_8_0:3% aten::stack(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
        %torch.2_8_0:3% aten::stack(tensors:list{<32xbf16>{1}, <32xbf16>{1}}) -> <2x32xbf16>{32, 1}
        %torch.2_8_0:3% aten::stack(tensors:list{<32xf32>{1}, <32xf32>{1}}) -> <2x32xf32>{32, 1}
aten::isnan 2
        %torch.2_8_0:3% aten::isnan(self:<2xf32>{1}) -> <2xbool>{1}
        %torch.2_8_0:3% aten::isnan(self:<1xbf16>{1}) -> <1xbool>{1}
aten::any 1
        %torch.2_8_0:3% aten::any(self:<2xbool>{1}) -> <1xbool>{1}
aten::unbind 2
        %torch.2_8_0:3% aten::unbind(self:<2xf32>{1}) -> <1xf32>{1}, <1xf32>{1}
        %torch.2_8_0:3% aten::unbind(self:<2x32xf32>{32, 1}) -> <32xf32>{1}, <32xf32>{1}+32
c10d::allreduce_ 5
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<2xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<2xf32>{1}}, Work:distributed
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<2x32xbf16>{32, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<2x32xbf16>{32, 1}}, Work:distributed
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<1xi32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<1xi32>{1}}, Work:distributed
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<1xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<1xf32>{1}}, Work:distributed
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<3xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> list{<3xf32>{1}}, Work:distributed
aten::mul_ 3
        %torch.2_8_0:3% aten::mul_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul_(self:<3002413056xbf16>{1}, other:0_125:float) -> <3002413056xbf16>{1}
        %torch.2_8_0:3% aten::mul_(self:<352321536xbf16>{1}, other:0_125:float) -> <352321536xbf16>{1}
aten::div_ 1
        %torch.2_8_0:3% aten::div_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
aten::ones 1
        %torch.2_8_0:3% aten::ones(size:list{1:int}, device:cuda_3:device, pin_memory:False:bool) -> <1xf32>{1}
aten::select_backward 1
        %torch.2_8_0:3% aten::select_backward(grad_output:<1xf32>{1}, input_sizes:list{2:int}, dim:0:int, index:0:int) -> <2xf32>{1}
aten::equal 1
        %torch.2_8_0:3% aten::equal(self:<4096x1xf32>{1, 4096}, other:<1xf32>{1}) -> False:bool
apex:fused_weight_gradient_mlp_cuda::wgrad_gemm_accum_fp16 2
        %torch.2_8_0:3% apex:fused_weight_gradient_mlp_cuda::wgrad_gemm_accum_fp16(<4096x7168xbf16>{7168, 1}, <4096x129280xbf16>{129280, 1}, <129280x7168xbf16>{7168, 1}) -> None:
        %torch.2_8_0:3% apex:fused_weight_gradient_mlp_cuda::wgrad_gemm_accum_fp16(<4096x7168xbf16>{7168, 1}+29360128, <4096x129280xbf16>{129280, 1}, <129280x7168xbf16>{7168, 1}) -> None:
te::rmsnorm_bwd 16
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}+29360128, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+926679040, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+970955776, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+987740160, 0:int, False:bool) -> <4096x512xbf16>{512, 1}, <512xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1029618176, 0:int, False:bool) -> <4096x1536xbf16>{1536, 1}, <1536xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1158070272, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1260837888, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1260845056, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1260852224, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1305128960, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+1321913344, 0:int, False:bool) -> <4096x512xbf16>{512, 1}, <512xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1363791360, 0:int, False:bool) -> <4096x1536xbf16>{1536, 1}, <1536xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1492243456, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1888612352, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+1905396736, 0:int, False:bool) -> <4096x512xbf16>{512, 1}, <512xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1947274752, 0:int, False:bool) -> <4096x1536xbf16>{1536, 1}, <1536xbf16>{1}
        %te.2_7_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+2075726848, 0:int, False:bool) -> <4096x7168xbf16>{7168, 1}, <7168xbf16>{1}
aten::sigmoid 6
        %torch.2_8_0:3% aten::sigmoid(self:<32704x2048xbf16>{4096, 1}) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<32720x2048xbf16>{4096, 1}) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<32832x2048xbf16>{4096, 1}) -> <32832x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<33200x2048xbf16>{4096, 1}) -> <33200x2048xbf16>{2048, 1}
aten::rsub 6
        %torch.2_8_0:3% aten::rsub(self:<32704x2048xbf16>{2048, 1}, other:1:int) -> <32704x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::rsub(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::rsub(self:<32720x2048xbf16>{2048, 1}, other:1:int) -> <32720x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::rsub(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::rsub(self:<32832x2048xbf16>{2048, 1}, other:1:int) -> <32832x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::rsub(self:<33200x2048xbf16>{2048, 1}, other:1:int) -> <33200x2048xbf16>{2048, 1}
aten::squeeze 5
        %torch.2_8_0:3% aten::squeeze(self:<32704x1xf32>{1, 1}, dim:-1:int) -> <32704xf32>{1}
        %torch.2_8_0:3% aten::squeeze(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:-2:int) -> <4096x1x64xbf16>{64, 64, 1}
        %torch.2_8_0:3% aten::squeeze(self:<32720x1xf32>{1, 1}, dim:-1:int) -> <32720xf32>{1}
        %torch.2_8_0:3% aten::squeeze(self:<32832x1xf32>{1, 1}, dim:-1:int) -> <32832xf32>{1}
        %torch.2_8_0:3% aten::squeeze(self:<33200x1xf32>{1, 1}, dim:-1:int) -> <33200xf32>{1}
aten::scatter 1
        %torch.2_8_0:3% aten::scatter(self:<4096x32xf32>{32, 1}, dim:-1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
te::fused_moe_aux_loss_bwd 1
        %te.2_7_0:3% te::fused_moe_aux_loss_bwd(Const_buf:<1xf32>{1}, tokens_per_expert:<32xi64>{1}, num_rows:4096:int, num_cols:32:int, grad_aux_loss:<1xf32>{1}) -> <4096x32xf32>{32, 1}
te::fused_score_for_moe_aux_loss_bwd 1
        %te.2_7_0:3% te::fused_score_for_moe_aux_loss_bwd(num_tokens:4096:int, num_experts:32:int, intermediate_output:<4096x32xf32>{32, 1}, grad_scores:<4096x32xf32>{32, 1}, topk:8:int, score_function:sigmoid:str) -> <4096x32xf32>{32, 1}
te::fused_topk_with_score_function_bwd 1
        %te.2_7_0:3% te::fused_topk_with_score_function_bwd(4096:int, 32:int, <4096x32xbool>{32, 1}, <4096x32xf32>{32, 1}, <4096x32xf32>{32, 1}, 8:int, True:bool, 2_5:float, sigmoid:str) -> <4096x32xf32>{32, 1}
te::scaled_masked_softmax_backward 1
        %te.2_7_0:3% te::scaled_masked_softmax_backward(<1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}, <1xf32>{1}) -> <1x128x4096x4096xbf16>{2147483648, 16777216, 4096, 1}
aten::embedding_dense_backward 1
        %torch.2_8_0:3% aten::embedding_dense_backward(grad_output:<1x4096x7168xbf16>{7168, 7168, 1}, indices:<1x4096xi64>{4096, 1}, num_weights:129280:int, padding_idx:-1:int, scale_grad_by_freq:False:bool) -> <129280x7168xbf16>{7168, 1}
aten::linalg_vector_norm 2
        %torch.2_8_0:3% aten::linalg_vector_norm(self:<3002413056xbf16>{1}) -> <1xbf16>{1}
        %torch.2_8_0:3% aten::linalg_vector_norm(self:<352321536xbf16>{1}) -> <1xbf16>{1}
c10d::reduce_scatter_tensor_coalesced_ 2
        %torch.2_8_0:3% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<375301632xbf16>{1}+1125904896}, inputs:list{<3002413056xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
        %torch.2_8_0:3% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<352321536xbf16>{1}}, inputs:list{<352321536xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
aten::sign 1
        %torch.2_8_0:3% aten::sign(self:<2x32xbf16>{32, 1}) -> <2x32xbf16>{32, 1}
te::multi_tensor_l2norm 1
        %te.2_7_0:3% te::multi_tensor_l2norm(65536:int, <1xi32>{1}, list{list{<32165376xbf16>{1}+1125904896, <102760448xbf16>{1}+1158077440, <14680064xbf16>{1}+1260859392, <29360128xbf16>{1}+1275539456, <229376xbf16>{1}+1304899584, <16777216xbf16>{1}+1305136128, <4128768xbf16>{1}+1321913856, <37748736xbf16>{1}+1326042624, <11010048xbf16>{1}+1363792896, <117440512xbf16>{1}+1374802944, <8955904xbf16>{1}+1492250624, <7168xbf16>{1}+1158070272, <7168xbf16>{1}+1260837888, <7168xbf16>{1}+1260845056, <7168xbf16>{1}+1260852224, <7168xbf16>{1}+1305128960, <512xbf16>{1}+1321913344, <1536xbf16>{1}+1363791360, <7168xbf16>{1}+1492243456, <14680064xbf16>{1}, <14680064xbf16>{1}+14680064, <14680064xbf16>{1}+29360128, <14680064xbf16>{1}+44040192, <29360128xbf16>{1}+58720256, <29360128xbf16>{1}+88080384, <29360128xbf16>{1}+117440512, <29360128xbf16>{1}+146800640, <14680064xbf16>{1}+176160768, <14680064xbf16>{1}+190840832, <14680064xbf16>{1}+205520896, <14680064xbf16>{1}+220200960, <29360128xbf16>{1}+234881024, <29360128xbf16>{1}+264241152, <29360128xbf16>{1}+293601280, <29360128xbf16>{1}+322961408}}, False:bool) -> <1xf32>{1}, <0xf32>{1}
aten::pow 1
        %torch.2_8_0:3% aten::pow(self:<1xf32>{1}, exponent:2_0:float) -> <1xf32>{1}
te::multi_tensor_scale 4
        %te.2_7_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<32165376xbf16>{1}+1125904896, <102760448xbf16>{1}+1158077440, <14680064xbf16>{1}+1260859392, <29360128xbf16>{1}+1275539456, <229376xbf16>{1}+1304899584, <16777216xbf16>{1}+1305136128, <4128768xbf16>{1}+1321913856, <37748736xbf16>{1}+1326042624, <11010048xbf16>{1}+1363792896, <117440512xbf16>{1}+1374802944, <8955904xbf16>{1}+1492250624, <7168xbf16>{1}+1158070272, <7168xbf16>{1}+1260837888, <7168xbf16>{1}+1260845056, <7168xbf16>{1}+1260852224, <7168xbf16>{1}+1305128960, <512xbf16>{1}+1321913344, <1536xbf16>{1}+1363791360, <7168xbf16>{1}+1492243456}, list{<32165376xbf16>{1}+1125904896, <102760448xbf16>{1}+1158077440, <14680064xbf16>{1}+1260859392, <29360128xbf16>{1}+1275539456, <229376xbf16>{1}+1304899584, <16777216xbf16>{1}+1305136128, <4128768xbf16>{1}+1321913856, <37748736xbf16>{1}+1326042624, <11010048xbf16>{1}+1363792896, <117440512xbf16>{1}+1374802944, <8955904xbf16>{1}+1492250624, <7168xbf16>{1}+1158070272, <7168xbf16>{1}+1260837888, <7168xbf16>{1}+1260845056, <7168xbf16>{1}+1260852224, <7168xbf16>{1}+1305128960, <512xbf16>{1}+1321913344, <1536xbf16>{1}+1363791360, <7168xbf16>{1}+1492243456}}, 1_6496779671771087e-05:float) -> None:
        %te.2_7_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<14680064xbf16>{1}, <14680064xbf16>{1}+14680064, <14680064xbf16>{1}+29360128, <14680064xbf16>{1}+44040192, <29360128xbf16>{1}+58720256, <29360128xbf16>{1}+88080384, <29360128xbf16>{1}+117440512, <29360128xbf16>{1}+146800640, <14680064xbf16>{1}+176160768, <14680064xbf16>{1}+190840832, <14680064xbf16>{1}+205520896, <14680064xbf16>{1}+220200960, <29360128xbf16>{1}+234881024, <29360128xbf16>{1}+264241152, <29360128xbf16>{1}+293601280, <29360128xbf16>{1}+322961408}, list{<14680064xbf16>{1}, <14680064xbf16>{1}+14680064, <14680064xbf16>{1}+29360128, <14680064xbf16>{1}+44040192, <29360128xbf16>{1}+58720256, <29360128xbf16>{1}+88080384, <29360128xbf16>{1}+117440512, <29360128xbf16>{1}+146800640, <14680064xbf16>{1}+176160768, <14680064xbf16>{1}+190840832, <14680064xbf16>{1}+205520896, <14680064xbf16>{1}+220200960, <29360128xbf16>{1}+234881024, <29360128xbf16>{1}+264241152, <29360128xbf16>{1}+293601280, <29360128xbf16>{1}+322961408}}, 1_6496779671771087e-05:float) -> None:
        %te.2_7_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<32165376xbf16>{1}+1125904896, <102760448xbf16>{1}+1158077440, <14680064xbf16>{1}+1260859392, <29360128xbf16>{1}+1275539456, <229376xbf16>{1}+1304899584, <16777216xbf16>{1}+1305136128, <4128768xbf16>{1}+1321913856, <37748736xbf16>{1}+1326042624, <11010048xbf16>{1}+1363792896, <117440512xbf16>{1}+1374802944, <8955904xbf16>{1}+1492250624, <7168xbf16>{1}+1158070272, <7168xbf16>{1}+1260837888, <7168xbf16>{1}+1260845056, <7168xbf16>{1}+1260852224, <7168xbf16>{1}+1305128960, <512xbf16>{1}+1321913344, <1536xbf16>{1}+1363791360, <7168xbf16>{1}+1492243456}, list{<32165376xbf16>{1}+1125904896, <102760448xbf16>{1}+1158077440, <14680064xbf16>{1}+1260859392, <29360128xbf16>{1}+1275539456, <229376xbf16>{1}+1304899584, <16777216xbf16>{1}+1305136128, <4128768xbf16>{1}+1321913856, <37748736xbf16>{1}+1326042624, <11010048xbf16>{1}+1363792896, <117440512xbf16>{1}+1374802944, <8955904xbf16>{1}+1492250624, <7168xbf16>{1}+1158070272, <7168xbf16>{1}+1260837888, <7168xbf16>{1}+1260845056, <7168xbf16>{1}+1260852224, <7168xbf16>{1}+1305128960, <512xbf16>{1}+1321913344, <1536xbf16>{1}+1363791360, <7168xbf16>{1}+1492243456}}, 2_064570665765588e-05:float) -> None:
        %te.2_7_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<14680064xbf16>{1}, <14680064xbf16>{1}+14680064, <14680064xbf16>{1}+29360128, <14680064xbf16>{1}+44040192, <29360128xbf16>{1}+58720256, <29360128xbf16>{1}+88080384, <29360128xbf16>{1}+117440512, <29360128xbf16>{1}+146800640, <14680064xbf16>{1}+176160768, <14680064xbf16>{1}+190840832, <14680064xbf16>{1}+205520896, <14680064xbf16>{1}+220200960, <29360128xbf16>{1}+234881024, <29360128xbf16>{1}+264241152, <29360128xbf16>{1}+293601280, <29360128xbf16>{1}+322961408}, list{<14680064xbf16>{1}, <14680064xbf16>{1}+14680064, <14680064xbf16>{1}+29360128, <14680064xbf16>{1}+44040192, <29360128xbf16>{1}+58720256, <29360128xbf16>{1}+88080384, <29360128xbf16>{1}+117440512, <29360128xbf16>{1}+146800640, <14680064xbf16>{1}+176160768, <14680064xbf16>{1}+190840832, <14680064xbf16>{1}+205520896, <14680064xbf16>{1}+220200960, <29360128xbf16>{1}+234881024, <29360128xbf16>{1}+264241152, <29360128xbf16>{1}+293601280, <29360128xbf16>{1}+322961408}}, 2_064570665765588e-05:float) -> None:
profiler::_record_function_enter_new 4
        %torch.2_8_0:3% profiler::_record_function_enter_new(name:Optimizer_step#HybridDeviceOptimizer_step:str) -> Work:distributed
        %torch.2_8_0:3% profiler::_record_function_enter_new(name:Optimizer_step#FusedAdam_step:str) -> Work:distributed
        %torch.2_8_0:3% profiler::_record_function_enter_new(name:Optimizer_step#AdamW_step:str) -> Work:distributed
        %torch.2_8_0:3% profiler::_record_function_enter_new(name:enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str) -> Work:distributed
te::multi_tensor_adam 6
        %te.2_7_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float) -> None:
        %te.2_7_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_0:float) -> None:
        %te.2_7_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float) -> None:
        %te.2_7_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <229376xf32>{1}, <16777216xf32>{1}, <4128768xf32>{1}, <37748736xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <8955904xf32>{1}}}, 9_913533761814537e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float) -> None:
        %te.2_7_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <7168xf32>{1}, <512xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}}, 9_913533761814537e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_0:float) -> None:
        %te.2_7_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 9_913533761814537e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float) -> None:
profiler::_record_function_exit 1
        %torch.2_8_0:3% profiler::_record_function_exit(_0:Work:distributed) -> None:
aten::_foreach_add_ 2
        %torch.2_8_0:3% aten::_foreach_add_(self:list{<1xf32>{1}, <1xf32>{1}}, scalar:1:int) -> None:
        %torch.2_8_0:3% aten::_foreach_add_(self:list{<1xf32>{1}, <1xf32>{1}, <1xf32>{1}}, scalar:1:int) -> None:
aten::_fused_adamw_ 4
        %torch.2_8_0:3% aten::_fused_adamw_(self:list{<32165376xf32>{1}, <102760448xf32>{1}}, grads:list{<32165376xf32>{1}, <102760448xf32>{1}}, exp_avgs:list{<32165376xf32>{1}, <102760448xf32>{1}}, exp_avg_sqs:list{<32165376xf32>{1}, <102760448xf32>{1}}, max_exp_avg_sqs:list{}, state_steps:list{<1xf32>{1}, <1xf32>{1}}, lr:1e-05:float, beta1:0_9:float, beta2:0_95:float, weight_decay:0_1:float, eps:1e-08:float, amsgrad:False:bool, maximize:False:bool) -> None:
        %torch.2_8_0:3% aten::_fused_adamw_(self:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, grads:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, exp_avgs:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, exp_avg_sqs:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, max_exp_avg_sqs:list{}, state_steps:list{<1xf32>{1}, <1xf32>{1}, <1xf32>{1}}, lr:1e-05:float, beta1:0_9:float, beta2:0_95:float, weight_decay:0_1:float, eps:1e-08:float, amsgrad:False:bool, maximize:False:bool) -> None:
        %torch.2_8_0:3% aten::_fused_adamw_(self:list{<32165376xf32>{1}, <102760448xf32>{1}}, grads:list{<32165376xf32>{1}, <102760448xf32>{1}}, exp_avgs:list{<32165376xf32>{1}, <102760448xf32>{1}}, exp_avg_sqs:list{<32165376xf32>{1}, <102760448xf32>{1}}, max_exp_avg_sqs:list{}, state_steps:list{<1xf32>{1}, <1xf32>{1}}, lr:9_913533761814537e-06:float, beta1:0_9:float, beta2:0_95:float, weight_decay:0_1:float, eps:1e-08:float, amsgrad:False:bool, maximize:False:bool) -> None:
        %torch.2_8_0:3% aten::_fused_adamw_(self:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, grads:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, exp_avgs:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, exp_avg_sqs:list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}}, max_exp_avg_sqs:list{}, state_steps:list{<1xf32>{1}, <1xf32>{1}, <1xf32>{1}}, lr:9_913533761814537e-06:float, beta1:0_9:float, beta2:0_95:float, weight_decay:0_1:float, eps:1e-08:float, amsgrad:False:bool, maximize:False:bool) -> None:
c10d::allgather_into_tensor_coalesced_ 2
        %torch.2_8_0:3% c10d::allgather_into_tensor_coalesced_(outputs:list{<3002413056xbf16>{1}}, inputs:list{<375301632xbf16>{1}+1125904896}, process_group:ProcessGroup:distributed) -> Work:distributed
        %torch.2_8_0:3% c10d::allgather_into_tensor_coalesced_(outputs:list{<352321536xbf16>{1}}, inputs:list{<352321536xbf16>{1}}, process_group:ProcessGroup:distributed) -> Work:distributed
aten::mean 1
        %torch.2_8_0:3% aten::mean(self:<1xf32>{1}) -> <1xf32>{1}
c10d::barrier 1
        %torch.2_8_0:3% c10d::barrier(tensor:<1xu8>{1}, process_group:ProcessGroup:distributed, device_ids:list{}, async_op:False:bool) -> Work:distributed
c10d::_allgather_base_ 1
        %torch.2_8_0:3% c10d::_allgather_base_(output_tensor:<192xf32>{1}, input_tensor:<24xf32>{1}+72, process_group:ProcessGroup:distributed, async_op:False:bool) -> <192xf32>{1}, Work:distributed
aten::index 24
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+1, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+2, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+3, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+4, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+5, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+6, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+7, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+8, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+9, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+10, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+11, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+12, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+13, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+14, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+15, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+16, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+17, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+18, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+19, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+20, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+21, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+22, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+23, indices:list{<8xbool>{1}}) -> <0xf32>{1}
c10d::broadcast_ 3
        %torch.2_8_0:3% c10d::broadcast_(tensors:list{<1x4096xi64>{4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> list{<1x4096xi64>{4096, 1}}, Work:distributed
        %torch.2_8_0:3% c10d::broadcast_(tensors:list{<1x4096xf32>{4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> list{<1x4096xf32>{4096, 1}}, Work:distributed
        %torch.2_8_0:3% c10d::broadcast_(tensors:list{<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> list{<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}}, Work:distributed
aten::isinf 1
        %torch.2_8_0:3% aten::isinf(self:<1xbf16>{1}) -> <1xbool>{1}