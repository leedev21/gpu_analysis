================================================== trace ops ==================================================
te::get_cudnn_version
        %torch.2_8_0:5% te::get_cudnn_version() -> 91100:int
aten::embedding
        %torch.2_8_0:5% aten::embedding(weight:<129280x7168xbf16>{7168, 1}+1741560832, indices:<1x4096xi64>{4096, 1}) -> <1x4096x7168xbf16>{29360128, 7168, 1}
aten::transpose
        %torch.2_8_0:5% aten::transpose(self:<1x4096x7168xbf16>{29360128, 7168, 1}, dim0:0:int, dim1:1:int) -> <4096x1x7168xbf16>{7168, 29360128, 1}
        %torch.2_8_0:5% aten::transpose(self:<11254x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x11254xbool>{1, 4}
        %torch.2_8_0:5% aten::transpose(self:<4x11254xbool>{1, 4}, dim0:0:int, dim1:1:int) -> <11254x4xbool>{4, 1}
        %torch.2_8_0:5% aten::transpose(self:<1x4096xi64>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:5% aten::transpose(self:<4096x1xf32>{1, 1}, dim0:0:int, dim1:1:int) -> <1x4096xf32>{1, 1}
        %torch.2_8_0:5% aten::transpose(self:<1x4096xf32>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xf32>{1, 4096}
        %torch.2_8_0:5% aten::transpose(self:<4096x1x7168xbf16>{7168, 7168, 1}, dim0:0:int, dim1:1:int) -> <1x4096x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:5% aten::transpose(self:<12989x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x12989xbool>{1, 4}
        %torch.2_8_0:5% aten::transpose(self:<4x12989xbool>{1, 4}, dim0:0:int, dim1:1:int) -> <12989x4xbool>{4, 1}
te::rmsnorm_fwd
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1741553664, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1613101568, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x1536xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+1571223552, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x512xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1554439168, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1158070272, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1029618176, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x1536xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+987740160, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x512xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+970955776, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
        %torch.2_8_0:5% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+926679040, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
aten::cos
        %torch.2_8_0:5% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
aten::mul
        %torch.2_8_0:5% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{36864, 1}+18432) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x8xf32>{8, 1}, other:2_5:float) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:5% aten::mul(self:<1x32xf32>{32, 1}, other:<1x32xf32>{32, 1}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<4xf32>{1}, other:16:int) -> <4xf32>{1}
        %torch.2_8_0:5% aten::mul(self:<22288x2048xbf16>{2048, 1}, other:<22288x2048xbf16>{4096, 1}+2048) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<22288x2048xbf16>{2048, 1}, other:<22288x1xf32>{1, 1}) -> <22288x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<4096xf32>{1}, other:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:5% aten::mul(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:5% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::mul(self:<4096xf32>{0}, other:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:5% aten::mul(self:<22288x2048xf32>{2048, 1}, other:<22288x2048xbf16>{2048, 1}) -> <22288x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<22288x2048xbf16>{4096, 1}, other:<22288x2048xbf16>{2048, 1}) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<22288x2048xf32>{2048, 1}, other:<22288x2048xbf16>{4096, 1}+2048) -> <22288x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<22288x2048xbf16>{2048, 1}, other:<22288x2048xf32>{2048, 1}) -> <22288x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x2048xbf16>{4096, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<1xf32>{1}, other:0_001:float) -> <1xf32>{1}
        %torch.2_8_0:5% aten::mul(self:<1x32xf32>{1, 0}, other:<1x32xf32>{32, 1}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x8xf32>{8, 1}, other:<4096x8xf32>{8, 1}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::mul(self:<4096x18432xbf16>{36864, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::mul(self:<1x32xbf16>{32, 1}, other:0_001:float) -> <1x32xbf16>{32, 1}
        %torch.2_8_0:5% aten::mul(self:<28464x2048xbf16>{2048, 1}, other:<28464x2048xbf16>{4096, 1}+2048) -> <28464x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<28464x2048xbf16>{2048, 1}, other:<28464x1xf32>{1, 1}) -> <28464x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<28464x2048xf32>{2048, 1}, other:<28464x2048xbf16>{2048, 1}) -> <28464x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<28464x2048xbf16>{4096, 1}, other:<28464x2048xbf16>{2048, 1}) -> <28464x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<28464x2048xf32>{2048, 1}, other:<28464x2048xbf16>{4096, 1}+2048) -> <28464x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::mul(self:<28464x2048xbf16>{2048, 1}, other:<28464x2048xf32>{2048, 1}) -> <28464x2048xf32>{2048, 1}
aten::_to_copy
        %torch.2_8_0:5% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<32xbf16>{1}, dtype:torch_float32:dtype) -> <32xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x32xf32>{32, 1}, dtype:torch_bool:dtype) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x32xf32>{32, 1}, dtype:torch_int32:dtype) -> <4096x32xi32>{32, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x32xi32>{32, 1}, dtype:torch_bool:dtype) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<1xi64>{1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <1xi64>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+5, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+10, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+15, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+20, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+25, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+30, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<5xu8>{1}+35, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+108, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+216, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+324, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+432, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+540, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+648, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<108xu8>{1}+756, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<11254x4xi64>{4, 1}, dtype:torch_bool:dtype) -> <11254x4xbool>{4, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4xf32>{1}, dtype:torch_int64:dtype) -> <4xi64>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<4x11254xbool>{1, 4}, dtype:torch_int32:dtype) -> <4x11254xi32>{1, 4}
        %torch.2_8_0:5% aten::_to_copy(self:<0xf32>{1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <0xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<22288x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x1x129280xbf16>{129280, 129280, 1}, dtype:torch_float32:dtype) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096xbool>{1}, dtype:torch_float32:dtype) -> <4096xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x1x129280xf32>{129280, 129280, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x129280xbf16>{129280, 129280, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<22288x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <22288x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<22288x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <22288x4096xbf16>{4096, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<32x7168xbf16>{7168, 1}+970726400, dtype:torch_float32:dtype) -> <32x7168xf32>{7168, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x7168xbf16>{7168, 1}, dtype:torch_float32:dtype) -> <4096x7168xf32>{7168, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4096x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <4096x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<32x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <32x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<73903744xbf16>{1}+1667649920, dtype:torch_float32:dtype) -> <73903744xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<259619072xbf16>{1}+1741560832, dtype:torch_float32:dtype) -> <259619072xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<7168xbf16>{1}+1741553664, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<14680064xbf16>{1}, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<14680064xbf16>{1}+14680064, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<14680064xbf16>{1}+29360128, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<14680064xbf16>{1}+44040192, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<29360128xbf16>{1}+58720256, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<29360128xbf16>{1}+88080384, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<29360128xbf16>{1}+117440512, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<29360128xbf16>{1}+146800640, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<73903744xbf16>{1}, dtype:torch_float32:dtype) -> <73903744xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<259619072xbf16>{1}, dtype:torch_float32:dtype) -> <259619072xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<73903744xf32>{1}, dtype:torch_bfloat16:dtype) -> <73903744xbf16>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<259619072xf32>{1}, dtype:torch_bfloat16:dtype) -> <259619072xbf16>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<7168xbf16>{1}, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<7168xf32>{1}, dtype:torch_bfloat16:dtype) -> <7168xbf16>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<29360128xbf16>{1}, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<14680064xf32>{1}, dtype:torch_bfloat16:dtype) -> <14680064xbf16>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<29360128xf32>{1}, dtype:torch_bfloat16:dtype) -> <29360128xbf16>{1}
        %torch.2_8_0:5% aten::_to_copy(self:<1x4096xi64>{4096, 1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_5:device, non_blocking:True:bool) -> <1x4096xi64>{4096, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<1x4096xf32>{4096, 1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_5:device, non_blocking:True:bool) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_5:device, non_blocking:True:bool) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<12989x4xi64>{4, 1}, dtype:torch_bool:dtype) -> <12989x4xbool>{4, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<4x12989xbool>{1, 4}, dtype:torch_int32:dtype) -> <4x12989xi32>{1, 4}
        %torch.2_8_0:5% aten::_to_copy(self:<28464x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <28464x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<28464x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <28464x2048xf32>{2048, 1}
        %torch.2_8_0:5% aten::_to_copy(self:<28464x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <28464x4096xbf16>{4096, 1}
aten::sin
        %torch.2_8_0:5% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
te::quantize
        %torch.2_8_0:5% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<1536x7168xbf16>{7168, 1}+1613103104, <True, True, False>:Float8BlockQuantizer) -> <1536x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<576x7168xbf16>{7168, 1}+1571224064, <True, True, False>:Float8BlockQuantizer) -> <576x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<24576x1536xbf16>{1536, 1}+1575352832, <True, True, False>:Float8BlockQuantizer) -> <24576x1536xf8e4m3>
        %torch.2_8_0:5% te::quantize(<32768x512xbf16>{512, 1}+1554446336, <True, True, False>:Float8BlockQuantizer) -> <32768x512xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x16384xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x16384xbf16>{16384, 1}+1624113152, <True, True, False>:Float8BlockQuantizer) -> <7168x16384xf8e4m3>
        %torch.2_8_0:5% te::quantize(<36864x7168xbf16>{7168, 1}+1290198016, <True, True, False>:Float8BlockQuantizer) -> <36864x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x1x18432xbf16>{18432, 18432, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x18432xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x18432xbf16>{18432, 1}+1158077440, <True, True, False>:Float8BlockQuantizer) -> <7168x18432xf8e4m3>
        %torch.2_8_0:5% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, False>:Float8BlockQuantizer) -> <1536x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, False>:Float8BlockQuantizer) -> <576x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, False>:Float8BlockQuantizer) -> <24576x1536xf8e4m3>
        %torch.2_8_0:5% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, False>:Float8BlockQuantizer) -> <32768x512xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, False>:Float8BlockQuantizer) -> <7168x16384xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x4096xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x32768xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x24576xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x576xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1536xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x36864xbf16>{36864, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x36864xf8e4m3>
        %torch.2_8_0:5% te::quantize(<1536x7168xbf16>{7168, 1}+1613103104, <True, True, True>:Float8BlockQuantizer, <1536x7168xf8e4m3>, None:) -> <1536x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<576x7168xbf16>{7168, 1}+1571224064, <True, True, True>:Float8BlockQuantizer, <576x7168xf8e4m3>, None:) -> <576x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<24576x1536xbf16>{1536, 1}+1575352832, <True, True, True>:Float8BlockQuantizer, <24576x1536xf8e4m3>, None:) -> <24576x1536xf8e4m3>
        %torch.2_8_0:5% te::quantize(<32768x512xbf16>{512, 1}+1554446336, <True, True, True>:Float8BlockQuantizer, <32768x512xf8e4m3>, None:) -> <32768x512xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x16384xbf16>{16384, 1}+1624113152, <True, True, True>:Float8BlockQuantizer, <7168x16384xf8e4m3>, None:) -> <7168x16384xf8e4m3>
        %torch.2_8_0:5% te::quantize(<36864x7168xbf16>{7168, 1}+1290198016, <True, True, True>:Float8BlockQuantizer, <36864x7168xf8e4m3>, None:) -> <36864x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x18432xbf16>{18432, 1}+1158077440, <True, True, True>:Float8BlockQuantizer, <7168x18432xf8e4m3>, None:) -> <7168x18432xf8e4m3>
        %torch.2_8_0:5% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, True>:Float8BlockQuantizer, <1536x7168xf8e4m3>, None:) -> <1536x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, True>:Float8BlockQuantizer, <576x7168xf8e4m3>, None:) -> <576x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, True>:Float8BlockQuantizer, <24576x1536xf8e4m3>, None:) -> <24576x1536xf8e4m3>
        %torch.2_8_0:5% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, True>:Float8BlockQuantizer, <32768x512xf8e4m3>, None:) -> <32768x512xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, True>:Float8BlockQuantizer, <7168x16384xf8e4m3>, None:) -> <7168x16384xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:5% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
te::generic_gemm
        %torch.2_8_0:5% te::generic_gemm(<1536x7168xf8e4m3>, True:bool, <4096x1x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x1536xbf16>{1536, 1536, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<576x7168xf8e4m3>, True:bool, <4096x1x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x576xbf16>{576, 576, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<24576x1536xf8e4m3>, True:bool, <4096x1536xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x24576xbf16>{24576, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<32768x512xf8e4m3>, True:bool, <4096x512xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x32768xbf16>{32768, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<7168x16384xf8e4m3>, True:bool, <4096x1x16384xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<36864x7168xf8e4m3>, True:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x36864xbf16>{36864, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<7168x18432xf8e4m3>, True:bool, <4096x1x18432xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<32x7168xbf16>{7168, 1}+970726400, True:bool, <4096x7168xbf16>{7168, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x32xf32>{32, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x7168xf8e4m3>, True:bool, <4096x1x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x4096xbf16>{4096, 4096, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<7168x2048xf8e4m3>, True:bool, <4096x1x2048xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<7168x2048xf8e4m3>, False:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x2048xbf16>{2048, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1x2048xf8e4m3>, False:bool, <4096x7168xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<7168x2048xbf16>{2048, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x7168xf8e4m3>, False:bool, <4096x4096xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1x7168xf8e4m3>, False:bool, <4096x4096xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<32x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xf32>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, True:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<32x7168xf32>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<7168x16384xf8e4m3>, False:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x16384xbf16>{16384, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1x16384xf8e4m3>, False:bool, <4096x7168xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<7168x16384xbf16>{16384, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<32768x512xf8e4m3>, False:bool, <4096x32768xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x512xbf16>{512, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x512xf8e4m3>, False:bool, <4096x32768xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<32768x512xbf16>{512, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<24576x1536xf8e4m3>, False:bool, <4096x24576xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1536xbf16>{1536, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1536xf8e4m3>, False:bool, <4096x24576xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<24576x1536xbf16>{1536, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<576x7168xf8e4m3>, False:bool, <4096x576xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1x7168xf8e4m3>, False:bool, <4096x576xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<576x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<1536x7168xf8e4m3>, False:bool, <4096x1536xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1x7168xf8e4m3>, False:bool, <4096x1536xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<1536x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<7168x18432xf8e4m3>, False:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x18432xbf16>{18432, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x1x18432xf8e4m3>, False:bool, <4096x7168xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<7168x18432xbf16>{18432, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<36864x7168xf8e4m3>, False:bool, <4096x36864xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:5% te::generic_gemm(<4096x7168xf8e4m3>, False:bool, <4096x36864xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<36864x7168xbf16>{7168, 1}, None:, None:, None:}
aten::split_with_sizes
        %torch.2_8_0:5% aten::split_with_sizes(self:<4096x1x576xbf16>{576, 576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> list{<4096x1x512xbf16>{576, 576, 1}, <4096x1x64xbf16>{576, 576, 1}+512}
aten::clone
        %torch.2_8_0:5% aten::clone(self:<4096x1x512xbf16>{576, 512, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x512xbf16>{512, 512, 1}
        %torch.2_8_0:5% aten::clone(self:<4096x128x192xbf16>{24576, 192, 1}) -> <4096x128x192xbf16>{24576, 192, 1}
        %torch.2_8_0:5% aten::clone(self:<4096x2x16xf32>{2, 1, 0}, memory_format:torch_contiguous_format:memory_format) -> <4096x2x16xf32>{32, 16, 1}
        %torch.2_8_0:5% aten::clone(self:<4x11254xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x11254xbool>{11254, 1}
        %torch.2_8_0:5% aten::clone(self:<4x11254xf32>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x11254xf32>{11254, 1}
        %torch.2_8_0:5% aten::clone(self:<4096x1xi64>{1, 4096}) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:5% aten::clone(self:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:5% aten::clone(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::clone(self:<4x12989xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x12989xbool>{12989, 1}
        %torch.2_8_0:5% aten::clone(self:<4x12989xf32>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x12989xf32>{12989, 1}
aten::unsqueeze
        %torch.2_8_0:5% aten::unsqueeze(self:<4096x1x64xbf16>{576, 576, 1}+512, dim:-2:int) -> <4096x1x1x64xbf16>{576, 576, 64, 1}+512
        %torch.2_8_0:5% aten::unsqueeze(self:<4096x2xf32>{2, 1}, dim:-1:int) -> <4096x2x1xf32>{2, 1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+1
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+2
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+3
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+4
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+5
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+6
        %torch.2_8_0:5% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+7
        %torch.2_8_0:5% aten::unsqueeze(self:<4xi64>{1}, dim:1:int) -> <4x1xi64>{1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<11254xi64>{1}, dim:0:int) -> <1x11254xi64>{11254, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<22288xf32>{1}, dim:-1:int) -> <22288x1xf32>{1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<22288xi64>{1}, dim:1:int) -> <22288x1xi64>{1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<4096x1xf32>{1, 1}, dim:-1:int) -> <4096x1x1xf32>{1, 1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<4096x1xf32>{1, 1}+4096, dim:-1:int) -> <4096x1x1xf32>{1, 1, 1}+4096
        %torch.2_8_0:5% aten::unsqueeze(self:<4096x1xf32>{1, 4096}, dim:-1:int) -> <4096x1x1xf32>{1, 4096, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<1xf32>{1}, dim:1:int) -> <1x1xf32>{1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<1x32xf32>{32, 1}, dim:0:int) -> <1x1x32xf32>{32, 32, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<12989xi64>{1}, dim:0:int) -> <1x12989xi64>{12989, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<28464xf32>{1}, dim:-1:int) -> <28464x1xf32>{1, 1}
        %torch.2_8_0:5% aten::unsqueeze(self:<28464xi64>{1}, dim:1:int) -> <28464x1xi64>{1, 1}
aten::copy_
        %torch.2_8_0:5% aten::copy_(self:<4096x128x192xbf16>{24576, 192, 1}, src:<4096x128x192xbf16>{24576, 192, 1}) -> <4096x128x192xbf16>{24576, 192, 1}
        %torch.2_8_0:5% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::copy_(self:<32xf32>{1}, src:<32xf32>{1}) -> <32xf32>{1}
        %torch.2_8_0:5% aten::copy_(self:<73903744xbf16>{1}, src:<73903744xbf16>{1}) -> <73903744xbf16>{1}
        %torch.2_8_0:5% aten::copy_(self:<259619072xbf16>{1}, src:<259619072xbf16>{1}) -> <259619072xbf16>{1}
        %torch.2_8_0:5% aten::copy_(self:<7168xbf16>{1}, src:<7168xbf16>{1}) -> <7168xbf16>{1}
        %torch.2_8_0:5% aten::copy_(self:<73903744xbf16>{1}+1667649920, src:<73903744xf32>{1}) -> <73903744xbf16>{1}+1667649920
        %torch.2_8_0:5% aten::copy_(self:<259619072xbf16>{1}+1741560832, src:<259619072xf32>{1}) -> <259619072xbf16>{1}+1741560832
        %torch.2_8_0:5% aten::copy_(self:<7168xbf16>{1}+1741553664, src:<7168xf32>{1}) -> <7168xbf16>{1}+1741553664
        %torch.2_8_0:5% aten::copy_(self:<14680064xbf16>{1}, src:<14680064xbf16>{1}) -> <14680064xbf16>{1}
        %torch.2_8_0:5% aten::copy_(self:<29360128xbf16>{1}, src:<29360128xbf16>{1}) -> <29360128xbf16>{1}
        %torch.2_8_0:5% aten::copy_(self:<14680064xbf16>{1}, src:<14680064xf32>{1}) -> <14680064xbf16>{1}
        %torch.2_8_0:5% aten::copy_(self:<14680064xbf16>{1}+14680064, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:5% aten::copy_(self:<14680064xbf16>{1}+29360128, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:5% aten::copy_(self:<14680064xbf16>{1}+44040192, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:5% aten::copy_(self:<29360128xbf16>{1}+58720256, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:5% aten::copy_(self:<29360128xbf16>{1}+88080384, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:5% aten::copy_(self:<29360128xbf16>{1}+117440512, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:5% aten::copy_(self:<29360128xbf16>{1}+146800640, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+146800640
aten::zero_
        %torch.2_8_0:5% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
        %torch.2_8_0:5% aten::zero_(self:<32xbf16>{1}) -> <32xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<73903744xbf16>{1}) -> <73903744xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<259619072xbf16>{1}) -> <259619072xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<7168xbf16>{1}) -> <7168xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<14680064xbf16>{1}) -> <14680064xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<29360128xbf16>{1}) -> <29360128xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<2xf32>{1}) -> <2xf32>{1}
        %torch.2_8_0:5% aten::zero_(self:<2668239872xbf16>{1}) -> <2668239872xbf16>{1}
        %torch.2_8_0:5% aten::zero_(self:<176160768xbf16>{1}) -> <176160768xbf16>{1}
aten::arange
        %torch.2_8_0:5% aten::arange(start:0:int, end:8192:int, step:4096:int, dtype:torch_int32:dtype, device:cuda_5:device, pin_memory:False:bool) -> <2xi32>{1}
        %torch.2_8_0:5% aten::arange(end:11254:int, device:cuda_5:device, pin_memory:False:bool) -> <11254xi64>{1}
        %torch.2_8_0:5% aten::arange(start:0:int, end:4096:int, device:cuda_5:device, pin_memory:False:bool) -> <4096xi64>{1}
        %torch.2_8_0:5% aten::arange(end:12989:int, device:cuda_5:device, pin_memory:False:bool) -> <12989xi64>{1}
te::get_fused_attn_backend
        %torch.2_8_0:5% te::get_fused_attn_backend(True:bool, DType_kBFloat16:DType, DType_kBFloat16:DType, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, 0_0:float, 128:int, 128:int, 4096:int, 4096:int, 192:int, 128:int, -1:int, 0:int) -> NVTE_Fused_Attn_Backend_NVTE_F16_arbitrary_seqlen:NVTE_Fused_Attn_Backend
aten::slice
        %torch.2_8_0:5% aten::slice(self:<2xi32>{1}, dim:0:int, start:0:int, end:2:int) -> <2xi32>{1}
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:0:int, end:5:int) -> <5xu8>{1}
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:5:int, end:10:int) -> <5xu8>{1}+5
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:10:int, end:15:int) -> <5xu8>{1}+10
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:15:int, end:20:int) -> <5xu8>{1}+15
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:20:int, end:25:int) -> <5xu8>{1}+20
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:25:int, end:30:int) -> <5xu8>{1}+25
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:30:int, end:35:int) -> <5xu8>{1}+30
        %torch.2_8_0:5% aten::slice(self:<40xu8>{1}, dim:0:int, start:35:int, end:40:int) -> <5xu8>{1}+35
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:0:int, end:108:int) -> <108xu8>{1}
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:108:int, end:216:int) -> <108xu8>{1}+108
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:216:int, end:324:int) -> <108xu8>{1}+216
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:324:int, end:432:int) -> <108xu8>{1}+324
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:432:int, end:540:int) -> <108xu8>{1}+432
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:540:int, end:648:int) -> <108xu8>{1}+540
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:648:int, end:756:int) -> <108xu8>{1}+648
        %torch.2_8_0:5% aten::slice(self:<864xu8>{1}, dim:0:int, start:756:int, end:864:int) -> <108xu8>{1}+756
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:0:int, end:333529984:int) -> <333529984xbf16>{1}
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:333529984:int, end:667059968:int) -> <333529984xbf16>{1}+333529984
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:667059968:int, end:1000589952:int) -> <333529984xbf16>{1}+667059968
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1000589952:int, end:1334119936:int) -> <333529984xbf16>{1}+1000589952
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1334119936:int, end:1667649920:int) -> <333529984xbf16>{1}+1334119936
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1667649920:int, end:2001179904:int) -> <333529984xbf16>{1}+1667649920
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:2001179904:int, end:2334709888:int) -> <333529984xbf16>{1}+2001179904
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:2334709888:int, end:2668239872:int) -> <333529984xbf16>{1}+2334709888
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:0:int, end:176160768:int) -> <176160768xbf16>{1}
        %torch.2_8_0:5% aten::slice(self:<117440512xbf16>{1}+1624113152, dim:0:int, start:43536768:int, end:117440512:int) -> <73903744xbf16>{1}+1667649920
        %torch.2_8_0:5% aten::slice(self:<926679040xbf16>{1}+1741560832, dim:0:int, start:0:int, end:259619072:int) -> <259619072xbf16>{1}+1741560832
        %torch.2_8_0:5% aten::slice(self:<7168xbf16>{1}+1741553664, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1741553664
        %torch.2_8_0:5% aten::slice(self:<14680064xbf16>{1}, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}
        %torch.2_8_0:5% aten::slice(self:<14680064xbf16>{1}+14680064, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:5% aten::slice(self:<14680064xbf16>{1}+29360128, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:5% aten::slice(self:<14680064xbf16>{1}+44040192, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:5% aten::slice(self:<29360128xbf16>{1}+58720256, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:5% aten::slice(self:<29360128xbf16>{1}+88080384, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:5% aten::slice(self:<29360128xbf16>{1}+117440512, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:5% aten::slice(self:<29360128xbf16>{1}+146800640, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+146800640
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1667649920:int, end:1741553664:int) -> <73903744xbf16>{1}+1667649920
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1741560832:int, end:2001179904:int) -> <259619072xbf16>{1}+1741560832
        %torch.2_8_0:5% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1741553664:int, end:1741560832:int) -> <7168xbf16>{1}+1741553664
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:14680064:int, end:29360128:int) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:29360128:int, end:44040192:int) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:44040192:int, end:58720256:int) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:58720256:int, end:88080384:int) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:88080384:int, end:117440512:int) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:117440512:int, end:146800640:int) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:5% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:146800640:int, end:176160768:int) -> <29360128xbf16>{1}+146800640
        %torch.2_8_0:5% aten::slice(self:<24xf32>{1}+120, dim:0:int, start:0:int, end:9223372036854775807:int) -> <24xf32>{1}+120
        %torch.2_8_0:5% aten::slice(self:<8x24xf32>{24, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x24xf32>{24, 1}
te::fused_attn_fwd
        %torch.2_8_0:5% te::fused_attn_fwd(4096:int, 4096:int, True:bool, 0_1352337788608801:float, 0_0:float, True:bool, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, tuple{-1:int, 0:int}, <2xi32>{1}, <2xi32>{1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, torch_bfloat16:dtype, None:, None:, None:, None:, None:, None:, None:, None:, 16:int) -> list{<4096x1x128x128xbf16>{16384, 16384, 128, 1}, <1x128x4096x1xf32>{524288, 4096, 1, 1}, <2xi64>{1}}
aten::eq
        %torch.2_8_0:5% aten::eq(self:<5056xu8>{1}, other:<5056xu8>{1}) -> <5056xbool>{1}
        %torch.2_8_0:5% aten::eq(self:<4x11254xbool>{1, 4}, other:0:int) -> <4x11254xbool>{1, 4}
        %torch.2_8_0:5% aten::eq(self:<4x12989xbool>{1, 4}, other:0:int) -> <4x12989xbool>{1, 4}
aten::all
        %torch.2_8_0:5% aten::all(self:<5056xbool>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::all(self:<4xbool>{1}) -> <1xbool>{1}
aten::add
        %torch.2_8_0:5% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:5% aten::add(self:<4096x32xf32>{32, 1}, other:<32xf32>{1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::add(self:<4096x1xf32>{1, 1}, other:1e-20:float) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::add(self:<22288x2048xbf16>{2048, 1}, other:1:int) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::add(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{1, 0}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::add(self:<4096x8xf32>{8, 1}, other:<4096x8xf32>{1, 0}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:5% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::add(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::add(self:<1x32xf32>{32, 1}, other:<1x32xbf16>{32, 1}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:5% aten::add(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::add(self:<28464x2048xbf16>{2048, 1}, other:1:int) -> <28464x2048xbf16>{2048, 1}
aten::split
        %torch.2_8_0:5% aten::split(self:<4096x36864xbf16>{36864, 1}, split_size:18432:int, dim:-1:int) -> list{<4096x18432xbf16>{36864, 1}, <4096x18432xbf16>{36864, 1}+18432}
        %torch.2_8_0:5% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> list{<4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048}
        %torch.2_8_0:5% aten::split(self:<22288x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> list{<22288x2048xbf16>{4096, 1}, <22288x2048xbf16>{4096, 1}+2048}
        %torch.2_8_0:5% aten::split(self:<8192x1xf32>{1, 1}, split_size:4096:int) -> list{<4096x1xf32>{1, 1}, <4096x1xf32>{1, 1}+4096}
        %torch.2_8_0:5% aten::split(self:<28464x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> list{<28464x2048xbf16>{4096, 1}, <28464x2048xbf16>{4096, 1}+2048}
aten::silu
        %torch.2_8_0:5% aten::silu(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::silu(self:<22288x2048xbf16>{4096, 1}) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::silu(self:<28464x2048xbf16>{4096, 1}) -> <28464x2048xbf16>{2048, 1}
aten::sigmoid
        %torch.2_8_0:5% aten::sigmoid(self:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::sigmoid(self:<22288x2048xbf16>{4096, 1}) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::sigmoid(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::sigmoid(self:<28464x2048xbf16>{4096, 1}) -> <28464x2048xbf16>{2048, 1}
aten::topk
        %torch.2_8_0:5% aten::topk(self:<4096x2x16xf32>{32, 16, 1}, k:8:int) -> tuple{<4096x2x8xf32>{16, 8, 1}, <4096x2x8xi64>{16, 8, 1}}
        %torch.2_8_0:5% aten::topk(self:<4096x2xf32>{2, 1}, k:1:int, dim:-1:int, largest:True:bool, sorted:False:bool) -> tuple{<4096x1xf32>{1, 1}, <4096x1xi64>{1, 1}}
        %torch.2_8_0:5% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int) -> tuple{<4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}}
        %torch.2_8_0:5% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int, dim:1:int) -> tuple{<4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}}
aten::sum
        %torch.2_8_0:5% aten::sum(self:<4096x2x8xf32>{16, 8, 1}, dim:list{-1:int}) -> <4096x2xf32>{2, 1}
        %torch.2_8_0:5% aten::sum(self:<4096x8xf32>{8, 1}, dim:list{-1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
        %torch.2_8_0:5% aten::sum(self:<4096x32xf32>{32, 1}, dim:list{-1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<4096x1x32xbool>{32, 32, 1}, dim:list{0:int}, dtype:torch_float32:dtype) -> <1x32xf32>{32, 1}
        %torch.2_8_0:5% aten::sum(self:<1x32xf32>{32, 1}, dim:list{1:int}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::sum(self:<11254x8xbool>{8, 1}, dim:list{1:int}) -> <11254xi64>{1}
        %torch.2_8_0:5% aten::sum(self:<4x11254xbool>{1, 4}, dim:list{1:int}) -> <4xi64>{1}
        %torch.2_8_0:5% aten::sum(self:<4xi64>{1}) -> <1xi64>{1}
        %torch.2_8_0:5% aten::sum(self:<4096x1x129280xf32>{129280, 129280, 1}, dim:list{-1:int}) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<4096xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::sum(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::sum(self:<22288x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <22288x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<4096x32xf32>{32, 1}, dim:list{1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<4096x8xf32>{8, 1}, dim:list{1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<1x32xbf16>{32, 1}, dim:list{-1:int}, keepdim:True:bool) -> <1x1xbf16>{1, 1}
        %torch.2_8_0:5% aten::sum(self:<12989x8xbool>{8, 1}, dim:list{1:int}) -> <12989xi64>{1}
        %torch.2_8_0:5% aten::sum(self:<4x12989xbool>{1, 4}, dim:list{1:int}) -> <4xi64>{1}
        %torch.2_8_0:5% aten::sum(self:<28464x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <28464x1xf32>{1, 1}
aten::scatter_
        %torch.2_8_0:5% aten::scatter_(self:<4096x2xf32>{2, 1}, dim:1:int, index:<4096x1xi64>{1, 1}, value:1:int) -> <4096x2xf32>{2, 1}
aten::bitwise_not
        %torch.2_8_0:5% aten::bitwise_not(self:<4096x32xbool>{32, 1}) -> <4096x32xbool>{32, 1}
aten::masked_fill
        %torch.2_8_0:5% aten::masked_fill(self:<4096x32xf32>{32, 1}, mask:<4096x32xbool>{32, 1}, value:-inf:float) -> <4096x32xf32>{32, 1}
aten::gather
        %torch.2_8_0:5% aten::gather(self:<4096x32xf32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:5% aten::gather(self:<11254x7168xbf16>{7168, 1}, dim:0:int, index:<22288x7168xi64>{1, 0}) -> <22288x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::gather(self:<12989x7168xbf16>{7168, 1}, dim:0:int, index:<28464x7168xi64>{1, 0}) -> <28464x7168xbf16>{7168, 1}
aten::div
        %torch.2_8_0:5% aten::div(self:<4096x8xf32>{8, 1}, other:<4096x1xf32>{1, 1}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:5% aten::div(self:<4096x32xf32>{32, 1}, other:<4096x1xf32>{1, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::div(self:<1xf32>{1}, other:0_001:float) -> <1xf32>{1}
        %torch.2_8_0:5% aten::div(self:<4xi64>{1}, other:16:int) -> <4xf32>{1}
        %torch.2_8_0:5% aten::div(self:<2xf32>{1}, other:8:int) -> <2xf32>{1}
        %torch.2_8_0:5% aten::div(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:5% aten::div(self:<1xf32>{0}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:5% aten::div(self:<4096x1x32xf32>{0, 32, 1}, other:4096:int) -> <4096x1x32xf32>{32, 32, 1}
        %torch.2_8_0:5% aten::div(self:<1x1xbf16>{1, 1}, other:32:int) -> <1x1xbf16>{1, 1}
aten::scatter
        %torch.2_8_0:5% aten::scatter(self:<4096x32xf32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::scatter(self:<4096x32xi32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}, value:1:int) -> <4096x32xi32>{32, 1}
        %torch.2_8_0:5% aten::scatter(self:<4096x32xf32>{32, 1}, dim:-1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
aten::div_
        %torch.2_8_0:5% aten::div_(self:<1x32xf32>{32, 1}, other:1024_0:float) -> <1x32xf32>{32, 1}
        %torch.2_8_0:5% aten::div_(self:<4096x1x129280xf32>{129280, 129280, 1}, other:<4096x1x1xf32>{1, 1, 1}+4096) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:5% aten::div_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
aten::mean
        %torch.2_8_0:5% aten::mean(self:<4096x1x32xf32>{32, 32, 1}, dim:list{0:int}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:5% aten::mean(self:<1xf32>{1}) -> <1xf32>{1}
aten::mul_
        %torch.2_8_0:5% aten::mul_(self:<1xf32>{1}, other:0_001:float) -> <1xf32>{1}
        %torch.2_8_0:5% aten::mul_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:5% aten::mul_(self:<4096x1x129280xf32>{129280, 129280, 1}, other:<4096x1x1xf32>{1, 4096, 1}) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:5% aten::mul_(self:<2668239872xbf16>{1}, other:0_125:float) -> <2668239872xbf16>{1}
        %torch.2_8_0:5% aten::mul_(self:<176160768xbf16>{1}, other:0_125:float) -> <176160768xbf16>{1}
aten::zeros
        %torch.2_8_0:5% aten::zeros(size:list{2:int}, device:cuda_5:device, pin_memory:False:bool) -> <2xf32>{1}
        %torch.2_8_0:5% aten::zeros(size:list{8:int}, dtype:torch_int64:dtype, device:cuda:device, pin_memory:False:bool) -> <8xi64>{1}
        %torch.2_8_0:5% aten::zeros(size:list{11254:int, 4:int}, dtype:torch_int64:dtype, device:cuda_5:device, pin_memory:False:bool) -> <11254x4xi64>{4, 1}
        %torch.2_8_0:5% aten::zeros(size:list{11254:int, 4:int}, dtype:torch_float32:dtype, device:cuda_5:device, pin_memory:False:bool) -> <11254x4xf32>{4, 1}
        %torch.2_8_0:5% aten::zeros(size:list{11254:int, 7168:int}, dtype:torch_bfloat16:dtype, device:cuda_5:device, pin_memory:False:bool) -> <11254x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
        %torch.2_8_0:5% aten::zeros(size:list{11254:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <11254x8xi64>{8, 1}
        %torch.2_8_0:5% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:5% aten::zeros(size:list{1:int, 1:int, 4096:int, 4096:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:5% aten::zeros(size:list{320:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <320xf32>{1}
        %torch.2_8_0:5% aten::zeros(size:list{8:int, 24:int}, dtype:torch_float32:dtype, device:cuda_5:device, pin_memory:False:bool) -> <8x24xf32>{24, 1}
        %torch.2_8_0:5% aten::zeros(size:list{}, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <1xi32>{1}
        %torch.2_8_0:5% aten::zeros(size:list{12989:int, 4:int}, dtype:torch_int64:dtype, device:cuda_5:device, pin_memory:False:bool) -> <12989x4xi64>{4, 1}
        %torch.2_8_0:5% aten::zeros(size:list{12989:int, 4:int}, dtype:torch_float32:dtype, device:cuda_5:device, pin_memory:False:bool) -> <12989x4xf32>{4, 1}
        %torch.2_8_0:5% aten::zeros(size:list{12989:int, 7168:int}, dtype:torch_bfloat16:dtype, device:cuda_5:device, pin_memory:False:bool) -> <12989x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::zeros(size:list{12989:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <12989x8xi64>{8, 1}
aten::add_
        %torch.2_8_0:5% aten::add_(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::add_(self:<32xbf16>{1}, other:<32xi64>{1}) -> <32xbf16>{1}
        %torch.2_8_0:5% aten::add_(self:<1xi32>{1}, other:<1xi32>{1}) -> <1xi32>{1}
        %torch.2_8_0:5% aten::add_(self:<129280x7168xbf16>{7168, 1}, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::add_(self:<7168xbf16>{1}+926679040, other:<7168xbf16>{1}) -> <7168xbf16>{1}+926679040
        %torch.2_8_0:5% aten::add_(self:<7168x2048xbf16>{2048, 1}+44040192, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+44040192
        %torch.2_8_0:5% aten::add_(self:<7168x2048xbf16>{2048, 1}, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::add_(self:<7168x2048xbf16>{2048, 1}+29360128, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+29360128
        %torch.2_8_0:5% aten::add_(self:<7168x2048xbf16>{2048, 1}+14680064, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+14680064
        %torch.2_8_0:5% aten::add_(self:<4096x7168xbf16>{7168, 1}+146800640, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+146800640
        %torch.2_8_0:5% aten::add_(self:<4096x7168xbf16>{7168, 1}+88080384, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+88080384
        %torch.2_8_0:5% aten::add_(self:<4096x7168xbf16>{7168, 1}+58720256, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+58720256
        %torch.2_8_0:5% aten::add_(self:<4096x7168xbf16>{7168, 1}+117440512, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+117440512
        %torch.2_8_0:5% aten::add_(self:<7168x2048xbf16>{2048, 1}+926686208, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+926686208
        %torch.2_8_0:5% aten::add_(self:<4096x7168xbf16>{7168, 1}+941366272, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+941366272
        %torch.2_8_0:5% aten::add_(self:<32x7168xbf16>{7168, 1}+970726400, other:<32x7168xbf16>{7168, 1}) -> <32x7168xbf16>{7168, 1}+970726400
        %torch.2_8_0:5% aten::add_(self:<7168xbf16>{1}+970955776, other:<7168xbf16>{1}) -> <7168xbf16>{1}+970955776
        %torch.2_8_0:5% aten::add_(self:<7168x16384xbf16>{16384, 1}+1040629760, other:<7168x16384xbf16>{16384, 1}) -> <7168x16384xbf16>{16384, 1}+1040629760
        %torch.2_8_0:5% aten::add_(self:<512xbf16>{1}+987740160, other:<512xbf16>{1}) -> <512xbf16>{1}+987740160
        %torch.2_8_0:5% aten::add_(self:<32768x512xbf16>{512, 1}+970962944, other:<32768x512xbf16>{512, 1}) -> <32768x512xbf16>{512, 1}+970962944
        %torch.2_8_0:5% aten::add_(self:<1536xbf16>{1}+1029618176, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1029618176
        %torch.2_8_0:5% aten::add_(self:<24576x1536xbf16>{1536, 1}+991869440, other:<24576x1536xbf16>{1536, 1}) -> <24576x1536xbf16>{1536, 1}+991869440
        %torch.2_8_0:5% aten::add_(self:<576x7168xbf16>{7168, 1}+987740672, other:<576x7168xbf16>{7168, 1}) -> <576x7168xbf16>{7168, 1}+987740672
        %torch.2_8_0:5% aten::add_(self:<1536x7168xbf16>{7168, 1}+1029619712, other:<1536x7168xbf16>{7168, 1}) -> <1536x7168xbf16>{7168, 1}+1029619712
        %torch.2_8_0:5% aten::add_(self:<7168xbf16>{1}+1158070272, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:5% aten::add_(self:<7168x18432xbf16>{18432, 1}+1158077440, other:<7168x18432xbf16>{18432, 1}) -> <7168x18432xbf16>{18432, 1}+1158077440
        %torch.2_8_0:5% aten::add_(self:<7168xbf16>{1}+1554439168, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1554439168
        %torch.2_8_0:5% aten::add_(self:<36864x7168xbf16>{7168, 1}+1290198016, other:<36864x7168xbf16>{7168, 1}) -> <36864x7168xbf16>{7168, 1}+1290198016
        %torch.2_8_0:5% aten::add_(self:<7168x16384xbf16>{16384, 1}+1624113152, other:<7168x16384xbf16>{16384, 1}) -> <7168x16384xbf16>{16384, 1}+1624113152
        %torch.2_8_0:5% aten::add_(self:<512xbf16>{1}+1571223552, other:<512xbf16>{1}) -> <512xbf16>{1}+1571223552
        %torch.2_8_0:5% aten::add_(self:<32768x512xbf16>{512, 1}+1554446336, other:<32768x512xbf16>{512, 1}) -> <32768x512xbf16>{512, 1}+1554446336
        %torch.2_8_0:5% aten::add_(self:<1536xbf16>{1}+1613101568, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1613101568
        %torch.2_8_0:5% aten::add_(self:<24576x1536xbf16>{1536, 1}+1575352832, other:<24576x1536xbf16>{1536, 1}) -> <24576x1536xbf16>{1536, 1}+1575352832
        %torch.2_8_0:5% aten::add_(self:<576x7168xbf16>{7168, 1}+1571224064, other:<576x7168xbf16>{7168, 1}) -> <576x7168xbf16>{7168, 1}+1571224064
        %torch.2_8_0:5% aten::add_(self:<1536x7168xbf16>{7168, 1}+1613103104, other:<1536x7168xbf16>{7168, 1}) -> <1536x7168xbf16>{7168, 1}+1613103104
        %torch.2_8_0:5% aten::add_(self:<7168xbf16>{1}+1741553664, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1741553664
        %torch.2_8_0:5% aten::add_(self:<129280x7168xbf16>{7168, 1}+1741560832, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}+1741560832
c10d::allgather_
        %torch.2_8_0:5% c10d::allgather_(output_tensors:list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, input_tensors:list{<1xi64>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, Work:distributed}
        %torch.2_8_0:5% c10d::allgather_(output_tensors:list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, input_tensors:list{<5xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, Work:distributed}
        %torch.2_8_0:5% c10d::allgather_(output_tensors:list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, input_tensors:list{<108xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, Work:distributed}
aten::gt
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+1, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+2, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+3, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+4, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+5, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+6, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<1xi64>{1}+7, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+1, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+2, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+3, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+4, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+5, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+6, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+7, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+8, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+9, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+10, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+11, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+12, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+13, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+14, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+15, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+16, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+17, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+18, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+19, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+20, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+21, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+22, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:5% aten::gt(self:<8xf32>{24}+23, other:0_0:float) -> <8xbool>{1}
aten::resize_
        %torch.2_8_0:5% aten::resize_(self:<5xu8>{1}, size:list{5:int}) -> <5xu8>{1}
        %torch.2_8_0:5% aten::resize_(self:<108xu8>{1}, size:list{108:int}) -> <108xu8>{1}
aten::record_stream
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<11254x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=9:Stream) -> None:
        %torch.2_8_0:5% aten::record_stream(self:<12989x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=5,_stream_id=0:Stream) -> None:
aten::ne
        %torch.2_8_0:5% aten::ne(self:<11254x8xi64>{8, 1}, other:-1:int) -> <11254x8xbool>{8, 1}
        %torch.2_8_0:5% aten::ne(self:<12989x8xi64>{8, 1}, other:-1:int) -> <12989x8xbool>{8, 1}
aten::index
        %torch.2_8_0:5% aten::index(self:<11254x8xi64>{8, 1}, indices:list{<11254x8xbool>{8, 1}}) -> <22260xi64>{1}
        %torch.2_8_0:5% aten::index(self:<11254x8xf32>{8, 1}, indices:list{<11254x8xbool>{8, 1}}) -> <22260xf32>{1}
        %torch.2_8_0:5% aten::index(self:<4096x129280xf32>{129280, 1}, indices:list{<4096xi64>{1}, <4096xi64>{1}}) -> <4096xf32>{1}
        %torch.2_8_0:5% aten::index(self:<11254x4xf32>{1, 11254}, indices:list{<22260xi64>{1}, <22260xi64>{1}}) -> <22260xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+1, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+2, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+3, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+4, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+5, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+6, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+7, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+8, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+9, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+10, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+11, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+12, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+13, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+14, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+15, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+16, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+17, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+18, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+19, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+20, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+21, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+22, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<8xf32>{24}+23, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:5% aten::index(self:<12989x8xi64>{8, 1}, indices:list{<12989x8xbool>{8, 1}}) -> <28429xi64>{1}
        %torch.2_8_0:5% aten::index(self:<12989x8xf32>{8, 1}, indices:list{<12989x8xbool>{8, 1}}) -> <28429xf32>{1}
        %torch.2_8_0:5% aten::index(self:<12989x4xf32>{1, 12989}, indices:list{<28429xi64>{1}, <28429xi64>{1}}) -> <28429xf32>{1}
aten::repeat_interleave
        %torch.2_8_0:5% aten::repeat_interleave(repeats:<11254xi64>{1}) -> <22260xi64>{1}
        %torch.2_8_0:5% aten::repeat_interleave(repeats:<12989xi64>{1}) -> <28429xi64>{1}
aten::index_select
        %torch.2_8_0:5% aten::index_select(self:<11254xi64>{1}, dim:0:int, index:<22260xi64>{1}) -> <22260xi64>{1}
        %torch.2_8_0:5% aten::index_select(self:<11254x7168xbf16>{7168, 1}, dim:0:int, index:<22288xi64>{1}) -> <22288x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::index_select(self:<12989xi64>{1}, dim:0:int, index:<28429xi64>{1}) -> <28429xi64>{1}
        %torch.2_8_0:5% aten::index_select(self:<12989x7168xbf16>{7168, 1}, dim:0:int, index:<28464xi64>{1}) -> <28464x7168xbf16>{7168, 1}
aten::index_put_
        %torch.2_8_0:5% aten::index_put_(self:<11254x4xi64>{4, 1}, indices:list{<22260xi64>{1}, <22260xi64>{1}}, values:<1xi64>{1}) -> <11254x4xi64>{4, 1}
        %torch.2_8_0:5% aten::index_put_(self:<11254x4xf32>{4, 1}, indices:list{<22260xi64>{1}, <22260xi64>{1}}, values:<22260xf32>{1}) -> <11254x4xf32>{4, 1}
        %torch.2_8_0:5% aten::index_put_(self:<4x11254xbool>{1, 4}, indices:list{<4x11254xbool>{11254, 1}}, values:<1xbool>{1}) -> <4x11254xbool>{1, 4}
        %torch.2_8_0:5% aten::index_put_(self:<4096x1xi64>{1, 4096}, indices:list{<4096x1xbool>{1, 1}}, values:<1xi64>{1}) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:5% aten::index_put_(self:<4096x1xf32>{1, 1}, indices:list{<4096x1xbool>{1, 1}}, values:<1xf32>{1}) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::index_put_(self:<4096x129280xf32>{129280, 1}, indices:list{<4096xi64>{1}, <4096xi64>{1}}, values:<4096xf32>{1}) -> <4096x129280xf32>{129280, 1}
        %torch.2_8_0:5% aten::index_put_(self:<12989x4xi64>{4, 1}, indices:list{<28429xi64>{1}, <28429xi64>{1}}, values:<1xi64>{1}) -> <12989x4xi64>{4, 1}
        %torch.2_8_0:5% aten::index_put_(self:<12989x4xf32>{4, 1}, indices:list{<28429xi64>{1}, <28429xi64>{1}}, values:<28429xf32>{1}) -> <12989x4xf32>{4, 1}
        %torch.2_8_0:5% aten::index_put_(self:<4x12989xbool>{1, 4}, indices:list{<4x12989xbool>{12989, 1}}, values:<1xbool>{1}) -> <4x12989xbool>{1, 4}
aten::ceil
        %torch.2_8_0:5% aten::ceil(self:<4xf32>{1}) -> <4xf32>{1}
aten::le
        %torch.2_8_0:5% aten::le(self:<4xi64>{1}, other:11254:int) -> <4xbool>{1}
        %torch.2_8_0:5% aten::le(self:<4x11254xi64>{11254, 1}, other:<4x1xi64>{1, 1}) -> <4x11254xbool>{11254, 1}
        %torch.2_8_0:5% aten::le(self:<4xi64>{1}, other:12989:int) -> <4xbool>{1}
        %torch.2_8_0:5% aten::le(self:<4x12989xi64>{12989, 1}, other:<4x1xi64>{1, 1}) -> <4x12989xbool>{12989, 1}
aten::neg
        %torch.2_8_0:5% aten::neg(self:<4xi64>{1}) -> <4xi64>{1}
        %torch.2_8_0:5% aten::neg(self:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::neg(self:<4096x8xf32>{8, 1}) -> <4096x8xf32>{8, 1}
aten::remainder
        %torch.2_8_0:5% aten::remainder(self:<4xi64>{1}, other:16:int) -> <4xi64>{1}
aten::cumsum
        %torch.2_8_0:5% aten::cumsum(self:<4x11254xi32>{1, 4}, dim:1:int) -> <4x11254xi64>{11254, 1}
        %torch.2_8_0:5% aten::cumsum(self:<4x12989xi32>{1, 4}, dim:1:int) -> <4x12989xi64>{12989, 1}
aten::permute
        %torch.2_8_0:5% aten::permute(self:<11254x4xbool>{4, 1}, dims:list{1:int, 0:int}) -> <4x11254xbool>{1, 4}
        %torch.2_8_0:5% aten::permute(self:<11254x4xf32>{4, 1}, dims:list{1:int, 0:int}) -> <4x11254xf32>{1, 4}
        %torch.2_8_0:5% aten::permute(self:<4x11254xf32>{11254, 1}, dims:list{1:int, 0:int}) -> <11254x4xf32>{1, 11254}
        %torch.2_8_0:5% aten::permute(self:<12989x4xbool>{4, 1}, dims:list{1:int, 0:int}) -> <4x12989xbool>{1, 4}
        %torch.2_8_0:5% aten::permute(self:<12989x4xf32>{4, 1}, dims:list{1:int, 0:int}) -> <4x12989xf32>{1, 4}
        %torch.2_8_0:5% aten::permute(self:<4x12989xf32>{12989, 1}, dims:list{1:int, 0:int}) -> <12989x4xf32>{1, 12989}
aten::masked_select
        %torch.2_8_0:5% aten::masked_select(self:<4x11254xi64>{0, 1}, mask:<4x11254xbool>{11254, 1}) -> <22288xi64>{1}
        %torch.2_8_0:5% aten::masked_select(self:<4x11254xf32>{11254, 1}, mask:<4x11254xbool>{11254, 1}) -> <22288xf32>{1}
        %torch.2_8_0:5% aten::masked_select(self:<4x12989xi64>{0, 1}, mask:<4x12989xbool>{12989, 1}) -> <28464xi64>{1}
        %torch.2_8_0:5% aten::masked_select(self:<4x12989xf32>{12989, 1}, mask:<4x12989xbool>{12989, 1}) -> <28464xf32>{1}
te::split_quantize
        %torch.2_8_0:5% te::split_quantize(<22288x7168xbf16>{7168, 1}, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<2704x7168xf8e4m3>, <7008x7168xf8e4m3>, <6432x7168xf8e4m3>, <6144x7168xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<22288x2048xbf16>{2048, 1}, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<2704x2048xf8e4m3>, <7008x2048xf8e4m3>, <6432x2048xf8e4m3>, <6144x2048xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<22288x7168xbf16>{7168, 1}, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<2704x7168xf8e4m3>, <7008x7168xf8e4m3>, <6432x7168xf8e4m3>, <6144x7168xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<22288x4096xbf16>{4096, 1}, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<2704x4096xf8e4m3>, <7008x4096xf8e4m3>, <6432x4096xf8e4m3>, <6144x4096xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<28464x7168xbf16>{7168, 1}, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<5184x7168xf8e4m3>, <3312x7168xf8e4m3>, <9776x7168xf8e4m3>, <10192x7168xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<28464x2048xbf16>{2048, 1}, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<5184x2048xf8e4m3>, <3312x2048xf8e4m3>, <9776x2048xf8e4m3>, <10192x2048xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<28464x7168xbf16>{7168, 1}, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<5184x7168xf8e4m3>, <3312x7168xf8e4m3>, <9776x7168xf8e4m3>, <10192x7168xf8e4m3>}
        %torch.2_8_0:5% te::split_quantize(<28464x4096xbf16>{4096, 1}, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<5184x4096xf8e4m3>, <3312x4096xf8e4m3>, <9776x4096xf8e4m3>, <10192x4096xf8e4m3>}
te::get_num_cublas_streams
        %torch.2_8_0:5% te::get_num_cublas_streams() -> 4:int
te::te_general_grouped_gemm
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, True:bool, list{<2704x7168xf8e4m3>, <7008x7168xf8e4m3>, <6432x7168xf8e4m3>, <6144x7168xf8e4m3>}, False:bool, list{<22288x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, True:bool, list{<2704x2048xf8e4m3>, <7008x2048xf8e4m3>, <6432x2048xf8e4m3>, <6144x2048xf8e4m3>}, False:bool, list{<22288x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, False:bool, list{<2704x7168xf8e4m3>, <7008x7168xf8e4m3>, <6432x7168xf8e4m3>, <6144x7168xf8e4m3>}, False:bool, list{<22288x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<2704x2048xf8e4m3>, <7008x2048xf8e4m3>, <6432x2048xf8e4m3>, <6144x2048xf8e4m3>}, False:bool, list{<2704x7168xf8e4m3>, <7008x7168xf8e4m3>, <6432x7168xf8e4m3>, <6144x7168xf8e4m3>}, True:bool, list{<7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, False:bool, list{<2704x4096xf8e4m3>, <7008x4096xf8e4m3>, <6432x4096xf8e4m3>, <6144x4096xf8e4m3>}, False:bool, list{<22288x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<2704x7168xf8e4m3>, <7008x7168xf8e4m3>, <6432x7168xf8e4m3>, <6144x7168xf8e4m3>}, False:bool, list{<2704x4096xf8e4m3>, <7008x4096xf8e4m3>, <6432x4096xf8e4m3>, <6144x4096xf8e4m3>}, True:bool, list{<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{2704:int, 7008:int, 6432:int, 6144:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, True:bool, list{<5184x7168xf8e4m3>, <3312x7168xf8e4m3>, <9776x7168xf8e4m3>, <10192x7168xf8e4m3>}, False:bool, list{<28464x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, True:bool, list{<5184x2048xf8e4m3>, <3312x2048xf8e4m3>, <9776x2048xf8e4m3>, <10192x2048xf8e4m3>}, False:bool, list{<28464x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, False:bool, list{<5184x7168xf8e4m3>, <3312x7168xf8e4m3>, <9776x7168xf8e4m3>, <10192x7168xf8e4m3>}, False:bool, list{<28464x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<5184x2048xf8e4m3>, <3312x2048xf8e4m3>, <9776x2048xf8e4m3>, <10192x2048xf8e4m3>}, False:bool, list{<5184x7168xf8e4m3>, <3312x7168xf8e4m3>, <9776x7168xf8e4m3>, <10192x7168xf8e4m3>}, True:bool, list{<7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, False:bool, list{<5184x4096xf8e4m3>, <3312x4096xf8e4m3>, <9776x4096xf8e4m3>, <10192x4096xf8e4m3>}, False:bool, list{<28464x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:5% te::te_general_grouped_gemm(list{<5184x7168xf8e4m3>, <3312x7168xf8e4m3>, <9776x7168xf8e4m3>, <10192x7168xf8e4m3>}, False:bool, list{<5184x4096xf8e4m3>, <3312x4096xf8e4m3>, <9776x4096xf8e4m3>, <10192x4096xf8e4m3>}, True:bool, list{<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{5184:int, 3312:int, 9776:int, 10192:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
aten::scatter_add_
        %torch.2_8_0:5% aten::scatter_add_(self:<11254x7168xbf16>{7168, 1}, dim:0:int, index:<22288x7168xi64>{1, 0}, src:<22288x7168xbf16>{7168, 1}) -> <11254x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::scatter_add_(self:<12989x7168xbf16>{7168, 1}, dim:0:int, index:<28464x7168xi64>{1, 0}, src:<28464x7168xbf16>{7168, 1}) -> <12989x7168xbf16>{7168, 1}
aten::mm
        %torch.2_8_0:5% aten::mm(self:<4096x7168xbf16>{7168, 1}, mat2:<7168x129280xbf16>{1, 7168}) -> <4096x129280xbf16>{129280, 1}
        %torch.2_8_0:5% aten::mm(self:<4096x129280xbf16>{129280, 1}, mat2:<129280x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::mm(self:<129280x4096xbf16>{1, 129280}, mat2:<4096x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
aten::max
        %torch.2_8_0:5% aten::max(self:<4096x1x129280xf32>{129280, 129280, 1}, dim:-1:int) -> tuple{<4096x1xf32>{1, 1}, <4096x1xi64>{1, 1}}
c10d::allreduce_
        %torch.2_8_0:5% c10d::allreduce_(tensors:list{<4096x1xf32>{1, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<4096x1xf32>{1, 1}}, Work:distributed}
        %torch.2_8_0:5% c10d::allreduce_(tensors:list{<8192x1xf32>{1, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<8192x1xf32>{1, 1}}, Work:distributed}
        %torch.2_8_0:5% c10d::allreduce_(tensors:list{<2xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<2xf32>{1}}, Work:distributed}
        %torch.2_8_0:5% c10d::allreduce_(tensors:list{<1x32xbf16>{32, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<1x32xbf16>{32, 1}}, Work:distributed}
        %torch.2_8_0:5% c10d::allreduce_(tensors:list{<1xi32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<1xi32>{1}}, Work:distributed}
        %torch.2_8_0:5% c10d::allreduce_(tensors:list{<1xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<1xf32>{1}}, Work:distributed}
aten::sub_
        %torch.2_8_0:5% aten::sub_(self:<4096x1x129280xf32>{129280, 129280, 1}, other:<4096x1x1xf32>{1, 1, 1}) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:5% aten::sub_(self:<4096xf32>{1}, other:<4096xf32>{1}) -> <4096xf32>{1}
aten::lt
        %torch.2_8_0:5% aten::lt(self:<4096x1xi64>{1, 4096}, other:0:int) -> <4096x1xbool>{1, 4096}
aten::ge
        %torch.2_8_0:5% aten::ge(self:<4096x1xi64>{1, 4096}, other:129280:int) -> <4096x1xbool>{1, 4096}
aten::bitwise_or
        %torch.2_8_0:5% aten::bitwise_or(self:<4096x1xbool>{1, 4096}, other:<4096x1xbool>{1, 4096}) -> <4096x1xbool>{1, 1}
aten::sub
        %torch.2_8_0:5% aten::sub(self:<4096x1xi64>{1, 4096}, other:0:int) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:5% aten::sub(self:<4096x1xf32>{1, 1}, other:<4096x1xf32>{1, 1}) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:5% aten::sub(self:<1x1xbf16>{1, 1}, other:<1x32xbf16>{32, 1}) -> <1x32xbf16>{32, 1}
aten::exp
        %torch.2_8_0:5% aten::exp(self:<4096x1x129280xf32>{129280, 129280, 1}, out:<4096x1x129280xf32>{129280, 129280, 1}) -> <4096x1x129280xf32>{129280, 129280, 1}
aten::cat
        %torch.2_8_0:5% aten::cat(tensors:list{<4096x1xf32>{1, 1}, <4096x1xf32>{1, 1}}) -> <8192x1xf32>{1, 1}
        %torch.2_8_0:5% aten::cat(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
        %torch.2_8_0:5% aten::cat(tensors:list{<22288x2048xf32>{2048, 1}, <22288x2048xf32>{2048, 1}}, dim:-1:int) -> <22288x4096xf32>{4096, 1}
        %torch.2_8_0:5% aten::cat(tensors:list{<4096x2048xbf16>{2048, 1}, <4096x2048xbf16>{2048, 1}}, dim:-1:int) -> <4096x4096xbf16>{4096, 1}
        %torch.2_8_0:5% aten::cat(tensors:list{<4096x1x512xbf16>{512, 512, 1}, <4096x1x64xbf16>{64, 64, 1}}, dim:2:int) -> <4096x1x576xbf16>{576, 576, 1}
        %torch.2_8_0:5% aten::cat(tensors:list{<4096x18432xbf16>{18432, 1}, <4096x18432xbf16>{18432, 1}}, dim:-1:int) -> <4096x36864xbf16>{36864, 1}
        %torch.2_8_0:5% aten::cat(tensors:list{<1xf32>{1}}) -> <1xf32>{1}
        %torch.2_8_0:5% aten::cat(tensors:list{<28464x2048xf32>{2048, 1}, <28464x2048xf32>{2048, 1}}, dim:-1:int) -> <28464x4096xf32>{4096, 1}
aten::log
        %torch.2_8_0:5% aten::log(self:<4096x1xf32>{1, 1}+4096) -> <4096x1xf32>{1, 1}
aten::stack
        %torch.2_8_0:5% aten::stack(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
        %torch.2_8_0:5% aten::stack(tensors:list{<32xbf16>{1}}) -> <1x32xbf16>{32, 1}
        %torch.2_8_0:5% aten::stack(tensors:list{<32xf32>{1}}) -> <1x32xf32>{32, 1}
aten::isnan
        %torch.2_8_0:5% aten::isnan(self:<2xf32>{1}) -> <2xbool>{1}
        %torch.2_8_0:5% aten::isnan(self:<1xbf16>{1}) -> <1xbool>{1}
aten::any
        %torch.2_8_0:5% aten::any(self:<2xbool>{1}) -> <1xbool>{1}
aten::unbind
        %torch.2_8_0:5% aten::unbind(self:<2xf32>{1}) -> list{<1xf32>{1}, <1xf32>{1}}
        %torch.2_8_0:5% aten::unbind(self:<1x32xf32>{32, 1}) -> list{<32xf32>{1}}
aten::ones
        %torch.2_8_0:5% aten::ones(size:list{1:int}, device:cuda_5:device, pin_memory:False:bool) -> <1xf32>{1}
        %torch.2_8_0:5% aten::ones(size:list{1:int}, dtype:torch_float32:dtype, device:cuda_5:device, pin_memory:False:bool) -> <1xf32>{1}
aten::select_backward
        %torch.2_8_0:5% aten::select_backward(grad_output:<1xf32>{1}, input_sizes:list{2:int}, dim:0:int, index:0:int) -> <2xf32>{1}
aten::rsub
        %torch.2_8_0:5% aten::rsub(self:<4096xf32>{1}, other:1_0:float) -> <4096xf32>{1}
        %torch.2_8_0:5% aten::rsub(self:<22288x2048xbf16>{2048, 1}, other:1:int) -> <22288x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::rsub(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:5% aten::rsub(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:5% aten::rsub(self:<28464x2048xbf16>{2048, 1}, other:1:int) -> <28464x2048xbf16>{2048, 1}
te::rmsnorm_bwd
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+926679040, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+970955776, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+987740160, 0:int, False:bool) -> list{<4096x512xbf16>{512, 1}, <512xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1029618176, 0:int, False:bool) -> list{<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1158070272, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1554439168, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+1571223552, 0:int, False:bool) -> list{<4096x512xbf16>{512, 1}, <512xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1613101568, 0:int, False:bool) -> list{<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}}
        %torch.2_8_0:5% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1741553664, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
aten::squeeze
        %torch.2_8_0:5% aten::squeeze(self:<22288x1xf32>{1, 1}, dim:-1:int) -> <22288xf32>{1}
        %torch.2_8_0:5% aten::squeeze(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:-2:int) -> <4096x1x64xbf16>{64, 64, 1}
        %torch.2_8_0:5% aten::squeeze(self:<28464x1xf32>{1, 1}, dim:-1:int) -> <28464xf32>{1}
aten::new_zeros
        %torch.2_8_0:5% aten::new_zeros(self:<22288x7168xbf16>{7168, 1}, size:list{11254:int, 7168:int}, dtype:torch_bfloat16:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <11254x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::new_zeros(self:<22260xf32>{1}, size:list{11254:int, 8:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <11254x8xf32>{8, 1}
        %torch.2_8_0:5% aten::new_zeros(self:<4096x8xf32>{8, 1}, size:list{4096:int, 32:int}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:5% aten::new_zeros(self:<28464x7168xbf16>{7168, 1}, size:list{12989:int, 7168:int}, dtype:torch_bfloat16:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <12989x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::new_zeros(self:<28429xf32>{1}, size:list{12989:int, 8:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_5:device) -> <12989x8xf32>{8, 1}
aten::index_add
        %torch.2_8_0:5% aten::index_add(self:<11254x7168xbf16>{7168, 1}, dim:0:int, index:<22288xi64>{1}, source:<22288x7168xbf16>{7168, 1}) -> <11254x7168xbf16>{7168, 1}
        %torch.2_8_0:5% aten::index_add(self:<12989x7168xbf16>{7168, 1}, dim:0:int, index:<28464xi64>{1}, source:<28464x7168xbf16>{7168, 1}) -> <12989x7168xbf16>{7168, 1}
aten::masked_scatter
        %torch.2_8_0:5% aten::masked_scatter(self:<4x11254xf32>{11254, 1}, mask:<4x11254xbool>{11254, 1}, source:<22288xf32>{1}) -> <4x11254xf32>{11254, 1}
        %torch.2_8_0:5% aten::masked_scatter(self:<4x12989xf32>{12989, 1}, mask:<4x12989xbool>{12989, 1}, source:<28464xf32>{1}) -> <4x12989xf32>{12989, 1}
aten::index_put
        %torch.2_8_0:5% aten::index_put(self:<11254x8xf32>{8, 1}, indices:list{<11254x8xbool>{8, 1}}, values:<22260xf32>{1}, accumulate:True:bool) -> <11254x8xf32>{8, 1}
        %torch.2_8_0:5% aten::index_put(self:<12989x8xf32>{8, 1}, indices:list{<12989x8xbool>{8, 1}}, values:<28429xf32>{1}, accumulate:True:bool) -> <12989x8xf32>{8, 1}
aten::sigmoid_backward
        %torch.2_8_0:5% aten::sigmoid_backward(grad_output:<4096x32xf32>{32, 1}, output:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
aten::scatter_add
        %torch.2_8_0:5% aten::scatter_add(self:<4096x32xf32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
te::fused_attn_bwd
        %torch.2_8_0:5% te::fused_attn_bwd(4096:int, 4096:int, 0_1352337788608801:float, 0_0:float, True:bool, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, tuple{-1:int, 0:int}, False:bool, <2xi32>{1}, <2xi32>{1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, torch_bfloat16:dtype, DType_kBFloat16:DType, list{<1x128x4096x1xf32>{524288, 4096, 1, 1}, <2xi64>{1}}, None:, None:, None:, None:, None:) -> list{<4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, None:}
aten::embedding_dense_backward
        %torch.2_8_0:5% aten::embedding_dense_backward(grad_output:<1x4096x7168xbf16>{7168, 7168, 1}, indices:<1x4096xi64>{4096, 1}, num_weights:129280:int, padding_idx:-1:int, scale_grad_by_freq:False:bool) -> <129280x7168xbf16>{7168, 1}
aten::linalg_vector_norm
        %torch.2_8_0:5% aten::linalg_vector_norm(self:<2668239872xbf16>{1}) -> <1xbf16>{1}
        %torch.2_8_0:5% aten::linalg_vector_norm(self:<176160768xbf16>{1}) -> <1xbf16>{1}
c10d::reduce_scatter_tensor_coalesced_
        %torch.2_8_0:5% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<333529984xbf16>{1}+1667649920}, inputs:list{<2668239872xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
        %torch.2_8_0:5% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<176160768xbf16>{1}}, inputs:list{<176160768xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
aten::sign
        %torch.2_8_0:5% aten::sign(self:<1x32xbf16>{32, 1}) -> <1x32xbf16>{32, 1}
te::multi_tensor_l2norm
        %torch.2_8_0:5% te::multi_tensor_l2norm(65536:int, <1xi32>{1}, list{list{<73903744xf32>{1}, <259619072xf32>{1}, <7168xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, False:bool) -> tuple{<1xf32>{1}, <0xf32>{1}}
aten::pow
        %torch.2_8_0:5% aten::pow(self:<1xf32>{1}, exponent:2_0:float) -> <1xf32>{1}
te::multi_tensor_scale
        %torch.2_8_0:5% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<73903744xf32>{1}, <259619072xf32>{1}, <7168xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}, <7168xf32>{1}}}, 1_6138403784027427e-05:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 1_6138403784027427e-05:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<73903744xf32>{1}, <259619072xf32>{1}, <7168xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}, <7168xf32>{1}}}, 2_086718301331225e-05:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 2_086718301331225e-05:float) -> None:
profiler::_record_function_enter_new
        %torch.2_8_0:5% profiler::_record_function_enter_new(name:Optimizer_step#FusedAdam_step:str) -> Work:distributed
        %torch.2_8_0:5% profiler::_record_function_enter_new(name:enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str) -> Work:distributed
te::multi_tensor_adam
        %torch.2_8_0:5% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<73903744xf32>{1}, <259619072xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<7168xf32>{1}}, list{<7168xf32>{1}}, list{<7168xf32>{1}}, list{<7168xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_0:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<73903744xf32>{1}, <259619072xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}}, list{<73903744xf32>{1}, <259619072xf32>{1}}}, 9_779754323328192e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<7168xf32>{1}}, list{<7168xf32>{1}}, list{<7168xf32>{1}}, list{<7168xf32>{1}}}, 9_779754323328192e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_0:float) -> None:
        %torch.2_8_0:5% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 9_779754323328192e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float) -> None:
profiler::_record_function_exit
        %torch.2_8_0:5% profiler::_record_function_exit(_0:Work:distributed) -> None:
c10d::allgather_into_tensor_coalesced_
        %torch.2_8_0:5% c10d::allgather_into_tensor_coalesced_(outputs:list{<2668239872xbf16>{1}}, inputs:list{<333529984xbf16>{1}+1667649920}, process_group:ProcessGroup:distributed) -> Work:distributed
        %torch.2_8_0:5% c10d::allgather_into_tensor_coalesced_(outputs:list{<176160768xbf16>{1}}, inputs:list{<176160768xbf16>{1}}, process_group:ProcessGroup:distributed) -> Work:distributed
c10d::_allgather_base_
        %torch.2_8_0:5% c10d::_allgather_base_(output_tensor:<192xf32>{1}, input_tensor:<24xf32>{1}+120, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{<192xf32>{1}, Work:distributed}
c10d::barrier
        %torch.2_8_0:5% c10d::barrier(tensor:<1xu8>{1}, process_group:ProcessGroup:distributed, device_ids:list{}, async_op:False:bool) -> Work:distributed
c10d::broadcast_
        %torch.2_8_0:5% c10d::broadcast_(tensors:list{<1x4096xi64>{4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> tuple{list{<1x4096xi64>{4096, 1}}, Work:distributed}
        %torch.2_8_0:5% c10d::broadcast_(tensors:list{<1x4096xf32>{4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> tuple{list{<1x4096xf32>{4096, 1}}, Work:distributed}
        %torch.2_8_0:5% c10d::broadcast_(tensors:list{<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> tuple{list{<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}}, Work:distributed}
aten::isinf
        %torch.2_8_0:5% aten::isinf(self:<1xbf16>{1}) -> <1xbool>{1}
save trace data: 7 24 0 100
save trace data: 7 24 0 100
Generated target format ops.json with 100 operations
Generated target format ops.json with 100 operations
================================================== trace ops ==================================================
te::get_cudnn_version
        %torch.2_8_0:3% te::get_cudnn_version() -> 91100:int
aten::embedding
        %torch.2_8_0:3% aten::embedding(weight:<129280x7168xbf16>{7168, 1}+1741560832, indices:<1x4096xi64>{4096, 1}) -> <1x4096x7168xbf16>{29360128, 7168, 1}
aten::transpose
        %torch.2_8_0:3% aten::transpose(self:<1x4096x7168xbf16>{29360128, 7168, 1}, dim0:0:int, dim1:1:int) -> <4096x1x7168xbf16>{7168, 29360128, 1}
        %torch.2_8_0:3% aten::transpose(self:<19621x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x19621xbool>{1, 4}
        %torch.2_8_0:3% aten::transpose(self:<4x19621xbool>{1, 4}, dim0:0:int, dim1:1:int) -> <19621x4xbool>{4, 1}
        %torch.2_8_0:3% aten::transpose(self:<1x4096xi64>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:3% aten::transpose(self:<4096x1xf32>{1, 1}, dim0:0:int, dim1:1:int) -> <1x4096xf32>{1, 1}
        %torch.2_8_0:3% aten::transpose(self:<1x4096xf32>{4096, 1}, dim0:0:int, dim1:1:int) -> <4096x1xf32>{1, 4096}
        %torch.2_8_0:3% aten::transpose(self:<4096x1x7168xbf16>{7168, 7168, 1}, dim0:0:int, dim1:1:int) -> <1x4096x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::transpose(self:<18755x4xbool>{4, 1}, dim0:0:int, dim1:1:int) -> <4x18755xbool>{1, 4}
        %torch.2_8_0:3% aten::transpose(self:<4x18755xbool>{1, 4}, dim0:0:int, dim1:1:int) -> <18755x4xbool>{4, 1}
te::rmsnorm_fwd
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1741553664, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1613101568, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x1536xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+1571223552, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x512xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1554439168, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+1158070272, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}+1029618176, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x1536xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x512xbf16>{512, 1}, <512xbf16>{1}+987740160, 1e-06:float, None:, <True, True, True>:Float8BlockQuantizer, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x512xf8e4m3>, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+970955776, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
        %torch.2_8_0:3% te::rmsnorm_fwd(<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}+926679040, 1e-06:float, None:, None:, DType_kBFloat16:DType, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, <4096xf32>{1}}
aten::cos
        %torch.2_8_0:3% aten::cos(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
aten::mul
        %torch.2_8_0:3% aten::mul(self:<4096x1x1x64xf32>{64, 64, 64, 1}, other:1_0:float) -> <4096x1x1x64xf32>{64, 64, 64, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{36864, 1}+18432) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x8xf32>{8, 1}, other:2_5:float) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:3% aten::mul(self:<1x32xf32>{32, 1}, other:<1x32xf32>{32, 1}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{4096, 1}+2048) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4xf32>{1}, other:16:int) -> <4xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<38688x2048xbf16>{2048, 1}, other:<38688x2048xbf16>{4096, 1}+2048) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<38688x2048xbf16>{2048, 1}, other:<38688x1xf32>{1, 1}) -> <38688x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4096xf32>{1}, other:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<4096xf32>{0}, other:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<38688x2048xf32>{2048, 1}, other:<38688x2048xbf16>{2048, 1}) -> <38688x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<38688x2048xbf16>{4096, 1}, other:<38688x2048xbf16>{2048, 1}) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<38688x2048xf32>{2048, 1}, other:<38688x2048xbf16>{4096, 1}+2048) -> <38688x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<38688x2048xbf16>{2048, 1}, other:<38688x2048xf32>{2048, 1}) -> <38688x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x2048xbf16>{2048, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x2048xbf16>{4096, 1}, other:<4096x2048xbf16>{2048, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<1xf32>{1}, other:0_001:float) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul(self:<1x32xf32>{1, 0}, other:<1x32xf32>{32, 1}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x8xf32>{8, 1}, other:<4096x8xf32>{8, 1}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x18432xbf16>{18432, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::mul(self:<4096x18432xbf16>{36864, 1}, other:<4096x18432xbf16>{18432, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::mul(self:<1x32xbf16>{32, 1}, other:0_001:float) -> <1x32xbf16>{32, 1}
        %torch.2_8_0:3% aten::mul(self:<35904x2048xbf16>{2048, 1}, other:<35904x2048xbf16>{4096, 1}+2048) -> <35904x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<35904x2048xbf16>{2048, 1}, other:<35904x1xf32>{1, 1}) -> <35904x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<35904x2048xf32>{2048, 1}, other:<35904x2048xbf16>{2048, 1}) -> <35904x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<35904x2048xbf16>{4096, 1}, other:<35904x2048xbf16>{2048, 1}) -> <35904x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<35904x2048xf32>{2048, 1}, other:<35904x2048xbf16>{4096, 1}+2048) -> <35904x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::mul(self:<35904x2048xbf16>{2048, 1}, other:<35904x2048xf32>{2048, 1}) -> <35904x2048xf32>{2048, 1}
aten::_to_copy
        %torch.2_8_0:3% aten::_to_copy(self:<4096x1x1x64xf32>{64, 64, 64, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x1x64xbf16>{64, 64, 64, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32xbf16>{1}, dtype:torch_float32:dtype) -> <32xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x32xf32>{32, 1}, dtype:torch_bool:dtype) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x32xf32>{32, 1}, dtype:torch_int32:dtype) -> <4096x32xi32>{32, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x32xi32>{32, 1}, dtype:torch_bool:dtype) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1xi64>{1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <1xi64>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+5, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+10, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+15, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+20, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+25, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+30, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<5xu8>{1}+35, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <5xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+108, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+216, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+324, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+432, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+540, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+648, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<108xu8>{1}+756, dtype:torch_uint8:dtype, layout:torch_strided:layout, device:cpu:device) -> <108xu8>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<19621x4xi64>{4, 1}, dtype:torch_bool:dtype) -> <19621x4xbool>{4, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4xf32>{1}, dtype:torch_int64:dtype) -> <4xi64>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<4x19621xbool>{1, 4}, dtype:torch_int32:dtype) -> <4x19621xi32>{1, 4}
        %torch.2_8_0:3% aten::_to_copy(self:<0xf32>{1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <0xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<38688x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x1x129280xbf16>{129280, 129280, 1}, dtype:torch_float32:dtype) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096xbool>{1}, dtype:torch_float32:dtype) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x1x129280xf32>{129280, 129280, 1}, dtype:torch_bfloat16:dtype) -> <4096x1x129280xbf16>{129280, 129280, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<38688x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <38688x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<38688x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <38688x4096xbf16>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32x7168xbf16>{7168, 1}+970726400, dtype:torch_float32:dtype) -> <32x7168xf32>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x7168xbf16>{7168, 1}, dtype:torch_float32:dtype) -> <4096x7168xf32>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4096x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <4096x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<32x7168xf32>{7168, 1}, dtype:torch_bfloat16:dtype) -> <32x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<29028224xbf16>{1}+1000589952, dtype:torch_float32:dtype) -> <29028224xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<11010048xbf16>{1}+1029619712, dtype:torch_float32:dtype) -> <11010048xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<117440512xbf16>{1}+1040629760, dtype:torch_float32:dtype) -> <117440512xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<132120576xbf16>{1}+1158077440, dtype:torch_float32:dtype) -> <132120576xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<43921920xbf16>{1}+1290198016, dtype:torch_float32:dtype) -> <43921920xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1536xbf16>{1}+1029618176, dtype:torch_float32:dtype) -> <1536xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}+1158070272, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+14680064, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+29360128, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xbf16>{1}+44040192, dtype:torch_float32:dtype) -> <14680064xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+58720256, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+88080384, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+117440512, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}+146800640, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29028224xbf16>{1}, dtype:torch_float32:dtype) -> <29028224xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<11010048xbf16>{1}, dtype:torch_float32:dtype) -> <11010048xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<117440512xbf16>{1}, dtype:torch_float32:dtype) -> <117440512xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<132120576xbf16>{1}, dtype:torch_float32:dtype) -> <132120576xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<43921920xbf16>{1}, dtype:torch_float32:dtype) -> <43921920xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29028224xf32>{1}, dtype:torch_bfloat16:dtype) -> <29028224xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<11010048xf32>{1}, dtype:torch_bfloat16:dtype) -> <11010048xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<117440512xf32>{1}, dtype:torch_bfloat16:dtype) -> <117440512xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<132120576xf32>{1}, dtype:torch_bfloat16:dtype) -> <132120576xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<43921920xf32>{1}, dtype:torch_bfloat16:dtype) -> <43921920xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1536xbf16>{1}, dtype:torch_float32:dtype) -> <1536xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xbf16>{1}, dtype:torch_float32:dtype) -> <7168xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1536xf32>{1}, dtype:torch_bfloat16:dtype) -> <1536xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<7168xf32>{1}, dtype:torch_bfloat16:dtype) -> <7168xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xbf16>{1}, dtype:torch_float32:dtype) -> <29360128xf32>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<14680064xf32>{1}, dtype:torch_bfloat16:dtype) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<29360128xf32>{1}, dtype:torch_bfloat16:dtype) -> <29360128xbf16>{1}
        %torch.2_8_0:3% aten::_to_copy(self:<1x4096xi64>{4096, 1}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device, non_blocking:True:bool) -> <1x4096xi64>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<1x4096xf32>{4096, 1}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device, non_blocking:True:bool) -> <1x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device, non_blocking:True:bool) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<18755x4xi64>{4, 1}, dtype:torch_bool:dtype) -> <18755x4xbool>{4, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<4x18755xbool>{1, 4}, dtype:torch_int32:dtype) -> <4x18755xi32>{1, 4}
        %torch.2_8_0:3% aten::_to_copy(self:<35904x2048xf32>{2048, 1}, dtype:torch_bfloat16:dtype) -> <35904x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<35904x2048xbf16>{2048, 1}, dtype:torch_float32:dtype) -> <35904x2048xf32>{2048, 1}
        %torch.2_8_0:3% aten::_to_copy(self:<35904x4096xf32>{4096, 1}, dtype:torch_bfloat16:dtype) -> <35904x4096xbf16>{4096, 1}
aten::sin
        %torch.2_8_0:3% aten::sin(self:<4096x1x1x64xf32>{64, 64, 64, 1}) -> <4096x1x1x64xf32>{64, 64, 64, 1}
te::quantize
        %torch.2_8_0:3% te::quantize(<4096x1x7168xbf16>{7168, 7168, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1613103104, <True, True, False>:Float8BlockQuantizer) -> <1536x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<576x7168xbf16>{7168, 1}+1571224064, <True, True, False>:Float8BlockQuantizer) -> <576x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+1575352832, <True, True, False>:Float8BlockQuantizer) -> <24576x1536xf8e4m3>
        %torch.2_8_0:3% te::quantize(<32768x512xbf16>{512, 1}+1554446336, <True, True, False>:Float8BlockQuantizer) -> <32768x512xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x1x16384xbf16>{16384, 16384, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x16384xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1624113152, <True, True, False>:Float8BlockQuantizer) -> <7168x16384xf8e4m3>
        %torch.2_8_0:3% te::quantize(<36864x7168xbf16>{7168, 1}+1290198016, <True, True, False>:Float8BlockQuantizer) -> <36864x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x1x18432xbf16>{18432, 18432, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x18432xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x18432xbf16>{18432, 1}+1158077440, <True, True, False>:Float8BlockQuantizer) -> <7168x18432xf8e4m3>
        %torch.2_8_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, False>:Float8BlockQuantizer) -> <1536x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, False>:Float8BlockQuantizer) -> <576x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, False>:Float8BlockQuantizer) -> <24576x1536xf8e4m3>
        %torch.2_8_0:3% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, False>:Float8BlockQuantizer) -> <32768x512xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, False>:Float8BlockQuantizer) -> <7168x16384xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x1x2048xbf16>{2048, 2048, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, False>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, False>:Float8BlockQuantizer) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x4096xbf16>{4096, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x4096xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x32768xbf16>{32768, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x32768xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x24576xbf16>{24576, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x24576xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x576xbf16>{576, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x576xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x1536xbf16>{1536, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x1536xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x36864xbf16>{36864, 1}, <True, True, True>:Float8BlockQuantizer) -> <4096x36864xf8e4m3>
        %torch.2_8_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1613103104, <True, True, True>:Float8BlockQuantizer, <1536x7168xf8e4m3>, None:) -> <1536x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<576x7168xbf16>{7168, 1}+1571224064, <True, True, True>:Float8BlockQuantizer, <576x7168xf8e4m3>, None:) -> <576x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+1575352832, <True, True, True>:Float8BlockQuantizer, <24576x1536xf8e4m3>, None:) -> <24576x1536xf8e4m3>
        %torch.2_8_0:3% te::quantize(<32768x512xbf16>{512, 1}+1554446336, <True, True, True>:Float8BlockQuantizer, <32768x512xf8e4m3>, None:) -> <32768x512xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1624113152, <True, True, True>:Float8BlockQuantizer, <7168x16384xf8e4m3>, None:) -> <7168x16384xf8e4m3>
        %torch.2_8_0:3% te::quantize(<36864x7168xbf16>{7168, 1}+1290198016, <True, True, True>:Float8BlockQuantizer, <36864x7168xf8e4m3>, None:) -> <36864x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x18432xbf16>{18432, 1}+1158077440, <True, True, True>:Float8BlockQuantizer, <7168x18432xf8e4m3>, None:) -> <7168x18432xf8e4m3>
        %torch.2_8_0:3% te::quantize(<1536x7168xbf16>{7168, 1}+1029619712, <True, True, True>:Float8BlockQuantizer, <1536x7168xf8e4m3>, None:) -> <1536x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<576x7168xbf16>{7168, 1}+987740672, <True, True, True>:Float8BlockQuantizer, <576x7168xf8e4m3>, None:) -> <576x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<24576x1536xbf16>{1536, 1}+991869440, <True, True, True>:Float8BlockQuantizer, <24576x1536xf8e4m3>, None:) -> <24576x1536xf8e4m3>
        %torch.2_8_0:3% te::quantize(<32768x512xbf16>{512, 1}+970962944, <True, True, True>:Float8BlockQuantizer, <32768x512xf8e4m3>, None:) -> <32768x512xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x16384xbf16>{16384, 1}+1040629760, <True, True, True>:Float8BlockQuantizer, <7168x16384xf8e4m3>, None:) -> <7168x16384xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+941366272, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+926686208, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+146800640, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+117440512, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+88080384, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<4096x7168xbf16>{7168, 1}+58720256, <True, True, True>:Float8BlockQuantizer, <4096x7168xf8e4m3>, None:) -> <4096x7168xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+44040192, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+29360128, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}+14680064, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
        %torch.2_8_0:3% te::quantize(<7168x2048xbf16>{2048, 1}, <True, True, True>:Float8BlockQuantizer, <7168x2048xf8e4m3>, None:) -> <7168x2048xf8e4m3>
te::generic_gemm
        %torch.2_8_0:3% te::generic_gemm(<1536x7168xf8e4m3>, True:bool, <4096x1x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x1536xbf16>{1536, 1536, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<576x7168xf8e4m3>, True:bool, <4096x1x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x576xbf16>{576, 576, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<24576x1536xf8e4m3>, True:bool, <4096x1536xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x24576xbf16>{24576, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<32768x512xf8e4m3>, True:bool, <4096x512xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x32768xbf16>{32768, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<7168x16384xf8e4m3>, True:bool, <4096x1x16384xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<36864x7168xf8e4m3>, True:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x36864xbf16>{36864, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<7168x18432xf8e4m3>, True:bool, <4096x1x18432xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<32x7168xbf16>{7168, 1}+970726400, True:bool, <4096x7168xbf16>{7168, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x32xf32>{32, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x7168xf8e4m3>, True:bool, <4096x1x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x4096xbf16>{4096, 4096, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<7168x2048xf8e4m3>, True:bool, <4096x1x2048xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, False:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1x7168xbf16>{7168, 7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<7168x2048xf8e4m3>, False:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x2048xbf16>{2048, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1x2048xf8e4m3>, False:bool, <4096x7168xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<7168x2048xbf16>{2048, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x7168xf8e4m3>, False:bool, <4096x4096xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1x7168xf8e4m3>, False:bool, <4096x4096xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<32x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, False:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xf32>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x7168xf32>{7168, 1}, False:bool, <4096x32xf32>{32, 1}, True:bool, None:, None:, DType_kFloat32:DType, None:, DType_kBFloat16:DType, None:, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, False:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<32x7168xf32>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<7168x16384xf8e4m3>, False:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x16384xbf16>{16384, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1x16384xf8e4m3>, False:bool, <4096x7168xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<7168x16384xbf16>{16384, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<32768x512xf8e4m3>, False:bool, <4096x32768xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x512xbf16>{512, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x512xf8e4m3>, False:bool, <4096x32768xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<32768x512xbf16>{512, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<24576x1536xf8e4m3>, False:bool, <4096x24576xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x1536xbf16>{1536, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1536xf8e4m3>, False:bool, <4096x24576xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<24576x1536xbf16>{1536, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<576x7168xf8e4m3>, False:bool, <4096x576xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1x7168xf8e4m3>, False:bool, <4096x576xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<576x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<1536x7168xf8e4m3>, False:bool, <4096x1536xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1x7168xf8e4m3>, False:bool, <4096x1536xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<1536x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<7168x18432xf8e4m3>, False:bool, <4096x7168xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x18432xbf16>{18432, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x1x18432xf8e4m3>, False:bool, <4096x7168xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<7168x18432xbf16>{18432, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<36864x7168xf8e4m3>, False:bool, <4096x36864xf8e4m3>, False:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<4096x7168xbf16>{7168, 1}, None:, None:, None:}
        %torch.2_8_0:3% te::generic_gemm(<4096x7168xf8e4m3>, False:bool, <4096x36864xf8e4m3>, True:bool, None:, None:, DType_kBFloat16:DType, None:, DType_kBFloat16:DType, False:bool, None:, True:bool, <33554432xu8>{1}, 33554432:int, False:bool, True:bool, comm_overlap:None:, comm_type:None:, extra_output:None:, bulk_overlap:False:bool) -> list{<36864x7168xbf16>{7168, 1}, None:, None:, None:}
aten::split_with_sizes
        %torch.2_8_0:3% aten::split_with_sizes(self:<4096x1x576xbf16>{576, 576, 1}, split_sizes:list{512:int, 64:int}, dim:-1:int) -> list{<4096x1x512xbf16>{576, 576, 1}, <4096x1x64xbf16>{576, 576, 1}+512}
aten::clone
        %torch.2_8_0:3% aten::clone(self:<4096x1x512xbf16>{576, 512, 1}, memory_format:torch_contiguous_format:memory_format) -> <4096x1x512xbf16>{512, 512, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x128x192xbf16>{24576, 192, 1}) -> <4096x128x192xbf16>{24576, 192, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x2x16xf32>{2, 1, 0}, memory_format:torch_contiguous_format:memory_format) -> <4096x2x16xf32>{32, 16, 1}
        %torch.2_8_0:3% aten::clone(self:<4x19621xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x19621xbool>{19621, 1}
        %torch.2_8_0:3% aten::clone(self:<4x19621xf32>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x19621xf32>{19621, 1}
        %torch.2_8_0:3% aten::clone(self:<4096x1xi64>{1, 4096}) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:3% aten::clone(self:<4096xf32>{1}) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::clone(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::clone(self:<4x18755xbool>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x18755xbool>{18755, 1}
        %torch.2_8_0:3% aten::clone(self:<4x18755xf32>{1, 4}, memory_format:torch_contiguous_format:memory_format) -> <4x18755xf32>{18755, 1}
aten::unsqueeze
        %torch.2_8_0:3% aten::unsqueeze(self:<4096x1x64xbf16>{576, 576, 1}+512, dim:-2:int) -> <4096x1x1x64xbf16>{576, 576, 64, 1}+512
        %torch.2_8_0:3% aten::unsqueeze(self:<4096x2xf32>{2, 1}, dim:-1:int) -> <4096x2x1xf32>{2, 1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+1
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+2
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+3
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+4
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+5
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+6
        %torch.2_8_0:3% aten::unsqueeze(self:<1xi64>{1}, dim:0:int) -> <1xi64>{1}+7
        %torch.2_8_0:3% aten::unsqueeze(self:<4xi64>{1}, dim:1:int) -> <4x1xi64>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<19621xi64>{1}, dim:0:int) -> <1x19621xi64>{19621, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<38688xf32>{1}, dim:-1:int) -> <38688x1xf32>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<38688xi64>{1}, dim:1:int) -> <38688x1xi64>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<4096x1xf32>{1, 1}, dim:-1:int) -> <4096x1x1xf32>{1, 1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<4096x1xf32>{1, 1}+4096, dim:-1:int) -> <4096x1x1xf32>{1, 1, 1}+4096
        %torch.2_8_0:3% aten::unsqueeze(self:<4096x1xf32>{1, 4096}, dim:-1:int) -> <4096x1x1xf32>{1, 4096, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<1xf32>{1}, dim:1:int) -> <1x1xf32>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<1x32xf32>{32, 1}, dim:0:int) -> <1x1x32xf32>{32, 32, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<18755xi64>{1}, dim:0:int) -> <1x18755xi64>{18755, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<35904xf32>{1}, dim:-1:int) -> <35904x1xf32>{1, 1}
        %torch.2_8_0:3% aten::unsqueeze(self:<35904xi64>{1}, dim:1:int) -> <35904x1xi64>{1, 1}
aten::copy_
        %torch.2_8_0:3% aten::copy_(self:<4096x128x192xbf16>{24576, 192, 1}, src:<4096x128x192xbf16>{24576, 192, 1}) -> <4096x128x192xbf16>{24576, 192, 1}
        %torch.2_8_0:3% aten::copy_(self:<1xf32>{1}, src:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<32xf32>{1}, src:<32xf32>{1}) -> <32xf32>{1}
        %torch.2_8_0:3% aten::copy_(self:<29028224xbf16>{1}, src:<29028224xbf16>{1}) -> <29028224xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<11010048xbf16>{1}, src:<11010048xbf16>{1}) -> <11010048xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<117440512xbf16>{1}, src:<117440512xbf16>{1}) -> <117440512xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<132120576xbf16>{1}, src:<132120576xbf16>{1}) -> <132120576xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<43921920xbf16>{1}, src:<43921920xbf16>{1}) -> <43921920xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<1536xbf16>{1}, src:<1536xbf16>{1}) -> <1536xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}, src:<7168xbf16>{1}) -> <7168xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<29028224xbf16>{1}+1000589952, src:<29028224xf32>{1}) -> <29028224xbf16>{1}+1000589952
        %torch.2_8_0:3% aten::copy_(self:<11010048xbf16>{1}+1029619712, src:<11010048xf32>{1}) -> <11010048xbf16>{1}+1029619712
        %torch.2_8_0:3% aten::copy_(self:<117440512xbf16>{1}+1040629760, src:<117440512xf32>{1}) -> <117440512xbf16>{1}+1040629760
        %torch.2_8_0:3% aten::copy_(self:<132120576xbf16>{1}+1158077440, src:<132120576xf32>{1}) -> <132120576xbf16>{1}+1158077440
        %torch.2_8_0:3% aten::copy_(self:<43921920xbf16>{1}+1290198016, src:<43921920xf32>{1}) -> <43921920xbf16>{1}+1290198016
        %torch.2_8_0:3% aten::copy_(self:<1536xbf16>{1}+1029618176, src:<1536xf32>{1}) -> <1536xbf16>{1}+1029618176
        %torch.2_8_0:3% aten::copy_(self:<7168xbf16>{1}+1158070272, src:<7168xf32>{1}) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}, src:<14680064xbf16>{1}) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}, src:<29360128xbf16>{1}) -> <29360128xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}, src:<14680064xf32>{1}) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+14680064, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+29360128, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:3% aten::copy_(self:<14680064xbf16>{1}+44040192, src:<14680064xf32>{1}) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+58720256, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+88080384, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+117440512, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:3% aten::copy_(self:<29360128xbf16>{1}+146800640, src:<29360128xf32>{1}) -> <29360128xbf16>{1}+146800640
aten::zero_
        %torch.2_8_0:3% aten::zero_(self:<67108864xi32>{1}) -> <67108864xi32>{1}
        %torch.2_8_0:3% aten::zero_(self:<32xbf16>{1}) -> <32xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<29028224xbf16>{1}) -> <29028224xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<11010048xbf16>{1}) -> <11010048xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<117440512xbf16>{1}) -> <117440512xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<132120576xbf16>{1}) -> <132120576xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<43921920xbf16>{1}) -> <43921920xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<1536xbf16>{1}) -> <1536xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<7168xbf16>{1}) -> <7168xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<14680064xbf16>{1}) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<29360128xbf16>{1}) -> <29360128xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<2xf32>{1}) -> <2xf32>{1}
        %torch.2_8_0:3% aten::zero_(self:<2668239872xbf16>{1}) -> <2668239872xbf16>{1}
        %torch.2_8_0:3% aten::zero_(self:<176160768xbf16>{1}) -> <176160768xbf16>{1}
aten::arange
        %torch.2_8_0:3% aten::arange(start:0:int, end:8192:int, step:4096:int, dtype:torch_int32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <2xi32>{1}
        %torch.2_8_0:3% aten::arange(end:19621:int, device:cuda_3:device, pin_memory:False:bool) -> <19621xi64>{1}
        %torch.2_8_0:3% aten::arange(start:0:int, end:4096:int, device:cuda_3:device, pin_memory:False:bool) -> <4096xi64>{1}
        %torch.2_8_0:3% aten::arange(end:18755:int, device:cuda_3:device, pin_memory:False:bool) -> <18755xi64>{1}
te::get_fused_attn_backend
        %torch.2_8_0:3% te::get_fused_attn_backend(True:bool, DType_kBFloat16:DType, DType_kBFloat16:DType, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, 0_0:float, 128:int, 128:int, 4096:int, 4096:int, 192:int, 128:int, -1:int, 0:int) -> NVTE_Fused_Attn_Backend_NVTE_F16_arbitrary_seqlen:NVTE_Fused_Attn_Backend
aten::slice
        %torch.2_8_0:3% aten::slice(self:<2xi32>{1}, dim:0:int, start:0:int, end:2:int) -> <2xi32>{1}
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:0:int, end:5:int) -> <5xu8>{1}
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:5:int, end:10:int) -> <5xu8>{1}+5
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:10:int, end:15:int) -> <5xu8>{1}+10
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:15:int, end:20:int) -> <5xu8>{1}+15
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:20:int, end:25:int) -> <5xu8>{1}+20
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:25:int, end:30:int) -> <5xu8>{1}+25
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:30:int, end:35:int) -> <5xu8>{1}+30
        %torch.2_8_0:3% aten::slice(self:<40xu8>{1}, dim:0:int, start:35:int, end:40:int) -> <5xu8>{1}+35
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:0:int, end:108:int) -> <108xu8>{1}
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:108:int, end:216:int) -> <108xu8>{1}+108
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:216:int, end:324:int) -> <108xu8>{1}+216
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:324:int, end:432:int) -> <108xu8>{1}+324
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:432:int, end:540:int) -> <108xu8>{1}+432
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:540:int, end:648:int) -> <108xu8>{1}+540
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:648:int, end:756:int) -> <108xu8>{1}+648
        %torch.2_8_0:3% aten::slice(self:<864xu8>{1}, dim:0:int, start:756:int, end:864:int) -> <108xu8>{1}+756
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:0:int, end:333529984:int) -> <333529984xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:333529984:int, end:667059968:int) -> <333529984xbf16>{1}+333529984
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:667059968:int, end:1000589952:int) -> <333529984xbf16>{1}+667059968
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1000589952:int, end:1334119936:int) -> <333529984xbf16>{1}+1000589952
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1334119936:int, end:1667649920:int) -> <333529984xbf16>{1}+1334119936
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1667649920:int, end:2001179904:int) -> <333529984xbf16>{1}+1667649920
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:2001179904:int, end:2334709888:int) -> <333529984xbf16>{1}+2001179904
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:2334709888:int, end:2668239872:int) -> <333529984xbf16>{1}+2334709888
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:0:int, end:176160768:int) -> <176160768xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<37748736xbf16>{1}+991869440, dim:0:int, start:8720512:int, end:37748736:int) -> <29028224xbf16>{1}+1000589952
        %torch.2_8_0:3% aten::slice(self:<11010048xbf16>{1}+1029619712, dim:0:int, start:0:int, end:11010048:int) -> <11010048xbf16>{1}+1029619712
        %torch.2_8_0:3% aten::slice(self:<117440512xbf16>{1}+1040629760, dim:0:int, start:0:int, end:117440512:int) -> <117440512xbf16>{1}+1040629760
        %torch.2_8_0:3% aten::slice(self:<132120576xbf16>{1}+1158077440, dim:0:int, start:0:int, end:132120576:int) -> <132120576xbf16>{1}+1158077440
        %torch.2_8_0:3% aten::slice(self:<264241152xbf16>{1}+1290198016, dim:0:int, start:0:int, end:43921920:int) -> <43921920xbf16>{1}+1290198016
        %torch.2_8_0:3% aten::slice(self:<1536xbf16>{1}+1029618176, dim:0:int, start:0:int, end:1536:int) -> <1536xbf16>{1}+1029618176
        %torch.2_8_0:3% aten::slice(self:<7168xbf16>{1}+1158070272, dim:0:int, start:0:int, end:7168:int) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+14680064, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+29360128, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:3% aten::slice(self:<14680064xbf16>{1}+44040192, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+58720256, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+88080384, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+117440512, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:3% aten::slice(self:<29360128xbf16>{1}+146800640, dim:0:int, start:0:int, end:29360128:int) -> <29360128xbf16>{1}+146800640
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1000589952:int, end:1029618176:int) -> <29028224xbf16>{1}+1000589952
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1029619712:int, end:1040629760:int) -> <11010048xbf16>{1}+1029619712
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1040629760:int, end:1158070272:int) -> <117440512xbf16>{1}+1040629760
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1158077440:int, end:1290198016:int) -> <132120576xbf16>{1}+1158077440
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1290198016:int, end:1334119936:int) -> <43921920xbf16>{1}+1290198016
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1029618176:int, end:1029619712:int) -> <1536xbf16>{1}+1029618176
        %torch.2_8_0:3% aten::slice(self:<2668239872xbf16>{1}, dim:0:int, start:1158070272:int, end:1158077440:int) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:0:int, end:14680064:int) -> <14680064xbf16>{1}
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:14680064:int, end:29360128:int) -> <14680064xbf16>{1}+14680064
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:29360128:int, end:44040192:int) -> <14680064xbf16>{1}+29360128
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:44040192:int, end:58720256:int) -> <14680064xbf16>{1}+44040192
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:58720256:int, end:88080384:int) -> <29360128xbf16>{1}+58720256
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:88080384:int, end:117440512:int) -> <29360128xbf16>{1}+88080384
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:117440512:int, end:146800640:int) -> <29360128xbf16>{1}+117440512
        %torch.2_8_0:3% aten::slice(self:<176160768xbf16>{1}, dim:0:int, start:146800640:int, end:176160768:int) -> <29360128xbf16>{1}+146800640
        %torch.2_8_0:3% aten::slice(self:<24xf32>{1}+72, dim:0:int, start:0:int, end:9223372036854775807:int) -> <24xf32>{1}+72
        %torch.2_8_0:3% aten::slice(self:<8x24xf32>{24, 1}, dim:0:int, start:0:int, end:9223372036854775807:int) -> <8x24xf32>{24, 1}
te::fused_attn_fwd
        %torch.2_8_0:3% te::fused_attn_fwd(4096:int, 4096:int, True:bool, 0_1352337788608801:float, 0_0:float, True:bool, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, tuple{-1:int, 0:int}, <2xi32>{1}, <2xi32>{1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, torch_bfloat16:dtype, None:, None:, None:, None:, None:, None:, None:, None:, 16:int) -> list{<4096x1x128x128xbf16>{16384, 16384, 128, 1}, <1x128x4096x1xf32>{524288, 4096, 1, 1}, <2xi64>{1}}
aten::eq
        %torch.2_8_0:3% aten::eq(self:<5056xu8>{1}, other:<5056xu8>{1}) -> <5056xbool>{1}
        %torch.2_8_0:3% aten::eq(self:<4x19621xbool>{1, 4}, other:0:int) -> <4x19621xbool>{1, 4}
        %torch.2_8_0:3% aten::eq(self:<4x18755xbool>{1, 4}, other:0:int) -> <4x18755xbool>{1, 4}
aten::all
        %torch.2_8_0:3% aten::all(self:<5056xbool>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::all(self:<4xbool>{1}) -> <1xbool>{1}
aten::add
        %torch.2_8_0:3% aten::add(self:<4096x1x7168xbf16>{7168, 7168, 1}, other:<4096x1x7168xbf16>{7168, 7168, 1}) -> <4096x1x7168xbf16>{7168, 7168, 1}
        %torch.2_8_0:3% aten::add(self:<4096x32xf32>{32, 1}, other:<32xf32>{1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::add(self:<4096x1xf32>{1, 1}, other:1e-20:float) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::add(self:<38688x2048xbf16>{2048, 1}, other:1:int) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{1, 0}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::add(self:<4096x8xf32>{8, 1}, other:<4096x8xf32>{1, 0}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:3% aten::add(self:<4096x32xf32>{32, 1}, other:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::add(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::add(self:<1x32xf32>{32, 1}, other:<1x32xbf16>{32, 1}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:3% aten::add(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::add(self:<35904x2048xbf16>{2048, 1}, other:1:int) -> <35904x2048xbf16>{2048, 1}
aten::split
        %torch.2_8_0:3% aten::split(self:<4096x36864xbf16>{36864, 1}, split_size:18432:int, dim:-1:int) -> list{<4096x18432xbf16>{36864, 1}, <4096x18432xbf16>{36864, 1}+18432}
        %torch.2_8_0:3% aten::split(self:<4096x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> list{<4096x2048xbf16>{4096, 1}, <4096x2048xbf16>{4096, 1}+2048}
        %torch.2_8_0:3% aten::split(self:<38688x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> list{<38688x2048xbf16>{4096, 1}, <38688x2048xbf16>{4096, 1}+2048}
        %torch.2_8_0:3% aten::split(self:<8192x1xf32>{1, 1}, split_size:4096:int) -> list{<4096x1xf32>{1, 1}, <4096x1xf32>{1, 1}+4096}
        %torch.2_8_0:3% aten::split(self:<35904x4096xbf16>{4096, 1}, split_size:2048:int, dim:-1:int) -> list{<35904x2048xbf16>{4096, 1}, <35904x2048xbf16>{4096, 1}+2048}
aten::silu
        %torch.2_8_0:3% aten::silu(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::silu(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::silu(self:<38688x2048xbf16>{4096, 1}) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::silu(self:<35904x2048xbf16>{4096, 1}) -> <35904x2048xbf16>{2048, 1}
aten::sigmoid
        %torch.2_8_0:3% aten::sigmoid(self:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<38688x2048xbf16>{4096, 1}) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<4096x2048xbf16>{4096, 1}) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<4096x18432xbf16>{36864, 1}) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::sigmoid(self:<35904x2048xbf16>{4096, 1}) -> <35904x2048xbf16>{2048, 1}
aten::topk
        %torch.2_8_0:3% aten::topk(self:<4096x2x16xf32>{32, 16, 1}, k:8:int) -> tuple{<4096x2x8xf32>{16, 8, 1}, <4096x2x8xi64>{16, 8, 1}}
        %torch.2_8_0:3% aten::topk(self:<4096x2xf32>{2, 1}, k:1:int, dim:-1:int, largest:True:bool, sorted:False:bool) -> tuple{<4096x1xf32>{1, 1}, <4096x1xi64>{1, 1}}
        %torch.2_8_0:3% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int) -> tuple{<4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}}
        %torch.2_8_0:3% aten::topk(self:<4096x32xf32>{32, 1}, k:8:int, dim:1:int) -> tuple{<4096x8xf32>{8, 1}, <4096x8xi64>{8, 1}}
aten::sum
        %torch.2_8_0:3% aten::sum(self:<4096x2x8xf32>{16, 8, 1}, dim:list{-1:int}) -> <4096x2xf32>{2, 1}
        %torch.2_8_0:3% aten::sum(self:<4096x8xf32>{8, 1}, dim:list{-1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<4096x32xbool>{32, 1}, dim:list{0:int}) -> <32xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<4096x32xf32>{32, 1}, dim:list{-1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<4096x1x32xbool>{32, 32, 1}, dim:list{0:int}, dtype:torch_float32:dtype) -> <1x32xf32>{32, 1}
        %torch.2_8_0:3% aten::sum(self:<1x32xf32>{32, 1}, dim:list{1:int}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<19621x8xbool>{8, 1}, dim:list{1:int}) -> <19621xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<4x19621xbool>{1, 4}, dim:list{1:int}) -> <4xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<4xi64>{1}) -> <1xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<4096x1x129280xf32>{129280, 129280, 1}, dim:list{-1:int}) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<4096xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::sum(self:<38688x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <38688x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<4096x32xf32>{32, 1}, dim:list{1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<4096x8xf32>{8, 1}, dim:list{1:int}, keepdim:True:bool) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<1x32xbf16>{32, 1}, dim:list{-1:int}, keepdim:True:bool) -> <1x1xbf16>{1, 1}
        %torch.2_8_0:3% aten::sum(self:<18755x8xbool>{8, 1}, dim:list{1:int}) -> <18755xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<4x18755xbool>{1, 4}, dim:list{1:int}) -> <4xi64>{1}
        %torch.2_8_0:3% aten::sum(self:<35904x2048xf32>{2048, 1}, dim:list{-1:int}, keepdim:True:bool) -> <35904x1xf32>{1, 1}
aten::scatter_
        %torch.2_8_0:3% aten::scatter_(self:<4096x2xf32>{2, 1}, dim:1:int, index:<4096x1xi64>{1, 1}, value:1:int) -> <4096x2xf32>{2, 1}
aten::bitwise_not
        %torch.2_8_0:3% aten::bitwise_not(self:<4096x32xbool>{32, 1}) -> <4096x32xbool>{32, 1}
aten::masked_fill
        %torch.2_8_0:3% aten::masked_fill(self:<4096x32xf32>{32, 1}, mask:<4096x32xbool>{32, 1}, value:-inf:float) -> <4096x32xf32>{32, 1}
aten::gather
        %torch.2_8_0:3% aten::gather(self:<4096x32xf32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:3% aten::gather(self:<19621x7168xbf16>{7168, 1}, dim:0:int, index:<38688x7168xi64>{1, 0}) -> <38688x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::gather(self:<18755x7168xbf16>{7168, 1}, dim:0:int, index:<35904x7168xi64>{1, 0}) -> <35904x7168xbf16>{7168, 1}
aten::div
        %torch.2_8_0:3% aten::div(self:<4096x8xf32>{8, 1}, other:<4096x1xf32>{1, 1}) -> <4096x8xf32>{8, 1}
        %torch.2_8_0:3% aten::div(self:<4096x32xf32>{32, 1}, other:<4096x1xf32>{1, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:0_001:float) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<4xi64>{1}, other:16:int) -> <4xf32>{1}
        %torch.2_8_0:3% aten::div(self:<2xf32>{1}, other:8:int) -> <2xf32>{1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<1xf32>{0}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::div(self:<4096x1x32xf32>{0, 32, 1}, other:4096:int) -> <4096x1x32xf32>{32, 32, 1}
        %torch.2_8_0:3% aten::div(self:<1x1xbf16>{1, 1}, other:32:int) -> <1x1xbf16>{1, 1}
aten::scatter
        %torch.2_8_0:3% aten::scatter(self:<4096x32xf32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::scatter(self:<4096x32xi32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}, value:1:int) -> <4096x32xi32>{32, 1}
        %torch.2_8_0:3% aten::scatter(self:<4096x32xf32>{32, 1}, dim:-1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
aten::div_
        %torch.2_8_0:3% aten::div_(self:<1x32xf32>{32, 1}, other:1024_0:float) -> <1x32xf32>{32, 1}
        %torch.2_8_0:3% aten::div_(self:<4096x1x129280xf32>{129280, 129280, 1}, other:<4096x1x1xf32>{1, 1, 1}+4096) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:3% aten::div_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
aten::mean
        %torch.2_8_0:3% aten::mean(self:<4096x1x32xf32>{32, 32, 1}, dim:list{0:int}) -> <1x32xf32>{32, 1}
        %torch.2_8_0:3% aten::mean(self:<1xf32>{1}) -> <1xf32>{1}
aten::mul_
        %torch.2_8_0:3% aten::mul_(self:<1xf32>{1}, other:0_001:float) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul_(self:<1xf32>{1}, other:1:int) -> <1xf32>{1}
        %torch.2_8_0:3% aten::mul_(self:<4096x1x129280xf32>{129280, 129280, 1}, other:<4096x1x1xf32>{1, 4096, 1}) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:3% aten::mul_(self:<2668239872xbf16>{1}, other:0_125:float) -> <2668239872xbf16>{1}
        %torch.2_8_0:3% aten::mul_(self:<176160768xbf16>{1}, other:0_125:float) -> <176160768xbf16>{1}
aten::zeros
        %torch.2_8_0:3% aten::zeros(size:list{2:int}, device:cuda_3:device, pin_memory:False:bool) -> <2xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{8:int}, dtype:torch_int64:dtype, device:cuda:device, pin_memory:False:bool) -> <8xi64>{1}
        %torch.2_8_0:3% aten::zeros(size:list{19621:int, 4:int}, dtype:torch_int64:dtype, device:cuda_3:device, pin_memory:False:bool) -> <19621x4xi64>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{19621:int, 4:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <19621x4xf32>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{19621:int, 7168:int}, dtype:torch_bfloat16:dtype, device:cuda_3:device, pin_memory:False:bool) -> <19621x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::zeros(size:list{4:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cpu:device) -> <4xi64>{1}
        %torch.2_8_0:3% aten::zeros(size:list{19621:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <19621x8xi64>{8, 1}
        %torch.2_8_0:3% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::zeros(size:list{4096:int, 32:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <4096x32xbool>{32, 1}
        %torch.2_8_0:3% aten::zeros(size:list{1:int, 1:int, 4096:int, 4096:int}, dtype:torch_bool:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}
        %torch.2_8_0:3% aten::zeros(size:list{320:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <320xf32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{8:int, 24:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <8x24xf32>{24, 1}
        %torch.2_8_0:3% aten::zeros(size:list{}, dtype:torch_int32:dtype, device:cuda:device, pin_memory:False:bool) -> <1xi32>{1}
        %torch.2_8_0:3% aten::zeros(size:list{18755:int, 4:int}, dtype:torch_int64:dtype, device:cuda_3:device, pin_memory:False:bool) -> <18755x4xi64>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{18755:int, 4:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <18755x4xf32>{4, 1}
        %torch.2_8_0:3% aten::zeros(size:list{18755:int, 7168:int}, dtype:torch_bfloat16:dtype, device:cuda_3:device, pin_memory:False:bool) -> <18755x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::zeros(size:list{18755:int, 8:int}, dtype:torch_int64:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <18755x8xi64>{8, 1}
aten::add_
        %torch.2_8_0:3% aten::add_(self:<1xf32>{1}, other:<1xf32>{1}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::add_(self:<32xbf16>{1}, other:<32xi64>{1}) -> <32xbf16>{1}
        %torch.2_8_0:3% aten::add_(self:<1xi32>{1}, other:<1xi32>{1}) -> <1xi32>{1}
        %torch.2_8_0:3% aten::add_(self:<129280x7168xbf16>{7168, 1}, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+926679040, other:<7168xbf16>{1}) -> <7168xbf16>{1}+926679040
        %torch.2_8_0:3% aten::add_(self:<7168x2048xbf16>{2048, 1}+44040192, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+44040192
        %torch.2_8_0:3% aten::add_(self:<7168x2048xbf16>{2048, 1}, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::add_(self:<7168x2048xbf16>{2048, 1}+29360128, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+29360128
        %torch.2_8_0:3% aten::add_(self:<7168x2048xbf16>{2048, 1}+14680064, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+14680064
        %torch.2_8_0:3% aten::add_(self:<4096x7168xbf16>{7168, 1}+146800640, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+146800640
        %torch.2_8_0:3% aten::add_(self:<4096x7168xbf16>{7168, 1}+88080384, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+88080384
        %torch.2_8_0:3% aten::add_(self:<4096x7168xbf16>{7168, 1}+58720256, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+58720256
        %torch.2_8_0:3% aten::add_(self:<4096x7168xbf16>{7168, 1}+117440512, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+117440512
        %torch.2_8_0:3% aten::add_(self:<7168x2048xbf16>{2048, 1}+926686208, other:<7168x2048xbf16>{2048, 1}) -> <7168x2048xbf16>{2048, 1}+926686208
        %torch.2_8_0:3% aten::add_(self:<4096x7168xbf16>{7168, 1}+941366272, other:<4096x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}+941366272
        %torch.2_8_0:3% aten::add_(self:<32x7168xbf16>{7168, 1}+970726400, other:<32x7168xbf16>{7168, 1}) -> <32x7168xbf16>{7168, 1}+970726400
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+970955776, other:<7168xbf16>{1}) -> <7168xbf16>{1}+970955776
        %torch.2_8_0:3% aten::add_(self:<7168x16384xbf16>{16384, 1}+1040629760, other:<7168x16384xbf16>{16384, 1}) -> <7168x16384xbf16>{16384, 1}+1040629760
        %torch.2_8_0:3% aten::add_(self:<512xbf16>{1}+987740160, other:<512xbf16>{1}) -> <512xbf16>{1}+987740160
        %torch.2_8_0:3% aten::add_(self:<32768x512xbf16>{512, 1}+970962944, other:<32768x512xbf16>{512, 1}) -> <32768x512xbf16>{512, 1}+970962944
        %torch.2_8_0:3% aten::add_(self:<1536xbf16>{1}+1029618176, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1029618176
        %torch.2_8_0:3% aten::add_(self:<24576x1536xbf16>{1536, 1}+991869440, other:<24576x1536xbf16>{1536, 1}) -> <24576x1536xbf16>{1536, 1}+991869440
        %torch.2_8_0:3% aten::add_(self:<576x7168xbf16>{7168, 1}+987740672, other:<576x7168xbf16>{7168, 1}) -> <576x7168xbf16>{7168, 1}+987740672
        %torch.2_8_0:3% aten::add_(self:<1536x7168xbf16>{7168, 1}+1029619712, other:<1536x7168xbf16>{7168, 1}) -> <1536x7168xbf16>{7168, 1}+1029619712
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1158070272, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1158070272
        %torch.2_8_0:3% aten::add_(self:<7168x18432xbf16>{18432, 1}+1158077440, other:<7168x18432xbf16>{18432, 1}) -> <7168x18432xbf16>{18432, 1}+1158077440
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1554439168, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1554439168
        %torch.2_8_0:3% aten::add_(self:<36864x7168xbf16>{7168, 1}+1290198016, other:<36864x7168xbf16>{7168, 1}) -> <36864x7168xbf16>{7168, 1}+1290198016
        %torch.2_8_0:3% aten::add_(self:<7168x16384xbf16>{16384, 1}+1624113152, other:<7168x16384xbf16>{16384, 1}) -> <7168x16384xbf16>{16384, 1}+1624113152
        %torch.2_8_0:3% aten::add_(self:<512xbf16>{1}+1571223552, other:<512xbf16>{1}) -> <512xbf16>{1}+1571223552
        %torch.2_8_0:3% aten::add_(self:<32768x512xbf16>{512, 1}+1554446336, other:<32768x512xbf16>{512, 1}) -> <32768x512xbf16>{512, 1}+1554446336
        %torch.2_8_0:3% aten::add_(self:<1536xbf16>{1}+1613101568, other:<1536xbf16>{1}) -> <1536xbf16>{1}+1613101568
        %torch.2_8_0:3% aten::add_(self:<24576x1536xbf16>{1536, 1}+1575352832, other:<24576x1536xbf16>{1536, 1}) -> <24576x1536xbf16>{1536, 1}+1575352832
        %torch.2_8_0:3% aten::add_(self:<576x7168xbf16>{7168, 1}+1571224064, other:<576x7168xbf16>{7168, 1}) -> <576x7168xbf16>{7168, 1}+1571224064
        %torch.2_8_0:3% aten::add_(self:<1536x7168xbf16>{7168, 1}+1613103104, other:<1536x7168xbf16>{7168, 1}) -> <1536x7168xbf16>{7168, 1}+1613103104
        %torch.2_8_0:3% aten::add_(self:<7168xbf16>{1}+1741553664, other:<7168xbf16>{1}) -> <7168xbf16>{1}+1741553664
        %torch.2_8_0:3% aten::add_(self:<129280x7168xbf16>{7168, 1}+1741560832, other:<129280x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}+1741560832
c10d::allgather_
        %torch.2_8_0:3% c10d::allgather_(output_tensors:list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, input_tensors:list{<1xi64>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{list{list{<1xi64>{1}, <1xi64>{1}+1, <1xi64>{1}+2, <1xi64>{1}+3, <1xi64>{1}+4, <1xi64>{1}+5, <1xi64>{1}+6, <1xi64>{1}+7}}, Work:distributed}
        %torch.2_8_0:3% c10d::allgather_(output_tensors:list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, input_tensors:list{<5xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{list{list{<5xu8>{1}, <5xu8>{1}+5, <5xu8>{1}+10, <5xu8>{1}+15, <5xu8>{1}+20, <5xu8>{1}+25, <5xu8>{1}+30, <5xu8>{1}+35}}, Work:distributed}
        %torch.2_8_0:3% c10d::allgather_(output_tensors:list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, input_tensors:list{<108xu8>{1}}, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{list{list{<108xu8>{1}, <108xu8>{1}+108, <108xu8>{1}+216, <108xu8>{1}+324, <108xu8>{1}+432, <108xu8>{1}+540, <108xu8>{1}+648, <108xu8>{1}+756}}, Work:distributed}
aten::gt
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+1, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+2, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+3, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+4, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+5, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+6, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<1xi64>{1}+7, other:<1xi64>{1}) -> <1xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+1, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+2, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+3, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+4, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+5, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+6, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+7, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+8, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+9, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+10, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+11, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+12, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+13, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+14, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+15, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+16, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+17, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+18, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+19, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+20, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+21, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+22, other:0_0:float) -> <8xbool>{1}
        %torch.2_8_0:3% aten::gt(self:<8xf32>{24}+23, other:0_0:float) -> <8xbool>{1}
aten::resize_
        %torch.2_8_0:3% aten::resize_(self:<5xu8>{1}, size:list{5:int}) -> <5xu8>{1}
        %torch.2_8_0:3% aten::resize_(self:<108xu8>{1}, size:list{108:int}) -> <108xu8>{1}
aten::record_stream
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<32xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xbool>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<8x10xi32>{10, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xi32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<4096x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<19621x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755x7168xbf16>{7168, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755xi32>{1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755x8xi64>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=9:Stream) -> None:
        %torch.2_8_0:3% aten::record_stream(self:<18755x8xf32>{8, 1}, s:torch_Stream_device_type=cuda,_device_inde_=3,_stream_id=0:Stream) -> None:
aten::ne
        %torch.2_8_0:3% aten::ne(self:<19621x8xi64>{8, 1}, other:-1:int) -> <19621x8xbool>{8, 1}
        %torch.2_8_0:3% aten::ne(self:<18755x8xi64>{8, 1}, other:-1:int) -> <18755x8xbool>{8, 1}
aten::index
        %torch.2_8_0:3% aten::index(self:<19621x8xi64>{8, 1}, indices:list{<19621x8xbool>{8, 1}}) -> <38661xi64>{1}
        %torch.2_8_0:3% aten::index(self:<19621x8xf32>{8, 1}, indices:list{<19621x8xbool>{8, 1}}) -> <38661xf32>{1}
        %torch.2_8_0:3% aten::index(self:<4096x129280xf32>{129280, 1}, indices:list{<4096xi64>{1}, <4096xi64>{1}}) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::index(self:<19621x4xf32>{1, 19621}, indices:list{<38661xi64>{1}, <38661xi64>{1}}) -> <38661xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+1, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+2, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+3, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+4, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+5, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+6, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+7, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+8, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+9, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+10, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+11, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+12, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+13, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+14, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+15, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+16, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+17, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+18, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+19, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+20, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+21, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+22, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<8xf32>{24}+23, indices:list{<8xbool>{1}}) -> <0xf32>{1}
        %torch.2_8_0:3% aten::index(self:<18755x8xi64>{8, 1}, indices:list{<18755x8xbool>{8, 1}}) -> <35869xi64>{1}
        %torch.2_8_0:3% aten::index(self:<18755x8xf32>{8, 1}, indices:list{<18755x8xbool>{8, 1}}) -> <35869xf32>{1}
        %torch.2_8_0:3% aten::index(self:<18755x4xf32>{1, 18755}, indices:list{<35869xi64>{1}, <35869xi64>{1}}) -> <35869xf32>{1}
aten::repeat_interleave
        %torch.2_8_0:3% aten::repeat_interleave(repeats:<19621xi64>{1}) -> <38661xi64>{1}
        %torch.2_8_0:3% aten::repeat_interleave(repeats:<18755xi64>{1}) -> <35869xi64>{1}
aten::index_select
        %torch.2_8_0:3% aten::index_select(self:<19621xi64>{1}, dim:0:int, index:<38661xi64>{1}) -> <38661xi64>{1}
        %torch.2_8_0:3% aten::index_select(self:<19621x7168xbf16>{7168, 1}, dim:0:int, index:<38688xi64>{1}) -> <38688x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::index_select(self:<18755xi64>{1}, dim:0:int, index:<35869xi64>{1}) -> <35869xi64>{1}
        %torch.2_8_0:3% aten::index_select(self:<18755x7168xbf16>{7168, 1}, dim:0:int, index:<35904xi64>{1}) -> <35904x7168xbf16>{7168, 1}
aten::index_put_
        %torch.2_8_0:3% aten::index_put_(self:<19621x4xi64>{4, 1}, indices:list{<38661xi64>{1}, <38661xi64>{1}}, values:<1xi64>{1}) -> <19621x4xi64>{4, 1}
        %torch.2_8_0:3% aten::index_put_(self:<19621x4xf32>{4, 1}, indices:list{<38661xi64>{1}, <38661xi64>{1}}, values:<38661xf32>{1}) -> <19621x4xf32>{4, 1}
        %torch.2_8_0:3% aten::index_put_(self:<4x19621xbool>{1, 4}, indices:list{<4x19621xbool>{19621, 1}}, values:<1xbool>{1}) -> <4x19621xbool>{1, 4}
        %torch.2_8_0:3% aten::index_put_(self:<4096x1xi64>{1, 4096}, indices:list{<4096x1xbool>{1, 1}}, values:<1xi64>{1}) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:3% aten::index_put_(self:<4096x1xf32>{1, 1}, indices:list{<4096x1xbool>{1, 1}}, values:<1xf32>{1}) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::index_put_(self:<4096x129280xf32>{129280, 1}, indices:list{<4096xi64>{1}, <4096xi64>{1}}, values:<4096xf32>{1}) -> <4096x129280xf32>{129280, 1}
        %torch.2_8_0:3% aten::index_put_(self:<18755x4xi64>{4, 1}, indices:list{<35869xi64>{1}, <35869xi64>{1}}, values:<1xi64>{1}) -> <18755x4xi64>{4, 1}
        %torch.2_8_0:3% aten::index_put_(self:<18755x4xf32>{4, 1}, indices:list{<35869xi64>{1}, <35869xi64>{1}}, values:<35869xf32>{1}) -> <18755x4xf32>{4, 1}
        %torch.2_8_0:3% aten::index_put_(self:<4x18755xbool>{1, 4}, indices:list{<4x18755xbool>{18755, 1}}, values:<1xbool>{1}) -> <4x18755xbool>{1, 4}
aten::ceil
        %torch.2_8_0:3% aten::ceil(self:<4xf32>{1}) -> <4xf32>{1}
aten::le
        %torch.2_8_0:3% aten::le(self:<4xi64>{1}, other:19621:int) -> <4xbool>{1}
        %torch.2_8_0:3% aten::le(self:<4x19621xi64>{19621, 1}, other:<4x1xi64>{1, 1}) -> <4x19621xbool>{19621, 1}
        %torch.2_8_0:3% aten::le(self:<4xi64>{1}, other:18755:int) -> <4xbool>{1}
        %torch.2_8_0:3% aten::le(self:<4x18755xi64>{18755, 1}, other:<4x1xi64>{1, 1}) -> <4x18755xbool>{18755, 1}
aten::neg
        %torch.2_8_0:3% aten::neg(self:<4xi64>{1}) -> <4xi64>{1}
        %torch.2_8_0:3% aten::neg(self:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::neg(self:<4096x8xf32>{8, 1}) -> <4096x8xf32>{8, 1}
aten::remainder
        %torch.2_8_0:3% aten::remainder(self:<4xi64>{1}, other:16:int) -> <4xi64>{1}
aten::cumsum
        %torch.2_8_0:3% aten::cumsum(self:<4x19621xi32>{1, 4}, dim:1:int) -> <4x19621xi64>{19621, 1}
        %torch.2_8_0:3% aten::cumsum(self:<4x18755xi32>{1, 4}, dim:1:int) -> <4x18755xi64>{18755, 1}
aten::permute
        %torch.2_8_0:3% aten::permute(self:<19621x4xbool>{4, 1}, dims:list{1:int, 0:int}) -> <4x19621xbool>{1, 4}
        %torch.2_8_0:3% aten::permute(self:<19621x4xf32>{4, 1}, dims:list{1:int, 0:int}) -> <4x19621xf32>{1, 4}
        %torch.2_8_0:3% aten::permute(self:<4x19621xf32>{19621, 1}, dims:list{1:int, 0:int}) -> <19621x4xf32>{1, 19621}
        %torch.2_8_0:3% aten::permute(self:<18755x4xbool>{4, 1}, dims:list{1:int, 0:int}) -> <4x18755xbool>{1, 4}
        %torch.2_8_0:3% aten::permute(self:<18755x4xf32>{4, 1}, dims:list{1:int, 0:int}) -> <4x18755xf32>{1, 4}
        %torch.2_8_0:3% aten::permute(self:<4x18755xf32>{18755, 1}, dims:list{1:int, 0:int}) -> <18755x4xf32>{1, 18755}
aten::masked_select
        %torch.2_8_0:3% aten::masked_select(self:<4x19621xi64>{0, 1}, mask:<4x19621xbool>{19621, 1}) -> <38688xi64>{1}
        %torch.2_8_0:3% aten::masked_select(self:<4x19621xf32>{19621, 1}, mask:<4x19621xbool>{19621, 1}) -> <38688xf32>{1}
        %torch.2_8_0:3% aten::masked_select(self:<4x18755xi64>{0, 1}, mask:<4x18755xbool>{18755, 1}) -> <35904xi64>{1}
        %torch.2_8_0:3% aten::masked_select(self:<4x18755xf32>{18755, 1}, mask:<4x18755xbool>{18755, 1}) -> <35904xf32>{1}
te::split_quantize
        %torch.2_8_0:3% te::split_quantize(<38688x7168xbf16>{7168, 1}, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<8560x7168xf8e4m3>, <10208x7168xf8e4m3>, <9808x7168xf8e4m3>, <10112x7168xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<38688x2048xbf16>{2048, 1}, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<8560x2048xf8e4m3>, <10208x2048xf8e4m3>, <9808x2048xf8e4m3>, <10112x2048xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<38688x7168xbf16>{7168, 1}, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<8560x7168xf8e4m3>, <10208x7168xf8e4m3>, <9808x7168xf8e4m3>, <10112x7168xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<38688x4096xbf16>{4096, 1}, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<8560x4096xf8e4m3>, <10208x4096xf8e4m3>, <9808x4096xf8e4m3>, <10112x4096xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<35904x7168xbf16>{7168, 1}, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<4624x7168xf8e4m3>, <10080x7168xf8e4m3>, <9792x7168xf8e4m3>, <11408x7168xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<35904x2048xbf16>{2048, 1}, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer, <True, True, False>:Float8BlockQuantizer}) -> list{<4624x2048xf8e4m3>, <10080x2048xf8e4m3>, <9792x2048xf8e4m3>, <11408x2048xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<35904x7168xbf16>{7168, 1}, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<4624x7168xf8e4m3>, <10080x7168xf8e4m3>, <9792x7168xf8e4m3>, <11408x7168xf8e4m3>}
        %torch.2_8_0:3% te::split_quantize(<35904x4096xbf16>{4096, 1}, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer, <True, True, True>:Float8BlockQuantizer}) -> list{<4624x4096xf8e4m3>, <10080x4096xf8e4m3>, <9792x4096xf8e4m3>, <11408x4096xf8e4m3>}
te::get_num_cublas_streams
        %torch.2_8_0:3% te::get_num_cublas_streams() -> 4:int
te::te_general_grouped_gemm
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, True:bool, list{<8560x7168xf8e4m3>, <10208x7168xf8e4m3>, <9808x7168xf8e4m3>, <10112x7168xf8e4m3>}, False:bool, list{<38688x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, True:bool, list{<8560x2048xf8e4m3>, <10208x2048xf8e4m3>, <9808x2048xf8e4m3>, <10112x2048xf8e4m3>}, False:bool, list{<38688x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, False:bool, list{<8560x7168xf8e4m3>, <10208x7168xf8e4m3>, <9808x7168xf8e4m3>, <10112x7168xf8e4m3>}, False:bool, list{<38688x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<8560x2048xf8e4m3>, <10208x2048xf8e4m3>, <9808x2048xf8e4m3>, <10112x2048xf8e4m3>}, False:bool, list{<8560x7168xf8e4m3>, <10208x7168xf8e4m3>, <9808x7168xf8e4m3>, <10112x7168xf8e4m3>}, True:bool, list{<7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, False:bool, list{<8560x4096xf8e4m3>, <10208x4096xf8e4m3>, <9808x4096xf8e4m3>, <10112x4096xf8e4m3>}, False:bool, list{<38688x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<8560x7168xf8e4m3>, <10208x7168xf8e4m3>, <9808x7168xf8e4m3>, <10112x7168xf8e4m3>}, False:bool, list{<8560x4096xf8e4m3>, <10208x4096xf8e4m3>, <9808x4096xf8e4m3>, <10112x4096xf8e4m3>}, True:bool, list{<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{8560:int, 10208:int, 9808:int, 10112:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, True:bool, list{<4624x7168xf8e4m3>, <10080x7168xf8e4m3>, <9792x7168xf8e4m3>, <11408x7168xf8e4m3>}, False:bool, list{<35904x4096xbf16>{4096, 1}}, DType_kBFloat16:DType, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, True:bool, list{<4624x2048xf8e4m3>, <10080x2048xf8e4m3>, <9792x2048xf8e4m3>, <11408x2048xf8e4m3>}, False:bool, list{<35904x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, False:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>, <7168x2048xf8e4m3>}, False:bool, list{<4624x7168xf8e4m3>, <10080x7168xf8e4m3>, <9792x7168xf8e4m3>, <11408x7168xf8e4m3>}, False:bool, list{<35904x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<4624x2048xf8e4m3>, <10080x2048xf8e4m3>, <9792x2048xf8e4m3>, <11408x2048xf8e4m3>}, False:bool, list{<4624x7168xf8e4m3>, <10080x7168xf8e4m3>, <9792x7168xf8e4m3>, <11408x7168xf8e4m3>}, True:bool, list{<7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}, <7168x2048xbf16>{2048, 1}}, DType_kBFloat16:DType, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>, <4096x7168xf8e4m3>}, False:bool, list{<4624x4096xf8e4m3>, <10080x4096xf8e4m3>, <9792x4096xf8e4m3>, <11408x4096xf8e4m3>}, False:bool, list{<35904x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, True:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
        %torch.2_8_0:3% te::te_general_grouped_gemm(list{<4624x7168xf8e4m3>, <10080x7168xf8e4m3>, <9792x7168xf8e4m3>, <11408x7168xf8e4m3>}, False:bool, list{<4624x4096xf8e4m3>, <10080x4096xf8e4m3>, <9792x4096xf8e4m3>, <11408x4096xf8e4m3>}, True:bool, list{<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}}, DType_kBFloat16:DType, list{4624:int, 10080:int, 9792:int, 11408:int}, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, DType_kBFloat16:DType, False:bool, list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}, True:bool, list{<33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}, <33554432xu8>{1}}, 33554432:int, False:bool, True:bool, 0:int) -> list{<0xf32>{1}, <0xf32>{1}, <0xf32>{1}, <0xf32>{1}}
aten::scatter_add_
        %torch.2_8_0:3% aten::scatter_add_(self:<19621x7168xbf16>{7168, 1}, dim:0:int, index:<38688x7168xi64>{1, 0}, src:<38688x7168xbf16>{7168, 1}) -> <19621x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::scatter_add_(self:<18755x7168xbf16>{7168, 1}, dim:0:int, index:<35904x7168xi64>{1, 0}, src:<35904x7168xbf16>{7168, 1}) -> <18755x7168xbf16>{7168, 1}
aten::mm
        %torch.2_8_0:3% aten::mm(self:<4096x7168xbf16>{7168, 1}, mat2:<7168x129280xbf16>{1, 7168}) -> <4096x129280xbf16>{129280, 1}
        %torch.2_8_0:3% aten::mm(self:<4096x129280xbf16>{129280, 1}, mat2:<129280x7168xbf16>{7168, 1}) -> <4096x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::mm(self:<129280x4096xbf16>{1, 129280}, mat2:<4096x7168xbf16>{7168, 1}) -> <129280x7168xbf16>{7168, 1}
aten::max
        %torch.2_8_0:3% aten::max(self:<4096x1x129280xf32>{129280, 129280, 1}, dim:-1:int) -> tuple{<4096x1xf32>{1, 1}, <4096x1xi64>{1, 1}}
c10d::allreduce_
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<4096x1xf32>{1, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<4096x1xf32>{1, 1}}, Work:distributed}
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<8192x1xf32>{1, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<8192x1xf32>{1, 1}}, Work:distributed}
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<2xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<2xf32>{1}}, Work:distributed}
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<1x32xbf16>{32, 1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<1x32xbf16>{32, 1}}, Work:distributed}
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<1xi32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<1xi32>{1}}, Work:distributed}
        %torch.2_8_0:3% c10d::allreduce_(tensors:list{<1xf32>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, sparse_indices:None:, async_op:False:bool) -> tuple{list{<1xf32>{1}}, Work:distributed}
aten::sub_
        %torch.2_8_0:3% aten::sub_(self:<4096x1x129280xf32>{129280, 129280, 1}, other:<4096x1x1xf32>{1, 1, 1}) -> <4096x1x129280xf32>{129280, 129280, 1}
        %torch.2_8_0:3% aten::sub_(self:<4096xf32>{1}, other:<4096xf32>{1}) -> <4096xf32>{1}
aten::lt
        %torch.2_8_0:3% aten::lt(self:<4096x1xi64>{1, 4096}, other:0:int) -> <4096x1xbool>{1, 4096}
aten::ge
        %torch.2_8_0:3% aten::ge(self:<4096x1xi64>{1, 4096}, other:129280:int) -> <4096x1xbool>{1, 4096}
aten::bitwise_or
        %torch.2_8_0:3% aten::bitwise_or(self:<4096x1xbool>{1, 4096}, other:<4096x1xbool>{1, 4096}) -> <4096x1xbool>{1, 1}
aten::sub
        %torch.2_8_0:3% aten::sub(self:<4096x1xi64>{1, 4096}, other:0:int) -> <4096x1xi64>{1, 4096}
        %torch.2_8_0:3% aten::sub(self:<4096x1xf32>{1, 1}, other:<4096x1xf32>{1, 1}) -> <4096x1xf32>{1, 1}
        %torch.2_8_0:3% aten::sub(self:<1x1xbf16>{1, 1}, other:<1x32xbf16>{32, 1}) -> <1x32xbf16>{32, 1}
aten::exp
        %torch.2_8_0:3% aten::exp(self:<4096x1x129280xf32>{129280, 129280, 1}, out:<4096x1x129280xf32>{129280, 129280, 1}) -> <4096x1x129280xf32>{129280, 129280, 1}
aten::cat
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x1xf32>{1, 1}, <4096x1xf32>{1, 1}}) -> <8192x1xf32>{1, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
        %torch.2_8_0:3% aten::cat(tensors:list{<38688x2048xf32>{2048, 1}, <38688x2048xf32>{2048, 1}}, dim:-1:int) -> <38688x4096xf32>{4096, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x2048xbf16>{2048, 1}, <4096x2048xbf16>{2048, 1}}, dim:-1:int) -> <4096x4096xbf16>{4096, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x1x512xbf16>{512, 512, 1}, <4096x1x64xbf16>{64, 64, 1}}, dim:2:int) -> <4096x1x576xbf16>{576, 576, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<4096x18432xbf16>{18432, 1}, <4096x18432xbf16>{18432, 1}}, dim:-1:int) -> <4096x36864xbf16>{36864, 1}
        %torch.2_8_0:3% aten::cat(tensors:list{<1xf32>{1}}) -> <1xf32>{1}
        %torch.2_8_0:3% aten::cat(tensors:list{<35904x2048xf32>{2048, 1}, <35904x2048xf32>{2048, 1}}, dim:-1:int) -> <35904x4096xf32>{4096, 1}
aten::log
        %torch.2_8_0:3% aten::log(self:<4096x1xf32>{1, 1}+4096) -> <4096x1xf32>{1, 1}
aten::stack
        %torch.2_8_0:3% aten::stack(tensors:list{<1xf32>{1}, <1xf32>{1}}) -> <2xf32>{1}
        %torch.2_8_0:3% aten::stack(tensors:list{<32xbf16>{1}}) -> <1x32xbf16>{32, 1}
        %torch.2_8_0:3% aten::stack(tensors:list{<32xf32>{1}}) -> <1x32xf32>{32, 1}
aten::isnan
        %torch.2_8_0:3% aten::isnan(self:<2xf32>{1}) -> <2xbool>{1}
        %torch.2_8_0:3% aten::isnan(self:<1xbf16>{1}) -> <1xbool>{1}
aten::any
        %torch.2_8_0:3% aten::any(self:<2xbool>{1}) -> <1xbool>{1}
aten::unbind
        %torch.2_8_0:3% aten::unbind(self:<2xf32>{1}) -> list{<1xf32>{1}, <1xf32>{1}}
        %torch.2_8_0:3% aten::unbind(self:<1x32xf32>{32, 1}) -> list{<32xf32>{1}}
aten::ones
        %torch.2_8_0:3% aten::ones(size:list{1:int}, device:cuda_3:device, pin_memory:False:bool) -> <1xf32>{1}
        %torch.2_8_0:3% aten::ones(size:list{1:int}, dtype:torch_float32:dtype, device:cuda_3:device, pin_memory:False:bool) -> <1xf32>{1}
aten::select_backward
        %torch.2_8_0:3% aten::select_backward(grad_output:<1xf32>{1}, input_sizes:list{2:int}, dim:0:int, index:0:int) -> <2xf32>{1}
aten::rsub
        %torch.2_8_0:3% aten::rsub(self:<4096xf32>{1}, other:1_0:float) -> <4096xf32>{1}
        %torch.2_8_0:3% aten::rsub(self:<38688x2048xbf16>{2048, 1}, other:1:int) -> <38688x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::rsub(self:<4096x2048xbf16>{2048, 1}, other:1:int) -> <4096x2048xbf16>{2048, 1}
        %torch.2_8_0:3% aten::rsub(self:<4096x18432xbf16>{18432, 1}, other:1:int) -> <4096x18432xbf16>{18432, 1}
        %torch.2_8_0:3% aten::rsub(self:<35904x2048xbf16>{2048, 1}, other:1:int) -> <35904x2048xbf16>{2048, 1}
te::rmsnorm_bwd
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+926679040, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+970955776, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+987740160, 0:int, False:bool) -> list{<4096x512xbf16>{512, 1}, <512xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1029618176, 0:int, False:bool) -> list{<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1158070272, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1554439168, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x512xbf16>{512, 1}, <4096x512xbf16>{512, 1}, <4096xf32>{1}, <512xbf16>{1}+1571223552, 0:int, False:bool) -> list{<4096x512xbf16>{512, 1}, <512xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x1536xbf16>{1536, 1}, <4096x1536xbf16>{1536, 1}, <4096xf32>{1}, <1536xbf16>{1}+1613101568, 0:int, False:bool) -> list{<4096x1536xbf16>{1536, 1}, <1536xbf16>{1}}
        %torch.2_8_0:3% te::rmsnorm_bwd(<4096x7168xbf16>{7168, 1}, <4096x7168xbf16>{7168, 1}, <4096xf32>{1}, <7168xbf16>{1}+1741553664, 0:int, False:bool) -> list{<4096x7168xbf16>{7168, 1}, <7168xbf16>{1}}
aten::squeeze
        %torch.2_8_0:3% aten::squeeze(self:<38688x1xf32>{1, 1}, dim:-1:int) -> <38688xf32>{1}
        %torch.2_8_0:3% aten::squeeze(self:<4096x1x1x64xbf16>{64, 64, 64, 1}, dim:-2:int) -> <4096x1x64xbf16>{64, 64, 1}
        %torch.2_8_0:3% aten::squeeze(self:<35904x1xf32>{1, 1}, dim:-1:int) -> <35904xf32>{1}
aten::new_zeros
        %torch.2_8_0:3% aten::new_zeros(self:<38688x7168xbf16>{7168, 1}, size:list{19621:int, 7168:int}, dtype:torch_bfloat16:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <19621x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::new_zeros(self:<38661xf32>{1}, size:list{19621:int, 8:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <19621x8xf32>{8, 1}
        %torch.2_8_0:3% aten::new_zeros(self:<4096x8xf32>{8, 1}, size:list{4096:int, 32:int}) -> <4096x32xf32>{32, 1}
        %torch.2_8_0:3% aten::new_zeros(self:<35904x7168xbf16>{7168, 1}, size:list{18755:int, 7168:int}, dtype:torch_bfloat16:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <18755x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::new_zeros(self:<35869xf32>{1}, size:list{18755:int, 8:int}, dtype:torch_float32:dtype, layout:torch_strided:layout, device:cuda_3:device) -> <18755x8xf32>{8, 1}
aten::index_add
        %torch.2_8_0:3% aten::index_add(self:<19621x7168xbf16>{7168, 1}, dim:0:int, index:<38688xi64>{1}, source:<38688x7168xbf16>{7168, 1}) -> <19621x7168xbf16>{7168, 1}
        %torch.2_8_0:3% aten::index_add(self:<18755x7168xbf16>{7168, 1}, dim:0:int, index:<35904xi64>{1}, source:<35904x7168xbf16>{7168, 1}) -> <18755x7168xbf16>{7168, 1}
aten::masked_scatter
        %torch.2_8_0:3% aten::masked_scatter(self:<4x19621xf32>{19621, 1}, mask:<4x19621xbool>{19621, 1}, source:<38688xf32>{1}) -> <4x19621xf32>{19621, 1}
        %torch.2_8_0:3% aten::masked_scatter(self:<4x18755xf32>{18755, 1}, mask:<4x18755xbool>{18755, 1}, source:<35904xf32>{1}) -> <4x18755xf32>{18755, 1}
aten::index_put
        %torch.2_8_0:3% aten::index_put(self:<19621x8xf32>{8, 1}, indices:list{<19621x8xbool>{8, 1}}, values:<38661xf32>{1}, accumulate:True:bool) -> <19621x8xf32>{8, 1}
        %torch.2_8_0:3% aten::index_put(self:<18755x8xf32>{8, 1}, indices:list{<18755x8xbool>{8, 1}}, values:<35869xf32>{1}, accumulate:True:bool) -> <18755x8xf32>{8, 1}
aten::sigmoid_backward
        %torch.2_8_0:3% aten::sigmoid_backward(grad_output:<4096x32xf32>{32, 1}, output:<4096x32xf32>{32, 1}) -> <4096x32xf32>{32, 1}
aten::scatter_add
        %torch.2_8_0:3% aten::scatter_add(self:<4096x32xf32>{32, 1}, dim:1:int, index:<4096x8xi64>{8, 1}, src:<4096x8xf32>{8, 1}) -> <4096x32xf32>{32, 1}
te::fused_attn_bwd
        %torch.2_8_0:3% te::fused_attn_bwd(4096:int, 4096:int, 0_1352337788608801:float, 0_0:float, True:bool, NVTE_QKV_Layout_NVTE_SBHD_SBHD_SBHD:NVTE_QKV_Layout, NVTE_Bias_Type_NVTE_NO_BIAS:NVTE_Bias_Type, NVTE_Mask_Type_NVTE_CAUSAL_MASK:NVTE_Mask_Type, tuple{-1:int, 0:int}, False:bool, <2xi32>{1}, <2xi32>{1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, torch_bfloat16:dtype, DType_kBFloat16:DType, list{<1x128x4096x1xf32>{524288, 4096, 1, 1}, <2xi64>{1}}, None:, None:, None:, None:, None:) -> list{<4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x192xbf16>{24576, 24576, 192, 1}, <4096x1x128x128xbf16>{16384, 16384, 128, 1}, None:}
aten::embedding_dense_backward
        %torch.2_8_0:3% aten::embedding_dense_backward(grad_output:<1x4096x7168xbf16>{7168, 7168, 1}, indices:<1x4096xi64>{4096, 1}, num_weights:129280:int, padding_idx:-1:int, scale_grad_by_freq:False:bool) -> <129280x7168xbf16>{7168, 1}
aten::linalg_vector_norm
        %torch.2_8_0:3% aten::linalg_vector_norm(self:<2668239872xbf16>{1}) -> <1xbf16>{1}
        %torch.2_8_0:3% aten::linalg_vector_norm(self:<176160768xbf16>{1}) -> <1xbf16>{1}
c10d::reduce_scatter_tensor_coalesced_
        %torch.2_8_0:3% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<333529984xbf16>{1}+1000589952}, inputs:list{<2668239872xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
        %torch.2_8_0:3% c10d::reduce_scatter_tensor_coalesced_(outputs:list{<176160768xbf16>{1}}, inputs:list{<176160768xbf16>{1}}, process_group:ProcessGroup:distributed, reduce_op:ReduceOp:distributed, async_op:False:bool) -> Work:distributed
aten::sign
        %torch.2_8_0:3% aten::sign(self:<1x32xbf16>{32, 1}) -> <1x32xbf16>{32, 1}
te::multi_tensor_l2norm
        %torch.2_8_0:3% te::multi_tensor_l2norm(65536:int, <1xi32>{1}, list{list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}, <1536xf32>{1}, <7168xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, False:bool) -> tuple{<1xf32>{1}, <0xf32>{1}}
aten::pow
        %torch.2_8_0:3% aten::pow(self:<1xf32>{1}, exponent:2_0:float) -> <1xf32>{1}
te::multi_tensor_scale
        %torch.2_8_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}}, 1_6138403784027427e-05:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 1_6138403784027427e-05:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}, <1536xf32>{1}, <7168xf32>{1}}}, 2_086718301331225e-05:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_scale(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 2_086718301331225e-05:float) -> None:
profiler::_record_function_enter_new
        %torch.2_8_0:3% profiler::_record_function_enter_new(name:Optimizer_step#FusedAdam_step:str) -> Work:distributed
        %torch.2_8_0:3% profiler::_record_function_enter_new(name:enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str) -> Work:distributed
te::multi_tensor_adam
        %torch.2_8_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<1536xf32>{1}, <7168xf32>{1}}, list{<1536xf32>{1}, <7168xf32>{1}}, list{<1536xf32>{1}, <7168xf32>{1}}, list{<1536xf32>{1}, <7168xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_0:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}, list{<29028224xf32>{1}, <11010048xf32>{1}, <117440512xf32>{1}, <132120576xf32>{1}, <43921920xf32>{1}}}, 9_779754323328192e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<1536xf32>{1}, <7168xf32>{1}}, list{<1536xf32>{1}, <7168xf32>{1}}, list{<1536xf32>{1}, <7168xf32>{1}}, list{<1536xf32>{1}, <7168xf32>{1}}}, 9_779754323328192e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_0:float) -> None:
        %torch.2_8_0:3% te::multi_tensor_adam(65536:int, <1xi32>{1}, list{list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}, list{<14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <14680064xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}, <29360128xf32>{1}}}, 9_779754323328192e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float) -> None:
profiler::_record_function_exit
        %torch.2_8_0:3% profiler::_record_function_exit(_0:Work:distributed) -> None:
c10d::allgather_into_tensor_coalesced_
        %torch.2_8_0:3% c10d::allgather_into_tensor_coalesced_(outputs:list{<2668239872xbf16>{1}}, inputs:list{<333529984xbf16>{1}+1000589952}, process_group:ProcessGroup:distributed) -> Work:distributed
        %torch.2_8_0:3% c10d::allgather_into_tensor_coalesced_(outputs:list{<176160768xbf16>{1}}, inputs:list{<176160768xbf16>{1}}, process_group:ProcessGroup:distributed) -> Work:distributed
c10d::_allgather_base_
        %torch.2_8_0:3% c10d::_allgather_base_(output_tensor:<192xf32>{1}, input_tensor:<24xf32>{1}+72, process_group:ProcessGroup:distributed, async_op:False:bool) -> tuple{<192xf32>{1}, Work:distributed}
c10d::barrier
        %torch.2_8_0:3% c10d::barrier(tensor:<1xu8>{1}, process_group:ProcessGroup:distributed, device_ids:list{}, async_op:False:bool) -> Work:distributed
c10d::broadcast_
        %torch.2_8_0:3% c10d::broadcast_(tensors:list{<1x4096xi64>{4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> tuple{list{<1x4096xi64>{4096, 1}}, Work:distributed}
        %torch.2_8_0:3% c10d::broadcast_(tensors:list{<1x4096xf32>{4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> tuple{list{<1x4096xf32>{4096, 1}}, Work:distributed}
        %torch.2_8_0:3% c10d::broadcast_(tensors:list{<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}}, process_group:ProcessGroup:distributed, root_rank:0:int, root_tensor:0:int, async_op:False:bool) -> tuple{list{<1x1x4096x4096xbool>{16777216, 16777216, 4096, 1}}, Work:distributed}
aten::isinf
        %torch.2_8_0:3% aten::isinf(self:<1xbf16>{1}) -> <1xbool>{1}