Loader add: model_bench <class 'model_theory.core.loaders.model_benchmark_loader.Loader'>
Loader add: model <class 'model_theory.core.loaders.model_loader.Loader'>
Loader add: nsys <class 'model_theory.core.loaders.nsys_profile.op_loader.Loader'>
objects add: ModelBenchmark <class 'model_theory.core.backend.model.benchmark.ModelBenchmarkObject'>
engine add: benchmark_verify <function benchmark_verify at 0x7fe6ef6b4b80>
engine add: mfu_mem_get <function mfu_mem_get at 0x7fe6ef6b4d30>
5 ['SDPA_fmha:fwd', 'op_name', 'OP::SDPA_fmha:fwd', '2D:1D', 'nvte_fused_attn_fwd']
5 ['SDPA_fmha:bwd', 'op_name', 'OP::SDPA_fmha:bwd', '2D:1D', 'nvte_fused_attn_bwd']
4 ['flash_attn_fwd', 'op_name', 'OP::flash_attn:fwd', '2D:1D']
4 ['flash_attn_bwd', 'op_name', 'OP::flash_attn:bwd', '2D:1D']
4 ['aten::fill_', 'op_name', 'OP::Fill', 'DMA']
4 ['aten::copy_', 'op_name', 'OP::Memcpy', 'DMA']
4 ['aten::masked_fill_', 'op_name', 'OP::masked_fill', '1D:DMA']
4 ['aten::_local_scalar_dense', 'op_name', 'OP::Memcpy', 'DMA']
OP::triton__0d1d2d3d
objects add: OpBenchmark <class 'model_theory.core.backend.op.benchmark.OPBenchmark'>
********************
0 aten::new_empty
1 aten::item
2 aten::squeeze
3 aten::transpose
4 aten::unflatten_dense_tensors
5 aten::view_as
6 aten::reshape
7 aten::detach
8 aten::result_type
9 aten::t
10 aten::split_with_sizes
11 aten::select
12 aten::narrow
13 aten::_unsafe_view
14 aten::expand
15 aten::empty_like
16 aten::lift_fresh
17 aten::slice
18 aten::detach_
19 aten::unsqueeze
20 aten::new_empty_strided
21 aten::split
22 aten::alias
23 aten::chunk
24 aten::is_nonzero
25 aten::slice_backward
26 aten::permute
27 aten::expand_as
28 aten::pin_memory
29 aten::is_same_size
30 aten::unbind
31 aten::empty_strided
32 aten::set_
33 aten::view
34 aten::_has_compatible_shallow_copy_type
35 aten::resize_
36 aten::_local_scalar_dense
37 aten::empty
38 aten::as_strided
39 aten::_reshape_alias
40 aten::is_pinned
41 aten::_pin_memory
42 aten::_to_copy
43 aten::clone
44 aten::ones_like
45 aten::zeros_like
46 apply
47 BackwardHookFunctionBackward
48 no_grad
49 set_grad_enabled
50 all_gather_into_tensor
51 forward
52 enable_grad
53 _register_hook
54 LinearWithGradAccumulationAndAsyncCommunicationBackward
********************
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_update_default_pg call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._update_default_pg(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55f82097f0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55f82097f0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%2:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%2:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%2:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa01b0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa01b0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0.BackendConfig(gloo:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cpu'__'gloo',_'cuda'__'gloo'_:dict)
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cpu'__'gloo',_'cuda'__'gloo'_:dict)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (cpu_gloo,cuda_gloo:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa06b0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa06b0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%4:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%4:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%4:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55f82cacb0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55f82cacb0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0.BackendConfig(gloo:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cpu'__'gloo',_'cuda'__'gloo'_:dict)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (cpu_gloo,cuda_gloo:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa08f0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa08f0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%5:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%5:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%5:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa26f0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa26f0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%6:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%6:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%6:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa2bb0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa2bb0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%7:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%7:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%7:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cuda'__'nccl'_:dict)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cuda'__'nccl'_:dict)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (cuda_nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa3170_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa3170_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cuda'__'nccl'_:dict)
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%8:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%8:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%8:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0._get_default_timeout(nccl:str))
          duration: -1
        - ----------->api::_get_default_timeout return:
          inputs: (nccl:str)
          outputs: (0_10_00:timedelta)
          duration: -1
        - ----------->api::_get_split_source call:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (torch.2_3_0._get_split_source(_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup))
          duration: -1
        - ----------->api::_get_split_source return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup, _torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa0ab0_:ProcessGroupNCCL)
          outputs: (_torch_distributed_distributed_c10d_ProcessGroupNCCL_object_at_0_7f55ebaa0ab0_:ProcessGroupNCCL)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_hash_ranks call:
          inputs: (%7:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (torch.2_3_0._hash_ranks(%7:list{0:int,1:int,2:int,3:int,4:int,5:int,6:int,7:int}))
          duration: -1
        - ----------->api::_hash_ranks return:
          inputs: (%7:list{0:int, 1:int, 2:int, 3:int, 4:int, 5:int, 6:int, 7:int})
          outputs: (fdeb0933b73aab77d495ea23df049e92973d6295:str)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_get_default_timeout return:
          inputs: (gloo:str)
          outputs: (0_30_00:timedelta)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (nccl:str)
          outputs: (torch.2_3_0.BackendConfig(nccl:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::is_nccl_available call:
          inputs: ()
          outputs: (torch.2_3_0.is_nccl_available())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0.BackendConfig(gloo:str))
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cpu'__'gloo',_'cuda'__'gloo'_:dict)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (_'cpu'__'gloo',_'cuda'__'gloo'_:dict)
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig call:
          inputs: ()
          outputs: (torch.2_3_0.BackendConfig())
          duration: -1
        - ----------->api::BackendConfig return:
          inputs: ()
          outputs: (cpu_gloo,cuda_gloo:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::_get_default_timeout call:
          inputs: (gloo:str)
          outputs: (torch.2_3_0._get_default_timeout(gloo:str))
          duration: -1
        - ----------->api::_get_default_timeout return:
          inputs: (gloo:str)
          outputs: (0_30_00:timedelta)
          duration: -1
        - ----------->api::_is_barrier_after_init call:
          inputs: ()
          outputs: (torch.2_3_0._is_barrier_after_init())
          duration: -1
        - ----------->api::_is_barrier_after_init return:
          inputs: ()
          outputs: (0:int)
          duration: -1
        - ----------->api::get_backend call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.get_backend(None:NoneType))
          duration: -1
        - ----------->api::get_backend return:
          inputs: (%1:tuple{nccl:str, _torch_distributed_distributed_c10d_Prefi_Store_object_at_0_7f55f82f7b30_:PrefixStore})
          outputs: (nccl:str)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (%9:list{cuda:device})
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%11:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %12:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%14:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %13:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%15:<1xCUSTOM_DATA_TYPE>{1}, RedOpType_MAX:RedOpType, None:NoneType, False:bool)
          outputs: (torch.2_3_0.all_reduce(%15:<1xCUSTOM_DATA_TYPE>{1},RedOpType_MAX:RedOpType,None:NoneType,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%16:list{%15:<1xCUSTOM_DATA_TYPE>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%17:tuple{%18:list{%15:<1xCUSTOM_DATA_TYPE>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%20:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %19:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%22:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %21:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::normal_:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%25:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 40:int, 2:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%26:<20xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%26:<20xf32>{1}, 40:int)
          outputs: (%24:<20xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%24:<20xf32>{1})
          outputs: (%27:<20xf32>{1})
          duration: -1
        - aten::reciprocal:
          inputs: (%27:<20xf32>{1})
          outputs: (%28:<20xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%28:<20xf32>{1}, 1_0:float)
          outputs: (%29:<20xf32>{1})
          duration: -1
        - ----------->api::get_cpu_offload_context call:
          inputs: (False:bool, 0:int, True:bool, True:bool)
          outputs: (torch.2_3_0.get_cpu_offload_context(False:bool,0:int,True:bool,True:bool))
          duration: -1
        - ----------->api::get_cpu_offload_context return:
          inputs: (False:bool, 0:int, True:bool, True:bool, _function_get_cpu_offload_conte_t__locals__tensor_need_offloading_checker_activations_at_0_7f55e84ffe20_:function, _function_get_cpu_offload_conte_t__locals__tensor_need_offloading_checker_weights_at_0_7f55e84ffd90_:function, _function_get_cpu_offload_conte_t__locals__tensor_need_offloading_checker_all_at_0_7f55e84ffd00_:function, _function_get_cpu_offload_conte_t__locals__tensor_need_offloading_checker_all_at_0_7f55e84ffd00_:function, _transformer_engine_pytorch_cpu_offload_AsyncDoubleBufferGroupOffloadHandler_object_at_0_7f55e8394c40_:AsyncDoubleBufferGroupOffloadHandler, _function_get_cpu_offload_conte_t__locals__group_prefetch_offload_commit_async_at_0_7f55e84ffc70_:function)
          outputs: (%30:tuple{_conte_tlib_nullconte_t_object_at_0_7f55e8394b80_:nullcontext,_function_get_cpu_offload_conte_t__locals__group_prefetch_offload_commit_async_at_0_7f55e84ffc70_:function})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (5120:int, 1e-06:float, True:bool, False:bool)
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(5120:int,1e-06:float,True:bool,False:bool))
          duration: -1
        - ----------->api::find_spec call:
          inputs: (fused_layer_norm_cuda:str, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.find_spec(fused_layer_norm_cuda:str,None:NoneType,None:NoneType))
          duration: -1
        - ----------->api::_find_spec call:
          inputs: (fused_layer_norm_cuda:str, /opt/ape_/fused_layer_norm_cuda:PosixPath)
          outputs: (torch.2_3_0._find_spec(fused_layer_norm_cuda:str,/opt/ape_/fused_layer_norm_cuda:PosixPath))
          duration: -1
        - ----------->api::_find_spec return:
          inputs: (fused_layer_norm_cuda:str, /opt/ape_/fused_layer_norm_cuda:PosixPath, /opt/ape_/fused_layer_norm_cuda/__init___py:PosixPath, _generator_object__EditableFinder__find_spec__locals___gene_pr__at_0_7f55e84bbe60_:generator, /opt/ape_/fused_layer_norm_cuda_cpython-310-_86_64-linu_-gnu_so:PosixPath)
          outputs: (ModuleSpec(name='fused_layer_norm_cuda',_loader=__frozen_importlib_e_ternal_E_tensionFileLoader_object_at_0_7f55e8394fd0_,_origin='/opt/ape_/fused_layer_norm_cuda_cpython-310-_86_64-linu_-gnu_so'):ModuleSpec)
          duration: -1
        - ----------->api::find_spec return:
          inputs: (%31:list{}, /opt/ape_/fused_layer_norm_cuda:str)
          outputs: (ModuleSpec(name='fused_layer_norm_cuda',_loader=__frozen_importlib_e_ternal_E_tensionFileLoader_object_at_0_7f55e8394fd0_,_origin='/opt/ape_/fused_layer_norm_cuda_cpython-310-_86_64-linu_-gnu_so'):ModuleSpec)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - aten::fill_:
          inputs: (%33:<5120xf32>{1}, 1_0:float)
          outputs: (%33:<5120xf32>{1})
          duration: -1
        - aten::normal_:
          inputs: (%36:<5120x16384xbf16>{16384, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%36:<5120x16384xbf16>{16384,1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 64:int, 2:int, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%35:<32xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::div:
          inputs: (%37:<32xf32>{1}, 64:int)
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%38:<32xf32>{1})
          outputs: (%39:<32xf32>{1})
          duration: -1
        - aten::reciprocal:
          inputs: (%39:<32xf32>{1})
          outputs: (%35:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%35:<32xf32>{1}, 1_0:float)
          outputs: (%40:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 64:int, 2:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%35:<32xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%35:<32xf32>{1}, 64:int)
          outputs: (%41:<32xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%41:<32xf32>{1})
          outputs: (%42:<32xf32>{1})
          duration: -1
        - aten::reciprocal:
          inputs: (%42:<32xf32>{1})
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%38:<32xf32>{1}, 1_0:float)
          outputs: (%43:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 64:int, 2:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%38:<32xf32>{1}, 64:int)
          outputs: (%44:<32xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%44:<32xf32>{1})
          outputs: (%41:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%41:<32xf32>{1}, 40:int)
          outputs: (%44:<32xf32>{1})
          duration: -1
        - aten::reciprocal:
          inputs: (%44:<32xf32>{1})
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%38:<32xf32>{1}, 1_0:float)
          outputs: (%45:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (32:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%41:<32xf32>{1})
          duration: -1
        - aten::sub:
          inputs: (%41:<32xf32>{1}, 10:int, alpha=1:int)
          outputs: (%46:<32xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%46:<32xf32>{1}, 13:int)
          outputs: (%47:<32xf32>{1})
          duration: -1
        - aten::clamp:
          inputs: (%47:<32xf32>{1}, 0:int, 1:int)
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::rsub:
          inputs: (%38:<32xf32>{1}, 1_0:float, 1:int)
          outputs: (%44:<32xf32>{1})
          duration: -1
        - aten::rsub:
          inputs: (%44:<32xf32>{1}, 1:int, 1:int)
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%45:<32xf32>{1}, %38:<32xf32>{1})
          outputs: (%48:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%43:<32xf32>{1}, %44:<32xf32>{1})
          outputs: (%38:<32xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%48:<32xf32>{1}, %38:<32xf32>{1}, alpha=1:int)
          outputs: (%46:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (163840:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%48:<163840xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%38:<163840x1xf32>{1, 1}, %46:<32xf32>{1})
          outputs: (%35:<163840x32xf32>{32,1})
          duration: -1
        - aten::cat:
          inputs: (%50:list{%35:<163840x32xf32>{32, 1}, %35:<163840x32xf32>{32, 1}}, -1:int)
          outputs: (%38:<163840x64xf32>{64,1})
          duration: -1
        - aten::cos:
          inputs: (%38:<163840x64xf32>{64, 1})
          outputs: (%51:<163840x64xf32>{64,1})
          duration: -1
        - aten::mul:
          inputs: (%51:<163840x64xf32>{64, 1}, 1_0:float)
          outputs: (%52:<163840x64xf32>{64,1})
          duration: -1
        - aten::sin:
          inputs: (%38:<163840x64xf32>{64, 1})
          outputs: (%53:<163840x64xf32>{64,1})
          duration: -1
        - aten::mul:
          inputs: (%53:<163840x64xf32>{64, 1}, 1_0:float)
          outputs: (%54:<163840x64xf32>{64,1})
          duration: -1
        - aten::normal_:
          inputs: (%38:<1536x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%38:<1536x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%47:<24576x1536xbf16>{1536, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%47:<24576x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%41:<576x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%41:<576x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%60:<32768x512xbf16>{512, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%60:<32768x512xbf16>{512,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (5120:int, 1e-06:float, True:bool, False:bool)
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(5120:int,1e-06:float,True:bool,False:bool))
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - aten::fill_:
          inputs: (%40:<5120xf32>{1}, 1_0:float)
          outputs: (%40:<5120xf32>{1})
          duration: -1
        - aten::normal_:
          inputs: (%65:<160x5120xf32>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%65:<160x5120xf32>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%67:<5120x3072xbf16>{3072, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%67:<5120x3072xbf16>{3072,1})
          duration: -1
        - aten::normal_:
          inputs: (%70:<6144x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%70:<6144x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%73:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%73:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%76:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%76:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%77:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%77:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%81:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%81:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%82:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%82:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%84:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%84:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%86:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%86:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%88:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%88:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%89:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%89:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%91:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%91:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%93:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%93:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%95:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%95:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%97:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%97:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%99:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%99:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%100:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%100:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%102:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%102:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%104:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%104:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%106:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%106:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%108:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%108:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%112:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%112:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%114:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%114:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%116:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%116:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%80:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%80:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%122:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%122:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%123:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%123:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%125:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%125:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%127:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%127:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%129:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%129:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%131:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%131:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%133:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%133:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%135:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%135:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%137:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%137:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%139:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%139:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%141:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%141:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%143:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%143:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::normal_:
          inputs: (%145:<5120x1536xbf16>{1536, 1}, 0_0:float, 0_00565685424949238:float, generator=None:NoneType)
          outputs: (%145:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::normal_:
          inputs: (%147:<3072x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%147:<3072x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (5120:int, 1e-06:float, True:bool, False:bool)
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(5120:int,1e-06:float,True:bool,False:bool))
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - aten::fill_:
          inputs: (%62:<5120xf32>{1}, 1_0:float)
          outputs: (%62:<5120xf32>{1})
          duration: -1
        - aten::normal_:
          inputs: (%152:<102400x5120xbf16>{5120, 1}, 0_0:float, 0_008:float, generator=None:NoneType)
          outputs: (%152:<102400x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: ()
          outputs: (torch.2_3_0.DeepseekV2RMSNorm())
          duration: -1
        - aten::zeros:
          inputs: (%169:list{1191296000:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%170:<1191296000xf32>{1})
          duration: -1
        - aten::zeros:
          inputs: (%194:list{526385152:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%178:<526385152xf32>{1})
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (%286:list{NotSurpot:dict, NotSurpot:dict}, 1e-05:float, True:bool, %287:tuple{0_9:float, 0_95:float}, 1e-08:float, True:bool, 0_1:float, False:bool, True:bool, False:bool, False:bool)
          outputs: (torch.2_3_0.FusedAdam(%286:list{NotSurpot:dict,NotSurpot:dict},1e-05:float,True:bool,%287:tuple{0_9:float,0_95:float},1e-08:float,True:bool,0_1:float,False:bool,True:bool,False:bool,False:bool))
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (%286:list{NotSurpot:dict, NotSurpot:dict}, _'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_1_:dict)
          outputs: (torch.2_3_0.FusedAdam(%286:list{NotSurpot:dict,NotSurpot:dict},_'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_1_:dict))
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (_list_iterator_object_at_0_7f55e83bfc10_:list_iterator)
          outputs: (torch.2_3_0.FusedAdam(_list_iterator_object_at_0_7f55e83bfc10_:list_iterator))
          duration: -1
        - ----------->api::FusedAdam return:
          inputs: (%152:<102400x5120xbf16>{5120, 1})
          outputs: (%290:list{None:NoneType,)
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (_list_iterator_object_at_0_7f55e83bfc10_:list_iterator)
          outputs: (torch.2_3_0.FusedAdam(_list_iterator_object_at_0_7f55e83bfc10_:list_iterator))
          duration: -1
        - ----------->api::FusedAdam return:
          inputs: (%62:<5120xbf16>{1})
          outputs: (%291:list{None:NoneType,)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig, None:NoneType, _function__get_megatron_optimizer_based_on_param_groups__locals__init_state_fn_at_0_7f55e83b05e0_:function)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig,)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig, None:NoneType, _function__get_megatron_optimizer_based_on_param_groups__locals__init_state_fn_at_0_7f55e83b05e0_:function)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig,)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig, _function__get_megatron_optimizer_based_on_param_groups__locals__init_state_fn_at_0_7f55e83b05e0_:function)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig,)
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (%309:list{NotSurpot:dict}, 1e-05:float, True:bool, %310:tuple{0_9:float, 0_95:float}, 1e-08:float, True:bool, 0_1:float, False:bool, True:bool, False:bool, False:bool)
          outputs: (torch.2_3_0.FusedAdam(%309:list{NotSurpot:dict},1e-05:float,True:bool,%310:tuple{0_9:float,0_95:float},1e-08:float,True:bool,0_1:float,False:bool,True:bool,False:bool,False:bool))
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (%309:list{NotSurpot:dict}, _'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_1_:dict)
          outputs: (torch.2_3_0.FusedAdam(%309:list{NotSurpot:dict},_'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_1_:dict))
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (_list_iterator_object_at_0_7f55e83bfbe0_:list_iterator)
          outputs: (torch.2_3_0.FusedAdam(_list_iterator_object_at_0_7f55e83bfbe0_:list_iterator))
          duration: -1
        - ----------->api::FusedAdam return:
          inputs: (%147:<3072x5120xbf16>{5120, 1})
          outputs: (%49:list{None:NoneType,)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig, None:NoneType, _function__get_megatron_optimizer_based_on_param_groups__locals__init_state_fn_at_0_7f55e83b3130_:function)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig,)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig, None:NoneType, _function__get_megatron_optimizer_based_on_param_groups__locals__init_state_fn_at_0_7f55e83b3130_:function)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig,)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig, _function__get_megatron_optimizer_based_on_param_groups__locals__init_state_fn_at_0_7f55e83b3130_:function)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(OptimizerConfig(optimizer='adam',_lr=1e-05,_min_lr=1e-06,_decoupled_lr=None,_decoupled_min_lr=None,_weight_decay=0_1,_fp16=False,_bf16=True,_params_dtype=torch_bfloat16,_loss_scale=None,_initial_loss_scale=4294967296,_min_loss_scale=1_0,_loss_scale_window=1000,_hysteresis=2,_adam_beta1=0_9,_adam_beta2=0_95,_adam_eps=1e-08,_sgd_momentum=0_9,_use_distributed_optimizer=False,_overlap_grad_reduce=False,_overlap_param_gather=False,_clip_grad=1_0,_log_num_zeros_in_grad=False,_barrier_with_L1_time=True,_timers=_megatron_core_timers_Timers_object_at_0_7f55f82f9120_):OptimizerConfig,)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%6:list{_megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55f82f94e0_:Float16OptimizerWithFloat16Params, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params})
          outputs: (torch.2_3_0.ChainedOptimizer(%6:list{_megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55f82f94e0_:Float16OptimizerWithFloat16Params,_megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (%309:list{NotSurpot:dict, _'params'__[tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device='cuda_6'),_tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device='cuda_6'),_tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device='cuda_6')],_'wd_mult'__0_0,_'lr_mult'__1_0,_'is_e_pert_parallel'__False,_'is_decoupled_lr'__False,_'ma__lr'__1e-05,_'min_lr'__1e-06,_'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_1_:dict, NotSurpot:dict}, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (%309:list{NotSurpot:dict,_'params'__[tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%387:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %386:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%52:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %194:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::zeros:
          inputs: (%194:list{8:int, 1:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%326:<8x1xf32>{1,1})
          duration: -1
        - aten::copy_:
          inputs: (%380:<i32>, %22:<i32>, False:bool)
          outputs: (%380:<i32>)
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%238:<8xf32>{1}, %359:<1xf32>{1}+6, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%238:<8xf32>{1},%359:<1xf32>{1}+6,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%238:<8xf32>{1}, %359:<1xf32>{1}+6, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%389:tuple{%238:<8xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gt:
          inputs: (%390:<8xf32>{1}, 0_0:float)
          outputs: (%391:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%390:<8xf32>{1}, %392:list{%391:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%359:<8xf32>{1})
          duration: -1
        - aten::min:
          inputs: (%359:<8xf32>{1})
          outputs: (%393:<i32>)
          duration: -1
        - aten::max:
          inputs: (%359:<8xf32>{1})
          outputs: (%390:<i32>)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%359:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %394:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%396:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %395:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%237:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %394:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%196:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %397:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%398:<3xCUSTOM_DATA_TYPE>{1}, 0:int, None:NoneType, False:bool)
          outputs: (torch.2_3_0.broadcast(%398:<3xCUSTOM_DATA_TYPE>{1},0:int,None:NoneType,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%399:list{%398:<3xCUSTOM_DATA_TYPE>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%400:tuple{%401:list{%398:<3xCUSTOM_DATA_TYPE>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::random_:
          inputs: (%398:<i32>, generator=None:NoneType)
          outputs: (%398:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%169:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%409:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%409:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%169:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%411:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%415:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %169:list{%415:<1024xCUSTOM_DATA_TYPE>{1}}, %410:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %169:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %169:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::random_:
          inputs: (%313:<i32>, generator=None:NoneType)
          outputs: (%313:<i32>)
          duration: -1
        - aten::tril:
          inputs: (%410:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%411:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%414:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%409:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::lt:
          inputs: (%412:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%404:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::tril:
          inputs: (%410:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%411:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%415:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::tril:
          inputs: (%410:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%411:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%410:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%411:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %409:list{%415:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%414:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::tril:
          inputs: (%409:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%410:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%410:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%411:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%414:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%402:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%412:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%411:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%412:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %417:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %409:list{%411:<1024xCUSTOM_DATA_TYPE>{1}}, %415:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::tril:
          inputs: (%410:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%411:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%414:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%409:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %415:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %409:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%414:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%418:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %409:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %409:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %415:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%418:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%412:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %409:list{%418:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::lt:
          inputs: (%411:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::lt:
          inputs: (%412:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %409:list{%418:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%411:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%415:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%403:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%415:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %409:list{%415:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %409:list{%411:<1024xCUSTOM_DATA_TYPE>{1}}, %412:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %408:list{%415:<1024xCUSTOM_DATA_TYPE>{1}}, %410:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %412:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %409:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%416:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%403:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %409:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %412:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1, %409:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::index_put_:
          inputs: (%403:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%403:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%419:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%418:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%418:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%419:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%418:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%418:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%419:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%419:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%421:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%420:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%420:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%410:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%410:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%419:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%419:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%419:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%419:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%419:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%260:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%260:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - c10d::barrier:
          inputs: (%407:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %406:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::ones:
          inputs: (%230:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%270:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%407:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%280:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%240:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%402:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%402:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%280:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::tril:
          inputs: (%270:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%240:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%407:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%262:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%280:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%364:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%280:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%364:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%230:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%408:<1024xf32>{1})
          duration: -1
        - aten::lt:
          inputs: (%409:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::lt:
          inputs: (%408:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%295:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%280:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%295:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%409:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%409:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%278:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%364:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%295:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %408:list{%295:<1024xCUSTOM_DATA_TYPE>{1}}, %280:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%262:<1024xf32>{1}, %407:list{%240:<1024xCUSTOM_DATA_TYPE>{1}}, %408:<i32>, False:bool)
          outputs: (%262:<1024xf32>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int, 1024:int}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%260:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%364:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%406:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%409:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%407:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::lt:
          inputs: (%409:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::lt:
          inputs: (%384:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %408:list{%295:<1024xCUSTOM_DATA_TYPE>{1}}, %280:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%384:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%295:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%295:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%406:<1024xCUSTOM_DATA_TYPE>{1}, %407:list{%409:<1024xCUSTOM_DATA_TYPE>{1}}, %408:<i32>, False:bool)
          outputs: (%406:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::lt:
          inputs: (%409:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%39:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%408:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%295:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xf32>{1}, %230:list{%384:<1024xCUSTOM_DATA_TYPE>{1}}, %240:<i32>, False:bool)
          outputs: (%408:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %408:list{%295:<1024xCUSTOM_DATA_TYPE>{1}}, %280:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %408:list{%295:<1024xCUSTOM_DATA_TYPE>{1}}, %280:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%292:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%295:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%39:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%411:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1, %407:list{%408:<1024xCUSTOM_DATA_TYPE>{1}}, %393:<i32>, False:bool)
          outputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %408:list{%295:<1024xCUSTOM_DATA_TYPE>{1}}, %280:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::tril:
          inputs: (%260:<1024x1024xf32>{1024, 1}, 0:int)
          outputs: (%280:<1024x1024xf32>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%292:<1024xCUSTOM_DATA_TYPE>{1}, %230:list{%295:<1024xCUSTOM_DATA_TYPE>{1}}, %240:<i32>, False:bool)
          outputs: (%292:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%411:<1024xCUSTOM_DATA_TYPE>{1}}, %39:<i32>, False:bool)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %410:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%411:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%39:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::ones:
          inputs: (%408:list{1024:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%39:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1, %230:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %240:<i32>, False:bool)
          outputs: (%364:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::index_put_:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%411:<1024xCUSTOM_DATA_TYPE>{1}}, %39:<i32>, False:bool)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%278:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%39:<1024xCUSTOM_DATA_TYPE>{1}}, %410:<i32>, False:bool)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::index_put_:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%39:<1024xCUSTOM_DATA_TYPE>{1}}, %411:<i32>, False:bool)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::lt:
          inputs: (%384:<1x1024x1024xf32>{1048576, 1024, 1}, 0_5:float)
          outputs: (%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1024,1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%280:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %408:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, %384:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::stack:
          inputs: (%421:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%420:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%420:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%421:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%421:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%39:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - aten::stack:
          inputs: (%422:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%421:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%421:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - aten::index_put_:
          inputs: (%407:<1024xCUSTOM_DATA_TYPE>{1}, %408:list{%39:<1024xCUSTOM_DATA_TYPE>{1}}, %384:<i32>, False:bool)
          outputs: (%407:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%421:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%421:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%384:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::stack:
          inputs: (%423:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%422:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%422:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1, %408:list{%384:<1024xCUSTOM_DATA_TYPE>{1}}, %280:<i32>, False:bool)
          outputs: (%240:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - c10d::barrier:
          inputs: (%409:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %408:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::zeros:
          inputs: (%410:list{8:int, 2:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%411:<8x2xf32>{2,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%408:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%265:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%265:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%403:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%421:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%421:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%402:<i32>, %412:<i32>, False:bool)
          outputs: (%402:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%404:<i32>, %402:<i32>, False:bool)
          outputs: (%404:<i32>)
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%413:<16xf32>{1}, %416:<2xf32>{1}+12, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%413:<16xf32>{1},%416:<2xf32>{1}+12,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%413:<16xf32>{1}, %416:<2xf32>{1}+12, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%417:tuple{%413:<16xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gt:
          inputs: (%413:<8xf32>{2}, 0_0:float)
          outputs: (%416:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%413:<8xf32>{2}, %418:list{%416:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%419:<8xf32>{1})
          duration: -1
        - aten::min:
          inputs: (%419:<8xf32>{1})
          outputs: (%420:<i32>)
          duration: -1
        - aten::max:
          inputs: (%419:<8xf32>{1})
          outputs: (%421:<i32>)
          duration: -1
        - aten::gt:
          inputs: (%413:<8xf32>{2}+1, 0_0:float)
          outputs: (%415:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%413:<8xf32>{2}+1, %422:list{%415:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%419:<8xf32>{1})
          duration: -1
        - aten::min:
          inputs: (%419:<8xf32>{1})
          outputs: (%402:<i32>)
          duration: -1
        - aten::max:
          inputs: (%419:<8xf32>{1})
          outputs: (%423:<i32>)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%402:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %424:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::stack:
          inputs: (%412:list{%406:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%411:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%411:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%413:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%413:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%413:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%413:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%413:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%413:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%412:list{%292:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%411:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%411:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%413:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%412:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%412:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%413:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%412:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%412:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%412:list{%407:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%411:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%411:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%240:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%415:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%415:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%240:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%415:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%415:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%364:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%413:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%413:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%415:list{%240:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%414:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%414:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%415:list{%240:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%414:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%414:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%364:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%413:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%413:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%240:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%415:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%415:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%240:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%413:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%413:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%426:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %425:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::zero_:
          inputs: (%170:<1191296000xf32>{1})
          outputs: (%170:<1191296000xf32>{1})
          duration: -1
        - aten::zero_:
          inputs: (%178:<526385152xf32>{1})
          outputs: (%178:<526385152xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (True:bool)
          outputs: (torch.2_3_0.ChainedOptimizer(True:bool))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (True:bool)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(True:bool))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (True:bool)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(True:bool))
          duration: -1
        - aten::zeros:
          inputs: (%425:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%337:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - aten::stack:
          inputs: (%418:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%417:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%417:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%410:<1024xf32>{1}}, 0:int, out=%419:<1x1024xf32>{1024, 1})
          outputs: (%419:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%420:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%420:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%423:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %414:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, %425:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%426:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}, %414:list{%426:<1024xCUSTOM_DATA_TYPE>{1}}, %425:<i32>, False:bool)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%423:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%423:<1024xCUSTOM_DATA_TYPE>{1}+1, %414:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, %425:<i32>, False:bool)
          outputs: (%423:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%416:list{%295:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%412:list{%409:<1024xf32>{1}}, 0:int, out=%417:<1x1024xf32>{1024, 1})
          outputs: (%417:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%418:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%417:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%417:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%412:list{%262:<1024xf32>{1}}, 0:int, out=%417:<1x1024xf32>{1024, 1})
          outputs: (%417:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%423:list{%404:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%422:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%422:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%421:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%421:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%429:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%429:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%417:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%416:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%416:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%413:<1024xf32>{1}}, 0:int, out=%424:<1x1024xf32>{1024, 1})
          outputs: (%424:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%413:<1024xf32>{1}}, 0:int, out=%424:<1x1024xf32>{1024, 1})
          outputs: (%424:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%278:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%418:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%418:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%410:<1024xf32>{1}}, 0:int, out=%419:<1x1024xf32>{1024, 1})
          outputs: (%419:<1x1024xf32>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %412:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}, %412:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, %425:<i32>, False:bool)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%425:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, %412:list{%425:<1024xCUSTOM_DATA_TYPE>{1}}, %422:<i32>, False:bool)
          outputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%417:list{%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%416:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%416:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%378:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%378:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%419:list{%412:<1024xf32>{1}}, 0:int, out=%423:<1x1024xf32>{1024, 1})
          outputs: (%423:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%419:list{%413:<1024xf32>{1}}, 0:int, out=%424:<1x1024xf32>{1024, 1})
          outputs: (%424:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%412:list{%408:<1024xf32>{1}}, 0:int, out=%417:<1x1024xf32>{1024, 1})
          outputs: (%417:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%413:list{%410:<1024xf32>{1}}, 0:int, out=%418:<1x1024xf32>{1024, 1})
          outputs: (%418:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%418:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%418:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%421:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%262:<1024xf32>{1}, %423:list{%421:<1024xCUSTOM_DATA_TYPE>{1}}, %422:<i32>, False:bool)
          outputs: (%262:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%384:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%384:<1024xCUSTOM_DATA_TYPE>{1}, %423:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, %422:<i32>, False:bool)
          outputs: (%384:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%425:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1, %423:list{%425:<1024xCUSTOM_DATA_TYPE>{1}}, %422:<i32>, False:bool)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%413:list{%409:<1024xf32>{1}}, 0:int, out=%418:<1x1024xf32>{1024, 1})
          outputs: (%418:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%430:list{%423:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%279:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%279:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%419:list{%413:<1024xf32>{1}}, 0:int, out=%421:<1x1024xf32>{1024, 1})
          outputs: (%421:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%425:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%425:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%425:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%425:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %431:list{%429:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, %431:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%433:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%203:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %430:list{%428:<1024xCUSTOM_DATA_TYPE>{1}}, %429:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%278:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, %430:list{%278:<1024xCUSTOM_DATA_TYPE>{1}}, %429:<i32>, False:bool)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%203:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%203:<1024xCUSTOM_DATA_TYPE>{1}+1, %430:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, %429:<i32>, False:bool)
          outputs: (%203:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::index_put_:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, %431:list{%433:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%416:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%420:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%420:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%420:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%423:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%423:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%409:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%418:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%418:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%423:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %414:list{%423:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%425:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}, %414:list{%425:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%421:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%426:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}+1, %414:list{%426:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%415:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%419:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%419:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%421:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%425:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%425:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%411:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %419:list{%429:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xf32>{1}, %422:list{%411:<1024xCUSTOM_DATA_TYPE>{1}}, %421:<i32>, False:bool)
          outputs: (%408:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%419:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, %419:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%419:<1024xCUSTOM_DATA_TYPE>{1}, %422:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %413:<i32>, False:bool)
          outputs: (%419:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, %419:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%413:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1, %422:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %413:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%262:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%393:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%262:<1024xCUSTOM_DATA_TYPE>{1}, %413:list{%393:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%262:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%425:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, %413:list{%425:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%416:list{%384:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%427:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%427:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%415:list{%278:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%419:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%419:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %169:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%424:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%424:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}, %425:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%369:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, %425:list{%369:<1024xCUSTOM_DATA_TYPE>{1}}, %426:<i32>, False:bool)
          outputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%420:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%278:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%278:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%402:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %419:list{%402:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%426:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%423:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%426:<1024xCUSTOM_DATA_TYPE>{1}, %419:list{%423:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%426:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1, %419:list{%429:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%416:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%423:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %425:list{%423:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%426:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}, %425:list{%426:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%416:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%416:<1024xCUSTOM_DATA_TYPE>{1}+1, %425:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%416:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%169:list{%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%424:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%424:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%197:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%417:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%417:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%428:list{%421:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%427:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%427:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%391:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%391:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%374:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%374:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%421:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%428:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%428:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%421:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%434:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%434:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%421:list{%413:<1024xf32>{1}}, 0:int, out=%425:<1x1024xf32>{1024, 1})
          outputs: (%425:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%415:list{%262:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%423:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%423:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%419:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%415:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%415:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%429:list{%420:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%428:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%428:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%410:<1024xf32>{1}}, 0:int, out=%418:<1x1024xf32>{1024, 1})
          outputs: (%418:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%412:<1024xf32>{1}}, 0:int, out=%274:<1x1024xf32>{1024, 1})
          outputs: (%274:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%230:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%423:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%427:list{%420:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%422:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%422:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%426:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%431:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%431:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::stack:
          inputs: (%169:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%429:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%429:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%433:list{%203:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%421:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%421:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%412:<1024xf32>{1}}, 0:int, out=%424:<1x1024xf32>{1024, 1})
          outputs: (%424:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%436:list{%428:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%435:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%435:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%429:list{%422:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%403:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%403:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%436:list{%428:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%435:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%435:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%230:list{%421:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%427:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%427:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%423:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%426:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%426:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%420:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%425:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%425:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%438:list{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%439:tuple{%440:list{%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::eq:
          inputs: (%429:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %430:list{%420:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%432:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%432:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}, %430:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%441:list{%432:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%442:tuple{%443:list{%432:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::stack:
          inputs: (%422:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%374:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%374:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%429:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%429:<1024xCUSTOM_DATA_TYPE>{1}+1, %430:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %424:<i32>, False:bool)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%434:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%434:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%444:list{%434:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%445:tuple{%446:list{%434:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::stack:
          inputs: (%429:list{%421:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%428:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%428:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%416:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%419:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%419:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%447:list{%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%439:tuple{%448:list{%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%427:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%432:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%432:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%237:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%447:list{%237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%449:tuple{%424:list{%237:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%450:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%450:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%237:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device)
          duration: -1
        - aten::stack:
          inputs: (%431:list{%416:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %424:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%451:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%451:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%237:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device)
          duration: -1
        - aten::eq:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%425:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%420:<1024xCUSTOM_DATA_TYPE>{1}, %424:list{%425:<1024xCUSTOM_DATA_TYPE>{1}}, %422:<i32>, False:bool)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - aten::index_put_:
          inputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1, %424:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %425:<i32>, False:bool)
          outputs: (%421:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::eq:
          inputs: (%425:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%426:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %428:list{%426:<1024xCUSTOM_DATA_TYPE>{1}}, %427:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%424:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%424:<1024xCUSTOM_DATA_TYPE>{1}, %428:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, %393:<i32>, False:bool)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%425:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%425:<1024xCUSTOM_DATA_TYPE>{1}+1, %428:list{%429:<1024xCUSTOM_DATA_TYPE>{1}}, %393:<i32>, False:bool)
          outputs: (%425:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%237:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%454:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%454:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%237:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_457,__245,_2569,_____,__245,_4706,__410]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[_457,__245,_2569,_____,__245,_4706,__410]],_device)
          duration: -1
54588 2024-12-10 17:47:52.482129 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n0,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%456:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%456:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
54597 2024-12-10 17:47:52.483308 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n0,rank6)
        - ----------->api::embedding call:
          inputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%422:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%425:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%425:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %431:list{%429:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}, %431:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%433:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, %431:list{%433:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%169:list{%428:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%243:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%243:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%427:list{%420:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%423:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%423:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%431:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%431:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%435:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%434:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%434:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%230:list{%429:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%384:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%384:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%428:list{%421:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%425:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%425:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%432:list{%425:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%374:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%374:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%437:list{%428:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%436:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%457:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%457:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
56182 2024-12-10 17:47:52.899013 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n0,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%456:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%463:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%467:tuple{%466:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%467:tuple{%466:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
56209 2024-12-10 17:47:52.904321 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n0,rank6)
        - ----------->api::dropout call:
          inputs: (%243:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%243:<1024x1x5120xbf16>{5120,5120,1},0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%243:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, True:bool, False:bool)
          outputs: (%243:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
56240 2024-12-10 17:47:52.914699 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n0,rank6)
        - ----------->api::Dropout return:
          inputs: (%467:tuple{%466:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%471:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
56254 2024-12-10 17:47:52.917947 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n0,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_457,__245,_2569,_____,__245,_4706,__410]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%254:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%467:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%467:tuple{1024:int}))
          duration: -1
56281 2024-12-10 17:47:52.922481 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n0,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%476:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%476:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%429:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%477:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%478:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%479:list{%478:<1024x20xf32>{20, 1}, %478:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%480:<1024x40xf32>{40,1})
          duration: -1
56430 2024-12-10 17:47:52.948819 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n0,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%467:tuple{1024:int})
          outputs: (%484:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
56486 2024-12-10 17:47:52.957253 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n0,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
56575 2024-12-10 17:47:52.969086 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n0,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%467:tuple{%487:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%467:tuple{%487:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
56583 2024-12-10 17:47:52.970073 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n0,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%491:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%491:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%491:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%491:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%494:tuple{%491:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%494:tuple{%491:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%494:tuple{%491:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%494:tuple{%491:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%497:tuple{%491:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, %496:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%497:tuple{%491:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},%496:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%491:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %494:tuple{%491:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%463:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%491:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%463:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
56918 2024-12-10 17:47:53.038350 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n0,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%467:tuple{%453:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.SelfAttention(%467:tuple{%453:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
56972 2024-12-10 17:47:53.046359 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n0,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%502:tuple{%263:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%502:tuple{%263:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
57011 2024-12-10 17:47:53.049656 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n0,rank6)
        - aten::mm:
          inputs: (%511:<1024x5120xbf16>{5120, 1}, %509:<5120x1536xbf16>{1, 5120})
          outputs: (%512:<1024x1536xbf16>{1536,1})
          duration: -1
57501 2024-12-10 17:47:53.152320 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n0,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%502:tuple{%263:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%508:tuple{%376:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%502:tuple{%376:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%502:tuple{%376:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
57515 2024-12-10 17:47:53.155911 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n1,rank6)
        - aten::mm:
          inputs: (%525:<1024x1536xbf16>{1536, 1}, %523:<1536x24576xbf16>{1, 1536})
          outputs: (%276:<1024x24576xbf16>{24576,1})
          duration: -1
57553 2024-12-10 17:47:53.166604 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n1,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%502:tuple{%376:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%522:tuple{%331:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%532:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %531:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%532:<1024x1x128x192xbf16>{24576,24576,192,1},%531:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%532:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %531:list{128:int, 64:int}, -1:int)
          outputs: (%508:tuple{%276:<1024x1x128x128xbf16>{24576,24576,192,1},%523:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%502:tuple{%263:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%502:tuple{%263:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
57586 2024-12-10 17:47:53.180814 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n2,rank6)
        - aten::mm:
          inputs: (%282:<1024x5120xbf16>{5120, 1}, %539:<5120x576xbf16>{1, 5120})
          outputs: (%541:<1024x576xbf16>{576,1})
          duration: -1
57629 2024-12-10 17:47:53.192952 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n2,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%502:tuple{%263:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%538:tuple{%545:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%545:<1024x1x576xbf16>{576, 576, 1}, %526:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%545:<1024x1x576xbf16>{576,576,1},%526:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%545:<1024x1x576xbf16>{576, 576, 1}, %526:list{512:int, 64:int}, -1:int)
          outputs: (%551:tuple{%550:<1024x1x512xbf16>{576,576,1},%541:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%502:tuple{%550:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%502:tuple{%550:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
57667 2024-12-10 17:47:53.205445 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n3,rank6)
        - aten::mm:
          inputs: (%539:<1024x512xbf16>{576, 1}, %555:<512x32768xbf16>{1, 512})
          outputs: (%557:<1024x32768xbf16>{32768,1})
          duration: -1
57775 2024-12-10 17:47:53.229054 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n3,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%502:tuple{%550:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%522:tuple{%557:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%280:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %563:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%280:<1024x1x128x256xbf16>{32768,32768,256,1},%563:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%280:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %563:list{128:int, 128:int}, -1:int)
          outputs: (%551:tuple{%565:<1024x1x128x128xbf16>{32768,32768,256,1},%566:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%553:tuple{%568:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%553:tuple{%568:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
57855 2024-12-10 17:47:53.245852 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n0,rank6)
        - aten::arange:
          inputs: (0:int, 64:int, 2:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%574:<32xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%574:<32xf32>{1}, 64:int)
          outputs: (%325:<32xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%325:<32xf32>{1})
          outputs: (%274:<32xf32>{1})
          duration: -1
        - aten::reciprocal:
          inputs: (%274:<32xf32>{1})
          outputs: (%575:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%575:<32xf32>{1}, 1_0:float)
          outputs: (%539:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 64:int, 2:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%325:<32xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%325:<32xf32>{1}, 64:int)
          outputs: (%576:<32xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%576:<32xf32>{1})
          outputs: (%574:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%574:<32xf32>{1}, 40:int)
          outputs: (%325:<32xf32>{1})
          duration: -1
        - aten::reciprocal:
          inputs: (%325:<32xf32>{1})
          outputs: (%577:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%577:<32xf32>{1}, 1_0:float)
          outputs: (%578:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (32:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%577:<32xf32>{1})
          duration: -1
        - aten::sub:
          inputs: (%577:<32xf32>{1}, 10:int, alpha=1:int)
          outputs: (%579:<32xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%579:<32xf32>{1}, 13:int)
          outputs: (%325:<32xf32>{1})
          duration: -1
        - aten::clamp:
          inputs: (%325:<32xf32>{1}, 0:int, 1:int)
          outputs: (%580:<32xf32>{1})
          duration: -1
        - aten::rsub:
          inputs: (%581:<32xf32>{1}, 1_0:float, 1:int)
          outputs: (%580:<32xf32>{1})
          duration: -1
        - aten::rsub:
          inputs: (%580:<32xf32>{1}, 1:int, 1:int)
          outputs: (%574:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%578:<32xf32>{1}, %574:<32xf32>{1})
          outputs: (%325:<32xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%539:<32xf32>{1}, %580:<32xf32>{1})
          outputs: (%274:<32xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%325:<32xf32>{1}, %274:<32xf32>{1}, alpha=1:int)
          outputs: (%582:<32xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%583:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%325:<1024x1xf32>{1, 1}, %582:<32xf32>{1})
          outputs: (%585:<1024x32xf32>{32,1})
          duration: -1
        - aten::cat:
          inputs: (%548:list{%585:<1024x32xf32>{32, 1}, %585:<1024x32xf32>{32, 1}}, -1:int)
          outputs: (%586:<1024x64xf32>{64,1})
          duration: -1
        - aten::cos:
          inputs: (%586:<1024x64xf32>{64, 1})
          outputs: (%577:<1024x64xf32>{64,1})
          duration: -1
        - aten::mul:
          inputs: (%577:<1024x64xf32>{64, 1}, 1_0:float)
          outputs: (%325:<1024x64xf32>{64,1})
          duration: -1
        - aten::sin:
          inputs: (%586:<1024x64xf32>{64, 1})
          outputs: (%588:<1024x64xf32>{64,1})
          duration: -1
        - aten::mul:
          inputs: (%588:<1024x64xf32>{64, 1}, 1_0:float)
          outputs: (%589:<1024x64xf32>{64,1})
          duration: -1
58119 2024-12-10 17:47:53.306135 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n0,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%553:tuple{%568:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%508:tuple{%583:<1024x64xbf16>{64,1},%160:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%583:<1024x64xbf16>{64, 1}, %593:list{%237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%585:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%160:<1024x64xbf16>{64, 1}, %592:list{%237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%595:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%597:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %594:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%325:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%274:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%600:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%593:list{%600:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %599:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%347:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%347:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %571:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%599:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%325:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %599:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%601:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%598:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %594:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%599:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%340:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%336:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%593:list{%336:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %274:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%602:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%602:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %571:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%336:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%599:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %336:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%602:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%276:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %598:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%276:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%596:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %601:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%596:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%336:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %340:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%336:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%605:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %604:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%605:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%550:<128x1024x192xbf16>{196608, 192, 1}, %583:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%602:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%276:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%601:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%555:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%604:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%555:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %593:list{%604:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %605:<i32>, False:bool)
          outputs: (%555:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%601:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %555:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%336:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%336:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%336:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%344:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%276:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%336:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %276:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%276:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%607:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%607:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%607:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, True:bool, False:bool)
          outputs: (%607:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%605:<128x1024x1024xbf16>{1048576, 1024, 1}, %160:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%331:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%502:tuple{%609:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%502:tuple{%609:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
59091 2024-12-10 17:47:53.534655 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n0,rank6)
        - aten::mm:
          inputs: (%614:<1024x16384xbf16>{16384, 1}, %612:<16384x5120xbf16>{1, 16384})
          outputs: (%615:<1024x5120xbf16>{5120,1})
          duration: -1
59139 2024-12-10 17:47:53.546889 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n0,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%502:tuple{%609:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%623:tuple{%621:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
59146 2024-12-10 17:47:53.550066 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n0,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%467:tuple{%453:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%522:tuple{%482:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::find_spec call:
          inputs: (z3:str, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.find_spec(z3:str,None:NoneType,None:NoneType))
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%626:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%626:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%627:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%627:tuple{None:NoneType,)
          duration: -1
        - ----------->api::grad call:
          inputs: (%628:list{NotSurpot:FunctionalTensor}, %629:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %630:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%628:list{NotSurpot:FunctionalTensor},%629:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%630:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%629:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%629:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%629:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%631:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%632:tuple{NotSurpot:FunctionalTensor}, %631:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%632:tuple{NotSurpot:FunctionalTensor},%631:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%632:tuple{NotSurpot:FunctionalTensor}, %631:tuple{NotSurpot:FunctionalTensor}, False:bool, %633:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%634:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%632:tuple{NotSurpot:FunctionalTensor}, %635:tuple{%634:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %636:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%632:tuple{NotSurpot:FunctionalTensor},%635:tuple{%634:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%636:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%632:tuple{NotSurpot:FunctionalTensor}, %635:tuple{%634:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %636:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%637:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%628:list{NotSurpot:FunctionalTensor}, %629:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %636:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %632:tuple{NotSurpot:FunctionalTensor}, %638:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %639:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %634:tuple{NotSurpot:FunctionalTensor}, %637:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%637:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%639:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%639:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::grad call:
          inputs: (%640:list{NotSurpot:FunctionalTensor}, %641:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %642:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%640:list{NotSurpot:FunctionalTensor},%641:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%642:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%641:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%641:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%641:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%643:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%644:tuple{NotSurpot:FunctionalTensor}, %643:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%644:tuple{NotSurpot:FunctionalTensor},%643:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%644:tuple{NotSurpot:FunctionalTensor}, %643:tuple{NotSurpot:FunctionalTensor}, False:bool, %645:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%646:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%644:tuple{NotSurpot:FunctionalTensor}, %647:tuple{%646:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %648:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%644:tuple{NotSurpot:FunctionalTensor},%647:tuple{%646:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%648:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%644:tuple{NotSurpot:FunctionalTensor}, %647:tuple{%646:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %648:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%649:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%640:list{NotSurpot:FunctionalTensor}, %641:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %648:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %644:tuple{NotSurpot:FunctionalTensor}, %650:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %651:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %646:tuple{NotSurpot:FunctionalTensor}, %649:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%649:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%651:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%651:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::GraphModule call:
          inputs: (%653:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor})
          outputs: (torch.2_3_0.GraphModule(%653:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::GraphModule return:
          inputs: (%653:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor})
          outputs: (%654:list{FakeTensor(___,_device='cuda_6',_size=(4,_4)):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%655:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%655:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::grad call:
          inputs: (%658:list{NotSurpot:FunctionalTensor}, %659:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %660:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%658:list{NotSurpot:FunctionalTensor},%659:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%660:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%659:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%659:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%659:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%661:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%662:tuple{NotSurpot:FunctionalTensor}, %661:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%662:tuple{NotSurpot:FunctionalTensor},%661:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%662:tuple{NotSurpot:FunctionalTensor}, %661:tuple{NotSurpot:FunctionalTensor}, False:bool, %663:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%664:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%662:tuple{NotSurpot:FunctionalTensor}, %665:tuple{%664:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %666:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%662:tuple{NotSurpot:FunctionalTensor},%665:tuple{%664:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%666:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%662:tuple{NotSurpot:FunctionalTensor}, %665:tuple{%664:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %666:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%667:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%658:list{NotSurpot:FunctionalTensor}, %659:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %666:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %662:tuple{NotSurpot:FunctionalTensor}, %668:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %669:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %664:tuple{NotSurpot:FunctionalTensor}, %667:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%667:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%669:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%669:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::GraphModule call:
          inputs: (%671:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor})
          outputs: (torch.2_3_0.GraphModule(%671:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::GraphModule return:
          inputs: (%671:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor})
          outputs: (%672:list{FakeTensor(___,_device='cuda_6',_size=(4,_4,_4)):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%673:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%673:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4,_4),_requires_grad=True):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::grad call:
          inputs: (%676:list{NotSurpot:FunctionalTensor}, %677:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %678:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%676:list{NotSurpot:FunctionalTensor},%677:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%678:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%677:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%677:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%677:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%679:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%680:tuple{NotSurpot:FunctionalTensor}, %679:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%680:tuple{NotSurpot:FunctionalTensor},%679:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%680:tuple{NotSurpot:FunctionalTensor}, %679:tuple{NotSurpot:FunctionalTensor}, False:bool, %681:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%682:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%680:tuple{NotSurpot:FunctionalTensor}, %683:tuple{%682:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %684:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%680:tuple{NotSurpot:FunctionalTensor},%683:tuple{%682:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%684:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%680:tuple{NotSurpot:FunctionalTensor}, %683:tuple{%682:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %684:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%685:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%676:list{NotSurpot:FunctionalTensor}, %677:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %684:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %680:tuple{NotSurpot:FunctionalTensor}, %686:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %687:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %682:tuple{NotSurpot:FunctionalTensor}, %685:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%685:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%685:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%685:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::GraphModule call:
          inputs: (%689:tuple{FakeTensor(___,_device='cuda_6',_size=(4,),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, 0_213377:float, 0_113377:float})
          outputs: (torch.2_3_0.GraphModule(%689:tuple{FakeTensor(___,_device='cuda_6',_size=(4,),_requires_grad=True):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::GraphModule return:
          inputs: (%689:tuple{FakeTensor(___,_device='cuda_6',_size=(4,),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, 0_213377:float, 0_113377:float})
          outputs: (%690:list{FakeTensor(___,_device='cuda_6',_size=(4,_4)):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%691:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor, FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%691:tuple{FakeTensor(___,_device='cuda_6',_size=(4,_4),_requires_grad=True):FakeTensor,FakeTensor(___,_device)
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55dc412bf0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55dc412bf0_:_InferenceMode)
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%501:tuple{%696:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%501:tuple{%696:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
68736 2024-12-10 17:48:01.306371 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n1,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%701:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%701:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%701:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%701:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%704:tuple{%701:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%704:tuple{%701:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%704:tuple{%701:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%704:tuple{%701:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%708:tuple{%701:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %344:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%708:tuple{%701:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%344:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%701:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %704:tuple{%701:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%706:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%701:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%706:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
68763 2024-12-10 17:48:01.366529 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n1,rank6)
        - ----------->api::MoELayer call:
          inputs: (%501:tuple{%711:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%501:tuple{%711:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
68771 2024-12-10 17:48:01.369658 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n0,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%717:tuple{%714:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%717:tuple{%714:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
68779 2024-12-10 17:48:01.372754 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n0,rank6)
        - aten::mm:
          inputs: (%723:<1024x5120xbf16>{5120, 1}, %722:<5120x160xbf16>{1, 5120})
          outputs: (%724:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%728:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%729:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%731:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%732:tuple{%733:<1024x6xbf16>{6,1},%734:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%735:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%736:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%738:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%739:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%739:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %740:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%739:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%739:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %738:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%741:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%736:<1024x160xf32>{160, 1}, %738:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%742:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%742:<160xf32>{1}, %741:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%740:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%740:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%743:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%743:<i32>, 2_5431315104166666e-07:float)
          outputs: (%744:<i32>)
          duration: -1
        - aten::div:
          inputs: (%744:<i32>, 0_01:float)
          outputs: (%745:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%738:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%746:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%747:<i32>, %749:<i32>, alpha=1:int)
          outputs: (%747:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%750:<i32>, %747:<i32>, False:bool)
          outputs: (%750:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%716:tuple{%744:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%716:tuple{%744:<i32>}))
          duration: -1
68937 2024-12-10 17:48:01.478572 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n0,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%717:tuple{%714:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%756:tuple{%727:<1024x6xbf16>{6,1},%753:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%747:<8192x5120xbf16>{5120, 1}, %729:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%747:<8192x5120xbf16>{5120,1},%729:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%747:<8192x5120xbf16>{5120, 1}, %729:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%754:tuple{%747:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%758:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %753:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%758:<8192x6xCUSTOM_DATA_TYPE>{6,1},%753:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%758:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %753:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%759:tuple{%758:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%758:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%760:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%758:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%719:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%760:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %719:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%761:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%758:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %761:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%762:<6352xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%764:<8192x6xbf16>{6, 1}, %727:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%764:<8192x6xbf16>{6,1},%727:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%764:<8192x6xbf16>{6, 1}, %727:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%697:tuple{%764:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%764:<8192x6xbf16>{6, 1}, %761:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%765:<6352xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%761:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%766:<6352x2xCUSTOM_DATA_TYPE>{1,6352})
          duration: -1
        - aten::gather:
          inputs: (%747:<8192x5120xbf16>{5120, 1}, 0:int, %771:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%772:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%762:<6352xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%732:tuple{%773:<6352xCUSTOM_DATA_TYPE>{1},%770:<6352xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%774:<6352xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%775:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%772:<6352x5120xbf16>{5120, 1}, 0:int, %776:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%779:<6352x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%732:tuple{%779:<6352x5120xbf16>{5120, 1}, %777:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%732:tuple{%779:<6352x5120xbf16>{5120,1},%777:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
70383 2024-12-10 17:48:11.171207 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n0,rank6)
        - aten::cumsum:
          inputs: (%736:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%782:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%783:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%784:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%783:list{%784:<1xCUSTOM_DATA_TYPE>{1}, %782:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%785:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%788:<75x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%717:tuple{%788:<75x5120xbf16>{5120,1}}))
          duration: -1
70478 2024-12-10 17:48:11.188888 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n0,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%721:tuple{%791:<75x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%721:tuple{%791:<75x5120xbf16>{5120,1}}))
          duration: -1
70502 2024-12-10 17:48:11.191993 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n4,rank6)
        - aten::mm:
          inputs: (%796:<75x5120xbf16>{5120, 1}, %800:<5120x3072xbf16>{1, 5120})
          outputs: (%801:<75x3072xbf16>{3072,1})
          duration: -1
70563 2024-12-10 17:48:11.199063 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n4,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%721:tuple{%791:<75x5120xbf16>{5120, 1}})
          outputs: (%752:tuple{%800:<75x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%758:<75x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%758:<75x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%758:<75x1536xbf16>{3072, 1})
          outputs: (%807:<75x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%758:<75x1536xbf16>{3072, 1}, False:bool)
          outputs: (%807:<75x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%807:<75x1536xbf16>{1536, 1}, %806:<75x1536xbf16>{3072, 1}+1536)
          outputs: (%808:<75x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%797:tuple{%808:<75x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%797:tuple{%808:<75x1536xbf16>{1536,1}}))
          duration: -1
70677 2024-12-10 17:48:11.218705 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n1,rank6)
        - aten::mm:
          inputs: (%810:<75x1536xbf16>{1536, 1}, %800:<1536x5120xbf16>{1, 1536})
          outputs: (%813:<75x5120xbf16>{5120,1})
          duration: -1
70734 2024-12-10 17:48:11.225688 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n1,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%797:tuple{%808:<75x1536xbf16>{1536, 1}})
          outputs: (%754:tuple{%816:<75x5120xbf16>{5120,1},None:NoneType})
          duration: -1
70766 2024-12-10 17:48:11.228786 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n0,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%788:<75x5120xbf16>{5120, 1}})
          outputs: (%752:tuple{%820:<75x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%816:<75x5120xbf16>{5120, 1}, %820:<75x5120xbf16>{5120, 1}, False:bool)
          outputs: (%816:<75x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%823:<523x5120xbf16>{5120, 1}+384000})
          outputs: (torch.2_3_0.MLP(%717:tuple{%823:<523x5120xbf16>{5120,1}+384000}))
          duration: -1
70867 2024-12-10 17:48:11.243782 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n1,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%712:tuple{%825:<523x5120xbf16>{5120, 1}+384000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%712:tuple{%825:<523x5120xbf16>{5120,1}+384000}))
          duration: -1
70904 2024-12-10 17:48:11.246854 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n5,rank6)
        - aten::mm:
          inputs: (%828:<523x5120xbf16>{5120, 1}+384000, %832:<5120x3072xbf16>{1, 5120})
          outputs: (%788:<523x3072xbf16>{3072,1})
          duration: -1
70984 2024-12-10 17:48:11.253713 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n5,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%712:tuple{%825:<523x5120xbf16>{5120, 1}+384000})
          outputs: (%831:tuple{%835:<523x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%832:<523x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%832:<523x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%832:<523x1536xbf16>{3072, 1})
          outputs: (%839:<523x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%832:<523x1536xbf16>{3072, 1}, False:bool)
          outputs: (%839:<523x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%839:<523x1536xbf16>{1536, 1}, %838:<523x1536xbf16>{3072, 1}+1536)
          outputs: (%840:<523x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%829:tuple{%840:<523x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%829:tuple{%840:<523x1536xbf16>{1536,1}}))
          duration: -1
71157 2024-12-10 17:48:11.268237 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n2,rank6)
        - aten::mm:
          inputs: (%842:<523x1536xbf16>{1536, 1}, %845:<1536x5120xbf16>{1, 1536})
          outputs: (%846:<523x5120xbf16>{5120,1})
          duration: -1
71250 2024-12-10 17:48:11.274986 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n2,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%829:tuple{%840:<523x1536xbf16>{1536, 1}})
          outputs: (%781:tuple{%849:<523x5120xbf16>{5120,1},None:NoneType})
          duration: -1
71295 2024-12-10 17:48:11.278016 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n1,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%823:<523x5120xbf16>{5120, 1}+384000})
          outputs: (%831:tuple{%853:<523x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%813:<523x5120xbf16>{5120, 1}+384000, %853:<523x5120xbf16>{5120, 1}, False:bool)
          outputs: (%813:<523x5120xbf16>{5120,1}+384000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%856:<243x5120xbf16>{5120, 1}+3061760})
          outputs: (torch.2_3_0.MLP(%717:tuple{%856:<243x5120xbf16>{5120,1}+3061760}))
          duration: -1
71498 2024-12-10 17:48:11.292764 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n2,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%792:tuple{%767:<243x5120xbf16>{5120, 1}+3061760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%792:tuple{%767:<243x5120xbf16>{5120,1}+3061760}))
          duration: -1
71528 2024-12-10 17:48:11.295810 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n6,rank6)
        - aten::mm:
          inputs: (%849:<243x5120xbf16>{5120, 1}+3061760, %862:<5120x3072xbf16>{1, 5120})
          outputs: (%813:<243x3072xbf16>{3072,1})
          duration: -1
71600 2024-12-10 17:48:11.302775 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n6,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%792:tuple{%767:<243x5120xbf16>{5120, 1}+3061760})
          outputs: (%754:tuple{%862:<243x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%868:<243x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%868:<243x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%868:<243x1536xbf16>{3072, 1})
          outputs: (%870:<243x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%868:<243x1536xbf16>{3072, 1}, False:bool)
          outputs: (%870:<243x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%870:<243x1536xbf16>{1536, 1}, %869:<243x1536xbf16>{3072, 1}+1536)
          outputs: (%871:<243x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%860:tuple{%871:<243x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%860:tuple{%871:<243x1536xbf16>{1536,1}}))
          duration: -1
71821 2024-12-10 17:48:11.318105 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n3,rank6)
        - aten::mm:
          inputs: (%873:<243x1536xbf16>{1536, 1}, %876:<1536x5120xbf16>{1, 1536})
          outputs: (%762:<243x5120xbf16>{5120,1})
          duration: -1
71907 2024-12-10 17:48:11.324912 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n3,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%860:tuple{%871:<243x1536xbf16>{1536, 1}})
          outputs: (%752:tuple{%879:<243x5120xbf16>{5120,1},None:NoneType})
          duration: -1
71955 2024-12-10 17:48:11.328025 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n2,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%856:<243x5120xbf16>{5120, 1}+3061760})
          outputs: (%754:tuple{%883:<243x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%767:<243x5120xbf16>{5120, 1}+3061760, %883:<243x5120xbf16>{5120, 1}, False:bool)
          outputs: (%767:<243x5120xbf16>{5120,1}+3061760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%855:<529x5120xbf16>{5120, 1}+4305920})
          outputs: (torch.2_3_0.MLP(%717:tuple{%855:<529x5120xbf16>{5120,1}+4305920}))
          duration: -1
72176 2024-12-10 17:48:11.342899 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n3,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%826:tuple{%888:<529x5120xbf16>{5120, 1}+4305920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%826:tuple{%888:<529x5120xbf16>{5120,1}+4305920}))
          duration: -1
72212 2024-12-10 17:48:11.345941 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n7,rank6)
        - aten::mm:
          inputs: (%891:<529x5120xbf16>{5120, 1}+4305920, %894:<5120x3072xbf16>{1, 5120})
          outputs: (%895:<529x3072xbf16>{3072,1})
          duration: -1
72280 2024-12-10 17:48:11.352797 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n7,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%826:tuple{%888:<529x5120xbf16>{5120, 1}+4305920})
          outputs: (%781:tuple{%898:<529x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%822:<529x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%822:<529x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%822:<529x1536xbf16>{3072, 1})
          outputs: (%903:<529x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%822:<529x1536xbf16>{3072, 1}, False:bool)
          outputs: (%903:<529x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%903:<529x1536xbf16>{1536, 1}, %902:<529x1536xbf16>{3072, 1}+1536)
          outputs: (%904:<529x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%892:tuple{%904:<529x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%892:tuple{%904:<529x1536xbf16>{1536,1}}))
          duration: -1
72489 2024-12-10 17:48:11.367395 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n4,rank6)
        - aten::mm:
          inputs: (%905:<529x1536xbf16>{1536, 1}, %908:<1536x5120xbf16>{1, 1536})
          outputs: (%816:<529x5120xbf16>{5120,1})
          duration: -1
72586 2024-12-10 17:48:11.374213 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n4,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%892:tuple{%904:<529x1536xbf16>{1536, 1}})
          outputs: (%831:tuple{%911:<529x5120xbf16>{5120,1},None:NoneType})
          duration: -1
72626 2024-12-10 17:48:11.377289 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n3,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%855:<529x5120xbf16>{5120, 1}+4305920})
          outputs: (%781:tuple{%915:<529x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%917:<529x5120xbf16>{5120, 1}+4305920, %915:<529x5120xbf16>{5120, 1}, False:bool)
          outputs: (%917:<529x5120xbf16>{5120,1}+4305920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%918:<132x5120xbf16>{5120, 1}+7014400})
          outputs: (torch.2_3_0.MLP(%717:tuple{%918:<132x5120xbf16>{5120,1}+7014400}))
          duration: -1
72994 2024-12-10 17:48:11.404002 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n4,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%858:tuple{%920:<132x5120xbf16>{5120, 1}+7014400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%858:tuple{%920:<132x5120xbf16>{5120,1}+7014400}))
          duration: -1
73041 2024-12-10 17:48:11.406998 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n8,rank6)
        - aten::mm:
          inputs: (%883:<132x5120xbf16>{5120, 1}+7014400, %925:<5120x3072xbf16>{1, 5120})
          outputs: (%926:<132x3072xbf16>{3072,1})
          duration: -1
73307 2024-12-10 17:48:11.425759 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n8,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%858:tuple{%920:<132x5120xbf16>{5120, 1}+7014400})
          outputs: (%752:tuple{%925:<132x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%932:<132x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%932:<132x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%932:<132x1536xbf16>{3072, 1})
          outputs: (%933:<132x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%932:<132x1536xbf16>{3072, 1}, False:bool)
          outputs: (%933:<132x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%933:<132x1536xbf16>{1536, 1}, %894:<132x1536xbf16>{3072, 1}+1536)
          outputs: (%934:<132x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%923:tuple{%934:<132x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%923:tuple{%934:<132x1536xbf16>{1536,1}}))
          duration: -1
73618 2024-12-10 17:48:11.447998 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n5,rank6)
        - aten::mm:
          inputs: (%728:<132x1536xbf16>{1536, 1}, %938:<1536x5120xbf16>{1, 1536})
          outputs: (%939:<132x5120xbf16>{5120,1})
          duration: -1
73707 2024-12-10 17:48:11.454679 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n5,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%923:tuple{%934:<132x1536xbf16>{1536, 1}})
          outputs: (%754:tuple{%942:<132x5120xbf16>{5120,1},None:NoneType})
          duration: -1
73752 2024-12-10 17:48:11.457696 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n4,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%918:<132x5120xbf16>{5120, 1}+7014400})
          outputs: (%752:tuple{%908:<132x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%879:<132x5120xbf16>{5120, 1}+7014400, %908:<132x5120xbf16>{5120, 1}, False:bool)
          outputs: (%879:<132x5120xbf16>{5120,1}+7014400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%885:<106x5120xbf16>{5120, 1}+7690240})
          outputs: (torch.2_3_0.MLP(%717:tuple{%885:<106x5120xbf16>{5120,1}+7690240}))
          duration: -1
73990 2024-12-10 17:48:11.472470 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n5,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%889:tuple{%949:<106x5120xbf16>{5120, 1}+7690240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%889:tuple{%949:<106x5120xbf16>{5120,1}+7690240}))
          duration: -1
74036 2024-12-10 17:48:11.475466 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n9,rank6)
        - aten::mm:
          inputs: (%942:<106x5120xbf16>{5120, 1}+7690240, %954:<5120x3072xbf16>{1, 5120})
          outputs: (%955:<106x3072xbf16>{3072,1})
          duration: -1
74133 2024-12-10 17:48:11.482864 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n9,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%889:tuple{%949:<106x5120xbf16>{5120, 1}+7690240})
          outputs: (%831:tuple{%958:<106x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%962:<106x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%962:<106x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%962:<106x1536xbf16>{3072, 1})
          outputs: (%964:<106x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%962:<106x1536xbf16>{3072, 1}, False:bool)
          outputs: (%964:<106x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%964:<106x1536xbf16>{1536, 1}, %963:<106x1536xbf16>{3072, 1}+1536)
          outputs: (%965:<106x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%952:tuple{%965:<106x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%952:tuple{%965:<106x1536xbf16>{1536,1}}))
          duration: -1
74330 2024-12-10 17:48:11.497239 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n6,rank6)
        - aten::mm:
          inputs: (%967:<106x1536xbf16>{1536, 1}, %970:<1536x5120xbf16>{1, 1536})
          outputs: (%971:<106x5120xbf16>{5120,1})
          duration: -1
74432 2024-12-10 17:48:11.505658 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n6,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%952:tuple{%965:<106x1536xbf16>{1536, 1}})
          outputs: (%781:tuple{%974:<106x5120xbf16>{5120,1},None:NoneType})
          duration: -1
74475 2024-12-10 17:48:11.508718 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n5,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%885:<106x5120xbf16>{5120, 1}+7690240})
          outputs: (%831:tuple{%978:<106x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%980:<106x5120xbf16>{5120, 1}+7690240, %978:<106x5120xbf16>{5120, 1}, False:bool)
          outputs: (%980:<106x5120xbf16>{5120,1}+7690240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%982:<157x5120xbf16>{5120, 1}+8232960})
          outputs: (torch.2_3_0.MLP(%717:tuple{%982:<157x5120xbf16>{5120,1}+8232960}))
          duration: -1
74712 2024-12-10 17:48:11.523626 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n6,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%921:tuple{%984:<157x5120xbf16>{5120, 1}+8232960})
          outputs: (torch.2_3_0.ColumnParallelLinear(%921:tuple{%984:<157x5120xbf16>{5120,1}+8232960}))
          duration: -1
74766 2024-12-10 17:48:11.526667 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n10,rank6)
        - aten::mm:
          inputs: (%987:<157x5120xbf16>{5120, 1}+8232960, %991:<5120x3072xbf16>{1, 5120})
          outputs: (%992:<157x3072xbf16>{3072,1})
          duration: -1
74875 2024-12-10 17:48:11.534099 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n10,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%921:tuple{%984:<157x5120xbf16>{5120, 1}+8232960})
          outputs: (%990:tuple{%991:<157x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%997:<157x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%997:<157x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%997:<157x1536xbf16>{3072, 1})
          outputs: (%999:<157x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%997:<157x1536xbf16>{3072, 1}, False:bool)
          outputs: (%999:<157x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%999:<157x1536xbf16>{1536, 1}, %998:<157x1536xbf16>{3072, 1}+1536)
          outputs: (%879:<157x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%988:tuple{%879:<157x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%988:tuple{%879:<157x1536xbf16>{1536,1}}))
          duration: -1
75077 2024-12-10 17:48:11.548573 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n7,rank6)
        - aten::mm:
          inputs: (%908:<157x1536xbf16>{1536, 1}, %1004:<1536x5120xbf16>{1, 1536})
          outputs: (%949:<157x5120xbf16>{5120,1})
          duration: -1
75169 2024-12-10 17:48:11.555304 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n7,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%988:tuple{%879:<157x1536xbf16>{1536, 1}})
          outputs: (%1003:tuple{%1007:<157x5120xbf16>{5120,1},None:NoneType})
          duration: -1
75205 2024-12-10 17:48:11.558320 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n6,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%982:<157x5120xbf16>{5120, 1}+8232960})
          outputs: (%990:tuple{%1010:<157x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%971:<157x5120xbf16>{5120, 1}+8232960, %1010:<157x5120xbf16>{5120, 1}, False:bool)
          outputs: (%971:<157x5120xbf16>{5120,1}+8232960)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%978:<446x5120xbf16>{5120, 1}+9036800})
          outputs: (torch.2_3_0.MLP(%717:tuple{%978:<446x5120xbf16>{5120,1}+9036800}))
          duration: -1
75425 2024-12-10 17:48:11.572852 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n7,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%950:tuple{%1013:<446x5120xbf16>{5120, 1}+9036800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%950:tuple{%1013:<446x5120xbf16>{5120,1}+9036800}))
          duration: -1
75478 2024-12-10 17:48:11.575836 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n11,rank6)
        - aten::mm:
          inputs: (%1004:<446x5120xbf16>{5120, 1}+9036800, %1018:<5120x3072xbf16>{1, 5120})
          outputs: (%1019:<446x3072xbf16>{3072,1})
          duration: -1
75573 2024-12-10 17:48:11.582666 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n11,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%950:tuple{%1013:<446x5120xbf16>{5120, 1}+9036800})
          outputs: (%781:tuple{%1022:<446x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%762:<446x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%762:<446x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%762:<446x1536xbf16>{3072, 1})
          outputs: (%1027:<446x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%762:<446x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1027:<446x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1027:<446x1536xbf16>{1536, 1}, %1026:<446x1536xbf16>{3072, 1}+1536)
          outputs: (%1028:<446x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1016:tuple{%1028:<446x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1016:tuple{%1028:<446x1536xbf16>{1536,1}}))
          duration: -1
75776 2024-12-10 17:48:11.597175 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n8,rank6)
        - aten::mm:
          inputs: (%1018:<446x1536xbf16>{1536, 1}, %1032:<1536x5120xbf16>{1, 1536})
          outputs: (%1033:<446x5120xbf16>{5120,1})
          duration: -1
75878 2024-12-10 17:48:11.604049 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n8,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1016:tuple{%1028:<446x1536xbf16>{1536, 1}})
          outputs: (%831:tuple{%1036:<446x5120xbf16>{5120,1},None:NoneType})
          duration: -1
75917 2024-12-10 17:48:11.607198 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n7,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%978:<446x5120xbf16>{5120, 1}+9036800})
          outputs: (%781:tuple{%1040:<446x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1013:<446x5120xbf16>{5120, 1}+9036800, %1040:<446x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1013:<446x5120xbf16>{5120,1}+9036800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%918:<644x5120xbf16>{5120, 1}+11320320})
          outputs: (torch.2_3_0.MLP(%717:tuple{%918:<644x5120xbf16>{5120,1}+11320320}))
          duration: -1
76139 2024-12-10 17:48:11.622199 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n8,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%985:tuple{%1044:<644x5120xbf16>{5120, 1}+11320320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%985:tuple{%1044:<644x5120xbf16>{5120,1}+11320320}))
          duration: -1
76190 2024-12-10 17:48:11.625254 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n12,rank6)
        - aten::mm:
          inputs: (%1036:<644x5120xbf16>{5120, 1}+11320320, %1049:<5120x3072xbf16>{1, 5120})
          outputs: (%1050:<644x3072xbf16>{3072,1})
          duration: -1
76294 2024-12-10 17:48:11.632059 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n12,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%985:tuple{%1044:<644x5120xbf16>{5120, 1}+11320320})
          outputs: (%1003:tuple{%1049:<644x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1055:<644x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1055:<644x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1055:<644x1536xbf16>{3072, 1})
          outputs: (%954:<644x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1055:<644x1536xbf16>{3072, 1}, False:bool)
          outputs: (%954:<644x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%954:<644x1536xbf16>{1536, 1}, %1056:<644x1536xbf16>{3072, 1}+1536)
          outputs: (%1057:<644x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1047:tuple{%1057:<644x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1047:tuple{%1057:<644x1536xbf16>{1536,1}}))
          duration: -1
76506 2024-12-10 17:48:11.647362 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n9,rank6)
        - aten::mm:
          inputs: (%1059:<644x1536xbf16>{1536, 1}, %1032:<1536x5120xbf16>{1, 1536})
          outputs: (%1062:<644x5120xbf16>{5120,1})
          duration: -1
76608 2024-12-10 17:48:11.654254 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n9,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1047:tuple{%1057:<644x1536xbf16>{1536, 1}})
          outputs: (%990:tuple{%1065:<644x5120xbf16>{5120,1},None:NoneType})
          duration: -1
76645 2024-12-10 17:48:11.657350 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n8,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%918:<644x5120xbf16>{5120, 1}+11320320})
          outputs: (%1003:tuple{%1069:<644x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%886:<644x5120xbf16>{5120, 1}+11320320, %1069:<644x5120xbf16>{5120, 1}, False:bool)
          outputs: (%886:<644x5120xbf16>{5120,1}+11320320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1072:<427x5120xbf16>{5120, 1}+14617600})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1072:<427x5120xbf16>{5120,1}+14617600}))
          duration: -1
76859 2024-12-10 17:48:11.672308 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n9,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1014:tuple{%949:<427x5120xbf16>{5120, 1}+14617600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1014:tuple{%949:<427x5120xbf16>{5120,1}+14617600}))
          duration: -1
        - aten::mm:
          inputs: (%925:<427x5120xbf16>{5120, 1}+14617600, %1044:<5120x3072xbf16>{1, 5120})
          outputs: (%1078:<427x3072xbf16>{3072,1})
          duration: -1
77016 2024-12-10 17:48:11.682182 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n13,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1014:tuple{%949:<427x5120xbf16>{5120, 1}+14617600})
          outputs: (%831:tuple{%1044:<427x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1084:<427x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1084:<427x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1084:<427x1536xbf16>{3072, 1})
          outputs: (%1086:<427x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1084:<427x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1086:<427x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1086:<427x1536xbf16>{1536, 1}, %1085:<427x1536xbf16>{3072, 1}+1536)
          outputs: (%1087:<427x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1076:tuple{%1087:<427x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1076:tuple{%1087:<427x1536xbf16>{1536,1}}))
          duration: -1
77226 2024-12-10 17:48:11.696848 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n10,rank6)
        - aten::mm:
          inputs: (%1089:<427x1536xbf16>{1536, 1}, %1044:<1536x5120xbf16>{1, 1536})
          outputs: (%1013:<427x5120xbf16>{5120,1})
          duration: -1
77333 2024-12-10 17:48:11.703838 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n10,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1076:tuple{%1087:<427x1536xbf16>{1536, 1}})
          outputs: (%781:tuple{%1094:<427x5120xbf16>{5120,1},None:NoneType})
          duration: -1
77377 2024-12-10 17:48:11.706986 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n9,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1072:<427x5120xbf16>{5120, 1}+14617600})
          outputs: (%831:tuple{%1097:<427x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1094:<427x5120xbf16>{5120, 1}+14617600, %1097:<427x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1094:<427x5120xbf16>{5120,1}+14617600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%918:<322x5120xbf16>{5120, 1}+16803840})
          outputs: (torch.2_3_0.MLP(%717:tuple{%918:<322x5120xbf16>{5120,1}+16803840}))
          duration: -1
77593 2024-12-10 17:48:11.722241 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n10,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1045:tuple{%1069:<322x5120xbf16>{5120, 1}+16803840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1045:tuple{%1069:<322x5120xbf16>{5120,1}+16803840}))
          duration: -1
77647 2024-12-10 17:48:11.725371 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n14,rank6)
        - aten::mm:
          inputs: (%1065:<322x5120xbf16>{5120, 1}+16803840, %1105:<5120x3072xbf16>{1, 5120})
          outputs: (%1106:<322x3072xbf16>{3072,1})
          duration: -1
77755 2024-12-10 17:48:11.732390 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n14,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1045:tuple{%1069:<322x5120xbf16>{5120, 1}+16803840})
          outputs: (%990:tuple{%1109:<322x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1113:<322x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1113:<322x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1113:<322x1536xbf16>{3072, 1})
          outputs: (%1115:<322x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1113:<322x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1115:<322x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1115:<322x1536xbf16>{1536, 1}, %1114:<322x1536xbf16>{3072, 1}+1536)
          outputs: (%1116:<322x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1103:tuple{%1116:<322x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1103:tuple{%1116:<322x1536xbf16>{1536,1}}))
          duration: -1
77965 2024-12-10 17:48:11.747762 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n11,rank6)
        - aten::mm:
          inputs: (%1117:<322x1536xbf16>{1536, 1}, %1120:<1536x5120xbf16>{1, 1536})
          outputs: (%1121:<322x5120xbf16>{5120,1})
          duration: -1
78072 2024-12-10 17:48:11.754674 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n11,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1103:tuple{%1116:<322x1536xbf16>{1536, 1}})
          outputs: (%1003:tuple{%1124:<322x5120xbf16>{5120,1},None:NoneType})
          duration: -1
78114 2024-12-10 17:48:11.757794 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n10,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%918:<322x5120xbf16>{5120, 1}+16803840})
          outputs: (%990:tuple{%1128:<322x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1130:<322x5120xbf16>{5120, 1}+16803840, %1128:<322x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1130:<322x5120xbf16>{5120,1}+16803840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1131:<282x5120xbf16>{5120, 1}+18452480})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1131:<282x5120xbf16>{5120,1}+18452480}))
          duration: -1
78327 2024-12-10 17:48:11.772964 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n11,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1074:tuple{%949:<282x5120xbf16>{5120, 1}+18452480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1074:tuple{%949:<282x5120xbf16>{5120,1}+18452480}))
          duration: -1
78381 2024-12-10 17:48:11.776103 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n15,rank6)
        - aten::mm:
          inputs: (%1135:<282x5120xbf16>{5120, 1}+18452480, %1138:<5120x3072xbf16>{1, 5120})
          outputs: (%1139:<282x3072xbf16>{3072,1})
          duration: -1
78495 2024-12-10 17:48:11.783105 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n15,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1074:tuple{%949:<282x5120xbf16>{5120, 1}+18452480})
          outputs: (%781:tuple{%1142:<282x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1146:<282x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1146:<282x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1146:<282x1536xbf16>{3072, 1})
          outputs: (%1148:<282x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1146:<282x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1148:<282x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1148:<282x1536xbf16>{1536, 1}, %1147:<282x1536xbf16>{3072, 1}+1536)
          outputs: (%1149:<282x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1136:tuple{%1149:<282x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1136:tuple{%1149:<282x1536xbf16>{1536,1}}))
          duration: -1
78708 2024-12-10 17:48:11.798501 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n12,rank6)
        - aten::mm:
          inputs: (%1142:<282x1536xbf16>{1536, 1}, %1153:<1536x5120xbf16>{1, 1536})
          outputs: (%1120:<282x5120xbf16>{5120,1})
          duration: -1
78824 2024-12-10 17:48:11.805239 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n12,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1136:tuple{%1149:<282x1536xbf16>{1536, 1}})
          outputs: (%831:tuple{%1156:<282x5120xbf16>{5120,1},None:NoneType})
          duration: -1
78865 2024-12-10 17:48:11.808247 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n11,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1131:<282x5120xbf16>{5120, 1}+18452480})
          outputs: (%781:tuple{%1160:<282x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%886:<282x5120xbf16>{5120, 1}+18452480, %1160:<282x5120xbf16>{5120, 1}, False:bool)
          outputs: (%886:<282x5120xbf16>{5120,1}+18452480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1162:<254x5120xbf16>{5120, 1}+19896320})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1162:<254x5120xbf16>{5120,1}+19896320}))
          duration: -1
79084 2024-12-10 17:48:11.823537 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n12,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1101:tuple{%1164:<254x5120xbf16>{5120, 1}+19896320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1101:tuple{%1164:<254x5120xbf16>{5120,1}+19896320}))
          duration: -1
79143 2024-12-10 17:48:11.827014 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n16,rank6)
        - aten::mm:
          inputs: (%276:<254x5120xbf16>{5120, 1}+19896320, %1169:<5120x3072xbf16>{1, 5120})
          outputs: (%1170:<254x3072xbf16>{3072,1})
          duration: -1
79258 2024-12-10 17:48:11.834653 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n16,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1101:tuple{%1164:<254x5120xbf16>{5120, 1}+19896320})
          outputs: (%1003:tuple{%1173:<254x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%971:<254x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%971:<254x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%971:<254x1536xbf16>{3072, 1})
          outputs: (%1178:<254x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%971:<254x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1178:<254x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1178:<254x1536xbf16>{1536, 1}, %1177:<254x1536xbf16>{3072, 1}+1536)
          outputs: (%1179:<254x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1167:tuple{%1179:<254x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1167:tuple{%1179:<254x1536xbf16>{1536,1}}))
          duration: -1
79511 2024-12-10 17:48:11.851447 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n13,rank6)
        - aten::mm:
          inputs: (%1181:<254x1536xbf16>{1536, 1}, %1184:<1536x5120xbf16>{1, 1536})
          outputs: (%1185:<254x5120xbf16>{5120,1})
          duration: -1
79614 2024-12-10 17:48:11.858615 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n13,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1167:tuple{%1179:<254x1536xbf16>{1536, 1}})
          outputs: (%990:tuple{%1121:<254x5120xbf16>{5120,1},None:NoneType})
          duration: -1
79649 2024-12-10 17:48:11.861661 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n12,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1162:<254x5120xbf16>{5120, 1}+19896320})
          outputs: (%1003:tuple{%1191:<254x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1193:<254x5120xbf16>{5120, 1}+19896320, %1191:<254x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1193:<254x5120xbf16>{5120,1}+19896320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1164:<351x5120xbf16>{5120, 1}+21196800})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1164:<351x5120xbf16>{5120,1}+21196800}))
          duration: -1
79883 2024-12-10 17:48:11.876701 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n13,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1133:tuple{%918:<351x5120xbf16>{5120, 1}+21196800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1133:tuple{%918:<351x5120xbf16>{5120,1}+21196800}))
          duration: -1
79931 2024-12-10 17:48:11.879793 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n17,rank6)
        - aten::mm:
          inputs: (%1062:<351x5120xbf16>{5120, 1}+21196800, %1200:<5120x3072xbf16>{1, 5120})
          outputs: (%1201:<351x3072xbf16>{3072,1})
          duration: -1
80021 2024-12-10 17:48:11.886710 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n17,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1133:tuple{%918:<351x5120xbf16>{5120, 1}+21196800})
          outputs: (%831:tuple{%1173:<351x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1206:<351x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1206:<351x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1206:<351x1536xbf16>{3072, 1})
          outputs: (%1208:<351x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1206:<351x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1208:<351x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1208:<351x1536xbf16>{1536, 1}, %1207:<351x1536xbf16>{3072, 1}+1536)
          outputs: (%1209:<351x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1198:tuple{%1209:<351x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1198:tuple{%1209:<351x1536xbf16>{1536,1}}))
          duration: -1
80247 2024-12-10 17:48:11.901283 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n14,rank6)
        - aten::mm:
          inputs: (%1200:<351x1536xbf16>{1536, 1}, %1213:<1536x5120xbf16>{1, 1536})
          outputs: (%1214:<351x5120xbf16>{5120,1})
          duration: -1
80359 2024-12-10 17:48:11.908103 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n14,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1198:tuple{%1209:<351x1536xbf16>{1536, 1}})
          outputs: (%781:tuple{%1216:<351x5120xbf16>{5120,1},None:NoneType})
          duration: -1
80397 2024-12-10 17:48:11.911153 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n13,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1164:<351x5120xbf16>{5120, 1}+21196800})
          outputs: (%831:tuple{%1220:<351x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1222:<351x5120xbf16>{5120, 1}+21196800, %1220:<351x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1222:<351x5120xbf16>{5120,1}+21196800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%775:<688x5120xbf16>{5120, 1}+22993920})
          outputs: (torch.2_3_0.MLP(%717:tuple{%775:<688x5120xbf16>{5120,1}+22993920}))
          duration: -1
80623 2024-12-10 17:48:11.925735 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n14,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1165:tuple{%1225:<688x5120xbf16>{5120, 1}+22993920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1165:tuple{%1225:<688x5120xbf16>{5120,1}+22993920}))
          duration: -1
80673 2024-12-10 17:48:11.928700 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n18,rank6)
        - aten::mm:
          inputs: (%1216:<688x5120xbf16>{5120, 1}+22993920, %1230:<5120x3072xbf16>{1, 5120})
          outputs: (%1231:<688x3072xbf16>{3072,1})
          duration: -1
80772 2024-12-10 17:48:11.935438 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n18,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1165:tuple{%1225:<688x5120xbf16>{5120, 1}+22993920})
          outputs: (%990:tuple{%1234:<688x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1238:<688x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1238:<688x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1238:<688x1536xbf16>{3072, 1})
          outputs: (%1230:<688x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1238:<688x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1230:<688x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1230:<688x1536xbf16>{1536, 1}, %1185:<688x1536xbf16>{3072, 1}+1536)
          outputs: (%1239:<688x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1228:tuple{%1239:<688x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1228:tuple{%1239:<688x1536xbf16>{1536,1}}))
          duration: -1
80989 2024-12-10 17:48:11.949474 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n15,rank6)
        - aten::mm:
          inputs: (%1234:<688x1536xbf16>{1536, 1}, %1243:<1536x5120xbf16>{1, 1536})
          outputs: (%1244:<688x5120xbf16>{5120,1})
          duration: -1
81089 2024-12-10 17:48:11.956104 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n15,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1228:tuple{%1239:<688x1536xbf16>{1536, 1}})
          outputs: (%1003:tuple{%1247:<688x5120xbf16>{5120,1},None:NoneType})
          duration: -1
81127 2024-12-10 17:48:11.959074 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n14,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%775:<688x5120xbf16>{5120, 1}+22993920})
          outputs: (%990:tuple{%1251:<688x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1225:<688x5120xbf16>{5120, 1}+22993920, %1251:<688x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1225:<688x5120xbf16>{5120,1}+22993920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1160:<195x5120xbf16>{5120, 1}+26516480})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1160:<195x5120xbf16>{5120,1}+26516480}))
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1196:tuple{%1256:<195x5120xbf16>{5120, 1}+26516480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1196:tuple{%1256:<195x5120xbf16>{5120,1}+26516480}))
          duration: -1
81394 2024-12-10 17:48:11.976292 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n19,rank6)
        - aten::mm:
          inputs: (%1259:<195x5120xbf16>{5120, 1}+26516480, %1262:<5120x3072xbf16>{1, 5120})
          outputs: (%1263:<195x3072xbf16>{3072,1})
          duration: -1
81480 2024-12-10 17:48:11.982861 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n19,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1196:tuple{%1256:<195x5120xbf16>{5120, 1}+26516480})
          outputs: (%781:tuple{%1266:<195x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1270:<195x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1270:<195x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1270:<195x1536xbf16>{3072, 1})
          outputs: (%1271:<195x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1270:<195x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1271:<195x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1271:<195x1536xbf16>{1536, 1}, %1220:<195x1536xbf16>{3072, 1}+1536)
          outputs: (%1262:<195x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1260:tuple{%1262:<195x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1260:tuple{%1262:<195x1536xbf16>{1536,1}}))
          duration: -1
81706 2024-12-10 17:48:11.997451 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n16,rank6)
        - aten::mm:
          inputs: (%1272:<195x1536xbf16>{1536, 1}, %1275:<1536x5120xbf16>{1, 1536})
          outputs: (%1276:<195x5120xbf16>{5120,1})
          duration: -1
81807 2024-12-10 17:48:12.004022 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n16,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1260:tuple{%1262:<195x1536xbf16>{1536, 1}})
          outputs: (%831:tuple{%1275:<195x5120xbf16>{5120,1},None:NoneType})
          duration: -1
81843 2024-12-10 17:48:12.006937 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n15,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1160:<195x5120xbf16>{5120, 1}+26516480})
          outputs: (%781:tuple{%1282:<195x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1275:<195x5120xbf16>{5120, 1}+26516480, %1282:<195x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1275:<195x5120xbf16>{5120,1}+26516480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1285:<337x5120xbf16>{5120, 1}+27514880})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1285:<337x5120xbf16>{5120,1}+27514880}))
          duration: -1
82070 2024-12-10 17:48:12.021275 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n16,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1226:tuple{%1266:<337x5120xbf16>{5120, 1}+27514880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1226:tuple{%1266:<337x5120xbf16>{5120,1}+27514880}))
          duration: -1
82122 2024-12-10 17:48:12.024163 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n20,rank6)
        - aten::mm:
          inputs: (%1289:<337x5120xbf16>{5120, 1}+27514880, %1292:<5120x3072xbf16>{1, 5120})
          outputs: (%1225:<337x3072xbf16>{3072,1})
          duration: -1
82221 2024-12-10 17:48:12.030802 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n20,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1226:tuple{%1266:<337x5120xbf16>{5120, 1}+27514880})
          outputs: (%1003:tuple{%1295:<337x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1298:<337x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1298:<337x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1298:<337x1536xbf16>{3072, 1})
          outputs: (%1300:<337x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1298:<337x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1300:<337x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1300:<337x1536xbf16>{1536, 1}, %1299:<337x1536xbf16>{3072, 1}+1536)
          outputs: (%1301:<337x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1290:tuple{%1301:<337x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1290:tuple{%1301:<337x1536xbf16>{1536,1}}))
          duration: -1
82423 2024-12-10 17:48:12.045020 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n17,rank6)
        - aten::mm:
          inputs: (%1303:<337x1536xbf16>{1536, 1}, %1306:<1536x5120xbf16>{1, 1536})
          outputs: (%1292:<337x5120xbf16>{5120,1})
          duration: -1
82532 2024-12-10 17:48:12.051680 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n17,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1290:tuple{%1301:<337x1536xbf16>{1536, 1}})
          outputs: (%990:tuple{%1309:<337x5120xbf16>{5120,1},None:NoneType})
          duration: -1
82574 2024-12-10 17:48:12.054595 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n16,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1285:<337x5120xbf16>{5120, 1}+27514880})
          outputs: (%1003:tuple{%1313:<337x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%886:<337x5120xbf16>{5120, 1}+27514880, %1313:<337x5120xbf16>{5120, 1}, False:bool)
          outputs: (%886:<337x5120xbf16>{5120,1}+27514880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1276:<85x5120xbf16>{5120, 1}+29240320})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1276:<85x5120xbf16>{5120,1}+29240320}))
          duration: -1
82799 2024-12-10 17:48:12.068856 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n17,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1257:tuple{%1282:<85x5120xbf16>{5120, 1}+29240320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1257:tuple{%1282:<85x5120xbf16>{5120,1}+29240320}))
          duration: -1
82845 2024-12-10 17:48:12.071761 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n21,rank6)
        - aten::mm:
          inputs: (%1318:<85x5120xbf16>{5120, 1}+29240320, %1321:<5120x3072xbf16>{1, 5120})
          outputs: (%735:<85x3072xbf16>{3072,1})
          duration: -1
82947 2024-12-10 17:48:12.078310 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n21,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1257:tuple{%1282:<85x5120xbf16>{5120, 1}+29240320})
          outputs: (%831:tuple{%1324:<85x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1214:<85x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1214:<85x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1214:<85x1536xbf16>{3072, 1})
          outputs: (%1306:<85x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1214:<85x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1306:<85x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1306:<85x1536xbf16>{1536, 1}, %1328:<85x1536xbf16>{3072, 1}+1536)
          outputs: (%1329:<85x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1319:tuple{%1329:<85x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1319:tuple{%1329:<85x1536xbf16>{1536,1}}))
          duration: -1
83163 2024-12-10 17:48:12.092034 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n18,rank6)
        - aten::mm:
          inputs: (%1331:<85x1536xbf16>{1536, 1}, %1334:<1536x5120xbf16>{1, 1536})
          outputs: (%1335:<85x5120xbf16>{5120,1})
          duration: -1
83279 2024-12-10 17:48:12.099331 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n18,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1319:tuple{%1329:<85x1536xbf16>{1536, 1}})
          outputs: (%781:tuple{%1338:<85x5120xbf16>{5120,1},None:NoneType})
          duration: -1
83324 2024-12-10 17:48:12.102484 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n17,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1276:<85x5120xbf16>{5120, 1}+29240320})
          outputs: (%831:tuple{%886:<85x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1282:<85x5120xbf16>{5120, 1}+29240320, %886:<85x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1282:<85x5120xbf16>{5120,1}+29240320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1344:<213x5120xbf16>{5120, 1}+29675520})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1344:<213x5120xbf16>{5120,1}+29675520}))
          duration: -1
83556 2024-12-10 17:48:12.117672 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n18,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1287:tuple{%1346:<213x5120xbf16>{5120, 1}+29675520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1287:tuple{%1346:<213x5120xbf16>{5120,1}+29675520}))
          duration: -1
83613 2024-12-10 17:48:12.120868 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n22,rank6)
        - aten::mm:
          inputs: (%876:<213x5120xbf16>{5120, 1}+29675520, %1351:<5120x3072xbf16>{1, 5120})
          outputs: (%1352:<213x3072xbf16>{3072,1})
          duration: -1
83723 2024-12-10 17:48:12.127919 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n22,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1287:tuple{%1346:<213x5120xbf16>{5120, 1}+29675520})
          outputs: (%990:tuple{%1022:<213x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%853:<213x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%853:<213x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%853:<213x1536xbf16>{3072, 1})
          outputs: (%1357:<213x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%853:<213x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1357:<213x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1357:<213x1536xbf16>{1536, 1}, %1351:<213x1536xbf16>{3072, 1}+1536)
          outputs: (%1358:<213x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1349:tuple{%1358:<213x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1349:tuple{%1358:<213x1536xbf16>{1536,1}}))
          duration: -1
83950 2024-12-10 17:48:12.142912 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n19,rank6)
        - aten::mm:
          inputs: (%1359:<213x1536xbf16>{1536, 1}, %1362:<1536x5120xbf16>{1, 1536})
          outputs: (%1363:<213x5120xbf16>{5120,1})
          duration: -1
84051 2024-12-10 17:48:12.149559 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n19,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1349:tuple{%1358:<213x1536xbf16>{1536, 1}})
          outputs: (%1003:tuple{%1366:<213x5120xbf16>{5120,1},None:NoneType})
          duration: -1
84089 2024-12-10 17:48:12.152553 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n18,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1344:<213x5120xbf16>{5120, 1}+29675520})
          outputs: (%990:tuple{%1370:<213x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1128:<213x5120xbf16>{5120, 1}+29675520, %1370:<213x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1128:<213x5120xbf16>{5120,1}+29675520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%717:tuple{%1372:<343x5120xbf16>{5120, 1}+30766080})
          outputs: (torch.2_3_0.MLP(%717:tuple{%1372:<343x5120xbf16>{5120,1}+30766080}))
          duration: -1
84329 2024-12-10 17:48:12.166897 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n19,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1316:tuple{%981:<343x5120xbf16>{5120, 1}+30766080})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1316:tuple{%981:<343x5120xbf16>{5120,1}+30766080}))
          duration: -1
84380 2024-12-10 17:48:12.169793 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n23,rank6)
        - aten::mm:
          inputs: (%980:<343x5120xbf16>{5120, 1}+30766080, %1378:<5120x3072xbf16>{1, 5120})
          outputs: (%918:<343x3072xbf16>{3072,1})
          duration: -1
84476 2024-12-10 17:48:12.176220 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n23,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1316:tuple{%981:<343x5120xbf16>{5120, 1}+30766080})
          outputs: (%781:tuple{%1366:<343x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1383:<343x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1383:<343x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1383:<343x1536xbf16>{3072, 1})
          outputs: (%1384:<343x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1383:<343x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1384:<343x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1384:<343x1536xbf16>{1536, 1}, %1378:<343x1536xbf16>{3072, 1}+1536)
          outputs: (%1385:<343x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1376:tuple{%1385:<343x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1376:tuple{%1385:<343x1536xbf16>{1536,1}}))
          duration: -1
84689 2024-12-10 17:48:12.189881 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n20,rank6)
        - aten::mm:
          inputs: (%938:<343x1536xbf16>{1536, 1}, %1388:<1536x5120xbf16>{1, 1536})
          outputs: (%1389:<343x5120xbf16>{5120,1})
          duration: -1
84780 2024-12-10 17:48:12.196352 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n20,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1376:tuple{%1385:<343x1536xbf16>{1536, 1}})
          outputs: (%831:tuple{%1391:<343x5120xbf16>{5120,1},None:NoneType})
          duration: -1
84821 2024-12-10 17:48:12.199294 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n19,rank6)
        - ----------->api::MLP return:
          inputs: (%717:tuple{%1372:<343x5120xbf16>{5120, 1}+30766080})
          outputs: (%781:tuple{%1395:<343x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1397:<343x5120xbf16>{5120, 1}+30766080, %1395:<343x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1397:<343x5120xbf16>{5120,1}+30766080)
          duration: -1
84953 2024-12-10 17:48:12.207206 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n0,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%732:tuple{%779:<6352x5120xbf16>{5120, 1}, %777:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%831:tuple{%785:<6352x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%784:<6352x5120xbf16>{5120, 1}, 0:int, %776:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, %785:<6352x5120xbf16>{5120, 1})
          outputs: (%729:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%1394:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1315:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%1315:<8192x5120xbf16>{5120, 1}, 0:int, %771:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, %731:<6352x5120xbf16>{5120, 1})
          outputs: (%1372:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%1397:<1024x5120xbf16>{5120, 1}, %1372:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%1397:<1024x5120xbf16>{5120,1},%1372:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%1397:<1024x5120xbf16>{5120, 1}, %1372:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%1397:<1024x5120xbf16>{5120,1},%1372:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%1397:<1024x5120xbf16>{5120, 1}, %1372:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%757:tuple{%1397:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1396:tuple{%714:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%1396:tuple{%714:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
86089 2024-12-10 17:48:12.366131 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n20,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1374:tuple{%1315:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1374:tuple{%1315:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
86152 2024-12-10 17:48:12.369021 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n24,rank6)
        - aten::mm:
          inputs: (%429:<1024x5120xbf16>{5120, 1}, %1411:<5120x6144xbf16>{1, 5120})
          outputs: (%816:<1024x6144xbf16>{6144,1})
          duration: -1
86364 2024-12-10 17:48:12.380300 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n24,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1374:tuple{%1315:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1410:tuple{%1411:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1372:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1372:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1372:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%1363:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1372:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%1363:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%1363:<1024x1x3072xbf16>{3072, 3072, 1}, %429:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%1391:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1406:tuple{%1391:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1406:tuple{%1391:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
86528 2024-12-10 17:48:12.393855 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n21,rank6)
        - aten::mm:
          inputs: (%1422:<1024x3072xbf16>{3072, 1}, %1420:<3072x5120xbf16>{1, 3072})
          outputs: (%1423:<1024x5120xbf16>{5120,1})
          duration: -1
86728 2024-12-10 17:48:12.404925 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n21,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1406:tuple{%1391:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%1410:tuple{%1429:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
86779 2024-12-10 17:48:12.407838 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n20,rank6)
        - ----------->api::MLP return:
          inputs: (%1396:tuple{%714:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%831:tuple{%1432:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%1402:<1024x1x5120xbf16>{5120, 5120, 1}, %1422:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%1315:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
86869 2024-12-10 17:48:12.413539 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n0,rank6)
        - ----------->api::MoELayer return:
          inputs: (%501:tuple{%711:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1400:tuple{%777:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::stack:
          inputs: (%432:list{%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%241:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%241:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%416:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%430:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%430:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FakeTensor)
          duration: -1
        - aten::stack:
          inputs: (%428:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%431:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%431:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%413:<1024xf32>{1}}, 0:int, out=%434:<1x1024xf32>{1024, 1})
          outputs: (%434:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - aten::stack:
          inputs: (%418:list{%410:<1024xf32>{1}}, 0:int, out=%431:<1x1024xf32>{1024, 1})
          outputs: (%431:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%416:list{%410:<1024xf32>{1}}, 0:int, out=%432:<1x1024xf32>{1024, 1})
          outputs: (%432:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%1437:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%1437:tuple{None:NoneType,)
          duration: -1
        - aten::stack:
          inputs: (%433:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%435:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%435:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%429:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%432:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%432:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%295:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%415:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - aten::stack:
          inputs: (%434:list{%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%241:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%241:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%1438:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%1438:tuple{None:NoneType,)
          duration: -1
        - aten::stack:
          inputs: (%430:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%433:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%433:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::grad call:
          inputs: (%1439:list{NotSurpot:FunctionalTensor}, %1440:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %1441:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%1439:list{NotSurpot:FunctionalTensor},%1440:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%1441:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%1440:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%1440:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%1440:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%1442:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%1443:tuple{NotSurpot:FunctionalTensor}, %1442:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%1443:tuple{NotSurpot:FunctionalTensor},%1442:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%1443:tuple{NotSurpot:FunctionalTensor}, %1442:tuple{NotSurpot:FunctionalTensor}, False:bool, %1444:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%1445:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%1443:tuple{NotSurpot:FunctionalTensor}, %1446:tuple{%1445:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %1447:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%1443:tuple{NotSurpot:FunctionalTensor},%1446:tuple{%1445:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%1447:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%1443:tuple{NotSurpot:FunctionalTensor}, %1446:tuple{%1445:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %1447:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%1448:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%1439:list{NotSurpot:FunctionalTensor}, %1440:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %1447:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %1443:tuple{NotSurpot:FunctionalTensor}, %1449:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %1450:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %1445:tuple{NotSurpot:FunctionalTensor}, %1448:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%1448:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%1450:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%1450:tuple{None:NoneType,)
          duration: -1
        - aten::stack:
          inputs: (%416:list{%262:<1024xf32>{1}}, 0:int, out=%417:<1x1024xf32>{1024, 1})
          outputs: (%417:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%413:<1024xf32>{1}}, 0:int, out=%203:<1x1024xf32>{1024, 1})
          outputs: (%203:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%429:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%429:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%230:list{%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%430:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%430:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%429:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%436:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%437:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%437:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%424:list{%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%427:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%427:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%427:list{%410:<1024xf32>{1}}, 0:int, out=%430:<1x1024xf32>{1024, 1})
          outputs: (%430:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%427:list{%409:<1024xf32>{1}}, 0:int, out=%431:<1x1024xf32>{1024, 1})
          outputs: (%431:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%428:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%428:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%408:<1024xf32>{1}}, 0:int, out=%428:<1x1024xf32>{1024, 1})
          outputs: (%428:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55dc2e5770_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55dc2e5770_:_InferenceMode)
          duration: -1
        - aten::stack:
          inputs: (%415:list{%410:<1024xf32>{1}}, 0:int, out=%429:<1x1024xf32>{1024, 1})
          outputs: (%429:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%428:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%431:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%431:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
90489 2024-12-10 17:48:13.184987 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n0,rank6)
        - aten::stack:
          inputs: (%426:list{%409:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%429:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%429:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%1460:tuple{%1458:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1456:tuple{%1458:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1456:tuple{%1458:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
90559 2024-12-10 17:48:13.198532 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n2,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1462:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1462:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1462:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1462:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%1465:tuple{%1462:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%1465:tuple{%1462:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%1465:tuple{%1462:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1465:tuple{%1462:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%418:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%418:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%1470:tuple{%1462:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %1469:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%1470:tuple{%1462:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%1469:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1462:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %1465:tuple{%1462:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1467:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1462:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1467:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
90965 2024-12-10 17:48:13.250298 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n2,rank6)
90994 2024-12-10 17:48:13.252736 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n0,rank6)
        - aten::stack:
          inputs: (%230:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%711:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1474:tuple{%711:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%1474:tuple{%711:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
91067 2024-12-10 17:48:13.264017 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n25,rank6)
        - aten::mm:
          inputs: (%1489:<1024x5120xbf16>{5120, 1}, %1487:<5120x102400xbf16>{1, 5120})
          outputs: (%800:<1024x102400xbf16>{102400,1})
          duration: -1
91239 2024-12-10 17:48:13.282119 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n25,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1474:tuple{%711:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%1486:tuple{%1494:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%1498:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%1499:tuple{%724:<1024x1xf32>{1,1},%1500:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%409:<1024xf32>{1}}, 0:int, out=%417:<1x1024xf32>{1024, 1})
          outputs: (%417:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%724:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%724:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1501:list{%724:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%1502:tuple{%1503:list{%724:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%404:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%437:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%437:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::sub_:
          inputs: (%1498:<1024x1x102400xf32>{102400, 102400, 1}, %1504:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%1498:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%1497:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%1500:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%1497:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%1505:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%1500:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %1505:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%1506:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1507:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%1508:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%1508:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %1509:list{%1506:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1479:<i32>, False:bool)
          outputs: (%1508:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1512:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1510:<1024x102400xf32>{102400, 1}, %1503:list{%1512:<1024xCUSTOM_DATA_TYPE>{1}, %1511:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1513:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%1514:<1024x1xf32>{1, 1}, %1503:list{%1506:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1513:<i32>, False:bool)
          outputs: (%1514:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%1498:<1024x1x102400xf32>{102400, 102400, 1}, out=%1498:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%1498:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%1498:<1024x1x102400xf32>{102400, 102400, 1}, %1509:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%1515:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1514:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1514:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1509:list{%1514:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%1486:tuple{%1493:list{%1514:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1515:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1515:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1516:list{%1515:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%1517:tuple{%1518:list{%1515:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::stack:
          inputs: (%421:list{%413:<1024xf32>{1}}, 0:int, out=%438:<1x1024xf32>{1024, 1})
          outputs: (%438:<1x1024xf32>{1024,1})
          duration: -1
        - aten::log:
          inputs: (%1515:<1024x1xf32>{1, 1})
          outputs: (%1519:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1519:<1024x1xf32>{1, 1}, %1514:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%1520:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%1498:<1024x1x102400xf32>{102400, 102400, 1}, %1513:<1024x1x1xf32>{1, 1, 1})
          outputs: (%1498:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%1521:tuple{%1498:<1024x1x102400xf32>{102400, 102400, 1}, %1506:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %1511:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%1521:tuple{%1498:<1024x1x102400xf32>{102400,102400,1},%1506:<1024x1xCUSTOM_DATA_TYPE>{1,1},%1511:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%454:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device='cuda_6')_:dict)
          outputs: (%1523:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1523:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1523:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1523:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%451:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device='cuda_6')_:dict)
          outputs: (%1523:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%450:tuple{%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %237:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %436:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_245,_2569,_4500,_____,_4706,__410,_3507]],_device='cuda_6')_:dict)
          outputs: (%1523:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%484:<1024xf32>{1}, %436:<1024xf32>{1})
          outputs: (%1497:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%1497:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1491:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%436:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1525:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1491:<i32>, %1525:<i32>)
          outputs: (%457:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%457:<i32>)
          outputs: (%1513:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%1472:list{%1527:<1xf32>{1}}, 0:int)
          outputs: (%1528:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1528:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1528:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%428:list{%278:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%429:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%429:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%436:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%439:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%439:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%430:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%434:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%434:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%415:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%39:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%39:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%420:list{%412:<1024xf32>{1}}, 0:int, out=%435:<1x1024xf32>{1024, 1})
          outputs: (%435:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%413:<1024xf32>{1}}, 0:int, out=%233:<1x1024xf32>{1024, 1})
          outputs: (%233:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%428:list{%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%424:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%424:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%436:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::eq:
          inputs: (%438:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%436:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%412:<1024xf32>{1}, %420:list{%436:<1024xCUSTOM_DATA_TYPE>{1}}, %435:<i32>, False:bool)
          outputs: (%412:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%434:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}, %420:list{%434:<1024xCUSTOM_DATA_TYPE>{1}}, %439:<i32>, False:bool)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%438:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%434:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%438:<1024xCUSTOM_DATA_TYPE>{1}+1, %420:list{%434:<1024xCUSTOM_DATA_TYPE>{1}}, %440:<i32>, False:bool)
          outputs: (%438:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - aten::stack:
          inputs: (%230:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%380:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%380:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%413:<1024xf32>{1}}, 0:int, out=%432:<1x1024xf32>{1024, 1})
          outputs: (%432:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%442:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%442:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%431:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%433:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%433:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%443:list{%438:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%402:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%402:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%430:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%433:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%433:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%412:<1024xf32>{1}}, 0:int, out=%434:<1x1024xf32>{1024, 1})
          outputs: (%434:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%169:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%438:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%438:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%435:list{%412:<1024xf32>{1}}, 0:int, out=%244:<1x1024xf32>{1024, 1})
          outputs: (%244:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%432:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%435:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%435:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%437:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%274:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%274:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - c10d::allreduce_:
          inputs: (%1529:list{%1528:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%1530:tuple{%1531:list{%1528:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%1528:<1xf32>{1}, 8:int)
          outputs: (%1532:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%457:<i32>, 1:int)
          outputs: (%1533:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%1533:<i32>, 1:int)
          outputs: (%1533:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%1534:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%457:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%457:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%457:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %457:<1xf32>{1})
          outputs: (%1535:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%1535:<1xf32>{1}, 1:int)
          outputs: (%1536:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%337:<i32>, 0:int, alpha=1:int)
          outputs: (%337:<i32>)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%1533:<i32>)
          outputs: (torch.2_3_0.ChainedOptimizer(%1533:<i32>))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %1533:<i32>)
          outputs: (%1537:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%1538:<1xf32>{1}, %164:<1xf32>{1})
          outputs: (%1539:<1xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%1539:<1xf32>{1}, dtype=None:NoneType)
          outputs: (%1540:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1540:<i32>, 1:int)
          outputs: (%1541:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1541:<i32>, 1:int)
          outputs: (%1542:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1542:<i32>, %1525:<i32>)
          outputs: (%1541:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1544:<1024xf32>{0}, %436:<1024xf32>{1})
          outputs: (%1535:<1024xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1548:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::rsub:
          inputs: (%1370:<1024xf32>{1}, 1_0:float, 1:int)
          outputs: (%432:<1024xf32>{1})
          duration: -1
        - aten::index:
          inputs: (%1547:<1024x102400xf32>{102400, 1}, %1545:list{%1548:<1024xCUSTOM_DATA_TYPE>{1}, %1511:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<1024xf32>{1})
          duration: -1
        - aten::sub_:
          inputs: (%1540:<1024xf32>{1}, %432:<1024xf32>{1}, alpha=1:int)
          outputs: (%1540:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%1547:<1024x102400xf32>{102400, 1}, %1543:list{%1548:<1024xCUSTOM_DATA_TYPE>{1}, %1511:<1024xCUSTOM_DATA_TYPE>{1}}, %1540:<1024xf32>{1}, False:bool)
          outputs: (%1547:<1024x102400xf32>{102400,1})
          duration: -1
        - aten::mul_:
          inputs: (%1498:<1024x1x102400xf32>{102400, 102400, 1}, %1550:<1024x1x1xf32>{1, 1024, 1})
          outputs: (%1498:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
94300 2024-12-10 17:48:17.240558 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.output_layer,n25,rank6)
        - aten::mm:
          inputs: (%1506:<1024x102400xbf16>{102400, 1}, %152:<102400x5120xbf16>{5120, 1})
          outputs: (%1508:<1024x5120xbf16>{5120,1})
          duration: -1
94576 2024-12-10 17:48:17.256399 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.output_layer,n25,rank6)
94582 2024-12-10 17:48:17.256470 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Backward:deepseek_v2.decoder,n0,rank6)
94583 2024-12-10 17:48:17.256517 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Backward:deepseek_v2.decoder,n0,rank6)
94587 2024-12-10 17:48:17.256572 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Backward:deepseek_v2.decoder.final_layernorm,n2,rank6)
        - aten::add_:
          inputs: (%175:<5120xf32>{1}+524288000, %1483:<5120xbf16>{1}, alpha=1:int)
          outputs: (%175:<5120xf32>{1}+524288000)
          duration: -1
94708 2024-12-10 17:48:17.258868 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Backward:deepseek_v2.decoder.final_layernorm,n2,rank6)
94713 2024-12-10 17:48:17.258936 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Backward:deepseek_v2.decoder.layers.0,n0,rank6)
94717 2024-12-10 17:48:17.258977 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Backward:deepseek_v2.decoder.layers.0,n0,rank6)
        - profiler::_record_function_enter_new:
          inputs: (compile_f___locals__bw_compiler_(dynamo_timed):str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
94773 2024-12-10 17:48:17.260181 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Backward:deepseek_v2.decoder.layers.0.mlp,n0,rank6)
94794 2024-12-10 17:48:17.260589 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n20,rank6)
94800 2024-12-10 17:48:17.260650 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n21,rank6)
        - aten::mm:
          inputs: (%1555:<1024x5120xbf16>{5120, 1}, %67:<5120x3072xbf16>{3072, 1})
          outputs: (%1514:<1024x3072xbf16>{3072,1})
          duration: -1
94916 2024-12-10 17:48:17.263089 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n21,rank6)
        - aten::mul:
          inputs: (%1388:<1024x1x3072xbf16>{3072, 3072, 1}, %1363:<1024x1x3072xbf16>{3072, 3072, 1})
          outputs: (%1555:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%1388:<1024x1x3072xbf16>{3072, 3072, 1}, %429:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%1558:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1558:<1024x1x3072xbf16>{3072, 3072, 1}, %1372:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%1363:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::cat:
          inputs: (%1543:list{%1363:<1024x1x3072xbf16>{3072, 3072, 1}, %1555:<1024x1x3072xbf16>{3072, 3072, 1}}, 2:int)
          outputs: (%1525:<1024x1x6144xbf16>{6144,6144,1})
          duration: -1
94966 2024-12-10 17:48:17.264386 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n24,rank6)
        - aten::mm:
          inputs: (%1512:<1024x6144xbf16>{6144, 1}, %70:<6144x5120xbf16>{5120, 1})
          outputs: (%1388:<1024x5120xbf16>{5120,1})
          duration: -1
95020 2024-12-10 17:48:17.267046 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n24,rank6)
95021 2024-12-10 17:48:17.267107 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n20,rank6)
        - c10d::_allgather_base_:
          inputs: (%1370:<8192x5120xbf16>{5120, 1}, %1525:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%1559:tuple{%1370:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gather:
          inputs: (%1370:<8192x5120xbf16>{5120, 1}, 0:int, %771:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%1388:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%1388:<6352x5120xbf16>{5120, 1}, %729:<6352x5120xbf16>{5120, 1})
          outputs: (%1370:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%1388:<6352x5120xbf16>{5120, 1}, %784:<6352x1xbf16>{1, 1})
          outputs: (%1555:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::sum:
          inputs: (%1370:<6352x5120xbf16>{5120, 1}, %1554:list{1:int}, True:bool, dtype=None:NoneType)
          outputs: (%1560:<6352x1xbf16>{1,1})
          duration: -1
        - aten::gather:
          inputs: (%1555:<6352x5120xbf16>{5120, 1}, 0:int, %776:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%332:<6352x5120xbf16>{5120,1})
          duration: -1
95136 2024-12-10 17:48:17.271362 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts,n0,rank6)
        - aten::copy_:
          inputs: (%1388:<6352x5120xbf16>{5120, 1}, %332:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1370:<343x5120xbf16>{5120, 1}+30766080, %1550:<343x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1370:<343x5120xbf16>{5120,1}+30766080)
          duration: -1
95237 2024-12-10 17:48:17.273569 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n19,rank6)
95244 2024-12-10 17:48:17.273629 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n20,rank6)
        - aten::mm:
          inputs: (%432:<343x5120xbf16>{5120, 1}, %145:<5120x1536xbf16>{1536, 1})
          outputs: (%1562:<343x1536xbf16>{1536,1})
          duration: -1
95314 2024-12-10 17:48:17.274837 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n20,rank6)
        - aten::mul:
          inputs: (%1562:<343x1536xbf16>{1536, 1}, %1384:<343x1536xbf16>{1536, 1})
          outputs: (%1385:<343x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1562:<343x1536xbf16>{1536, 1}, %1378:<343x1536xbf16>{3072, 1}+1536)
          outputs: (%1564:<343x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1564:<343x1536xbf16>{1536, 1}, %1383:<343x1536xbf16>{3072, 1})
          outputs: (%1562:<343x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1345:list{%1562:<343x1536xbf16>{1536, 1}, %1385:<343x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1378:<343x3072xbf16>{3072,1})
          duration: -1
95388 2024-12-10 17:48:17.275968 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n23,rank6)
        - aten::mm:
          inputs: (%1378:<343x3072xbf16>{3072, 1}, %147:<3072x5120xbf16>{5120, 1})
          outputs: (%1383:<343x5120xbf16>{5120,1})
          duration: -1
95453 2024-12-10 17:48:17.277184 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n23,rank6)
95460 2024-12-10 17:48:17.277243 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n19,rank6)
        - aten::copy_:
          inputs: (%980:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%980:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1549:<213x5120xbf16>{5120, 1}+29675520, %1565:<213x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1549:<213x5120xbf16>{5120,1}+29675520)
          duration: -1
95586 2024-12-10 17:48:17.279858 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n18,rank6)
95593 2024-12-10 17:48:17.279922 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n19,rank6)
        - aten::mm:
          inputs: (%1372:<213x5120xbf16>{5120, 1}, %141:<5120x1536xbf16>{1536, 1})
          outputs: (%1566:<213x1536xbf16>{1536,1})
          duration: -1
95664 2024-12-10 17:48:17.281103 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n19,rank6)
        - aten::mul:
          inputs: (%1566:<213x1536xbf16>{1536, 1}, %1357:<213x1536xbf16>{1536, 1})
          outputs: (%1567:<213x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1566:<213x1536xbf16>{1536, 1}, %1351:<213x1536xbf16>{3072, 1}+1536)
          outputs: (%1565:<213x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1565:<213x1536xbf16>{1536, 1}, %853:<213x1536xbf16>{3072, 1})
          outputs: (%1351:<213x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1543:list{%1351:<213x1536xbf16>{1536, 1}, %1567:<213x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1388:<213x3072xbf16>{3072,1})
          duration: -1
95740 2024-12-10 17:48:17.282228 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n22,rank6)
        - aten::mm:
          inputs: (%1388:<213x3072xbf16>{3072, 1}, %143:<3072x5120xbf16>{5120, 1})
          outputs: (%1540:<213x5120xbf16>{5120,1})
          duration: -1
95804 2024-12-10 17:48:17.283384 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n22,rank6)
95810 2024-12-10 17:48:17.283443 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n18,rank6)
        - aten::add:
          inputs: (%1378:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1351:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1378:<6352x5120xbf16>{5120, 1}, %980:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1378:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<85x5120xbf16>{5120, 1}+29240320, %1548:<85x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<85x5120xbf16>{5120,1}+29240320)
          duration: -1
95951 2024-12-10 17:48:17.286325 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n17,rank6)
95956 2024-12-10 17:48:17.286385 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n18,rank6)
        - aten::mm:
          inputs: (%1567:<85x5120xbf16>{5120, 1}, %137:<5120x1536xbf16>{1536, 1})
          outputs: (%432:<85x1536xbf16>{1536,1})
          duration: -1
96026 2024-12-10 17:48:17.287551 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n18,rank6)
        - aten::mul:
          inputs: (%432:<85x1536xbf16>{1536, 1}, %1306:<85x1536xbf16>{1536, 1})
          outputs: (%1568:<85x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%432:<85x1536xbf16>{1536, 1}, %1328:<85x1536xbf16>{3072, 1}+1536)
          outputs: (%1569:<85x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1569:<85x1536xbf16>{1536, 1}, %1214:<85x1536xbf16>{3072, 1})
          outputs: (%432:<85x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1345:list{%432:<85x1536xbf16>{1536, 1}, %1568:<85x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1214:<85x3072xbf16>{3072,1})
          duration: -1
96100 2024-12-10 17:48:17.288631 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n21,rank6)
        - aten::mm:
          inputs: (%1214:<85x3072xbf16>{3072, 1}, %139:<3072x5120xbf16>{5120, 1})
          outputs: (%1352:<85x5120xbf16>{5120,1})
          duration: -1
96165 2024-12-10 17:48:17.290007 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n21,rank6)
96171 2024-12-10 17:48:17.290076 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n17,rank6)
        - aten::add:
          inputs: (%1351:<6352x5120xbf16>{5120, 1}, %980:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1388:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1378:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<337x5120xbf16>{5120, 1}+27514880, %1571:<337x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1570:<337x5120xbf16>{5120,1}+27514880)
          duration: -1
96316 2024-12-10 17:48:17.292978 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n16,rank6)
96320 2024-12-10 17:48:17.293058 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n17,rank6)
        - aten::mm:
          inputs: (%1366:<337x5120xbf16>{5120, 1}, %133:<5120x1536xbf16>{1536, 1})
          outputs: (%1378:<337x1536xbf16>{1536,1})
          duration: -1
96385 2024-12-10 17:48:17.294241 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n17,rank6)
        - aten::mul:
          inputs: (%1378:<337x1536xbf16>{1536, 1}, %1300:<337x1536xbf16>{1536, 1})
          outputs: (%1572:<337x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1378:<337x1536xbf16>{1536, 1}, %1299:<337x1536xbf16>{3072, 1}+1536)
          outputs: (%1301:<337x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1301:<337x1536xbf16>{1536, 1}, %1298:<337x1536xbf16>{3072, 1})
          outputs: (%1378:<337x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1556:list{%1378:<337x1536xbf16>{1536, 1}, %1572:<337x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1570:<337x3072xbf16>{3072,1})
          duration: -1
96452 2024-12-10 17:48:17.295324 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n20,rank6)
        - aten::mm:
          inputs: (%1570:<337x3072xbf16>{3072, 1}, %135:<3072x5120xbf16>{5120, 1})
          outputs: (%1225:<337x5120xbf16>{5120,1})
          duration: -1
96519 2024-12-10 17:48:17.296527 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n20,rank6)
96523 2024-12-10 17:48:17.296591 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n16,rank6)
        - aten::add:
          inputs: (%1388:<6352x5120xbf16>{5120, 1}, %1570:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1289:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1497:<6352x5120xbf16>{5120, 1}, %432:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1497:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1298:<195x5120xbf16>{5120, 1}+26516480, %1571:<195x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1298:<195x5120xbf16>{5120,1}+26516480)
          duration: -1
96663 2024-12-10 17:48:17.299506 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n15,rank6)
96668 2024-12-10 17:48:17.299566 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n16,rank6)
        - aten::mm:
          inputs: (%1301:<195x5120xbf16>{5120, 1}, %129:<5120x1536xbf16>{1536, 1})
          outputs: (%980:<195x1536xbf16>{1536,1})
          duration: -1
96724 2024-12-10 17:48:17.300738 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n16,rank6)
        - aten::mul:
          inputs: (%980:<195x1536xbf16>{1536, 1}, %1271:<195x1536xbf16>{1536, 1})
          outputs: (%1388:<195x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%980:<195x1536xbf16>{1536, 1}, %1220:<195x1536xbf16>{3072, 1}+1536)
          outputs: (%1262:<195x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1262:<195x1536xbf16>{1536, 1}, %1270:<195x1536xbf16>{3072, 1})
          outputs: (%1298:<195x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1561:list{%1298:<195x1536xbf16>{1536, 1}, %1388:<195x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%432:<195x3072xbf16>{3072,1})
          duration: -1
96788 2024-12-10 17:48:17.301869 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n19,rank6)
        - aten::mm:
          inputs: (%432:<195x3072xbf16>{3072, 1}, %131:<3072x5120xbf16>{5120, 1})
          outputs: (%436:<195x5120xbf16>{5120,1})
          duration: -1
96841 2024-12-10 17:48:17.303056 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n19,rank6)
96845 2024-12-10 17:48:17.303116 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n15,rank6)
        - aten::add:
          inputs: (%1289:<6352x5120xbf16>{5120, 1}, %1270:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1574:<6352x5120xbf16>{5120, 1}, %1497:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1574:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<688x5120xbf16>{5120, 1}+22993920, %1271:<688x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<688x5120xbf16>{5120,1}+22993920)
          duration: -1
96961 2024-12-10 17:48:17.305985 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n14,rank6)
96964 2024-12-10 17:48:17.306047 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n15,rank6)
        - aten::mm:
          inputs: (%1259:<688x5120xbf16>{5120, 1}, %125:<5120x1536xbf16>{1536, 1})
          outputs: (%1570:<688x1536xbf16>{1536,1})
          duration: -1
97022 2024-12-10 17:48:17.307234 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n15,rank6)
        - aten::mul:
          inputs: (%1570:<688x1536xbf16>{1536, 1}, %1230:<688x1536xbf16>{1536, 1})
          outputs: (%1298:<688x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1570:<688x1536xbf16>{1536, 1}, %1185:<688x1536xbf16>{3072, 1}+1536)
          outputs: (%1263:<688x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1263:<688x1536xbf16>{1536, 1}, %1238:<688x1536xbf16>{3072, 1})
          outputs: (%1497:<688x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1554:list{%1497:<688x1536xbf16>{1536, 1}, %1298:<688x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1388:<688x3072xbf16>{3072,1})
          duration: -1
97082 2024-12-10 17:48:17.308327 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n18,rank6)
        - aten::mm:
          inputs: (%1388:<688x3072xbf16>{3072, 1}, %127:<3072x5120xbf16>{5120, 1})
          outputs: (%1570:<688x5120xbf16>{5120,1})
          duration: -1
97142 2024-12-10 17:48:17.309647 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n18,rank6)
97144 2024-12-10 17:48:17.309714 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n14,rank6)
        - aten::add:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%436:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1574:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<351x5120xbf16>{5120, 1}+21196800, %1263:<351x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<351x5120xbf16>{5120,1}+21196800)
          duration: -1
97278 2024-12-10 17:48:17.312589 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n13,rank6)
97280 2024-12-10 17:48:17.312662 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n14,rank6)
        - aten::mm:
          inputs: (%1497:<351x5120xbf16>{5120, 1}, %122:<5120x1536xbf16>{1536, 1})
          outputs: (%1270:<351x1536xbf16>{1536,1})
          duration: -1
97352 2024-12-10 17:48:17.313866 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n14,rank6)
        - aten::mul:
          inputs: (%1270:<351x1536xbf16>{1536, 1}, %1208:<351x1536xbf16>{1536, 1})
          outputs: (%1238:<351x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1270:<351x1536xbf16>{1536, 1}, %1207:<351x1536xbf16>{3072, 1}+1536)
          outputs: (%1573:<351x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1573:<351x1536xbf16>{1536, 1}, %1206:<351x1536xbf16>{3072, 1})
          outputs: (%1270:<351x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1543:list{%1270:<351x1536xbf16>{1536, 1}, %1238:<351x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1352:<351x3072xbf16>{3072,1})
          duration: -1
97418 2024-12-10 17:48:17.314972 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n17,rank6)
        - aten::mm:
          inputs: (%1352:<351x3072xbf16>{3072, 1}, %123:<3072x5120xbf16>{5120, 1})
          outputs: (%1573:<351x5120xbf16>{5120,1})
          duration: -1
97490 2024-12-10 17:48:17.316179 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n17,rank6)
97493 2024-12-10 17:48:17.316240 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n13,rank6)
        - aten::add:
          inputs: (%436:<6352x5120xbf16>{5120, 1}, %1570:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1575:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<6352x5120xbf16>{5120, 1}, %432:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<254x5120xbf16>{5120, 1}+19896320, %1298:<254x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1570:<254x5120xbf16>{5120,1}+19896320)
          duration: -1
97640 2024-12-10 17:48:17.319100 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n12,rank6)
97643 2024-12-10 17:48:17.319160 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n13,rank6)
        - aten::mm:
          inputs: (%1574:<254x5120xbf16>{5120, 1}, %80:<5120x1536xbf16>{1536, 1})
          outputs: (%1207:<254x1536xbf16>{1536,1})
          duration: -1
97715 2024-12-10 17:48:17.320327 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n13,rank6)
        - aten::mul:
          inputs: (%1207:<254x1536xbf16>{1536, 1}, %1178:<254x1536xbf16>{1536, 1})
          outputs: (%980:<254x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1207:<254x1536xbf16>{1536, 1}, %1177:<254x1536xbf16>{3072, 1}+1536)
          outputs: (%1179:<254x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1179:<254x1536xbf16>{1536, 1}, %971:<254x1536xbf16>{3072, 1})
          outputs: (%1207:<254x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1345:list{%1207:<254x1536xbf16>{1536, 1}, %980:<254x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1497:<254x3072xbf16>{3072,1})
          duration: -1
97783 2024-12-10 17:48:17.321449 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n16,rank6)
        - aten::mm:
          inputs: (%1497:<254x3072xbf16>{3072, 1}, %120:<3072x5120xbf16>{5120, 1})
          outputs: (%1352:<254x5120xbf16>{5120,1})
          duration: -1
97852 2024-12-10 17:48:17.322648 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n16,rank6)
97858 2024-12-10 17:48:17.322722 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n12,rank6)
        - aten::add:
          inputs: (%1575:<6352x5120xbf16>{5120, 1}, %1497:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1207:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1574:<282x5120xbf16>{5120, 1}+18452480, %1201:<282x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1574:<282x5120xbf16>{5120,1}+18452480)
          duration: -1
98010 2024-12-10 17:48:17.325704 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n11,rank6)
98016 2024-12-10 17:48:17.325766 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n12,rank6)
        - aten::mm:
          inputs: (%1497:<282x5120xbf16>{5120, 1}, %114:<5120x1536xbf16>{1536, 1})
          outputs: (%436:<282x1536xbf16>{1536,1})
          duration: -1
98088 2024-12-10 17:48:17.326949 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n12,rank6)
        - aten::mul:
          inputs: (%436:<282x1536xbf16>{1536, 1}, %1148:<282x1536xbf16>{1536, 1})
          outputs: (%1388:<282x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%436:<282x1536xbf16>{1536, 1}, %1147:<282x1536xbf16>{3072, 1}+1536)
          outputs: (%1149:<282x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1149:<282x1536xbf16>{1536, 1}, %1146:<282x1536xbf16>{3072, 1})
          outputs: (%1574:<282x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1556:list{%1574:<282x1536xbf16>{1536, 1}, %1388:<282x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1146:<282x3072xbf16>{3072,1})
          duration: -1
98146 2024-12-10 17:48:17.328035 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n15,rank6)
        - aten::mm:
          inputs: (%1146:<282x3072xbf16>{3072, 1}, %116:<3072x5120xbf16>{5120, 1})
          outputs: (%1574:<282x5120xbf16>{5120,1})
          duration: -1
98209 2024-12-10 17:48:17.329230 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n15,rank6)
98213 2024-12-10 17:48:17.329290 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n11,rank6)
        - aten::add:
          inputs: (%1207:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1270:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<6352x5120xbf16>{5120, 1}, %432:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1570:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1352:<322x5120xbf16>{5120, 1}+16803840, %1298:<322x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1352:<322x5120xbf16>{5120,1}+16803840)
          duration: -1
98345 2024-12-10 17:48:17.332181 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n10,rank6)
98349 2024-12-10 17:48:17.332243 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n11,rank6)
        - aten::mm:
          inputs: (%1216:<322x5120xbf16>{5120, 1}, %110:<5120x1536xbf16>{1536, 1})
          outputs: (%432:<322x1536xbf16>{1536,1})
          duration: -1
98411 2024-12-10 17:48:17.333474 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n11,rank6)
        - aten::mul:
          inputs: (%432:<322x1536xbf16>{1536, 1}, %1115:<322x1536xbf16>{1536, 1})
          outputs: (%1352:<322x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%432:<322x1536xbf16>{1536, 1}, %1114:<322x1536xbf16>{3072, 1}+1536)
          outputs: (%1298:<322x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1298:<322x1536xbf16>{1536, 1}, %1113:<322x1536xbf16>{3072, 1})
          outputs: (%980:<322x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1561:list{%980:<322x1536xbf16>{1536, 1}, %1352:<322x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1388:<322x3072xbf16>{3072,1})
          duration: -1
98472 2024-12-10 17:48:17.334613 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n14,rank6)
        - aten::mm:
          inputs: (%1388:<322x3072xbf16>{3072, 1}, %112:<3072x5120xbf16>{5120, 1})
          outputs: (%980:<322x5120xbf16>{5120,1})
          duration: -1
98535 2024-12-10 17:48:17.335835 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n14,rank6)
98539 2024-12-10 17:48:17.335897 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n10,rank6)
        - aten::add:
          inputs: (%1270:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1352:<6352x5120xbf16>{5120, 1}, %1570:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1352:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%436:<427x5120xbf16>{5120, 1}+14617600, %1216:<427x5120xbf16>{5120, 1}, False:bool)
          outputs: (%436:<427x5120xbf16>{5120,1}+14617600)
          duration: -1
98672 2024-12-10 17:48:17.338815 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n9,rank6)
98676 2024-12-10 17:48:17.338878 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n10,rank6)
        - aten::mm:
          inputs: (%1574:<427x5120xbf16>{5120, 1}, %106:<5120x1536xbf16>{1536, 1})
          outputs: (%1497:<427x1536xbf16>{1536,1})
          duration: -1
98740 2024-12-10 17:48:17.340075 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n10,rank6)
        - aten::mul:
          inputs: (%1497:<427x1536xbf16>{1536, 1}, %1086:<427x1536xbf16>{1536, 1})
          outputs: (%1149:<427x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1497:<427x1536xbf16>{1536, 1}, %1085:<427x1536xbf16>{3072, 1}+1536)
          outputs: (%1089:<427x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1089:<427x1536xbf16>{1536, 1}, %1084:<427x1536xbf16>{3072, 1})
          outputs: (%1570:<427x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1554:list{%1570:<427x1536xbf16>{1536, 1}, %1149:<427x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1388:<427x3072xbf16>{3072,1})
          duration: -1
98808 2024-12-10 17:48:17.341189 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n13,rank6)
        - aten::mm:
          inputs: (%1388:<427x3072xbf16>{3072, 1}, %108:<3072x5120xbf16>{5120, 1})
          outputs: (%1085:<427x5120xbf16>{5120,1})
          duration: -1
98878 2024-12-10 17:48:17.342383 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n13,rank6)
98882 2024-12-10 17:48:17.342450 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n9,rank6)
        - aten::add:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1570:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1574:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%980:<6352x5120xbf16>{5120, 1}, %1352:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%980:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<644x5120xbf16>{5120, 1}+11320320, %1065:<644x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<644x5120xbf16>{5120,1}+11320320)
          duration: -1
99033 2024-12-10 17:48:17.345364 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n8,rank6)
99038 2024-12-10 17:48:17.345429 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n9,rank6)
        - aten::mm:
          inputs: (%1570:<644x5120xbf16>{5120, 1}, %102:<5120x1536xbf16>{1536, 1})
          outputs: (%1352:<644x1536xbf16>{1536,1})
          duration: -1
99107 2024-12-10 17:48:17.346620 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n9,rank6)
        - aten::mul:
          inputs: (%1352:<644x1536xbf16>{1536, 1}, %954:<644x1536xbf16>{1536, 1})
          outputs: (%1089:<644x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1352:<644x1536xbf16>{1536, 1}, %1056:<644x1536xbf16>{3072, 1}+1536)
          outputs: (%1185:<644x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1185:<644x1536xbf16>{1536, 1}, %1055:<644x1536xbf16>{3072, 1})
          outputs: (%436:<644x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1543:list{%436:<644x1536xbf16>{1536, 1}, %1089:<644x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1388:<644x3072xbf16>{3072,1})
          duration: -1
99176 2024-12-10 17:48:17.347698 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n12,rank6)
        - aten::mm:
          inputs: (%1388:<644x3072xbf16>{3072, 1}, %104:<3072x5120xbf16>{5120, 1})
          outputs: (%1207:<644x5120xbf16>{5120,1})
          duration: -1
99246 2024-12-10 17:48:17.348970 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n12,rank6)
99249 2024-12-10 17:48:17.349040 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n8,rank6)
        - aten::add:
          inputs: (%1574:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1558:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1270:<6352x5120xbf16>{5120, 1}, %980:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1270:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<446x5120xbf16>{5120, 1}+9036800, %1089:<446x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1570:<446x5120xbf16>{5120,1}+9036800)
          duration: -1
99401 2024-12-10 17:48:17.351896 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n7,rank6)
99405 2024-12-10 17:48:17.351956 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n8,rank6)
        - aten::mm:
          inputs: (%1352:<446x5120xbf16>{5120, 1}, %99:<5120x1536xbf16>{1536, 1})
          outputs: (%980:<446x1536xbf16>{1536,1})
          duration: -1
99473 2024-12-10 17:48:17.353137 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n8,rank6)
        - aten::mul:
          inputs: (%980:<446x1536xbf16>{1536, 1}, %1027:<446x1536xbf16>{1536, 1})
          outputs: (%1570:<446x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%980:<446x1536xbf16>{1536, 1}, %1026:<446x1536xbf16>{3072, 1}+1536)
          outputs: (%1089:<446x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1089:<446x1536xbf16>{1536, 1}, %762:<446x1536xbf16>{3072, 1})
          outputs: (%432:<446x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1345:list{%432:<446x1536xbf16>{1536, 1}, %1570:<446x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1512:<446x3072xbf16>{3072,1})
          duration: -1
99541 2024-12-10 17:48:17.354231 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n11,rank6)
        - aten::mm:
          inputs: (%1512:<446x3072xbf16>{3072, 1}, %100:<3072x5120xbf16>{5120, 1})
          outputs: (%1570:<446x5120xbf16>{5120,1})
          duration: -1
99611 2024-12-10 17:48:17.355434 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n11,rank6)
99615 2024-12-10 17:48:17.355498 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n7,rank6)
        - aten::add:
          inputs: (%1558:<6352x5120xbf16>{5120, 1}, %980:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1497:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1207:<6352x5120xbf16>{5120, 1}, %1270:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1207:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<157x5120xbf16>{5120, 1}+8232960, %432:<157x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<157x5120xbf16>{5120,1}+8232960)
          duration: -1
99770 2024-12-10 17:48:17.358409 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n6,rank6)
99773 2024-12-10 17:48:17.358473 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n7,rank6)
        - aten::mm:
          inputs: (%1512:<157x5120xbf16>{5120, 1}, %95:<5120x1536xbf16>{1536, 1})
          outputs: (%1576:<157x1536xbf16>{1536,1})
          duration: -1
99844 2024-12-10 17:48:17.359670 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n7,rank6)
        - aten::mul:
          inputs: (%1576:<157x1536xbf16>{1536, 1}, %999:<157x1536xbf16>{1536, 1})
          outputs: (%432:<157x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1576:<157x1536xbf16>{1536, 1}, %998:<157x1536xbf16>{3072, 1}+1536)
          outputs: (%1055:<157x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1055:<157x1536xbf16>{1536, 1}, %997:<157x1536xbf16>{3072, 1})
          outputs: (%1574:<157x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1556:list{%1574:<157x1536xbf16>{1536, 1}, %432:<157x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1055:<157x3072xbf16>{3072,1})
          duration: -1
99911 2024-12-10 17:48:17.360771 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n10,rank6)
        - aten::mm:
          inputs: (%1055:<157x3072xbf16>{3072, 1}, %97:<3072x5120xbf16>{5120, 1})
          outputs: (%1576:<157x5120xbf16>{5120,1})
          duration: -1
99979 2024-12-10 17:48:17.361935 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n10,rank6)
99983 2024-12-10 17:48:17.361996 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n6,rank6)
        - aten::add:
          inputs: (%1497:<6352x5120xbf16>{5120, 1}, %1388:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%987:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%762:<6352x5120xbf16>{5120, 1}, %1207:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%762:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1055:<106x5120xbf16>{5120, 1}+7690240, %998:<106x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1055:<106x5120xbf16>{5120,1}+7690240)
          duration: -1
100136 2024-12-10 17:48:17.364907 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n5,rank6)
100141 2024-12-10 17:48:17.364968 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n6,rank6)
        - aten::mm:
          inputs: (%1270:<106x5120xbf16>{5120, 1}, %91:<5120x1536xbf16>{1536, 1})
          outputs: (%1207:<106x1536xbf16>{1536,1})
          duration: -1
100209 2024-12-10 17:48:17.366146 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n6,rank6)
        - aten::mul:
          inputs: (%1207:<106x1536xbf16>{1536, 1}, %964:<106x1536xbf16>{1536, 1})
          outputs: (%1558:<106x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1207:<106x1536xbf16>{1536, 1}, %963:<106x1536xbf16>{3072, 1}+1536)
          outputs: (%980:<106x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%980:<106x1536xbf16>{1536, 1}, %962:<106x1536xbf16>{3072, 1})
          outputs: (%1207:<106x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1561:list{%1207:<106x1536xbf16>{1536, 1}, %1558:<106x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%962:<106x3072xbf16>{3072,1})
          duration: -1
100278 2024-12-10 17:48:17.367265 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n9,rank6)
        - aten::mm:
          inputs: (%962:<106x3072xbf16>{3072, 1}, %93:<3072x5120xbf16>{5120, 1})
          outputs: (%432:<106x5120xbf16>{5120,1})
          duration: -1
100347 2024-12-10 17:48:17.368472 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n9,rank6)
100352 2024-12-10 17:48:17.368538 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n5,rank6)
        - aten::add:
          inputs: (%987:<6352x5120xbf16>{5120, 1}, %1558:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%980:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1270:<6352x5120xbf16>{5120, 1}, %762:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1270:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%999:<132x5120xbf16>{5120, 1}+7014400, %987:<132x5120xbf16>{5120, 1}, False:bool)
          outputs: (%999:<132x5120xbf16>{5120,1}+7014400)
          duration: -1
100508 2024-12-10 17:48:17.371481 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n4,rank6)
100512 2024-12-10 17:48:17.371541 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n5,rank6)
        - aten::mm:
          inputs: (%1576:<132x5120xbf16>{5120, 1}, %88:<5120x1536xbf16>{1536, 1})
          outputs: (%987:<132x1536xbf16>{1536,1})
          duration: -1
100582 2024-12-10 17:48:17.372709 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n5,rank6)
        - aten::mul:
          inputs: (%987:<132x1536xbf16>{1536, 1}, %933:<132x1536xbf16>{1536, 1})
          outputs: (%1558:<132x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%987:<132x1536xbf16>{1536, 1}, %894:<132x1536xbf16>{3072, 1}+1536)
          outputs: (%1512:<132x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1512:<132x1536xbf16>{1536, 1}, %932:<132x1536xbf16>{3072, 1})
          outputs: (%999:<132x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1554:list{%999:<132x1536xbf16>{1536, 1}, %1558:<132x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1512:<132x3072xbf16>{3072,1})
          duration: -1
100647 2024-12-10 17:48:17.373800 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n8,rank6)
        - aten::mm:
          inputs: (%1512:<132x3072xbf16>{3072, 1}, %89:<3072x5120xbf16>{5120, 1})
          outputs: (%987:<132x5120xbf16>{5120,1})
          duration: -1
100712 2024-12-10 17:48:17.374978 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n8,rank6)
100717 2024-12-10 17:48:17.375037 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n4,rank6)
        - aten::add:
          inputs: (%980:<6352x5120xbf16>{5120, 1}, %1558:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%853:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1270:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1512:<529x5120xbf16>{5120, 1}+4305920, %987:<529x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1512:<529x5120xbf16>{5120,1}+4305920)
          duration: -1
100860 2024-12-10 17:48:17.377938 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n3,rank6)
100866 2024-12-10 17:48:17.377998 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n4,rank6)
        - aten::mm:
          inputs: (%1558:<529x5120xbf16>{5120, 1}, %84:<5120x1536xbf16>{1536, 1})
          outputs: (%987:<529x1536xbf16>{1536,1})
          duration: -1
100927 2024-12-10 17:48:17.379195 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n4,rank6)
        - aten::mul:
          inputs: (%987:<529x1536xbf16>{1536, 1}, %903:<529x1536xbf16>{1536, 1})
          outputs: (%962:<529x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%987:<529x1536xbf16>{1536, 1}, %902:<529x1536xbf16>{3072, 1}+1536)
          outputs: (%1576:<529x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1576:<529x1536xbf16>{1536, 1}, %822:<529x1536xbf16>{3072, 1})
          outputs: (%987:<529x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1543:list{%987:<529x1536xbf16>{1536, 1}, %962:<529x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1576:<529x3072xbf16>{3072,1})
          duration: -1
100989 2024-12-10 17:48:17.380282 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n7,rank6)
        - aten::mm:
          inputs: (%1576:<529x3072xbf16>{3072, 1}, %86:<3072x5120xbf16>{5120, 1})
          outputs: (%987:<529x5120xbf16>{5120,1})
          duration: -1
101061 2024-12-10 17:48:17.381497 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n7,rank6)
101066 2024-12-10 17:48:17.381557 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n3,rank6)
        - aten::add:
          inputs: (%853:<6352x5120xbf16>{5120, 1}, %962:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%987:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1270:<6352x5120xbf16>{5120, 1}, %432:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1270:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%853:<243x5120xbf16>{5120, 1}+3061760, %1576:<243x5120xbf16>{5120, 1}, False:bool)
          outputs: (%853:<243x5120xbf16>{5120,1}+3061760)
          duration: -1
101216 2024-12-10 17:48:17.384439 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n2,rank6)
101218 2024-12-10 17:48:17.384500 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n3,rank6)
        - aten::mm:
          inputs: (%962:<243x5120xbf16>{5120, 1}, %81:<5120x1536xbf16>{1536, 1})
          outputs: (%980:<243x1536xbf16>{1536,1})
          duration: -1
101285 2024-12-10 17:48:17.385677 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n3,rank6)
        - aten::mul:
          inputs: (%980:<243x1536xbf16>{1536, 1}, %870:<243x1536xbf16>{1536, 1})
          outputs: (%1558:<243x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%980:<243x1536xbf16>{1536, 1}, %869:<243x1536xbf16>{3072, 1}+1536)
          outputs: (%1576:<243x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1576:<243x1536xbf16>{1536, 1}, %868:<243x1536xbf16>{3072, 1})
          outputs: (%432:<243x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1345:list{%432:<243x1536xbf16>{1536, 1}, %1558:<243x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1512:<243x3072xbf16>{3072,1})
          duration: -1
101352 2024-12-10 17:48:17.386770 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n6,rank6)
        - aten::mm:
          inputs: (%1512:<243x3072xbf16>{3072, 1}, %82:<3072x5120xbf16>{5120, 1})
          outputs: (%869:<243x5120xbf16>{5120,1})
          duration: -1
101426 2024-12-10 17:48:17.387942 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n6,rank6)
101430 2024-12-10 17:48:17.388003 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n2,rank6)
        - aten::add:
          inputs: (%987:<6352x5120xbf16>{5120, 1}, %962:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%962:<6352x5120xbf16>{5120, 1}, %1270:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%962:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%987:<523x5120xbf16>{5120, 1}+384000, %1512:<523x5120xbf16>{5120, 1}, False:bool)
          outputs: (%987:<523x5120xbf16>{5120,1}+384000)
          duration: -1
101708 2024-12-10 17:48:17.393582 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n1,rank6)
101716 2024-12-10 17:48:17.393724 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n2,rank6)
        - aten::mm:
          inputs: (%1576:<523x5120xbf16>{5120, 1}, %77:<5120x1536xbf16>{1536, 1})
          outputs: (%1558:<523x1536xbf16>{1536,1})
          duration: -1
101863 2024-12-10 17:48:17.396406 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n2,rank6)
        - aten::mul:
          inputs: (%1558:<523x1536xbf16>{1536, 1}, %839:<523x1536xbf16>{1536, 1})
          outputs: (%1574:<523x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1558:<523x1536xbf16>{1536, 1}, %838:<523x1536xbf16>{3072, 1}+1536)
          outputs: (%987:<523x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%987:<523x1536xbf16>{1536, 1}, %832:<523x1536xbf16>{3072, 1})
          outputs: (%1512:<523x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1556:list{%1512:<523x1536xbf16>{1536, 1}, %1574:<523x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%987:<523x3072xbf16>{3072,1})
          duration: -1
101991 2024-12-10 17:48:17.398800 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n5,rank6)
        - aten::mm:
          inputs: (%987:<523x3072xbf16>{3072, 1}, %78:<3072x5120xbf16>{5120, 1})
          outputs: (%1574:<523x5120xbf16>{5120,1})
          duration: -1
102131 2024-12-10 17:48:17.401323 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n5,rank6)
102138 2024-12-10 17:48:17.401457 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n1,rank6)
        - aten::add:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %987:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%840:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%987:<6352x5120xbf16>{5120, 1}, %962:<6352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%987:<6352x5120xbf16>{5120,1})
          duration: -1
102317 2024-12-10 17:48:17.406083 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n0,rank6)
102319 2024-12-10 17:48:17.406144 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n1,rank6)
        - aten::mm:
          inputs: (%1558:<75x5120xbf16>{5120, 1}, %73:<5120x1536xbf16>{1536, 1})
          outputs: (%987:<75x1536xbf16>{1536,1})
          duration: -1
102364 2024-12-10 17:48:17.407313 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n1,rank6)
        - aten::mul:
          inputs: (%987:<75x1536xbf16>{1536, 1}, %807:<75x1536xbf16>{1536, 1})
          outputs: (%1577:<75x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%987:<75x1536xbf16>{1536, 1}, %806:<75x1536xbf16>{3072, 1}+1536)
          outputs: (%1576:<75x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1576:<75x1536xbf16>{1536, 1}, %758:<75x1536xbf16>{3072, 1})
          outputs: (%987:<75x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1561:list{%987:<75x1536xbf16>{1536, 1}, %1577:<75x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%962:<75x3072xbf16>{3072,1})
          duration: -1
102399 2024-12-10 17:48:17.408394 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n4,rank6)
        - aten::mm:
          inputs: (%962:<75x3072xbf16>{3072, 1}, %76:<3072x5120xbf16>{5120, 1})
          outputs: (%980:<75x5120xbf16>{5120,1})
          duration: -1
102436 2024-12-10 17:48:17.409579 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n4,rank6)
102439 2024-12-10 17:48:17.409640 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n0,rank6)
        - aten::add:
          inputs: (%840:<6352x5120xbf16>{5120, 1}, %987:<6352x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%962:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%1561:list{20:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cpu:device, pin_memory=None:NoneType)
          outputs: (%987:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
102473 2024-12-10 17:48:17.410843 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts,n0,rank6)
        - aten::new_zeros:
          inputs: (%962:<6352x5120xbf16>{5120, 1}, %1561:list{6352:int, 5120:int}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType)
          outputs: (%987:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%987:<6352x5120xbf16>{5120, 1}, 0:int, %776:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, %962:<6352x5120xbf16>{5120, 1})
          outputs: (%432:<6352x5120xbf16>{5120,1})
          duration: -1
        - aten::new_zeros:
          inputs: (%432:<6352x5120xbf16>{5120, 1}, %1554:list{8192:int, 5120:int}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType)
          outputs: (%1574:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%1574:<8192x5120xbf16>{5120, 1}, 0:int, %771:<6352x5120xCUSTOM_DATA_TYPE>{1, 0}, %432:<6352x5120xbf16>{5120, 1})
          outputs: (%962:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::masked_scatter:
          inputs: (%987:<8192x6xbf16>{6, 1}, %761:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %784:<6352xbf16>{1})
          outputs: (%840:<8192x6xbf16>{6,1})
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%987:<1024x6xbf16>{6, 1}, %840:<8192x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%1578:tuple{%987:<1024x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%1562:<1024x5120xbf16>{5120, 1}, %962:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%1559:tuple{%1562:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::add:
          inputs: (%1547:<1024x1x5120xbf16>{5120, 5120, 1}, %840:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%962:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%1561:list{1024:int, 6:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%1512:<1024x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
102710 2024-12-10 17:48:17.424875 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Backward:deepseek_v2.decoder.layers.0.mlp.router,n0,rank6)
        - aten::mul:
          inputs: (%840:<i32>, %1536:<1xf32>{1})
          outputs: (%1577:<1xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%1577:<1xf32>{1}, dtype=None:NoneType)
          outputs: (%1576:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1576:<i32>, 2_5431315104166666e-07:float)
          outputs: (%1574:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1579:<1024x160xf32>{0, 0}, %741:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%740:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%740:<1024x160xf32>{160, 1}, %1543:list{0:int}, True:bool, dtype=None:NoneType)
          outputs: (%1580:<1x160xf32>{160,1})
          duration: -1
        - aten::_softmax_backward_data:
          inputs: (%1558:<1024x160xf32>{0, 1}, %840:<1024x160xf32>{160, 1}, -1:int, torch_float32:dtype)
          outputs: (%740:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%1543:list{1024:int, 160:int}, dtype=torch_bfloat16:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%840:<1024x160xbf16>{160,1})
          duration: -1
        - aten::scatter:
          inputs: (%840:<1024x160xbf16>{160, 1}, -1:int, %734:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, %987:<1024x6xbf16>{6, 1})
          outputs: (%740:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax_backward_data:
          inputs: (%1505:<1024x160xf32>{160, 1}, %1579:<1024x160xf32>{160, 1}, 1:int, torch_float32:dtype)
          outputs: (%997:<1024x160xf32>{160,1})
          duration: -1
        - aten::add:
          inputs: (%1580:<1024x160xbf16>{160, 1}, %1579:<1024x160xbf16>{160, 1}, alpha=1:int)
          outputs: (%740:<1024x160xbf16>{160,1})
          duration: -1
        - aten::mm:
          inputs: (%1576:<160x1024xbf16>{1, 160}, %723:<1024x5120xbf16>{5120, 1})
          outputs: (%1577:<160x5120xbf16>{5120,1})
          duration: -1
        - aten::mm:
          inputs: (%1574:<1024x160xbf16>{160, 1}, %1580:<160x5120xbf16>{5120, 1})
          outputs: (%1576:<1024x5120xbf16>{5120,1})
          duration: -1
        - aten::add_:
          inputs: (%182:<160x5120xf32>{5120, 1}+571479040, %1505:<160x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%182:<160x5120xf32>{5120,1}+571479040)
          duration: -1
103274 2024-12-10 17:48:17.435141 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Backward:deepseek_v2.decoder.layers.0.mlp.router,n0,rank6)
        - aten::add:
          inputs: (%962:<1024x1x5120xbf16>{5120, 5120, 1}, %740:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%987:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
103294 2024-12-10 17:48:17.435496 module_return: A37503:None:0 torch.2_3_0.MoELayer(Backward:deepseek_v2.decoder.layers.0.mlp,n0,rank6)
103298 2024-12-10 17:48:17.435564 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Backward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n1,rank6)
        - aten::add_:
          inputs: (%184:<5120xf32>{1}+572298240, %701:<5120xbf16>{1}, alpha=1:int)
          outputs: (%184:<5120xf32>{1}+572298240)
          duration: -1
103423 2024-12-10 17:48:17.437690 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Backward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n1,rank6)
        - aten::add:
          inputs: (%1553:<1024x1x5120xbf16>{5120, 5120, 1}, %723:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%730:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (compile_f___locals__bw_compiler_(dynamo_timed):str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
103488 2024-12-10 17:48:17.439210 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Backward:deepseek_v2.decoder.layers.0.self_attention,n0,rank6)
103491 2024-12-10 17:48:17.439294 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n0,rank6)
        - aten::mm:
          inputs: (%997:<1024x5120xbf16>{5120, 1}, %36:<5120x16384xbf16>{16384, 1})
          outputs: (%1575:<1024x16384xbf16>{16384,1})
          duration: -1
103627 2024-12-10 17:48:17.443031 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n0,rank6)
        - aten::bmm:
          inputs: (%344:<128x1024x1024xbf16>{1048576, 1, 1024}, %997:<128x1024x128xbf16>{128, 16384, 1})
          outputs: (%734:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - aten::bmm:
          inputs: (%997:<128x1024x128xbf16>{128, 16384, 1}, %601:<128x128x1024xbf16>{256, 1, 32768}+128)
          outputs: (%1513:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::_softmax_backward_data:
          inputs: (%607:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, %344:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, torch_float32:dtype)
          outputs: (%1513:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%607:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%601:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%1576:<128x192x1024xbf16>{196608, 1, 192}, %607:<128x1024x1024xbf16>{1048576, 1024, 1})
          outputs: (%1512:<128x192x1024xbf16>{196608,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%607:<128x1024x1024xbf16>{1048576, 1024, 1}, %1558:<128x1024x192xbf16>{196608, 192, 1})
          outputs: (%1576:<128x1024x192xbf16>{196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1558:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %344:<1x128x1024x192xbf16>{25165824, 196608, 1, 1024}, False:bool)
          outputs: (%1558:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%325:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %1055:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%325:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::sum:
          inputs: (%806:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1543:list{0:int, 1:int}, True:bool, dtype=None:NoneType)
          outputs: (%1055:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%601:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %1558:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, False:bool)
          outputs: (%601:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%344:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %607:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, False:bool)
          outputs: (%344:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%801:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %696:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%801:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%1562:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %344:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, False:bool)
          outputs: (%1562:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::mul:
          inputs: (%762:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %571:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1576:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%807:<1x1x1024x32xbf16>{65536, 65536, 64, 1})
          outputs: (%1583:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::add:
          inputs: (%696:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %810:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%1576:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%762:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %594:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1582:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%1576:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1582:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%1558:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%806:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %571:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1576:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%762:<1x128x1024x32xbf16>{8388608, 65536, 64, 1})
          outputs: (%839:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::add:
          inputs: (%1512:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1583:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%1576:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%806:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %594:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1512:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%1576:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1512:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%594:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::cat:
          inputs: (%1543:list{%1177:<1024x1x128x128xbf16>{128, 16777216, 131072, 1}, %740:<1024x1x128x128xbf16>{128, 16777216, 131072, 1}}, 3:int)
          outputs: (%595:<1024x1x128x256xbf16>{32768,32768,256,1})
          duration: -1
104896 2024-12-10 17:48:17.480822 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n3,rank6)
        - aten::mm:
          inputs: (%1505:<1024x32768xbf16>{32768, 1}, %60:<32768x512xbf16>{512, 1})
          outputs: (%997:<1024x512xbf16>{512,1})
          duration: -1
105028 2024-12-10 17:48:17.483273 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n3,rank6)
        - aten::cat:
          inputs: (%1345:list{%838:<1024x1x512xbf16>{512, 512, 1}, %1055:<1024x1x64xbf16>{64, 65536, 1}}, 2:int)
          outputs: (%740:<1024x1x576xbf16>{576,576,1})
          duration: -1
105054 2024-12-10 17:48:17.483665 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n2,rank6)
        - aten::mm:
          inputs: (%595:<1024x576xbf16>{576, 1}, %41:<576x5120xbf16>{5120, 1})
          outputs: (%1576:<1024x5120xbf16>{5120,1})
          duration: -1
105183 2024-12-10 17:48:17.485977 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n2,rank6)
        - aten::cat:
          inputs: (%1556:list{%344:<1024x1x128x128xbf16>{128, 16777216, 131072, 1}, %1583:<1024x1x128x64xbf16>{64, 8388608, 65536, 1}}, 3:int)
          outputs: (%1467:<1024x1x128x192xbf16>{24576,24576,192,1})
          duration: -1
105225 2024-12-10 17:48:17.486824 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n1,rank6)
        - aten::mm:
          inputs: (%1139:<1024x24576xbf16>{24576, 1}, %47:<24576x1536xbf16>{1536, 1})
          outputs: (%734:<1024x1536xbf16>{1536,1})
          duration: -1
105370 2024-12-10 17:48:17.489641 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n1,rank6)
105374 2024-12-10 17:48:17.489721 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n0,rank6)
        - aten::mm:
          inputs: (%519:<1024x1536xbf16>{1536, 1}, %38:<1536x5120xbf16>{5120, 1})
          outputs: (%1467:<1024x5120xbf16>{5120,1})
          duration: -1
105512 2024-12-10 17:48:17.492053 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n0,rank6)
        - aten::add:
          inputs: (%1177:<1024x1x5120xbf16>{5120, 5120, 1}, %1055:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%271:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
105532 2024-12-10 17:48:17.492433 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Backward:deepseek_v2.decoder.layers.0.self_attention,n0,rank6)
105536 2024-12-10 17:48:17.492502 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Backward:deepseek_v2.decoder.layers.0.input_layernorm,n0,rank6)
        - aten::add_:
          inputs: (%189:<5120xf32>{1}+667002880, %1586:<5120xbf16>{1}, alpha=1:int)
          outputs: (%189:<5120xf32>{1}+667002880)
          duration: -1
105656 2024-12-10 17:48:17.494530 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Backward:deepseek_v2.decoder.layers.0.input_layernorm,n0,rank6)
        - aten::add:
          inputs: (%730:<1024x1x5120xbf16>{5120, 5120, 1}, %1177:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%1587:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
105684 2024-12-10 17:48:17.494923 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Backward:deepseek_v2.embedding,n0,rank6)
105687 2024-12-10 17:48:17.494972 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Backward:deepseek_v2.embedding,n0,rank6)
105694 2024-12-10 17:48:17.495033 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Backward:deepseek_v2.embedding.embedding_dropout,n0,rank6)
105698 2024-12-10 17:48:17.495093 module_return: A37503:None:0 torch.2_3_0.Dropout(Backward:deepseek_v2.embedding.embedding_dropout,n0,rank6)
105718 2024-12-10 17:48:17.495425 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Backward:deepseek_v2.embedding.word_embeddings,n0,rank6)
105723 2024-12-10 17:48:17.495483 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Backward:deepseek_v2.embedding.word_embeddings,n0,rank6)
        - aten::embedding_dense_backward:
          inputs: (%325:<1x1024x5120xbf16>{5120, 5120, 1}, %430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 102400:int, -1:int, False:bool)
          outputs: (%879:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::add_:
          inputs: (%186:<102400x5120xf32>{5120, 1}+667008000, %1177:<102400x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%186:<102400x5120xf32>{5120,1}+667008000)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (False:bool, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (False:bool)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%595:<1xi32>{1}, %1591:list{%31:list{%186:<102400x5120xf32>{5120, 1}+667008000, %167:<5120x16384xf32>{16384, 1}+583116800, %187:<1536x5120xf32>{5120, 1}+575252480, %185:<576x5120xf32>{5120, 1}+572303360, %182:<160x5120xf32>{5120, 1}+571479040, %179:<5120x3072xf32>{3072, 1}+555750400, %176:<6144x5120xf32>{5120, 1}+524293120, %173:<102400x5120xf32>{5120, 1}, %189:<5120xf32>{1}+667002880, %184:<5120xf32>{1}+572298240, %175:<5120xf32>{1}+524288000}}, %1592:tuple{False:bool})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_l2norm_of_PyCapsule_object_at_0_7f5650c631e0_:builtin_function_or_method,%595:<1xi32>{1},%1591:list{%31:list{%186:<102400x5120xf32>{5120,1}+667008000,%167:<5120x16384xf32>{16384,1}+583116800,%187:<1536x5120xf32>{5120,1}+575252480,%185:<576x5120xf32>{5120,1}+572303360,%182:<160x5120xf32>{5120,1}+571479040,%179:<5120x3072xf32>{3072,1}+555750400,%176:<6144x5120xf32>{5120,1}+524293120,%173:<102400x5120xf32>{5120,1},%189:<5120xf32>{1}+667002880,%184:<5120xf32>{1}+572298240,%175:<5120xf32>{1}+524288000}},%1592:tuple{False:bool}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - aten::zeros:
          inputs: (%1345:list{320:int}, dtype=torch_float32:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%1593:<320xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%1276:<1xf32>{1}, 2_0:float)
          outputs: (%885:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%885:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0fb0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%885:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0fb0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1035:list{%885:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%756:tuple{%1594:list{%885:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%1595:list{%315:<24576x1536xf32>{1536, 1}, %316:<32768x512xf32>{512, 1}, %317:<5120x1536xf32>{1536, 1}, %320:<3072x5120xf32>{5120, 1}, %321:<5120x1536xf32>{1536, 1}, %257:<3072x5120xf32>{5120, 1}, %323:<5120x1536xf32>{1536, 1}, %248:<3072x5120xf32>{5120, 1}, %328:<5120x1536xf32>{1536, 1}, %330:<3072x5120xf32>{5120, 1}, %307:<5120x1536xf32>{1536, 1}, %298:<3072x5120xf32>{5120, 1}, %251:<5120x1536xf32>{1536, 1}, %333:<3072x5120xf32>{5120, 1}, %272:<5120x1536xf32>{1536, 1}, %335:<3072x5120xf32>{5120, 1}, %338:<5120x1536xf32>{1536, 1}, %267:<3072x5120xf32>{5120, 1}, %341:<5120x1536xf32>{1536, 1}, %342:<3072x5120xf32>{5120, 1}, %343:<5120x1536xf32>{1536, 1}, %346:<3072x5120xf32>{5120, 1}, %348:<5120x1536xf32>{1536, 1}, %327:<3072x5120xf32>{5120, 1}, %305:<5120x1536xf32>{1536, 1}, %352:<3072x5120xf32>{5120, 1}, %355:<5120x1536xf32>{1536, 1}, %324:<3072x5120xf32>{5120, 1}, %357:<5120x1536xf32>{1536, 1}, %349:<3072x5120xf32>{5120, 1}, %268:<5120x1536xf32>{1536, 1}, %354:<3072x5120xf32>{5120, 1}, %366:<5120x1536xf32>{1536, 1}, %368:<3072x5120xf32>{5120, 1}, %371:<5120x1536xf32>{1536, 1}, %373:<3072x5120xf32>{5120, 1}, %375:<5120x1536xf32>{1536, 1}, %379:<3072x5120xf32>{5120, 1}, %383:<5120x1536xf32>{1536, 1}, %381:<3072x5120xf32>{5120, 1}, %255:<5120x1536xf32>{1536, 1}, %360:<3072x5120xf32>{5120, 1}}, %360:<3072x5120xf32>{5120, 1})
          outputs: (%1595:list{%315:<24576x1536xf32>{1536,1},%316:<32768x512xf32>{512,1},%317:<5120x1536xf32>{1536,1},%320:<3072x5120xf32>{5120,1},%321:<5120x1536xf32>{1536,1},%257:<3072x5120xf32>{5120,1},%323:<5120x1536xf32>{1536,1},%248:<3072x5120xf32>{5120,1},%328:<5120x1536xf32>{1536,1},%330:<3072x5120xf32>{5120,1},%307:<5120x1536xf32>{1536,1},%298:<3072x5120xf32>{5120,1},%251:<5120x1536xf32>{1536,1},%333:<3072x5120xf32>{5120,1},%272:<5120x1536xf32>{1536,1},%335:<3072x5120xf32>{5120,1},%338:<5120x1536xf32>{1536,1},%267:<3072x5120xf32>{5120,1},%341:<5120x1536xf32>{1536,1},%342:<3072x5120xf32>{5120,1},%343:<5120x1536xf32>{1536,1},%346:<3072x5120xf32>{5120,1},%348:<5120x1536xf32>{1536,1},%327:<3072x5120xf32>{5120,1},%305:<5120x1536xf32>{1536,1},%352:<3072x5120xf32>{5120,1},%355:<5120x1536xf32>{1536,1},%324:<3072x5120xf32>{5120,1},%357:<5120x1536xf32>{1536,1},%349:<3072x5120xf32>{5120,1},%268:<5120x1536xf32>{1536,1},%354:<3072x5120xf32>{5120,1},%366:<5120x1536xf32>{1536,1},%368:<3072x5120xf32>{5120,1},%371:<5120x1536xf32>{1536,1},%373:<3072x5120xf32>{5120,1},%375:<5120x1536xf32>{1536,1},%379:<3072x5120xf32>{5120,1},%383:<5120x1536xf32>{1536,1},%381:<3072x5120xf32>{5120,1},%255:<5120x1536xf32>{1536,1},%360:<3072x5120xf32>{5120,1}})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%1513:<1xi32>{1}, %1595:list{%1596:list{%229:<24576x1536xf32>{1536, 1}+488636416, %228:<32768x512xf32>{512, 1}+471859200, %227:<5120x1536xf32>{1536, 1}+463994880, %226:<3072x5120xf32>{5120, 1}+448266240, %225:<5120x1536xf32>{1536, 1}+440401920, %224:<3072x5120xf32>{5120, 1}+424673280, %223:<5120x1536xf32>{1536, 1}+416808960, %222:<3072x5120xf32>{5120, 1}+401080320, %221:<5120x1536xf32>{1536, 1}+393216000, %163:<3072x5120xf32>{5120, 1}+377487360, %220:<5120x1536xf32>{1536, 1}+369623040, %154:<3072x5120xf32>{5120, 1}+353894400, %219:<5120x1536xf32>{1536, 1}+346030080, %157:<3072x5120xf32>{5120, 1}+330301440, %218:<5120x1536xf32>{1536, 1}+322437120, %217:<3072x5120xf32>{5120, 1}+306708480, %158:<5120x1536xf32>{1536, 1}+298844160, %216:<3072x5120xf32>{5120, 1}+283115520, %215:<5120x1536xf32>{1536, 1}+275251200, %155:<3072x5120xf32>{5120, 1}+259522560, %153:<5120x1536xf32>{1536, 1}+251658240, %214:<3072x5120xf32>{5120, 1}+235929600, %213:<5120x1536xf32>{1536, 1}+228065280, %174:<3072x5120xf32>{5120, 1}+212336640, %171:<5120x1536xf32>{1536, 1}+204472320, %212:<3072x5120xf32>{5120, 1}+188743680, %156:<5120x1536xf32>{1536, 1}+180879360, %211:<3072x5120xf32>{5120, 1}+165150720, %210:<5120x1536xf32>{1536, 1}+157286400, %193:<3072x5120xf32>{5120, 1}+141557760, %209:<5120x1536xf32>{1536, 1}+133693440, %208:<3072x5120xf32>{5120, 1}+117964800, %207:<5120x1536xf32>{1536, 1}+110100480, %206:<3072x5120xf32>{5120, 1}+94371840, %183:<5120x1536xf32>{1536, 1}+86507520, %205:<3072x5120xf32>{5120, 1}+70778880, %180:<5120x1536xf32>{1536, 1}+62914560, %204:<3072x5120xf32>{5120, 1}+47185920, %177:<5120x1536xf32>{1536, 1}+39321600, %201:<3072x5120xf32>{5120, 1}+23592960, %165:<5120x1536xf32>{1536, 1}+15728640, %198:<3072x5120xf32>{5120, 1}}}, %1592:tuple{False:bool})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_l2norm_of_PyCapsule_object_at_0_7f5650c631e0_:builtin_function_or_method,%1513:<1xi32>{1},%1595:list{%1596:list{%229:<24576x1536xf32>{1536,1}+488636416,%228:<32768x512xf32>{512,1}+471859200,%227:<5120x1536xf32>{1536,1}+463994880,%226:<3072x5120xf32>{5120,1}+448266240,%225:<5120x1536xf32>{1536,1}+440401920,%224:<3072x5120xf32>{5120,1}+424673280,%223:<5120x1536xf32>{1536,1}+416808960,%222:<3072x5120xf32>{5120,1}+401080320,%221:<5120x1536xf32>{1536,1}+393216000,%163:<3072x5120xf32>{5120,1}+377487360,%220:<5120x1536xf32>{1536,1}+369623040,%154:<3072x5120xf32>{5120,1}+353894400,%219:<5120x1536xf32>{1536,1}+346030080,%157:<3072x5120xf32>{5120,1}+330301440,%218:<5120x1536xf32>{1536,1}+322437120,%217:<3072x5120xf32>{5120,1}+306708480,%158:<5120x1536xf32>{1536,1}+298844160,%216:<3072x5120xf32>{5120,1}+283115520,%215:<5120x1536xf32>{1536,1}+275251200,%155:<3072x5120xf32>{5120,1}+259522560,%153:<5120x1536xf32>{1536,1}+251658240,%214:<3072x5120xf32>{5120,1}+235929600,%213:<5120x1536xf32>{1536,1}+228065280,%174:<3072x5120xf32>{5120,1}+212336640,%171:<5120x1536xf32>{1536,1}+204472320,%212:<3072x5120xf32>{5120,1}+188743680,%156:<5120x1536xf32>{1536,1}+180879360,%211:<3072x5120xf32>{5120,1}+165150720,%210:<5120x1536xf32>{1536,1}+157286400,%193:<3072x5120xf32>{5120,1}+141557760,%209:<5120x1536xf32>{1536,1}+133693440,%208:<3072x5120xf32>{5120,1}+117964800,%207:<5120x1536xf32>{1536,1}+110100480,%206:<3072x5120xf32>{5120,1}+94371840,%183:<5120x1536xf32>{1536,1}+86507520,%205:<3072x5120xf32>{5120,1}+70778880,%180:<5120x1536xf32>{1536,1}+62914560,%204:<3072x5120xf32>{5120,1}+47185920,%177:<5120x1536xf32>{1536,1}+39321600,%201:<3072x5120xf32>{5120,1}+23592960,%165:<5120x1536xf32>{1536,1}+15728640,%198:<3072x5120xf32>{5120,1}}},%1592:tuple{False:bool}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - aten::zeros:
          inputs: (%1597:list{320:int}, dtype=torch_float32:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%1598:<320xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%1583:<1xf32>{1}, 2_0:float)
          outputs: (%1055:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1055:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0af0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1055:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0af0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1035:list{%1055:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%751:tuple{%1554:list{%1055:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%1596:list{%51:<102400x5120xf32>{5120, 1}, %297:<5120x16384xf32>{16384, 1}, %299:<1536x5120xf32>{5120, 1}, %300:<576x5120xf32>{5120, 1}, %303:<160x5120xf32>{5120, 1}, %304:<5120x3072xf32>{3072, 1}, %302:<6144x5120xf32>{5120, 1}, %234:<102400x5120xf32>{5120, 1}, %306:<5120xf32>{1}, %301:<5120xf32>{1}, %247:<5120xf32>{1}}, _'params'__[tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device='cuda_6'),_tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device='cuda_6'),_tensor([1_,_1_,_1_,_____,_1_,_1_,_1_],_device='cuda_6')],_'wd_mult'__0_0,_'lr_mult'__1_0,_'is_e_pert_parallel'__False,_'is_decoupled_lr'__False,_'ma__lr'__1e-05,_'min_lr'__1e-06,_'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_0_:dict, %247:<5120xf32>{1})
          outputs: (%1596:list{%51:<102400x5120xf32>{5120,1},%297:<5120x16384xf32>{16384,1},%299:<1536x5120xf32>{5120,1},%300:<576x5120xf32>{5120,1},%303:<160x5120xf32>{5120,1},%304:<5120x3072xf32>{3072,1},%302:<6144x5120xf32>{5120,1},%234:<102400x5120xf32>{5120,1},%306:<5120xf32>{1},%301:<5120xf32>{1},%247:<5120xf32>{1}})
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%949:<1xi32>{1}, %1035:list{%1603:list{%1519:<102400x5120xf32>{5120, 1}+667008000, %1505:<5120x16384xf32>{16384, 1}+583116800, %1513:<1536x5120xf32>{5120, 1}+575252480, %1599:<576x5120xf32>{5120, 1}+572303360, %1338:<160x5120xf32>{5120, 1}+571479040, %1139:<5120x3072xf32>{3072, 1}+555750400, %1600:<6144x5120xf32>{5120, 1}+524293120, %885:<102400x5120xf32>{5120, 1}, %1576:<5120xf32>{1}+667002880, %1491:<5120xf32>{1}+572298240, %1602:<5120xf32>{1}+524288000}, %1603:list{%1519:<102400x5120xf32>{5120, 1}+667008000, %1505:<5120x16384xf32>{16384, 1}+583116800, %1513:<1536x5120xf32>{5120, 1}+575252480, %1599:<576x5120xf32>{5120, 1}+572303360, %1338:<160x5120xf32>{5120, 1}+571479040, %1139:<5120x3072xf32>{3072, 1}+555750400, %1600:<6144x5120xf32>{5120, 1}+524293120, %885:<102400x5120xf32>{5120, 1}, %1576:<5120xf32>{1}+667002880, %1491:<5120xf32>{1}+572298240, %1602:<5120xf32>{1}+524288000}}, %718:tuple{0_060333048260884134:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_scale_of_PyCapsule_object_at_0_7f5650c63300_:builtin_function_or_method,%949:<1xi32>{1},%1035:list{%1603:list{%1519:<102400x5120xf32>{5120,1}+667008000,%1505:<5120x16384xf32>{16384,1}+583116800,%1513:<1536x5120xf32>{5120,1}+575252480,%1599:<576x5120xf32>{5120,1}+572303360,%1338:<160x5120xf32>{5120,1}+571479040,%1139:<5120x3072xf32>{3072,1}+555750400,%1600:<6144x5120xf32>{5120,1}+524293120,%885:<102400x5120xf32>{5120,1},%1576:<5120xf32>{1}+667002880,%1491:<5120xf32>{1}+572298240,%1602:<5120xf32>{1}+524288000},%1603:list{%1519:<102400x5120xf32>{5120,1}+667008000,%1505:<5120x16384xf32>{16384,1}+583116800,%1513:<1536x5120xf32>{5120,1}+575252480,%1599:<576x5120xf32>{5120,1}+572303360,%1338:<160x5120xf32>{5120,1}+571479040,%1139:<5120x3072xf32>{3072,1}+555750400,%1600:<6144x5120xf32>{5120,1}+524293120,%885:<102400x5120xf32>{5120,1},%1576:<5120xf32>{1}+667002880,%1491:<5120xf32>{1}+572298240,%1602:<5120xf32>{1}+524288000}},%718:tuple{0_060333048260884134:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%1604:list{%315:<24576x1536xf32>{1536, 1}, %316:<32768x512xf32>{512, 1}, %317:<5120x1536xf32>{1536, 1}, %320:<3072x5120xf32>{5120, 1}, %321:<5120x1536xf32>{1536, 1}, %257:<3072x5120xf32>{5120, 1}, %323:<5120x1536xf32>{1536, 1}, %248:<3072x5120xf32>{5120, 1}, %328:<5120x1536xf32>{1536, 1}, %330:<3072x5120xf32>{5120, 1}, %307:<5120x1536xf32>{1536, 1}, %298:<3072x5120xf32>{5120, 1}, %251:<5120x1536xf32>{1536, 1}, %333:<3072x5120xf32>{5120, 1}, %272:<5120x1536xf32>{1536, 1}, %335:<3072x5120xf32>{5120, 1}, %338:<5120x1536xf32>{1536, 1}, %267:<3072x5120xf32>{5120, 1}, %341:<5120x1536xf32>{1536, 1}, %342:<3072x5120xf32>{5120, 1}, %343:<5120x1536xf32>{1536, 1}, %346:<3072x5120xf32>{5120, 1}, %348:<5120x1536xf32>{1536, 1}, %327:<3072x5120xf32>{5120, 1}, %305:<5120x1536xf32>{1536, 1}, %352:<3072x5120xf32>{5120, 1}, %355:<5120x1536xf32>{1536, 1}, %324:<3072x5120xf32>{5120, 1}, %357:<5120x1536xf32>{1536, 1}, %349:<3072x5120xf32>{5120, 1}, %268:<5120x1536xf32>{1536, 1}, %354:<3072x5120xf32>{5120, 1}, %366:<5120x1536xf32>{1536, 1}, %368:<3072x5120xf32>{5120, 1}, %371:<5120x1536xf32>{1536, 1}, %373:<3072x5120xf32>{5120, 1}, %375:<5120x1536xf32>{1536, 1}, %379:<3072x5120xf32>{5120, 1}, %383:<5120x1536xf32>{1536, 1}, %381:<3072x5120xf32>{5120, 1}, %255:<5120x1536xf32>{1536, 1}, %360:<3072x5120xf32>{5120, 1}}, %360:<3072x5120xf32>{5120, 1})
          outputs: (%1604:list{%315:<24576x1536xf32>{1536,1},%316:<32768x512xf32>{512,1},%317:<5120x1536xf32>{1536,1},%320:<3072x5120xf32>{5120,1},%321:<5120x1536xf32>{1536,1},%257:<3072x5120xf32>{5120,1},%323:<5120x1536xf32>{1536,1},%248:<3072x5120xf32>{5120,1},%328:<5120x1536xf32>{1536,1},%330:<3072x5120xf32>{5120,1},%307:<5120x1536xf32>{1536,1},%298:<3072x5120xf32>{5120,1},%251:<5120x1536xf32>{1536,1},%333:<3072x5120xf32>{5120,1},%272:<5120x1536xf32>{1536,1},%335:<3072x5120xf32>{5120,1},%338:<5120x1536xf32>{1536,1},%267:<3072x5120xf32>{5120,1},%341:<5120x1536xf32>{1536,1},%342:<3072x5120xf32>{5120,1},%343:<5120x1536xf32>{1536,1},%346:<3072x5120xf32>{5120,1},%348:<5120x1536xf32>{1536,1},%327:<3072x5120xf32>{5120,1},%305:<5120x1536xf32>{1536,1},%352:<3072x5120xf32>{5120,1},%355:<5120x1536xf32>{1536,1},%324:<3072x5120xf32>{5120,1},%357:<5120x1536xf32>{1536,1},%349:<3072x5120xf32>{5120,1},%268:<5120x1536xf32>{1536,1},%354:<3072x5120xf32>{5120,1},%366:<5120x1536xf32>{1536,1},%368:<3072x5120xf32>{5120,1},%371:<5120x1536xf32>{1536,1},%373:<3072x5120xf32>{5120,1},%375:<5120x1536xf32>{1536,1},%379:<3072x5120xf32>{5120,1},%383:<5120x1536xf32>{1536,1},%381:<3072x5120xf32>{5120,1},%255:<5120x1536xf32>{1536,1},%360:<3072x5120xf32>{5120,1}})
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%958:<1xi32>{1}, %1595:list{%1603:list{%997:<24576x1536xf32>{1536, 1}+488636416, %1606:<32768x512xf32>{512, 1}+471859200, %1608:<5120x1536xf32>{1536, 1}+463994880, %601:<3072x5120xf32>{5120, 1}+448266240, %429:<5120x1536xf32>{1536, 1}+440401920, %491:<3072x5120xf32>{5120, 1}+424673280, %706:<5120x1536xf32>{1536, 1}+416808960, %496:<3072x5120xf32>{5120, 1}+401080320, %463:<5120x1536xf32>{1536, 1}+393216000, %600:<3072x5120xf32>{5120, 1}+377487360, %487:<5120x1536xf32>{1536, 1}+369623040, %1581:<3072x5120xf32>{5120, 1}+353894400, %949:<5120x1536xf32>{1536, 1}+346030080, %1602:<3072x5120xf32>{5120, 1}+330301440, %1467:<5120x1536xf32>{1536, 1}+322437120, %761:<3072x5120xf32>{5120, 1}+306708480, %744:<5120x1536xf32>{1536, 1}+298844160, %1599:<3072x5120xf32>{5120, 1}+283115520, %1607:<5120x1536xf32>{1536, 1}+275251200, %1600:<3072x5120xf32>{5120, 1}+259522560, %478:<5120x1536xf32>{1536, 1}+251658240, %1611:<3072x5120xf32>{5120, 1}+235929600, %1194:<5120x1536xf32>{1536, 1}+228065280, %1527:<3072x5120xf32>{5120, 1}+212336640, %1512:<5120x1536xf32>{1536, 1}+204472320, %1491:<3072x5120xf32>{5120, 1}+188743680, %1576:<5120x1536xf32>{1536, 1}+180879360, %1505:<3072x5120xf32>{5120, 1}+165150720, %758:<5120x1536xf32>{1536, 1}+157286400, %1139:<3072x5120xf32>{5120, 1}+141557760, %282:<5120x1536xf32>{1536, 1}+133693440, %711:<3072x5120xf32>{5120, 1}+117964800, %1613:<5120x1536xf32>{1536, 1}+110100480, %1558:<3072x5120xf32>{5120, 1}+94371840, %252:<5120x1536xf32>{1536, 1}+86507520, %724:<3072x5120xf32>{5120, 1}+70778880, %978:<5120x1536xf32>{1536, 1}+62914560, %768:<3072x5120xf32>{5120, 1}+47185920, %1616:<5120x1536xf32>{1536, 1}+39321600, %1618:<3072x5120xf32>{5120, 1}+23592960, %1177:<5120x1536xf32>{1536, 1}+15728640, %1619:<3072x5120xf32>{5120, 1}}, %1603:list{%997:<24576x1536xf32>{1536, 1}+488636416, %1606:<32768x512xf32>{512, 1}+471859200, %1608:<5120x1536xf32>{1536, 1}+463994880, %601:<3072x5120xf32>{5120, 1}+448266240, %429:<5120x1536xf32>{1536, 1}+440401920, %491:<3072x5120xf32>{5120, 1}+424673280, %706:<5120x1536xf32>{1536, 1}+416808960, %496:<3072x5120xf32>{5120, 1}+401080320, %463:<5120x1536xf32>{1536, 1}+393216000, %600:<3072x5120xf32>{5120, 1}+377487360, %487:<5120x1536xf32>{1536, 1}+369623040, %1581:<3072x5120xf32>{5120, 1}+353894400, %949:<5120x1536xf32>{1536, 1}+346030080, %1602:<3072x5120xf32>{5120, 1}+330301440, %1467:<5120x1536xf32>{1536, 1}+322437120, %761:<3072x5120xf32>{5120, 1}+306708480, %744:<5120x1536xf32>{1536, 1}+298844160, %1599:<3072x5120xf32>{5120, 1}+283115520, %1607:<5120x1536xf32>{1536, 1}+275251200, %1600:<3072x5120xf32>{5120, 1}+259522560, %478:<5120x1536xf32>{1536, 1}+251658240, %1611:<3072x5120xf32>{5120, 1}+235929600, %1194:<5120x1536xf32>{1536, 1}+228065280, %1527:<3072x5120xf32>{5120, 1}+212336640, %1512:<5120x1536xf32>{1536, 1}+204472320, %1491:<3072x5120xf32>{5120, 1}+188743680, %1576:<5120x1536xf32>{1536, 1}+180879360, %1505:<3072x5120xf32>{5120, 1}+165150720, %758:<5120x1536xf32>{1536, 1}+157286400, %1139:<3072x5120xf32>{5120, 1}+141557760, %282:<5120x1536xf32>{1536, 1}+133693440, %711:<3072x5120xf32>{5120, 1}+117964800, %1613:<5120x1536xf32>{1536, 1}+110100480, %1558:<3072x5120xf32>{5120, 1}+94371840, %252:<5120x1536xf32>{1536, 1}+86507520, %724:<3072x5120xf32>{5120, 1}+70778880, %978:<5120x1536xf32>{1536, 1}+62914560, %768:<3072x5120xf32>{5120, 1}+47185920, %1616:<5120x1536xf32>{1536, 1}+39321600, %1618:<3072x5120xf32>{5120, 1}+23592960, %1177:<5120x1536xf32>{1536, 1}+15728640, %1619:<3072x5120xf32>{5120, 1}}}, %718:tuple{0_060333048260884134:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_scale_of_PyCapsule_object_at_0_7f5650c63300_:builtin_function_or_method,%958:<1xi32>{1},%1595:list{%1603:list{%997:<24576x1536xf32>{1536,1}+488636416,%1606:<32768x512xf32>{512,1}+471859200,%1608:<5120x1536xf32>{1536,1}+463994880,%601:<3072x5120xf32>{5120,1}+448266240,%429:<5120x1536xf32>{1536,1}+440401920,%491:<3072x5120xf32>{5120,1}+424673280,%706:<5120x1536xf32>{1536,1}+416808960,%496:<3072x5120xf32>{5120,1}+401080320,%463:<5120x1536xf32>{1536,1}+393216000,%600:<3072x5120xf32>{5120,1}+377487360,%487:<5120x1536xf32>{1536,1}+369623040,%1581:<3072x5120xf32>{5120,1}+353894400,%949:<5120x1536xf32>{1536,1}+346030080,%1602:<3072x5120xf32>{5120,1}+330301440,%1467:<5120x1536xf32>{1536,1}+322437120,%761:<3072x5120xf32>{5120,1}+306708480,%744:<5120x1536xf32>{1536,1}+298844160,%1599:<3072x5120xf32>{5120,1}+283115520,%1607:<5120x1536xf32>{1536,1}+275251200,%1600:<3072x5120xf32>{5120,1}+259522560,%478:<5120x1536xf32>{1536,1}+251658240,%1611:<3072x5120xf32>{5120,1}+235929600,%1194:<5120x1536xf32>{1536,1}+228065280,%1527:<3072x5120xf32>{5120,1}+212336640,%1512:<5120x1536xf32>{1536,1}+204472320,%1491:<3072x5120xf32>{5120,1}+188743680,%1576:<5120x1536xf32>{1536,1}+180879360,%1505:<3072x5120xf32>{5120,1}+165150720,%758:<5120x1536xf32>{1536,1}+157286400,%1139:<3072x5120xf32>{5120,1}+141557760,%282:<5120x1536xf32>{1536,1}+133693440,%711:<3072x5120xf32>{5120,1}+117964800,%1613:<5120x1536xf32>{1536,1}+110100480,%1558:<3072x5120xf32>{5120,1}+94371840,%252:<5120x1536xf32>{1536,1}+86507520,%724:<3072x5120xf32>{5120,1}+70778880,%978:<5120x1536xf32>{1536,1}+62914560,%768:<3072x5120xf32>{5120,1}+47185920,%1616:<5120x1536xf32>{1536,1}+39321600,%1618:<3072x5120xf32>{5120,1}+23592960,%1177:<5120x1536xf32>{1536,1}+15728640,%1619:<3072x5120xf32>{5120,1}},%1603:list{%997:<24576x1536xf32>{1536,1}+488636416,%1606:<32768x512xf32>{512,1}+471859200,%1608:<5120x1536xf32>{1536,1}+463994880,%601:<3072x5120xf32>{5120,1}+448266240,%429:<5120x1536xf32>{1536,1}+440401920,%491:<3072x5120xf32>{5120,1}+424673280,%706:<5120x1536xf32>{1536,1}+416808960,%496:<3072x5120xf32>{5120,1}+401080320,%463:<5120x1536xf32>{1536,1}+393216000,%600:<3072x5120xf32>{5120,1}+377487360,%487:<5120x1536xf32>{1536,1}+369623040,%1581:<3072x5120xf32>{5120,1}+353894400,%949:<5120x1536xf32>{1536,1}+346030080,%1602:<3072x5120xf32>{5120,1}+330301440,%1467:<5120x1536xf32>{1536,1}+322437120,%761:<3072x5120xf32>{5120,1}+306708480,%744:<5120x1536xf32>{1536,1}+298844160,%1599:<3072x5120xf32>{5120,1}+283115520,%1607:<5120x1536xf32>{1536,1}+275251200,%1600:<3072x5120xf32>{5120,1}+259522560,%478:<5120x1536xf32>{1536,1}+251658240,%1611:<3072x5120xf32>{5120,1}+235929600,%1194:<5120x1536xf32>{1536,1}+228065280,%1527:<3072x5120xf32>{5120,1}+212336640,%1512:<5120x1536xf32>{1536,1}+204472320,%1491:<3072x5120xf32>{5120,1}+188743680,%1576:<5120x1536xf32>{1536,1}+180879360,%1505:<3072x5120xf32>{5120,1}+165150720,%758:<5120x1536xf32>{1536,1}+157286400,%1139:<3072x5120xf32>{5120,1}+141557760,%282:<5120x1536xf32>{1536,1}+133693440,%711:<3072x5120xf32>{5120,1}+117964800,%1613:<5120x1536xf32>{1536,1}+110100480,%1558:<3072x5120xf32>{5120,1}+94371840,%252:<5120x1536xf32>{1536,1}+86507520,%724:<3072x5120xf32>{5120,1}+70778880,%978:<5120x1536xf32>{1536,1}+62914560,%768:<3072x5120xf32>{5120,1}+47185920,%1616:<5120x1536xf32>{1536,1}+39321600,%1618:<3072x5120xf32>{5120,1}+23592960,%1177:<5120x1536xf32>{1536,1}+15728640,%1619:<3072x5120xf32>{5120,1}}},%718:tuple{0_060333048260884134:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::wrapper call:
          inputs: (%1620:tuple{NotSurpot:FusedAdam}, _function_FusedAdam_step_at_0_7f5650cb89d0_:function)
          outputs: (torch.2_3_0.wrapper(%1620:tuple{NotSurpot:FusedAdam},_function_FusedAdam_step_at_0_7f5650cb89d0_:function))
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (Optimizer_step#FusedAdam_step:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.FusedAdam(None:NoneType,)
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%285:<1xi32>{1}, %1624:list{%1625:list{%1606:<102400x5120xf32>{5120, 1}+667008000, %595:<5120x16384xf32>{16384, 1}+583116800, %1194:<1536x5120xf32>{5120, 1}+575252480, %1600:<576x5120xf32>{5120, 1}+572303360, %519:<160x5120xf32>{5120, 1}+571479040, %1623:<5120x3072xf32>{3072, 1}+555750400, %1513:<6144x5120xf32>{5120, 1}+524293120, %252:<102400x5120xf32>{5120, 1}}, %1626:list{%1607:<102400x5120xf32>{5120, 1}, %487:<5120x16384xf32>{16384, 1}, %429:<1536x5120xf32>{5120, 1}, %761:<576x5120xf32>{5120, 1}, %1622:<160x5120xf32>{5120, 1}, %1583:<5120x3072xf32>{3072, 1}, %340:<6144x5120xf32>{5120, 1}, %282:<102400x5120xf32>{5120, 1}}, %1627:list{%706:<102400x5120xf32>{5120, 1}, %606:<5120x16384xf32>{16384, 1}, %1611:<1536x5120xf32>{5120, 1}, %601:<576x5120xf32>{5120, 1}, %1621:<160x5120xf32>{5120, 1}, %1527:<5120x3072xf32>{3072, 1}, %1558:<6144x5120xf32>{5120, 1}, %1537:<102400x5120xf32>{5120, 1}}, %1628:list{%740:<102400x5120xf32>{5120, 1}, %496:<5120x16384xf32>{16384, 1}, %457:<1536x5120xf32>{5120, 1}, %1344:<576x5120xf32>{5120, 1}, %1487:<160x5120xf32>{5120, 1}, %1613:<5120x3072xf32>{3072, 1}, %1576:<6144x5120xf32>{5120, 1}, %711:<102400x5120xf32>{5120, 1}}}, %1629:tuple{1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_adam_of_PyCapsule_object_at_0_7f5650c63030_:builtin_function_or_method,%285:<1xi32>{1},%1624:list{%1625:list{%1606:<102400x5120xf32>{5120,1}+667008000,%595:<5120x16384xf32>{16384,1}+583116800,%1194:<1536x5120xf32>{5120,1}+575252480,%1600:<576x5120xf32>{5120,1}+572303360,%519:<160x5120xf32>{5120,1}+571479040,%1623:<5120x3072xf32>{3072,1}+555750400,%1513:<6144x5120xf32>{5120,1}+524293120,%252:<102400x5120xf32>{5120,1}},%1626:list{%1607:<102400x5120xf32>{5120,1},%487:<5120x16384xf32>{16384,1},%429:<1536x5120xf32>{5120,1},%761:<576x5120xf32>{5120,1},%1622:<160x5120xf32>{5120,1},%1583:<5120x3072xf32>{3072,1},%340:<6144x5120xf32>{5120,1},%282:<102400x5120xf32>{5120,1}},%1627:list{%706:<102400x5120xf32>{5120,1},%606:<5120x16384xf32>{16384,1},%1611:<1536x5120xf32>{5120,1},%601:<576x5120xf32>{5120,1},%1621:<160x5120xf32>{5120,1},%1527:<5120x3072xf32>{3072,1},%1558:<6144x5120xf32>{5120,1},%1537:<102400x5120xf32>{5120,1}},%1628:list{%740:<102400x5120xf32>{5120,1},%496:<5120x16384xf32>{16384,1},%457:<1536x5120xf32>{5120,1},%1344:<576x5120xf32>{5120,1},%1487:<160x5120xf32>{5120,1},%1613:<5120x3072xf32>{3072,1},%1576:<6144x5120xf32>{5120,1},%711:<102400x5120xf32>{5120,1}}},%1629:tuple{1e-05:float,0_9:float,0_95:float,1e-08:float,1:int,1:int,1:int,0_1:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%285:<1xi32>{1}, %1626:list{%1597:list{%487:<5120xf32>{1}+667002880, %1467:<5120xf32>{1}+572298240, %1500:<5120xf32>{1}+524288000}, %1630:list{%478:<5120xf32>{1}, %1579:<5120xf32>{1}, %1583:<5120xf32>{1}}, %1493:list{%761:<5120xf32>{1}, %1194:<5120xf32>{1}, %1572:<5120xf32>{1}}, %1631:list{%519:<5120xf32>{1}, %1513:<5120xf32>{1}, %1622:<5120xf32>{1}}}, %1629:tuple{1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_0:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_adam_of_PyCapsule_object_at_0_7f5650c63030_:builtin_function_or_method,%285:<1xi32>{1},%1626:list{%1597:list{%487:<5120xf32>{1}+667002880,%1467:<5120xf32>{1}+572298240,%1500:<5120xf32>{1}+524288000},%1630:list{%478:<5120xf32>{1},%1579:<5120xf32>{1},%1583:<5120xf32>{1}},%1493:list{%761:<5120xf32>{1},%1194:<5120xf32>{1},%1572:<5120xf32>{1}},%1631:list{%519:<5120xf32>{1},%1513:<5120xf32>{1},%1622:<5120xf32>{1}}},%1629:tuple{1e-05:float,0_9:float,0_95:float,1e-08:float,1:int,1:int,1:int,0_0:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%1636:list{%487:<102400x5120xf32>{5120, 1}, %724:<5120x16384xf32>{16384, 1}, %984:<1536x5120xf32>{5120, 1}, %1619:<576x5120xf32>{5120, 1}, %237:<160x5120xf32>{5120, 1}, %282:<5120x3072xf32>{3072, 1}, %337:<6144x5120xf32>{5120, 1}, %325:<102400x5120xf32>{5120, 1}, %963:<5120xf32>{1}, %1616:<5120xf32>{1}, %772:<5120xf32>{1}}, %1589:list{%306:<5120xf32>{1}, %301:<5120xf32>{1}, %247:<5120xf32>{1}}, %247:<5120xf32>{1})
          outputs: (%1638:tuple{%1639:list{%1607:<102400x5120xbf16>{5120,1},%1579:<5120x16384xbf16>{16384,1},%1500:<1536x5120xbf16>{5120,1},%730:<576x5120xbf16>{5120,1},%1467:<160x5120xbf16>{5120,1},%340:<5120x3072xbf16>{3072,1},%252:<6144x5120xbf16>{5120,1},%284:<102400x5120xbf16>{5120,1},%947:<5120xbf16>{1},%1617:<5120xbf16>{1},%999:<5120xbf16>{1}},%1636:list{%487:<102400x5120xf32>{5120,1},%724:<5120x16384xf32>{16384,1},%984:<1536x5120xf32>{5120,1},%1619:<576x5120xf32>{5120,1},%237:<160x5120xf32>{5120,1},%282:<5120x3072xf32>{3072,1},%337:<6144x5120xf32>{5120,1},%325:<102400x5120xf32>{5120,1},%963:<5120xf32>{1},%1616:<5120xf32>{1},%772:<5120xf32>{1}}})
          duration: -1
        - ----------->api::_multi_tensor_copy_this_to_that call:
          inputs: (%1636:list{%487:<102400x5120xf32>{5120, 1}, %724:<5120x16384xf32>{16384, 1}, %984:<1536x5120xf32>{5120, 1}, %1619:<576x5120xf32>{5120, 1}, %237:<160x5120xf32>{5120, 1}, %282:<5120x3072xf32>{3072, 1}, %337:<6144x5120xf32>{5120, 1}, %325:<102400x5120xf32>{5120, 1}, %963:<5120xf32>{1}, %1616:<5120xf32>{1}, %772:<5120xf32>{1}}, %1639:list{%1607:<102400x5120xbf16>{5120, 1}, %1579:<5120x16384xbf16>{16384, 1}, %1500:<1536x5120xbf16>{5120, 1}, %730:<576x5120xbf16>{5120, 1}, %1467:<160x5120xbf16>{5120, 1}, %340:<5120x3072xbf16>{3072, 1}, %252:<6144x5120xbf16>{5120, 1}, %284:<102400x5120xbf16>{5120, 1}, %947:<5120xbf16>{1}, %1617:<5120xbf16>{1}, %999:<5120xbf16>{1}}, None:NoneType)
          outputs: (torch.2_3_0._multi_tensor_copy_this_to_that(%1636:list{%487:<102400x5120xf32>{5120,1},%724:<5120x16384xf32>{16384,1},%984:<1536x5120xf32>{5120,1},%1619:<576x5120xf32>{5120,1},%237:<160x5120xf32>{5120,1},%282:<5120x3072xf32>{3072,1},%337:<6144x5120xf32>{5120,1},%325:<102400x5120xf32>{5120,1},%963:<5120xf32>{1},%1616:<5120xf32>{1},%772:<5120xf32>{1}},%1639:list{%1607:<102400x5120xbf16>{5120,1},%1579:<5120x16384xbf16>{16384,1},%1500:<1536x5120xbf16>{5120,1},%730:<576x5120xbf16>{5120,1},%1467:<160x5120xbf16>{5120,1},%340:<5120x3072xbf16>{3072,1},%252:<6144x5120xbf16>{5120,1},%284:<102400x5120xbf16>{5120,1},%947:<5120xbf16>{1},%1617:<5120xbf16>{1},%999:<5120xbf16>{1}},None:NoneType))
          duration: -1
        - aten::copy_:
          inputs: (%1607:<102400x5120xbf16>{5120, 1}, %487:<102400x5120xf32>{5120, 1}, False:bool)
          outputs: (%1607:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1579:<5120x16384xbf16>{16384, 1}, %724:<5120x16384xf32>{16384, 1}, False:bool)
          outputs: (%1579:<5120x16384xbf16>{16384,1})
          duration: -1
        - aten::copy_:
          inputs: (%1500:<1536x5120xbf16>{5120, 1}, %984:<1536x5120xf32>{5120, 1}, False:bool)
          outputs: (%1500:<1536x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%730:<576x5120xbf16>{5120, 1}, %1619:<576x5120xf32>{5120, 1}, False:bool)
          outputs: (%730:<576x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1467:<160x5120xbf16>{5120, 1}, %237:<160x5120xf32>{5120, 1}, False:bool)
          outputs: (%1467:<160x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%340:<5120x3072xbf16>{3072, 1}, %282:<5120x3072xf32>{3072, 1}, False:bool)
          outputs: (%340:<5120x3072xbf16>{3072,1})
          duration: -1
        - aten::copy_:
          inputs: (%252:<6144x5120xbf16>{5120, 1}, %337:<6144x5120xf32>{5120, 1}, False:bool)
          outputs: (%252:<6144x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%284:<102400x5120xbf16>{5120, 1}, %325:<102400x5120xf32>{5120, 1}, False:bool)
          outputs: (%284:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%947:<5120xbf16>{1}, %963:<5120xf32>{1}, False:bool)
          outputs: (%947:<5120xbf16>{1})
          duration: -1
        - aten::copy_:
          inputs: (%1617:<5120xbf16>{1}, %1616:<5120xf32>{1}, False:bool)
          outputs: (%1617:<5120xbf16>{1})
          duration: -1
        - aten::copy_:
          inputs: (%999:<5120xbf16>{1}, %772:<5120xf32>{1}, False:bool)
          outputs: (%999:<5120xbf16>{1})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::wrapper call:
          inputs: (%1620:tuple{NotSurpot:FusedAdam}, _function_FusedAdam_step_at_0_7f5650cb89d0_:function)
          outputs: (torch.2_3_0.wrapper(%1620:tuple{NotSurpot:FusedAdam},_function_FusedAdam_step_at_0_7f5650cb89d0_:function))
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.FusedAdam(None:NoneType,)
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%42:<1xi32>{1}, %1700:list{%1633:list{%429:<24576x1536xf32>{1536, 1}+488636416, %1467:<32768x512xf32>{512, 1}+471859200, %237:<5120x1536xf32>{1536, 1}+463994880, %958:<3072x5120xf32>{5120, 1}+448266240, %947:<5120x1536xf32>{1536, 1}+440401920, %1505:<3072x5120xf32>{5120, 1}+424673280, %1610:<5120x1536xf32>{1536, 1}+416808960, %1643:<3072x5120xf32>{5120, 1}+401080320, %991:<5120x1536xf32>{1536, 1}+393216000, %595:<3072x5120xf32>{5120, 1}+377487360, %1651:<5120x1536xf32>{1536, 1}+369623040, %1647:<3072x5120xf32>{5120, 1}+353894400, %832:<5120x1536xf32>{1536, 1}+346030080, %978:<3072x5120xf32>{5120, 1}+330301440, %1645:<5120x1536xf32>{1536, 1}+322437120, %1653:<3072x5120xf32>{5120, 1}+306708480, %1583:<5120x1536xf32>{1536, 1}+298844160, %1378:<3072x5120xf32>{5120, 1}+283115520, %1659:<5120x1536xf32>{1536, 1}+275251200, %1664:<3072x5120xf32>{5120, 1}+259522560, %332:<5120x1536xf32>{1536, 1}+251658240, %1363:<3072x5120xf32>{5120, 1}+235929600, %1670:<5120x1536xf32>{1536, 1}+228065280, %1497:<3072x5120xf32>{5120, 1}+212336640, %1271:<5120x1536xf32>{1536, 1}+204472320, %1675:<3072x5120xf32>{5120, 1}+188743680, %1388:<5120x1536xf32>{1536, 1}+180879360, %1084:<3072x5120xf32>{5120, 1}+165150720, %1679:<5120x1536xf32>{1536, 1}+157286400, %1685:<3072x5120xf32>{5120, 1}+141557760, %1688:<5120x1536xf32>{1536, 1}+133693440, %1138:<3072x5120xf32>{5120, 1}+117964800, %574:<5120x1536xf32>{1536, 1}+110100480, %1120:<3072x5120xf32>{5120, 1}+94371840, %1693:<5120x1536xf32>{1536, 1}+86507520, %1692:<3072x5120xf32>{5120, 1}+70778880, %1538:<5120x1536xf32>{1536, 1}+62914560, %600:<3072x5120xf32>{5120, 1}+47185920, %1149:<5120x1536xf32>{1536, 1}+39321600, %1078:<3072x5120xf32>{5120, 1}+23592960, %583:<5120x1536xf32>{1536, 1}+15728640, %908:<3072x5120xf32>{5120, 1}}, %1701:list{%284:<24576x1536xf32>{1536, 1}, %337:<32768x512xf32>{512, 1}, %344:<5120x1536xf32>{1536, 1}, %1618:<3072x5120xf32>{5120, 1}, %1491:<5120x1536xf32>{1536, 1}, %1586:<3072x5120xf32>{5120, 1}, %1584:<5120x1536xf32>{1536, 1}, %774:<3072x5120xf32>{5120, 1}, %820:<5120x1536xf32>{1536, 1}, %855:<3072x5120xf32>{5120, 1}, %1652:<5120x1536xf32>{1536, 1}, %807:<3072x5120xf32>{5120, 1}, %806:<5120x1536xf32>{1536, 1}, %1648:<3072x5120xf32>{5120, 1}, %1577:<5120x1536xf32>{1536, 1}, %1539:<3072x5120xf32>{5120, 1}, %1357:<5120x1536xf32>{1536, 1}, %1658:<3072x5120xf32>{5120, 1}, %1385:<5120x1536xf32>{1536, 1}, %1666:<3072x5120xf32>{5120, 1}, %1668:<5120x1536xf32>{1536, 1}, %1384:<3072x5120xf32>{5120, 1}, %478:<5120x1536xf32>{1536, 1}, %1672:<3072x5120xf32>{5120, 1}, %1673:<5120x1536xf32>{1536, 1}, %1214:<3072x5120xf32>{5120, 1}, %1676:<5120x1536xf32>{1536, 1}, %1678:<3072x5120xf32>{5120, 1}, %1682:<5120x1536xf32>{1536, 1}, %911:<3072x5120xf32>{5120, 1}, %1010:<5120x1536xf32>{1536, 1}, %1689:<3072x5120xf32>{5120, 1}, %1690:<5120x1536xf32>{1536, 1}, %1691:<3072x5120xf32>{5120, 1}, %1694:<5120x1536xf32>{1536, 1}, %1022:<3072x5120xf32>{5120, 1}, %1285:<5120x1536xf32>{1536, 1}, %1117:<3072x5120xf32>{5120, 1}, %1555:<5120x1536xf32>{1536, 1}, %1309:<3072x5120xf32>{5120, 1}, %883:<5120x1536xf32>{1536, 1}, %894:<3072x5120xf32>{5120, 1}}, %1702:list{%1607:<24576x1536xf32>{1536, 1}, %282:<32768x512xf32>{512, 1}, %1619:<5120x1536xf32>{1536, 1}, %1616:<3072x5120xf32>{5120, 1}, %1617:<5120x1536xf32>{1536, 1}, %885:<3072x5120xf32>{5120, 1}, %1303:<5120x1536xf32>{1536, 1}, %813:<3072x5120xf32>{5120, 1}, %1644:<5120x1536xf32>{1536, 1}, %823:<3072x5120xf32>{5120, 1}, %1649:<5120x1536xf32>{1536, 1}, %838:<3072x5120xf32>{5120, 1}, %810:<5120x1536xf32>{1536, 1}, %801:<3072x5120xf32>{5120, 1}, %828:<5120x1536xf32>{1536, 1}, %1358:<3072x5120xf32>{5120, 1}, %1654:<5120x1536xf32>{1536, 1}, %1557:<3072x5120xf32>{5120, 1}, %1660:<5120x1536xf32>{1536, 1}, %1656:<3072x5120xf32>{5120, 1}, %1542:<5120x1536xf32>{1536, 1}, %1669:<3072x5120xf32>{5120, 1}, %1671:<5120x1536xf32>{1536, 1}, %1372:<3072x5120xf32>{5120, 1}, %1114:<5120x1536xf32>{1536, 1}, %876:<3072x5120xf32>{5120, 1}, %1131:<5120x1536xf32>{1536, 1}, %1036:<3072x5120xf32>{5120, 1}, %1680:<5120x1536xf32>{1536, 1}, %1684:<3072x5120xf32>{5120, 1}, %888:<5120x1536xf32>{1536, 1}, %1686:<3072x5120xf32>{5120, 1}, %1153:<5120x1536xf32>{1536, 1}, %1225:<3072x5120xf32>{5120, 1}, %1173:<5120x1536xf32>{1536, 1}, %1013:<3072x5120xf32>{5120, 1}, %1254:<5120x1536xf32>{1536, 1}, %1142:<3072x5120xf32>{5120, 1}, %1699:<5120x1536xf32>{1536, 1}, %891:<3072x5120xf32>{5120, 1}, %904:<5120x1536xf32>{1536, 1}, %1209:<3072x5120xf32>{5120, 1}}, %1703:list{%325:<24576x1536xf32>{1536, 1}, %252:<32768x512xf32>{512, 1}, %340:<5120x1536xf32>{1536, 1}, %999:<3072x5120xf32>{5120, 1}, %1579:<5120x1536xf32>{1536, 1}, %1514:<3072x5120xf32>{5120, 1}, %1588:<5120x1536xf32>{1536, 1}, %1642:<3072x5120xf32>{5120, 1}, %917:<5120x1536xf32>{1536, 1}, %1646:<3072x5120xf32>{5120, 1}, %839:<5120x1536xf32>{1536, 1}, %869:<3072x5120xf32>{5120, 1}, %1650:<5120x1536xf32>{1536, 1}, %784:<3072x5120xf32>{5120, 1}, %1270:<5120x1536xf32>{1536, 1}, %1306:<3072x5120xf32>{5120, 1}, %1533:<5120x1536xf32>{1536, 1}, %1657:<3072x5120xf32>{5120, 1}, %1661:<5120x1536xf32>{1536, 1}, %1665:<3072x5120xf32>{5120, 1}, %1331:<5120x1536xf32>{1536, 1}, %1352:<3072x5120xf32>{5120, 1}, %1329:<5120x1536xf32>{1536, 1}, %1087:<3072x5120xf32>{5120, 1}, %1208:<5120x1536xf32>{1536, 1}, %1674:<3072x5120xf32>{5120, 1}, %898:<5120x1536xf32>{1536, 1}, %1677:<3072x5120xf32>{5120, 1}, %1681:<5120x1536xf32>{1536, 1}, %1683:<3072x5120xf32>{5120, 1}, %1099:<5120x1536xf32>{1536, 1}, %1032:<3072x5120xf32>{5120, 1}, %1121:<5120x1536xf32>{1536, 1}, %1184:<3072x5120xf32>{5120, 1}, %1042:<5120x1536xf32>{1536, 1}, %1697:<3072x5120xf32>{5120, 1}, %1295:<5120x1536xf32>{1536, 1}, %1247:<3072x5120xf32>{5120, 1}, %938:<5120x1536xf32>{1536, 1}, %873:<3072x5120xf32>{5120, 1}, %918:<5120x1536xf32>{1536, 1}, %1243:<3072x5120xf32>{5120, 1}}}, %1629:tuple{1e-05:float, 0_9:float, 0_95:float, 1e-08:float, 1:int, 1:int, 1:int, 0_1:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_adam_of_PyCapsule_object_at_0_7f5650c63030_:builtin_function_or_method,%42:<1xi32>{1},%1700:list{%1633:list{%429:<24576x1536xf32>{1536,1}+488636416,%1467:<32768x512xf32>{512,1}+471859200,%237:<5120x1536xf32>{1536,1}+463994880,%958:<3072x5120xf32>{5120,1}+448266240,%947:<5120x1536xf32>{1536,1}+440401920,%1505:<3072x5120xf32>{5120,1}+424673280,%1610:<5120x1536xf32>{1536,1}+416808960,%1643:<3072x5120xf32>{5120,1}+401080320,%991:<5120x1536xf32>{1536,1}+393216000,%595:<3072x5120xf32>{5120,1}+377487360,%1651:<5120x1536xf32>{1536,1}+369623040,%1647:<3072x5120xf32>{5120,1}+353894400,%832:<5120x1536xf32>{1536,1}+346030080,%978:<3072x5120xf32>{5120,1}+330301440,%1645:<5120x1536xf32>{1536,1}+322437120,%1653:<3072x5120xf32>{5120,1}+306708480,%1583:<5120x1536xf32>{1536,1}+298844160,%1378:<3072x5120xf32>{5120,1}+283115520,%1659:<5120x1536xf32>{1536,1}+275251200,%1664:<3072x5120xf32>{5120,1}+259522560,%332:<5120x1536xf32>{1536,1}+251658240,%1363:<3072x5120xf32>{5120,1}+235929600,%1670:<5120x1536xf32>{1536,1}+228065280,%1497:<3072x5120xf32>{5120,1}+212336640,%1271:<5120x1536xf32>{1536,1}+204472320,%1675:<3072x5120xf32>{5120,1}+188743680,%1388:<5120x1536xf32>{1536,1}+180879360,%1084:<3072x5120xf32>{5120,1}+165150720,%1679:<5120x1536xf32>{1536,1}+157286400,%1685:<3072x5120xf32>{5120,1}+141557760,%1688:<5120x1536xf32>{1536,1}+133693440,%1138:<3072x5120xf32>{5120,1}+117964800,%574:<5120x1536xf32>{1536,1}+110100480,%1120:<3072x5120xf32>{5120,1}+94371840,%1693:<5120x1536xf32>{1536,1}+86507520,%1692:<3072x5120xf32>{5120,1}+70778880,%1538:<5120x1536xf32>{1536,1}+62914560,%600:<3072x5120xf32>{5120,1}+47185920,%1149:<5120x1536xf32>{1536,1}+39321600,%1078:<3072x5120xf32>{5120,1}+23592960,%583:<5120x1536xf32>{1536,1}+15728640,%908:<3072x5120xf32>{5120,1}},%1701:list{%284:<24576x1536xf32>{1536,1},%337:<32768x512xf32>{512,1},%344:<5120x1536xf32>{1536,1},%1618:<3072x5120xf32>{5120,1},%1491:<5120x1536xf32>{1536,1},%1586:<3072x5120xf32>{5120,1},%1584:<5120x1536xf32>{1536,1},%774:<3072x5120xf32>{5120,1},%820:<5120x1536xf32>{1536,1},%855:<3072x5120xf32>{5120,1},%1652:<5120x1536xf32>{1536,1},%807:<3072x5120xf32>{5120,1},%806:<5120x1536xf32>{1536,1},%1648:<3072x5120xf32>{5120,1},%1577:<5120x1536xf32>{1536,1},%1539:<3072x5120xf32>{5120,1},%1357:<5120x1536xf32>{1536,1},%1658:<3072x5120xf32>{5120,1},%1385:<5120x1536xf32>{1536,1},%1666:<3072x5120xf32>{5120,1},%1668:<5120x1536xf32>{1536,1},%1384:<3072x5120xf32>{5120,1},%478:<5120x1536xf32>{1536,1},%1672:<3072x5120xf32>{5120,1},%1673:<5120x1536xf32>{1536,1},%1214:<3072x5120xf32>{5120,1},%1676:<5120x1536xf32>{1536,1},%1678:<3072x5120xf32>{5120,1},%1682:<5120x1536xf32>{1536,1},%911:<3072x5120xf32>{5120,1},%1010:<5120x1536xf32>{1536,1},%1689:<3072x5120xf32>{5120,1},%1690:<5120x1536xf32>{1536,1},%1691:<3072x5120xf32>{5120,1},%1694:<5120x1536xf32>{1536,1},%1022:<3072x5120xf32>{5120,1},%1285:<5120x1536xf32>{1536,1},%1117:<3072x5120xf32>{5120,1},%1555:<5120x1536xf32>{1536,1},%1309:<3072x5120xf32>{5120,1},%883:<5120x1536xf32>{1536,1},%894:<3072x5120xf32>{5120,1}},%1702:list{%1607:<24576x1536xf32>{1536,1},%282:<32768x512xf32>{512,1},%1619:<5120x1536xf32>{1536,1},%1616:<3072x5120xf32>{5120,1},%1617:<5120x1536xf32>{1536,1},%885:<3072x5120xf32>{5120,1},%1303:<5120x1536xf32>{1536,1},%813:<3072x5120xf32>{5120,1},%1644:<5120x1536xf32>{1536,1},%823:<3072x5120xf32>{5120,1},%1649:<5120x1536xf32>{1536,1},%838:<3072x5120xf32>{5120,1},%810:<5120x1536xf32>{1536,1},%801:<3072x5120xf32>{5120,1},%828:<5120x1536xf32>{1536,1},%1358:<3072x5120xf32>{5120,1},%1654:<5120x1536xf32>{1536,1},%1557:<3072x5120xf32>{5120,1},%1660:<5120x1536xf32>{1536,1},%1656:<3072x5120xf32>{5120,1},%1542:<5120x1536xf32>{1536,1},%1669:<3072x5120xf32>{5120,1},%1671:<5120x1536xf32>{1536,1},%1372:<3072x5120xf32>{5120,1},%1114:<5120x1536xf32>{1536,1},%876:<3072x5120xf32>{5120,1},%1131:<5120x1536xf32>{1536,1},%1036:<3072x5120xf32>{5120,1},%1680:<5120x1536xf32>{1536,1},%1684:<3072x5120xf32>{5120,1},%888:<5120x1536xf32>{1536,1},%1686:<3072x5120xf32>{5120,1},%1153:<5120x1536xf32>{1536,1},%1225:<3072x5120xf32>{5120,1},%1173:<5120x1536xf32>{1536,1},%1013:<3072x5120xf32>{5120,1},%1254:<5120x1536xf32>{1536,1},%1142:<3072x5120xf32>{5120,1},%1699:<5120x1536xf32>{1536,1},%891:<3072x5120xf32>{5120,1},%904:<5120x1536xf32>{1536,1},%1209:<3072x5120xf32>{5120,1}},%1703:list{%325:<24576x1536xf32>{1536,1},%252:<32768x512xf32>{512,1},%340:<5120x1536xf32>{1536,1},%999:<3072x5120xf32>{5120,1},%1579:<5120x1536xf32>{1536,1},%1514:<3072x5120xf32>{5120,1},%1588:<5120x1536xf32>{1536,1},%1642:<3072x5120xf32>{5120,1},%917:<5120x1536xf32>{1536,1},%1646:<3072x5120xf32>{5120,1},%839:<5120x1536xf32>{1536,1},%869:<3072x5120xf32>{5120,1},%1650:<5120x1536xf32>{1536,1},%784:<3072x5120xf32>{5120,1},%1270:<5120x1536xf32>{1536,1},%1306:<3072x5120xf32>{5120,1},%1533:<5120x1536xf32>{1536,1},%1657:<3072x5120xf32>{5120,1},%1661:<5120x1536xf32>{1536,1},%1665:<3072x5120xf32>{5120,1},%1331:<5120x1536xf32>{1536,1},%1352:<3072x5120xf32>{5120,1},%1329:<5120x1536xf32>{1536,1},%1087:<3072x5120xf32>{5120,1},%1208:<5120x1536xf32>{1536,1},%1674:<3072x5120xf32>{5120,1},%898:<5120x1536xf32>{1536,1},%1677:<3072x5120xf32>{5120,1},%1681:<5120x1536xf32>{1536,1},%1683:<3072x5120xf32>{5120,1},%1099:<5120x1536xf32>{1536,1},%1032:<3072x5120xf32>{5120,1},%1121:<5120x1536xf32>{1536,1},%1184:<3072x5120xf32>{5120,1},%1042:<5120x1536xf32>{1536,1},%1697:<3072x5120xf32>{5120,1},%1295:<5120x1536xf32>{1536,1},%1247:<3072x5120xf32>{5120,1},%938:<5120x1536xf32>{1536,1},%873:<3072x5120xf32>{5120,1},%918:<5120x1536xf32>{1536,1},%1243:<3072x5120xf32>{5120,1}}},%1629:tuple{1e-05:float,0_9:float,0_95:float,1e-08:float,1:int,1:int,1:int,0_1:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%1633:list{%879:<24576x1536xf32>{1536, 1}, %978:<32768x512xf32>{512, 1}, %595:<5120x1536xf32>{1536, 1}, %1453:<3072x5120xf32>{5120, 1}, %947:<5120x1536xf32>{1536, 1}, %478:<3072x5120xf32>{5120, 1}, %585:<5120x1536xf32>{1536, 1}, %237:<3072x5120xf32>{5120, 1}, %741:<5120x1536xf32>{1536, 1}, %971:<3072x5120xf32>{5120, 1}, %344:<5120x1536xf32>{1536, 1}, %284:<3072x5120xf32>{5120, 1}, %855:<5120x1536xf32>{1536, 1}, %1652:<3072x5120xf32>{5120, 1}, %807:<5120x1536xf32>{1536, 1}, %1648:<3072x5120xf32>{5120, 1}, %1553:<5120x1536xf32>{1536, 1}, %1645:<3072x5120xf32>{5120, 1}, %583:<5120x1536xf32>{1536, 1}, %1707:<3072x5120xf32>{5120, 1}, %1324:<5120x1536xf32>{1536, 1}, %1385:<3072x5120xf32>{5120, 1}, %1668:<5120x1536xf32>{1536, 1}, %1378:<3072x5120xf32>{5120, 1}, %1708:<5120x1536xf32>{1536, 1}, %1670:<3072x5120xf32>{5120, 1}, %796:<5120x1536xf32>{1536, 1}, %1577:<3072x5120xf32>{5120, 1}, %1491:<5120x1536xf32>{1536, 1}, %1709:<3072x5120xf32>{5120, 1}, %1711:<5120x1536xf32>{1536, 1}, %1653:<3072x5120xf32>{5120, 1}, %1659:<5120x1536xf32>{1536, 1}, %1214:<3072x5120xf32>{5120, 1}, %1676:<5120x1536xf32>{1536, 1}, %1678:<3072x5120xf32>{5120, 1}, %911:<5120x1536xf32>{1536, 1}, %1689:<3072x5120xf32>{5120, 1}, %1691:<5120x1536xf32>{1536, 1}, %1022:<3072x5120xf32>{5120, 1}, %1117:<5120x1536xf32>{1536, 1}, %1309:<3072x5120xf32>{5120, 1}}, %1590:list{%315:<24576x1536xf32>{1536, 1}, %316:<32768x512xf32>{512, 1}, %317:<5120x1536xf32>{1536, 1}, %320:<3072x5120xf32>{5120, 1}, %321:<5120x1536xf32>{1536, 1}, %257:<3072x5120xf32>{5120, 1}, %323:<5120x1536xf32>{1536, 1}, %248:<3072x5120xf32>{5120, 1}, %328:<5120x1536xf32>{1536, 1}, %330:<3072x5120xf32>{5120, 1}, %307:<5120x1536xf32>{1536, 1}, %298:<3072x5120xf32>{5120, 1}, %251:<5120x1536xf32>{1536, 1}, %333:<3072x5120xf32>{5120, 1}, %272:<5120x1536xf32>{1536, 1}, %335:<3072x5120xf32>{5120, 1}, %338:<5120x1536xf32>{1536, 1}, %267:<3072x5120xf32>{5120, 1}, %341:<5120x1536xf32>{1536, 1}, %342:<3072x5120xf32>{5120, 1}, %343:<5120x1536xf32>{1536, 1}, %346:<3072x5120xf32>{5120, 1}, %348:<5120x1536xf32>{1536, 1}, %327:<3072x5120xf32>{5120, 1}, %305:<5120x1536xf32>{1536, 1}, %352:<3072x5120xf32>{5120, 1}, %355:<5120x1536xf32>{1536, 1}, %324:<3072x5120xf32>{5120, 1}, %357:<5120x1536xf32>{1536, 1}, %349:<3072x5120xf32>{5120, 1}, %268:<5120x1536xf32>{1536, 1}, %354:<3072x5120xf32>{5120, 1}, %366:<5120x1536xf32>{1536, 1}, %368:<3072x5120xf32>{5120, 1}, %371:<5120x1536xf32>{1536, 1}, %373:<3072x5120xf32>{5120, 1}, %375:<5120x1536xf32>{1536, 1}, %379:<3072x5120xf32>{5120, 1}, %383:<5120x1536xf32>{1536, 1}, %381:<3072x5120xf32>{5120, 1}, %255:<5120x1536xf32>{1536, 1}, %360:<3072x5120xf32>{5120, 1}}, %360:<3072x5120xf32>{5120, 1})
          outputs: (%1713:tuple{%1702:list{%958:<24576x1536xbf16>{1536,1},%1651:<32768x512xbf16>{512,1},%1084:<5120x1536xbf16>{1536,1},%991:<3072x5120xbf16>{5120,1},%1618:<5120x1536xbf16>{1536,1},%429:<3072x5120xbf16>{5120,1},%574:<5120x1536xbf16>{1536,1},%1497:<3072x5120xbf16>{5120,1},%332:<5120x1536xbf16>{1536,1},%600:<3072x5120xbf16>{5120,1},%997:<5120x1536xbf16>{1536,1},%337:<3072x5120xbf16>{5120,1},%970:<5120x1536xbf16>{1536,1},%820:<3072x5120xbf16>{5120,1},%786:<5120x1536xbf16>{1536,1},%806:<3072x5120xbf16>{5120,1},%1647:<5120x1536xbf16>{1536,1},%1641:<3072x5120xbf16>{5120,1},%832:<5120x1536xbf16>{1536,1},%1539:<3072x5120xbf16>{5120,1},%1357:<5120x1536xbf16>{1536,1},%1658:<3072x5120xbf16>{5120,1},%1666:<5120x1536xbf16>{1536,1},%1384:<3072x5120xbf16>{5120,1},%1664:<5120x1536xbf16>{1536,1},%1388:<3072x5120xbf16>{5120,1},%1363:<5120x1536xbf16>{1536,1},%1505:<3072x5120xbf16>{5120,1},%1643:<5120x1536xbf16>{1536,1},%1528:<3072x5120xbf16>{5120,1},%1710:<5120x1536xbf16>{1536,1},%731:<3072x5120xbf16>{5120,1},%1712:<5120x1536xbf16>{1536,1},%1609:<3072x5120xbf16>{5120,1},%1673:<5120x1536xbf16>{1536,1},%1672:<3072x5120xbf16>{5120,1},%1682:<5120x1536xbf16>{1536,1},%1010:<3072x5120xbf16>{5120,1},%1690:<5120x1536xbf16>{1536,1},%1694:<3072x5120xbf16>{5120,1},%1285:<5120x1536xbf16>{1536,1},%1555:<3072x5120xbf16>{5120,1}},%1633:list{%879:<24576x1536xf32>{1536,1},%978:<32768x512xf32>{512,1},%595:<5120x1536xf32>{1536,1},%1453:<3072x5120xf32>{5120,1},%947:<5120x1536xf32>{1536,1},%478:<3072x5120xf32>{5120,1},%585:<5120x1536xf32>{1536,1},%237:<3072x5120xf32>{5120,1},%741:<5120x1536xf32>{1536,1},%971:<3072x5120xf32>{5120,1},%344:<5120x1536xf32>{1536,1},%284:<3072x5120xf32>{5120,1},%855:<5120x1536xf32>{1536,1},%1652:<3072x5120xf32>{5120,1},%807:<5120x1536xf32>{1536,1},%1648:<3072x5120xf32>{5120,1},%1553:<5120x1536xf32>{1536,1},%1645:<3072x5120xf32>{5120,1},%583:<5120x1536xf32>{1536,1},%1707:<3072x5120xf32>{5120,1},%1324:<5120x1536xf32>{1536,1},%1385:<3072x5120xf32>{5120,1},%1668:<5120x1536xf32>{1536,1},%1378:<3072x5120xf32>{5120,1},%1708:<5120x1536xf32>{1536,1},%1670:<3072x5120xf32>{5120,1},%796:<5120x1536xf32>{1536,1},%1577:<3072x5120xf32>{5120,1},%1491:<5120x1536xf32>{1536,1},%1709:<3072x5120xf32>{5120,1},%1711:<5120x1536xf32>{1536,1},%1653:<3072x5120xf32>{5120,1},%1659:<5120x1536xf32>{1536,1},%1214:<3072x5120xf32>{5120,1},%1676:<5120x1536xf32>{1536,1},%1678:<3072x5120xf32>{5120,1},%911:<5120x1536xf32>{1536,1},%1689:<3072x5120xf32>{5120,1},%1691:<5120x1536xf32>{1536,1},%1022:<3072x5120xf32>{5120,1},%1117:<5120x1536xf32>{1536,1},%1309:<3072x5120xf32>{5120,1}}})
          duration: -1
        - ----------->api::_multi_tensor_copy_this_to_that call:
          inputs: (%1633:list{%879:<24576x1536xf32>{1536, 1}, %978:<32768x512xf32>{512, 1}, %595:<5120x1536xf32>{1536, 1}, %1453:<3072x5120xf32>{5120, 1}, %947:<5120x1536xf32>{1536, 1}, %478:<3072x5120xf32>{5120, 1}, %585:<5120x1536xf32>{1536, 1}, %237:<3072x5120xf32>{5120, 1}, %741:<5120x1536xf32>{1536, 1}, %971:<3072x5120xf32>{5120, 1}, %344:<5120x1536xf32>{1536, 1}, %284:<3072x5120xf32>{5120, 1}, %855:<5120x1536xf32>{1536, 1}, %1652:<3072x5120xf32>{5120, 1}, %807:<5120x1536xf32>{1536, 1}, %1648:<3072x5120xf32>{5120, 1}, %1553:<5120x1536xf32>{1536, 1}, %1645:<3072x5120xf32>{5120, 1}, %583:<5120x1536xf32>{1536, 1}, %1707:<3072x5120xf32>{5120, 1}, %1324:<5120x1536xf32>{1536, 1}, %1385:<3072x5120xf32>{5120, 1}, %1668:<5120x1536xf32>{1536, 1}, %1378:<3072x5120xf32>{5120, 1}, %1708:<5120x1536xf32>{1536, 1}, %1670:<3072x5120xf32>{5120, 1}, %796:<5120x1536xf32>{1536, 1}, %1577:<3072x5120xf32>{5120, 1}, %1491:<5120x1536xf32>{1536, 1}, %1709:<3072x5120xf32>{5120, 1}, %1711:<5120x1536xf32>{1536, 1}, %1653:<3072x5120xf32>{5120, 1}, %1659:<5120x1536xf32>{1536, 1}, %1214:<3072x5120xf32>{5120, 1}, %1676:<5120x1536xf32>{1536, 1}, %1678:<3072x5120xf32>{5120, 1}, %911:<5120x1536xf32>{1536, 1}, %1689:<3072x5120xf32>{5120, 1}, %1691:<5120x1536xf32>{1536, 1}, %1022:<3072x5120xf32>{5120, 1}, %1117:<5120x1536xf32>{1536, 1}, %1309:<3072x5120xf32>{5120, 1}}, %1702:list{%958:<24576x1536xbf16>{1536, 1}, %1651:<32768x512xbf16>{512, 1}, %1084:<5120x1536xbf16>{1536, 1}, %991:<3072x5120xbf16>{5120, 1}, %1618:<5120x1536xbf16>{1536, 1}, %429:<3072x5120xbf16>{5120, 1}, %574:<5120x1536xbf16>{1536, 1}, %1497:<3072x5120xbf16>{5120, 1}, %332:<5120x1536xbf16>{1536, 1}, %600:<3072x5120xbf16>{5120, 1}, %997:<5120x1536xbf16>{1536, 1}, %337:<3072x5120xbf16>{5120, 1}, %970:<5120x1536xbf16>{1536, 1}, %820:<3072x5120xbf16>{5120, 1}, %786:<5120x1536xbf16>{1536, 1}, %806:<3072x5120xbf16>{5120, 1}, %1647:<5120x1536xbf16>{1536, 1}, %1641:<3072x5120xbf16>{5120, 1}, %832:<5120x1536xbf16>{1536, 1}, %1539:<3072x5120xbf16>{5120, 1}, %1357:<5120x1536xbf16>{1536, 1}, %1658:<3072x5120xbf16>{5120, 1}, %1666:<5120x1536xbf16>{1536, 1}, %1384:<3072x5120xbf16>{5120, 1}, %1664:<5120x1536xbf16>{1536, 1}, %1388:<3072x5120xbf16>{5120, 1}, %1363:<5120x1536xbf16>{1536, 1}, %1505:<3072x5120xbf16>{5120, 1}, %1643:<5120x1536xbf16>{1536, 1}, %1528:<3072x5120xbf16>{5120, 1}, %1710:<5120x1536xbf16>{1536, 1}, %731:<3072x5120xbf16>{5120, 1}, %1712:<5120x1536xbf16>{1536, 1}, %1609:<3072x5120xbf16>{5120, 1}, %1673:<5120x1536xbf16>{1536, 1}, %1672:<3072x5120xbf16>{5120, 1}, %1682:<5120x1536xbf16>{1536, 1}, %1010:<3072x5120xbf16>{5120, 1}, %1690:<5120x1536xbf16>{1536, 1}, %1694:<3072x5120xbf16>{5120, 1}, %1285:<5120x1536xbf16>{1536, 1}, %1555:<3072x5120xbf16>{5120, 1}}, None:NoneType)
          outputs: (torch.2_3_0._multi_tensor_copy_this_to_that(%1633:list{%879:<24576x1536xf32>{1536,1},%978:<32768x512xf32>{512,1},%595:<5120x1536xf32>{1536,1},%1453:<3072x5120xf32>{5120,1},%947:<5120x1536xf32>{1536,1},%478:<3072x5120xf32>{5120,1},%585:<5120x1536xf32>{1536,1},%237:<3072x5120xf32>{5120,1},%741:<5120x1536xf32>{1536,1},%971:<3072x5120xf32>{5120,1},%344:<5120x1536xf32>{1536,1},%284:<3072x5120xf32>{5120,1},%855:<5120x1536xf32>{1536,1},%1652:<3072x5120xf32>{5120,1},%807:<5120x1536xf32>{1536,1},%1648:<3072x5120xf32>{5120,1},%1553:<5120x1536xf32>{1536,1},%1645:<3072x5120xf32>{5120,1},%583:<5120x1536xf32>{1536,1},%1707:<3072x5120xf32>{5120,1},%1324:<5120x1536xf32>{1536,1},%1385:<3072x5120xf32>{5120,1},%1668:<5120x1536xf32>{1536,1},%1378:<3072x5120xf32>{5120,1},%1708:<5120x1536xf32>{1536,1},%1670:<3072x5120xf32>{5120,1},%796:<5120x1536xf32>{1536,1},%1577:<3072x5120xf32>{5120,1},%1491:<5120x1536xf32>{1536,1},%1709:<3072x5120xf32>{5120,1},%1711:<5120x1536xf32>{1536,1},%1653:<3072x5120xf32>{5120,1},%1659:<5120x1536xf32>{1536,1},%1214:<3072x5120xf32>{5120,1},%1676:<5120x1536xf32>{1536,1},%1678:<3072x5120xf32>{5120,1},%911:<5120x1536xf32>{1536,1},%1689:<3072x5120xf32>{5120,1},%1691:<5120x1536xf32>{1536,1},%1022:<3072x5120xf32>{5120,1},%1117:<5120x1536xf32>{1536,1},%1309:<3072x5120xf32>{5120,1}},%1702:list{%958:<24576x1536xbf16>{1536,1},%1651:<32768x512xbf16>{512,1},%1084:<5120x1536xbf16>{1536,1},%991:<3072x5120xbf16>{5120,1},%1618:<5120x1536xbf16>{1536,1},%429:<3072x5120xbf16>{5120,1},%574:<5120x1536xbf16>{1536,1},%1497:<3072x5120xbf16>{5120,1},%332:<5120x1536xbf16>{1536,1},%600:<3072x5120xbf16>{5120,1},%997:<5120x1536xbf16>{1536,1},%337:<3072x5120xbf16>{5120,1},%970:<5120x1536xbf16>{1536,1},%820:<3072x5120xbf16>{5120,1},%786:<5120x1536xbf16>{1536,1},%806:<3072x5120xbf16>{5120,1},%1647:<5120x1536xbf16>{1536,1},%1641:<3072x5120xbf16>{5120,1},%832:<5120x1536xbf16>{1536,1},%1539:<3072x5120xbf16>{5120,1},%1357:<5120x1536xbf16>{1536,1},%1658:<3072x5120xbf16>{5120,1},%1666:<5120x1536xbf16>{1536,1},%1384:<3072x5120xbf16>{5120,1},%1664:<5120x1536xbf16>{1536,1},%1388:<3072x5120xbf16>{5120,1},%1363:<5120x1536xbf16>{1536,1},%1505:<3072x5120xbf16>{5120,1},%1643:<5120x1536xbf16>{1536,1},%1528:<3072x5120xbf16>{5120,1},%1710:<5120x1536xbf16>{1536,1},%731:<3072x5120xbf16>{5120,1},%1712:<5120x1536xbf16>{1536,1},%1609:<3072x5120xbf16>{5120,1},%1673:<5120x1536xbf16>{1536,1},%1672:<3072x5120xbf16>{5120,1},%1682:<5120x1536xbf16>{1536,1},%1010:<3072x5120xbf16>{5120,1},%1690:<5120x1536xbf16>{1536,1},%1694:<3072x5120xbf16>{5120,1},%1285:<5120x1536xbf16>{1536,1},%1555:<3072x5120xbf16>{5120,1}},None:NoneType))
          duration: -1
        - aten::copy_:
          inputs: (%958:<24576x1536xbf16>{1536, 1}, %879:<24576x1536xf32>{1536, 1}, False:bool)
          outputs: (%958:<24576x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1651:<32768x512xbf16>{512, 1}, %978:<32768x512xf32>{512, 1}, False:bool)
          outputs: (%1651:<32768x512xbf16>{512,1})
          duration: -1
        - aten::copy_:
          inputs: (%1084:<5120x1536xbf16>{1536, 1}, %595:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1084:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%991:<3072x5120xbf16>{5120, 1}, %1453:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%991:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1618:<5120x1536xbf16>{1536, 1}, %947:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1618:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%429:<3072x5120xbf16>{5120, 1}, %478:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%429:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%574:<5120x1536xbf16>{1536, 1}, %585:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%574:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1497:<3072x5120xbf16>{5120, 1}, %237:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1497:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%332:<5120x1536xbf16>{1536, 1}, %741:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%332:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%600:<3072x5120xbf16>{5120, 1}, %971:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%600:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%997:<5120x1536xbf16>{1536, 1}, %344:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%997:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%337:<3072x5120xbf16>{5120, 1}, %284:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%337:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%970:<5120x1536xbf16>{1536, 1}, %855:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%970:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%820:<3072x5120xbf16>{5120, 1}, %1652:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%820:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%786:<5120x1536xbf16>{1536, 1}, %807:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%786:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%806:<3072x5120xbf16>{5120, 1}, %1648:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%806:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1647:<5120x1536xbf16>{1536, 1}, %1553:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1647:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1641:<3072x5120xbf16>{5120, 1}, %1645:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1641:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%832:<5120x1536xbf16>{1536, 1}, %583:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%832:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1539:<3072x5120xbf16>{5120, 1}, %1707:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1539:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1357:<5120x1536xbf16>{1536, 1}, %1324:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1357:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1658:<3072x5120xbf16>{5120, 1}, %1385:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1658:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1666:<5120x1536xbf16>{1536, 1}, %1668:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1666:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1384:<3072x5120xbf16>{5120, 1}, %1378:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1384:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1664:<5120x1536xbf16>{1536, 1}, %1708:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1664:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<3072x5120xbf16>{5120, 1}, %1670:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1388:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1363:<5120x1536xbf16>{1536, 1}, %796:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1363:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1505:<3072x5120xbf16>{5120, 1}, %1577:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1505:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1643:<5120x1536xbf16>{1536, 1}, %1491:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1643:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1528:<3072x5120xbf16>{5120, 1}, %1709:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1528:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1710:<5120x1536xbf16>{1536, 1}, %1711:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1710:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%731:<3072x5120xbf16>{5120, 1}, %1653:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%731:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1712:<5120x1536xbf16>{1536, 1}, %1659:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1712:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1609:<3072x5120xbf16>{5120, 1}, %1214:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1609:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1673:<5120x1536xbf16>{1536, 1}, %1676:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1673:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1672:<3072x5120xbf16>{5120, 1}, %1678:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1672:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1682:<5120x1536xbf16>{1536, 1}, %911:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1682:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1010:<3072x5120xbf16>{5120, 1}, %1689:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1010:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1690:<5120x1536xbf16>{1536, 1}, %1691:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1690:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1694:<3072x5120xbf16>{5120, 1}, %1022:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1694:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1285:<5120x1536xbf16>{1536, 1}, %1117:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1285:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1555:<3072x5120xbf16>{5120, 1}, %1309:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1555:<3072x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (True:bool, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (True:bool)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (%1604:list{NotSurpot:dict, _'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6')],_'wd_mult'__0_0,_'lr_mult'__1_0,_'is_e_pert_parallel'__False,_'is_decoupled_lr'__False,_'ma__lr'__1e-05,_'min_lr'__1e-06,_'lr'__1e-05,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_0,_'step'__1_:dict, NotSurpot:dict}, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (%1604:list{NotSurpot:dict,_'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device)
          duration: -1
        - aten::add:
          inputs: (%1479:<i32>, 0:int, alpha=1:int)
          outputs: (%978:<i32>)
          duration: -1
        - aten::div:
          inputs: (%978:<i32>, 1:int)
          outputs: (%1497:<i32>)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (%1633:list{NotSurpot:dict, _'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6')],_'wd_mult'__0_0,_'lr_mult'__1_0,_'is_e_pert_parallel'__False,_'is_decoupled_lr'__False,_'ma__lr'__1e-05,_'min_lr'__1e-06,_'lr'__5_5e-06,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_0,_'step'__1_:dict, NotSurpot:dict}, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (%1633:list{NotSurpot:dict,_'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device)
          duration: -1
        - aten::add:
          inputs: (%741:<1xf32>{1}, %1497:<i32>, alpha=1:int)
          outputs: (%970:<1xf32>{1})
          duration: -1
        - aten::zeros:
          inputs: (%1634:list{8:int, 24:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%978:<8x24xf32>{24,1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%595:<192xf32>{1}, %1584:<24xf32>{1}+144, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%595:<192xf32>{1},%1584:<24xf32>{1}+144,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%595:<192xf32>{1}, %1584:<24xf32>{1}+144, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%732:tuple{%595:<192xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gt:
          inputs: (%963:<8xf32>{24}, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%963:<8xf32>{24}, %1704:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1453:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%963:<8xf32>{24}+1, 0_0:float)
          outputs: (%772:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%963:<8xf32>{24}+1, %1714:list{%772:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+2, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+2, %1704:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+3, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+3, %1714:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+4, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+4, %1704:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+5, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+5, %1714:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+6, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+6, %1704:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+7, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+7, %1714:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+8, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+8, %1704:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+9, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+9, %1714:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+10, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+10, %1704:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+11, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+11, %1714:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+12, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+12, %1704:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+13, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+13, %1714:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+14, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+14, %1704:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+15, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+15, %1714:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+16, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+16, %1704:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+17, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+17, %1714:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+18, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+18, %1704:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+19, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+19, %1714:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+20, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+20, %1704:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1467:<8xf32>{24}+21, 0_0:float)
          outputs: (%1453:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1467:<8xf32>{24}+21, %1714:list{%1453:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%879:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1584:<8xf32>{24}+22, 0_0:float)
          outputs: (%1586:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1584:<8xf32>{24}+22, %1704:list{%1586:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1467:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%879:<8xf32>{24}+23, 0_0:float)
          outputs: (%595:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%879:<8xf32>{24}+23, %1714:list{%595:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1584:<0xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%746:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2530_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%746:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2530_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1714:list{%746:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%751:tuple{%1715:list{%746:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::zero_:
          inputs: (%746:<1xf32>{1})
          outputs: (%746:<1xf32>{1})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%1467:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %1715:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%879:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %1716:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::zeros:
          inputs: (%1068:list{8:int, 24:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%574:<8x24xf32>{24,1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%276:<192xf32>{1}, %1540:<24xf32>{1}+144, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%276:<192xf32>{1},%1540:<24xf32>{1}+144,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%276:<192xf32>{1}, %1540:<24xf32>{1}+144, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%1717:tuple{%276:<192xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gt:
          inputs: (%926:<8xf32>{24}, 0_0:float)
          outputs: (%276:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%926:<8xf32>{24}, %1704:list{%276:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%600:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1540:<8xf32>{24}+1, 0_0:float)
          outputs: (%237:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1540:<8xf32>{24}+1, %1624:list{%237:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%600:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1718:<8xf32>{24}+2, 0_0:float)
          outputs: (%276:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1718:<8xf32>{24}+2, %1704:list{%276:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%276:<8xf32>{24}+3, 0_0:float)
          outputs: (%237:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%276:<8xf32>{24}+3, %1624:list{%237:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%970:<8xf32>{24}+4, 0_0:float)
          outputs: (%600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%970:<8xf32>{24}+4, %1704:list{%600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%600:<8xf32>{24}+5, 0_0:float)
          outputs: (%237:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%600:<8xf32>{24}+5, %1624:list{%237:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1698:<8xf32>{24}+6, 0_0:float)
          outputs: (%926:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1698:<8xf32>{24}+6, %1704:list{%926:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%926:<8xf32>{24}+7, 0_0:float)
          outputs: (%237:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%926:<8xf32>{24}+7, %1624:list{%237:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%237:<8xf32>{24}+8, 0_0:float)
          outputs: (%600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%237:<8xf32>{24}+8, %1704:list{%600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%600:<8xf32>{24}+9, 0_0:float)
          outputs: (%970:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%600:<8xf32>{24}+9, %1624:list{%970:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%600:<8xf32>{24}+10, 0_0:float)
          outputs: (%1600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%600:<8xf32>{24}+10, %1704:list{%1600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1698:<8xf32>{24}+11, 0_0:float)
          outputs: (%926:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1698:<8xf32>{24}+11, %1624:list{%926:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1698:<8xf32>{24}+12, 0_0:float)
          outputs: (%1600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1698:<8xf32>{24}+12, %1704:list{%1600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1600:<8xf32>{24}+13, 0_0:float)
          outputs: (%970:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1600:<8xf32>{24}+13, %1624:list{%970:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%970:<8xf32>{24}+14, 0_0:float)
          outputs: (%600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%970:<8xf32>{24}+14, %1704:list{%600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%600:<8xf32>{24}+15, 0_0:float)
          outputs: (%237:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%600:<8xf32>{24}+15, %1624:list{%237:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1698:<8xf32>{24}+16, 0_0:float)
          outputs: (%926:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1698:<8xf32>{24}+16, %1704:list{%926:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%926:<8xf32>{24}+17, 0_0:float)
          outputs: (%237:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%926:<8xf32>{24}+17, %1624:list{%237:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%237:<8xf32>{24}+18, 0_0:float)
          outputs: (%600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%237:<8xf32>{24}+18, %1704:list{%600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%600:<8xf32>{24}+19, 0_0:float)
          outputs: (%970:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%600:<8xf32>{24}+19, %1624:list{%970:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%600:<8xf32>{24}+20, 0_0:float)
          outputs: (%1600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%600:<8xf32>{24}+20, %1704:list{%1600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1698:<8xf32>{24}+21, 0_0:float)
          outputs: (%926:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1698:<8xf32>{24}+21, %1624:list{%926:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1698:<8xf32>{24}+22, 0_0:float)
          outputs: (%1600:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1698:<8xf32>{24}+22, %1704:list{%1600:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1600:<8xf32>{24}+23, 0_0:float)
          outputs: (%970:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1600:<8xf32>{24}+23, %1624:list{%970:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1540:<0xf32>{1})
          duration: -1
        - aten::zero_:
          inputs: (%170:<1191296000xf32>{1})
          outputs: (%170:<1191296000xf32>{1})
          duration: -1
        - aten::zero_:
          inputs: (%178:<526385152xf32>{1})
          outputs: (%178:<526385152xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (True:bool)
          outputs: (torch.2_3_0.ChainedOptimizer(True:bool))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (True:bool)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(True:bool))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: (True:bool)
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params(True:bool))
          duration: -1
        - aten::zeros:
          inputs: (%1719:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%237:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%438:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%413:<1024xf32>{1}, %442:list{%438:<1024xCUSTOM_DATA_TYPE>{1}}, %432:<i32>, False:bool)
          outputs: (%413:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%441:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%439:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%441:<1024xCUSTOM_DATA_TYPE>{1}, %442:list{%439:<1024xCUSTOM_DATA_TYPE>{1}}, %432:<i32>, False:bool)
          outputs: (%441:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1, %442:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %403:<i32>, False:bool)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%1727:list{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1728:tuple{%1551:list{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%775:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%775:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%1729:list{%775:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1730:tuple{%1719:list{%775:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1695:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1695:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%1727:list{%1695:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1717:tuple{%1068:list{%1695:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%1729:list{%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1728:tuple{%1731:list{%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1726:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%1732:list{%1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1730:tuple{%1551:list{%1726:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%1733:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%1733:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1726:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%1734:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%1734:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1726:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1726:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%1735:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%1735:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1726:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__185,_21118,___279,_____,___338,__4345,_12143]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[__185,_21118,___279,_____,___338,__4345,_12143]],_device)
          duration: -1
122943 2024-12-10 17:48:24.121155 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n1,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%721:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%721:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
122955 2024-12-10 17:48:24.121895 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n1,rank6)
        - ----------->api::embedding call:
          inputs: (%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%1721:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%1736:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%1736:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
123136 2024-12-10 17:48:24.136489 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n1,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%721:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%1741:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%721:tuple{%758:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%721:tuple{%758:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
123163 2024-12-10 17:48:24.140272 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n1,rank6)
        - ----------->api::dropout call:
          inputs: (%1160:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%1160:<1024x1x5120xbf16>{5120,5120,1},0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%1160:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, True:bool, False:bool)
          outputs: (%1160:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
123296 2024-12-10 17:48:24.148144 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n1,rank6)
        - ----------->api::Dropout return:
          inputs: (%721:tuple{%758:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%1318:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
123332 2024-12-10 17:48:24.150603 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n1,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__185,_21118,___279,_____,___338,__4345,_12143]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%1741:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%721:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%721:tuple{1024:int}))
          duration: -1
123383 2024-12-10 17:48:24.153917 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n1,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1748:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%1748:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%1504:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%842:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%1749:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%1731:list{%1749:<1024x20xf32>{20, 1}, %1749:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%1520:<1024x40xf32>{40,1})
          duration: -1
123488 2024-12-10 17:48:24.164669 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n1,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%721:tuple{1024:int})
          outputs: (%1751:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
123547 2024-12-10 17:48:24.170947 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n1,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
123646 2024-12-10 17:48:24.180247 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n1,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%721:tuple{%1318:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%721:tuple{%1318:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
123653 2024-12-10 17:48:24.180970 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n3,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1756:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - aten::stack:
          inputs: (%443:list{%441:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%434:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%434:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1756:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%1759:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%1759:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%1759:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1759:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%1763:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, %1762:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%1763:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1},%33:<5120xbf16>{1},%1762:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %1759:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1760:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1760:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
123944 2024-12-10 17:48:24.226518 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n3,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%721:tuple{%1765:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.SelfAttention(%721:tuple{%1765:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
124088 2024-12-10 17:48:24.232874 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n1,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1770:tuple{%1540:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1770:tuple{%1540:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
124138 2024-12-10 17:48:24.235395 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n26,rank6)
        - aten::mm:
          inputs: (%1779:<1024x5120xbf16>{5120, 1}, %1777:<5120x1536xbf16>{1, 5120})
          outputs: (%1780:<1024x1536xbf16>{1536,1})
          duration: -1
124346 2024-12-10 17:48:24.245957 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n26,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1770:tuple{%1540:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1776:tuple{%1785:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1770:tuple{%1785:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1770:tuple{%1785:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
124381 2024-12-10 17:48:24.248700 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n27,rank6)
        - aten::mm:
          inputs: (%1794:<1024x1536xbf16>{1536, 1}, %1792:<1536x24576xbf16>{1, 1536})
          outputs: (%735:<1024x24576xbf16>{24576,1})
          duration: -1
124494 2024-12-10 17:48:24.257021 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n27,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1770:tuple{%1785:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%1791:tuple{%1798:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%1785:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %1801:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%1785:<1024x1x128x192xbf16>{24576,24576,192,1},%1801:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%1785:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %1801:list{128:int, 64:int}, -1:int)
          outputs: (%1776:tuple{%1803:<1024x1x128x128xbf16>{24576,24576,192,1},%1798:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1770:tuple{%1540:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1770:tuple{%1540:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
124771 2024-12-10 17:48:24.275355 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n28,rank6)
        - aten::mm:
          inputs: (%1811:<1024x5120xbf16>{5120, 1}, %1809:<5120x576xbf16>{1, 5120})
          outputs: (%1812:<1024x576xbf16>{576,1})
          duration: -1
124920 2024-12-10 17:48:24.284954 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n28,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1770:tuple{%1540:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1808:tuple{%1817:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%1817:<1024x1x576xbf16>{576, 576, 1}, %1058:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%1817:<1024x1x576xbf16>{576,576,1},%1058:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%1817:<1024x1x576xbf16>{576, 576, 1}, %1058:list{512:int, 64:int}, -1:int)
          outputs: (%1822:tuple{%1750:<1024x1x512xbf16>{576,576,1},%1792:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1770:tuple{%1750:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1770:tuple{%1750:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
125053 2024-12-10 17:48:24.294826 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n29,rank6)
        - aten::mm:
          inputs: (%737:<1024x512xbf16>{576, 1}, %1826:<512x32768xbf16>{1, 512})
          outputs: (%1809:<1024x32768xbf16>{32768,1})
          duration: -1
125145 2024-12-10 17:48:24.303105 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n29,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1770:tuple{%1750:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%1791:tuple{%1826:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%1276:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %1833:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%1276:<1024x1x128x256xbf16>{32768,32768,256,1},%1833:list{128:int,128:int},-1:int))
          duration: -1
        - aten::stack:
          inputs: (%444:list{%428:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%313:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%313:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::split return:
          inputs: (%1276:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %1833:list{128:int, 128:int}, -1:int)
          outputs: (%1822:tuple{%1826:<1024x1x128x128xbf16>{32768,32768,256,1},%1835:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%1824:tuple{%1837:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%1824:tuple{%1837:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
125367 2024-12-10 17:48:24.316399 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n1,rank6)
125448 2024-12-10 17:48:24.321262 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n1,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%1824:tuple{%1837:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%1776:tuple{%1836:<1024x64xbf16>{64,1},%1843:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%1836:<1024x64xbf16>{64, 1}, %1847:list{%1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%1848:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%1843:<1024x64xbf16>{64, 1}, %1846:list{%1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%1809:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1792:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1840:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1855:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%1853:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%1857:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%1847:list{%1857:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %1856:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%1858:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1858:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1849:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1857:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%1855:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1857:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%1655:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1852:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1840:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1859:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%1855:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%1861:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%1847:list{%1861:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %1860:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%1862:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1862:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1849:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1861:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%1859:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1861:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%1602:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%1865:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %1859:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%1865:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1863:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %1655:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%1863:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%1863:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %1792:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%1863:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1868:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %1869:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%1868:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%1862:<128x1024x192xbf16>{196608, 192, 1}, %1792:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%1859:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%1870:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%856:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%1785:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%1869:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%1785:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %1847:list{%1869:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %1851:<i32>, False:bool)
          outputs: (%1785:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%856:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %1785:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%1836:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%1836:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%1836:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%1870:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%1863:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%1836:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %1863:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%1863:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%1569:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%1569:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%1569:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, True:bool, False:bool)
          outputs: (%1569:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%1864:<128x1024x1024xbf16>{1048576, 1024, 1}, %1836:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%856:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1770:tuple{%1870:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1770:tuple{%1870:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
126963 2024-12-10 17:48:24.445173 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n22,rank6)
        - aten::mm:
          inputs: (%1875:<1024x16384xbf16>{16384, 1}, %1548:<16384x5120xbf16>{1, 16384})
          outputs: (%1876:<1024x5120xbf16>{5120,1})
          duration: -1
127075 2024-12-10 17:48:24.454877 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n22,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1770:tuple{%1870:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%1885:tuple{%1883:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
127092 2024-12-10 17:48:24.457398 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n1,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%721:tuple{%1765:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1791:tuple{%1865:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%1887:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%1887:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%1888:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%1888:tuple{None:NoneType,)
          duration: -1
        - ----------->api::grad call:
          inputs: (%1889:list{NotSurpot:FunctionalTensor}, %1890:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %1891:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%1889:list{NotSurpot:FunctionalTensor},%1890:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%1891:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%1890:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%1890:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%1890:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%1892:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%1893:tuple{NotSurpot:FunctionalTensor}, %1892:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%1893:tuple{NotSurpot:FunctionalTensor},%1892:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%1893:tuple{NotSurpot:FunctionalTensor}, %1892:tuple{NotSurpot:FunctionalTensor}, False:bool, %1894:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%1895:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%1893:tuple{NotSurpot:FunctionalTensor}, %1896:tuple{%1895:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %1897:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%1893:tuple{NotSurpot:FunctionalTensor},%1896:tuple{%1895:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%1897:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%1893:tuple{NotSurpot:FunctionalTensor}, %1896:tuple{%1895:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %1897:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%1898:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%1889:list{NotSurpot:FunctionalTensor}, %1890:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %1897:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %1893:tuple{NotSurpot:FunctionalTensor}, %1899:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %1900:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %1895:tuple{NotSurpot:FunctionalTensor}, %1898:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%1898:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%1900:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%1900:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55e8382830_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55e8382830_:_InferenceMode)
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%805:tuple{%271:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%805:tuple{%271:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
129884 2024-12-10 17:48:25.113337 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n4,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1904:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1904:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1904:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%958:tuple{%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%958:tuple{%1904:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%958:tuple{%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%958:tuple{%1904:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%1910:tuple{%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %1909:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%1910:tuple{%1904:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%1909:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %958:tuple{%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1574:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1904:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1574:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
130211 2024-12-10 17:48:25.159577 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n4,rank6)
        - ----------->api::MoELayer call:
          inputs: (%805:tuple{%855:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%805:tuple{%855:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
130250 2024-12-10 17:48:25.162128 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n1,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%1917:tuple{%1914:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%1917:tuple{%1914:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
130282 2024-12-10 17:48:25.164627 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n1,rank6)
        - aten::mm:
          inputs: (%1922:<1024x5120xbf16>{5120, 1}, %1921:<5120x160xbf16>{1, 5120})
          outputs: (%1923:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%1927:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%1928:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%1930:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%1931:tuple{%1615:<1024x6xbf16>{6,1},%1483:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%1932:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%1933:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%1935:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%1936:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%1936:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %1937:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%1936:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%1936:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %1935:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%1878:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%1933:<1024x160xf32>{160, 1}, %1935:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%1865:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%1865:<160xf32>{1}, %1878:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%1938:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%1938:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%1939:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1939:<i32>, 2_5431315104166666e-07:float)
          outputs: (%1940:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1940:<i32>, 0_01:float)
          outputs: (%1941:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%1942:<i32>, %1943:<i32>, alpha=1:int)
          outputs: (%1942:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%1944:<i32>, %1942:<i32>, False:bool)
          outputs: (%1944:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%1916:tuple{%1940:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%1916:tuple{%1940:<i32>}))
          duration: -1
130875 2024-12-10 17:48:25.202795 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n1,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%1917:tuple{%1914:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1949:tuple{%1937:<1024x6xbf16>{6,1},%1933:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%1853:<8192x5120xbf16>{5120, 1}, %332:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%1853:<8192x5120xbf16>{5120,1},%332:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%1853:<8192x5120xbf16>{5120, 1}, %332:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%1947:tuple{%1853:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%1943:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %1933:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%1943:<8192x6xCUSTOM_DATA_TYPE>{6,1},%1933:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%1943:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %1933:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%1952:tuple{%1943:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%1943:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%1930:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%1943:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%571:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%1930:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %571:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1868:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%1943:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %1868:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1954:<6955xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%1956:<8192x6xbf16>{6, 1}, %1937:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%1956:<8192x6xbf16>{6,1},%1937:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%1956:<8192x6xbf16>{6, 1}, %1937:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%1902:tuple{%1956:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%1956:<8192x6xbf16>{6, 1}, %1868:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1957:<6955xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%1868:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1958:<6955x2xCUSTOM_DATA_TYPE>{1,6955})
          duration: -1
        - aten::gather:
          inputs: (%1853:<8192x5120xbf16>{5120, 1}, 0:int, %243:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%766:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%1954:<6955xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%1963:tuple{%571:<6955xCUSTOM_DATA_TYPE>{1},%1959:<6955xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%336:<6955xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%571:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%766:<6955x5120xbf16>{5120, 1}, 0:int, %1964:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%1966:<6955x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%1963:tuple{%1966:<6955x5120xbf16>{5120, 1}, %1850:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%1963:tuple{%1966:<6955x5120xbf16>{5120,1},%1850:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
131640 2024-12-10 17:48:25.319641 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n1,rank6)
        - aten::cumsum:
          inputs: (%1943:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%1548:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%1969:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%1854:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%1969:list{%1854:<1xCUSTOM_DATA_TYPE>{1}, %1548:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%1540:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1676:<326x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1676:<326x5120xbf16>{5120,1}}))
          duration: -1
131861 2024-12-10 17:48:25.333128 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n21,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%462:tuple{%336:<326x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%462:tuple{%336:<326x5120xbf16>{5120,1}}))
          duration: -1
131924 2024-12-10 17:48:25.335573 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n30,rank6)
        - aten::mm:
          inputs: (%1977:<326x5120xbf16>{5120, 1}, %1979:<5120x3072xbf16>{1, 5120})
          outputs: (%1301:<326x3072xbf16>{3072,1})
          duration: -1
132050 2024-12-10 17:48:25.341144 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n30,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%462:tuple{%336:<326x5120xbf16>{5120, 1}})
          outputs: (%1931:tuple{%1084:<326x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1552:<326x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1552:<326x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1552:<326x1536xbf16>{3072, 1})
          outputs: (%1309:<326x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1552:<326x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1309:<326x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1309:<326x1536xbf16>{1536, 1}, %1200:<326x1536xbf16>{3072, 1}+1536)
          outputs: (%942:<326x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1978:tuple{%942:<326x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1978:tuple{%942:<326x1536xbf16>{1536,1}}))
          duration: -1
132187 2024-12-10 17:48:25.352691 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n23,rank6)
        - aten::mm:
          inputs: (%1469:<326x1536xbf16>{1536, 1}, %471:<1536x5120xbf16>{1, 1536})
          outputs: (%1986:<326x5120xbf16>{5120,1})
          duration: -1
132312 2024-12-10 17:48:25.358204 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n23,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1978:tuple{%942:<326x1536xbf16>{1536, 1}})
          outputs: (%1951:tuple{%1234:<326x5120xbf16>{5120,1},None:NoneType})
          duration: -1
132367 2024-12-10 17:48:25.360645 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n21,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1676:<326x5120xbf16>{5120, 1}})
          outputs: (%1931:tuple{%471:<326x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1992:<326x5120xbf16>{5120, 1}, %471:<326x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1992:<326x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%332:<535x5120xbf16>{5120, 1}+1669120})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%332:<535x5120xbf16>{5120,1}+1669120}))
          duration: -1
132577 2024-12-10 17:48:25.372412 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n22,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%464:tuple{%1994:<535x5120xbf16>{5120, 1}+1669120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%464:tuple{%1994:<535x5120xbf16>{5120,1}+1669120}))
          duration: -1
132640 2024-12-10 17:48:25.374826 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n31,rank6)
        - aten::mm:
          inputs: (%984:<535x5120xbf16>{5120, 1}+1669120, %430:<5120x3072xbf16>{1, 5120})
          outputs: (%254:<535x3072xbf16>{3072,1})
          duration: -1
132765 2024-12-10 17:48:25.380288 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n31,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%464:tuple{%1994:<535x5120xbf16>{5120, 1}+1669120})
          outputs: (%1998:tuple{%1162:<535x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%430:<535x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%430:<535x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%430:<535x1536xbf16>{3072, 1})
          outputs: (%600:<535x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%430:<535x1536xbf16>{3072, 1}, False:bool)
          outputs: (%600:<535x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%600:<535x1536xbf16>{1536, 1}, %2001:<535x1536xbf16>{3072, 1}+1536)
          outputs: (%605:<535x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1997:tuple{%605:<535x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1997:tuple{%605:<535x1536xbf16>{1536,1}}))
          duration: -1
132904 2024-12-10 17:48:25.391843 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n24,rank6)
        - aten::mm:
          inputs: (%574:<535x1536xbf16>{1536, 1}, %1723:<1536x5120xbf16>{1, 1536})
          outputs: (%2004:<535x5120xbf16>{5120,1})
          duration: -1
133028 2024-12-10 17:48:25.397253 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n24,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1997:tuple{%605:<535x1536xbf16>{1536, 1}})
          outputs: (%1968:tuple{%1722:<535x5120xbf16>{5120,1},None:NoneType})
          duration: -1
133080 2024-12-10 17:48:25.399664 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n22,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%332:<535x5120xbf16>{5120, 1}+1669120})
          outputs: (%1998:tuple{%1720:<535x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1722:<535x5120xbf16>{5120, 1}+1669120, %1720:<535x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1722:<535x5120xbf16>{5120,1}+1669120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1992:<308x5120xbf16>{5120, 1}+4408320})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1992:<308x5120xbf16>{5120,1}+4408320}))
          duration: -1
133293 2024-12-10 17:48:25.411406 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n23,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1974:tuple{%2011:<308x5120xbf16>{5120, 1}+4408320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1974:tuple{%2011:<308x5120xbf16>{5120,1}+4408320}))
          duration: -1
133353 2024-12-10 17:48:25.413828 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n32,rank6)
        - aten::mm:
          inputs: (%434:<308x5120xbf16>{5120, 1}+4408320, %534:<5120x3072xbf16>{1, 5120})
          outputs: (%1994:<308x3072xbf16>{3072,1})
          duration: -1
133483 2024-12-10 17:48:25.419290 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n32,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1974:tuple{%2011:<308x5120xbf16>{5120, 1}+4408320})
          outputs: (%1951:tuple{%566:<308x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%487:<308x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%487:<308x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%487:<308x1536xbf16>{3072, 1})
          outputs: (%1346:<308x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%487:<308x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1346:<308x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1346:<308x1536xbf16>{1536, 1}, %726:<308x1536xbf16>{3072, 1}+1536)
          outputs: (%2018:<308x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2014:tuple{%2018:<308x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2014:tuple{%2018:<308x1536xbf16>{1536,1}}))
          duration: -1
133620 2024-12-10 17:48:25.430770 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n25,rank6)
        - aten::mm:
          inputs: (%2020:<308x1536xbf16>{1536, 1}, %2022:<1536x5120xbf16>{1, 1536})
          outputs: (%1667:<308x5120xbf16>{5120,1})
          duration: -1
133746 2024-12-10 17:48:25.436216 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n25,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2014:tuple{%2018:<308x1536xbf16>{1536, 1}})
          outputs: (%1931:tuple{%2025:<308x5120xbf16>{5120,1},None:NoneType})
          duration: -1
133798 2024-12-10 17:48:25.438650 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n23,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1992:<308x5120xbf16>{5120, 1}+4408320})
          outputs: (%1951:tuple{%2028:<308x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2004:<308x5120xbf16>{5120, 1}+4408320, %2028:<308x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2004:<308x5120xbf16>{5120,1}+4408320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1084:<294x5120xbf16>{5120, 1}+5985280})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1084:<294x5120xbf16>{5120,1}+5985280}))
          duration: -1
134009 2024-12-10 17:48:25.450414 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n24,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1995:tuple{%571:<294x5120xbf16>{5120, 1}+5985280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1995:tuple{%571:<294x5120xbf16>{5120,1}+5985280}))
          duration: -1
134070 2024-12-10 17:48:25.452813 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n33,rank6)
        - aten::mm:
          inputs: (%2034:<294x5120xbf16>{5120, 1}+5985280, %2036:<5120x3072xbf16>{1, 5120})
          outputs: (%2037:<294x3072xbf16>{3072,1})
          duration: -1
134201 2024-12-10 17:48:25.458307 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n33,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1995:tuple{%571:<294x5120xbf16>{5120, 1}+5985280})
          outputs: (%1968:tuple{%2040:<294x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%488:<294x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%488:<294x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%488:<294x1536xbf16>{3072, 1})
          outputs: (%2043:<294x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%488:<294x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2043:<294x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2043:<294x1536xbf16>{1536, 1}, %2025:<294x1536xbf16>{3072, 1}+1536)
          outputs: (%2036:<294x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2035:tuple{%2036:<294x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2035:tuple{%2036:<294x1536xbf16>{1536,1}}))
          duration: -1
134337 2024-12-10 17:48:25.469796 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n26,rank6)
        - aten::mm:
          inputs: (%2045:<294x1536xbf16>{1536, 1}, %2040:<1536x5120xbf16>{1, 1536})
          outputs: (%2048:<294x5120xbf16>{5120,1})
          duration: -1
134475 2024-12-10 17:48:25.475842 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n26,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2035:tuple{%2036:<294x1536xbf16>{1536, 1}})
          outputs: (%1998:tuple{%2009:<294x5120xbf16>{5120,1},None:NoneType})
          duration: -1
134526 2024-12-10 17:48:25.478322 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n24,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1084:<294x5120xbf16>{5120, 1}+5985280})
          outputs: (%1968:tuple{%2053:<294x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2009:<294x5120xbf16>{5120, 1}+5985280, %2053:<294x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2009:<294x5120xbf16>{5120,1}+5985280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2056:<462x5120xbf16>{5120, 1}+7490560})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2056:<462x5120xbf16>{5120,1}+7490560}))
          duration: -1
134739 2024-12-10 17:48:25.490137 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n25,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2012:tuple{%1724:<462x5120xbf16>{5120, 1}+7490560})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2012:tuple{%1724:<462x5120xbf16>{5120,1}+7490560}))
          duration: -1
134798 2024-12-10 17:48:25.492563 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n34,rank6)
        - aten::mm:
          inputs: (%2060:<462x5120xbf16>{5120, 1}+7490560, %2062:<5120x3072xbf16>{1, 5120})
          outputs: (%2063:<462x3072xbf16>{3072,1})
          duration: -1
134928 2024-12-10 17:48:25.498049 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n34,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2012:tuple{%1724:<462x5120xbf16>{5120, 1}+7490560})
          outputs: (%1931:tuple{%2066:<462x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2068:<462x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2068:<462x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2068:<462x1536xbf16>{3072, 1})
          outputs: (%2070:<462x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2068:<462x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2070:<462x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2070:<462x1536xbf16>{1536, 1}, %2069:<462x1536xbf16>{3072, 1}+1536)
          outputs: (%2071:<462x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2061:tuple{%2071:<462x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2061:tuple{%2071:<462x1536xbf16>{1536,1}}))
          duration: -1
135082 2024-12-10 17:48:25.510091 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n27,rank6)
        - aten::mm:
          inputs: (%2073:<462x1536xbf16>{1536, 1}, %2076:<1536x5120xbf16>{1, 1536})
          outputs: (%2077:<462x5120xbf16>{5120,1})
          duration: -1
135203 2024-12-10 17:48:25.515509 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n27,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2061:tuple{%2071:<462x1536xbf16>{1536, 1}})
          outputs: (%1951:tuple{%1600:<462x5120xbf16>{5120,1},None:NoneType})
          duration: -1
135254 2024-12-10 17:48:25.517947 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n25,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2056:<462x5120xbf16>{5120, 1}+7490560})
          outputs: (%1931:tuple{%2082:<462x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2009:<462x5120xbf16>{5120, 1}+7490560, %2082:<462x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2009:<462x5120xbf16>{5120,1}+7490560)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1970:<48x5120xbf16>{5120, 1}+9856000})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1970:<48x5120xbf16>{5120,1}+9856000}))
          duration: -1
135468 2024-12-10 17:48:25.529741 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n26,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2032:tuple{%571:<48x5120xbf16>{5120, 1}+9856000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2032:tuple{%571:<48x5120xbf16>{5120,1}+9856000}))
          duration: -1
135528 2024-12-10 17:48:25.532153 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n35,rank6)
        - aten::mm:
          inputs: (%2087:<48x5120xbf16>{5120, 1}+9856000, %2048:<5120x3072xbf16>{1, 5120})
          outputs: (%2053:<48x3072xbf16>{3072,1})
          duration: -1
135657 2024-12-10 17:48:25.537664 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n35,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2032:tuple{%571:<48x5120xbf16>{5120, 1}+9856000})
          outputs: (%1998:tuple{%2092:<48x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2095:<48x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2095:<48x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2095:<48x1536xbf16>{3072, 1})
          outputs: (%1351:<48x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2095:<48x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1351:<48x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1351:<48x1536xbf16>{1536, 1}, %1334:<48x1536xbf16>{3072, 1}+1536)
          outputs: (%2096:<48x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2088:tuple{%2096:<48x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2088:tuple{%2096:<48x1536xbf16>{1536,1}}))
          duration: -1
135799 2024-12-10 17:48:25.549164 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n28,rank6)
        - aten::mm:
          inputs: (%2048:<48x1536xbf16>{1536, 1}, %1370:<1536x5120xbf16>{1, 1536})
          outputs: (%2098:<48x5120xbf16>{5120,1})
          duration: -1
135919 2024-12-10 17:48:25.554604 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n28,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2088:tuple{%2096:<48x1536xbf16>{1536, 1}})
          outputs: (%1968:tuple{%2092:<48x5120xbf16>{5120,1},None:NoneType})
          duration: -1
135971 2024-12-10 17:48:25.557081 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n26,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1970:<48x5120xbf16>{5120, 1}+9856000})
          outputs: (%1998:tuple{%747:<48x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%336:<48x5120xbf16>{5120, 1}+9856000, %747:<48x5120xbf16>{5120, 1}, False:bool)
          outputs: (%336:<48x5120xbf16>{5120,1}+9856000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1272:<733x5120xbf16>{5120, 1}+10101760})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1272:<733x5120xbf16>{5120,1}+10101760}))
          duration: -1
136185 2024-12-10 17:48:25.568882 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n27,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2058:tuple{%2105:<733x5120xbf16>{5120, 1}+10101760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2058:tuple{%2105:<733x5120xbf16>{5120,1}+10101760}))
          duration: -1
136245 2024-12-10 17:48:25.571316 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n36,rank6)
        - aten::mm:
          inputs: (%1177:<733x5120xbf16>{5120, 1}+10101760, %1362:<5120x3072xbf16>{1, 5120})
          outputs: (%1667:<733x3072xbf16>{3072,1})
          duration: -1
136372 2024-12-10 17:48:25.576759 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n36,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2058:tuple{%2105:<733x5120xbf16>{5120, 1}+10101760})
          outputs: (%1951:tuple{%2092:<733x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1362:<733x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1362:<733x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1362:<733x1536xbf16>{3072, 1})
          outputs: (%2114:<733x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1362:<733x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2114:<733x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2114:<733x1536xbf16>{1536, 1}, %2113:<733x1536xbf16>{3072, 1}+1536)
          outputs: (%2115:<733x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2108:tuple{%2115:<733x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2108:tuple{%2115:<733x1536xbf16>{1536,1}}))
          duration: -1
136517 2024-12-10 17:48:25.588296 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n29,rank6)
        - aten::mm:
          inputs: (%2117:<733x1536xbf16>{1536, 1}, %1384:<1536x5120xbf16>{1, 1536})
          outputs: (%1668:<733x5120xbf16>{5120,1})
          duration: -1
136636 2024-12-10 17:48:25.593762 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n29,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2108:tuple{%2115:<733x1536xbf16>{1536, 1}})
          outputs: (%1931:tuple{%1687:<733x5120xbf16>{5120,1},None:NoneType})
          duration: -1
136690 2024-12-10 17:48:25.596201 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n27,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1272:<733x5120xbf16>{5120, 1}+10101760})
          outputs: (%1951:tuple{%2092:<733x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1986:<733x5120xbf16>{5120, 1}+10101760, %2092:<733x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1986:<733x5120xbf16>{5120,1}+10101760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1670:<279x5120xbf16>{5120, 1}+13854720})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1670:<279x5120xbf16>{5120,1}+13854720}))
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2085:tuple{%2105:<279x5120xbf16>{5120, 1}+13854720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2085:tuple{%2105:<279x5120xbf16>{5120,1}+13854720}))
          duration: -1
136967 2024-12-10 17:48:25.610552 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n37,rank6)
        - aten::mm:
          inputs: (%1666:<279x5120xbf16>{5120, 1}+13854720, %1710:<5120x3072xbf16>{1, 5120})
          outputs: (%1707:<279x3072xbf16>{3072,1})
          duration: -1
137093 2024-12-10 17:48:25.615980 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n37,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2085:tuple{%2105:<279x5120xbf16>{5120, 1}+13854720})
          outputs: (%1968:tuple{%2130:<279x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%558:<279x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%558:<279x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%558:<279x1536xbf16>{3072, 1})
          outputs: (%1710:<279x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%558:<279x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1710:<279x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1710:<279x1536xbf16>{1536, 1}, %2082:<279x1536xbf16>{3072, 1}+1536)
          outputs: (%1384:<279x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2126:tuple{%1384:<279x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2126:tuple{%1384:<279x1536xbf16>{1536,1}}))
          duration: -1
137235 2024-12-10 17:48:25.627448 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n30,rank6)
        - aten::mm:
          inputs: (%2133:<279x1536xbf16>{1536, 1}, %1385:<1536x5120xbf16>{1, 1536})
          outputs: (%1663:<279x5120xbf16>{5120,1})
          duration: -1
137354 2024-12-10 17:48:25.632909 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n30,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2126:tuple{%1384:<279x1536xbf16>{1536, 1}})
          outputs: (%1998:tuple{%2138:<279x5120xbf16>{5120,1},None:NoneType})
          duration: -1
137409 2024-12-10 17:48:25.635363 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n28,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1670:<279x5120xbf16>{5120, 1}+13854720})
          outputs: (%1968:tuple{%1944:<279x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1923:<279x5120xbf16>{5120, 1}+13854720, %1944:<279x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1923:<279x5120xbf16>{5120,1}+13854720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2092:<115x5120xbf16>{5120, 1}+15283200})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2092:<115x5120xbf16>{5120,1}+15283200}))
          duration: -1
137620 2024-12-10 17:48:25.647165 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n29,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2106:tuple{%2105:<115x5120xbf16>{5120, 1}+15283200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2106:tuple{%2105:<115x5120xbf16>{5120,1}+15283200}))
          duration: -1
137681 2024-12-10 17:48:25.649583 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n38,rank6)
        - aten::mm:
          inputs: (%478:<115x5120xbf16>{5120, 1}+15283200, %2148:<5120x3072xbf16>{1, 5120})
          outputs: (%1939:<115x3072xbf16>{3072,1})
          duration: -1
137808 2024-12-10 17:48:25.655056 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n38,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2106:tuple{%2105:<115x5120xbf16>{5120, 1}+15283200})
          outputs: (%1931:tuple{%2151:<115x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1670:<115x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1670:<115x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1670:<115x1536xbf16>{3072, 1})
          outputs: (%2155:<115x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1670:<115x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2155:<115x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2155:<115x1536xbf16>{1536, 1}, %1986:<115x1536xbf16>{3072, 1}+1536)
          outputs: (%2156:<115x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2146:tuple{%2156:<115x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2146:tuple{%2156:<115x1536xbf16>{1536,1}}))
          duration: -1
137952 2024-12-10 17:48:25.666505 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n31,rank6)
        - aten::mm:
          inputs: (%1363:<115x1536xbf16>{1536, 1}, %970:<1536x5120xbf16>{1, 1536})
          outputs: (%2160:<115x5120xbf16>{5120,1})
          duration: -1
138069 2024-12-10 17:48:25.671911 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n31,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2146:tuple{%2156:<115x1536xbf16>{1536, 1}})
          outputs: (%1951:tuple{%1385:<115x5120xbf16>{5120,1},None:NoneType})
          duration: -1
138124 2024-12-10 17:48:25.674372 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n29,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2092:<115x5120xbf16>{5120, 1}+15283200})
          outputs: (%1931:tuple{%980:<115x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2077:<115x5120xbf16>{5120, 1}+15283200, %980:<115x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2077:<115x5120xbf16>{5120,1}+15283200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2105:<232x5120xbf16>{5120, 1}+15872000})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2105:<232x5120xbf16>{5120,1}+15872000}))
          duration: -1
138337 2024-12-10 17:48:25.686128 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n30,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2124:tuple{%1388:<232x5120xbf16>{5120, 1}+15872000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2124:tuple{%1388:<232x5120xbf16>{5120,1}+15872000}))
          duration: -1
138398 2024-12-10 17:48:25.688525 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n39,rank6)
        - aten::mm:
          inputs: (%2169:<232x5120xbf16>{5120, 1}+15872000, %970:<5120x3072xbf16>{1, 5120})
          outputs: (%2148:<232x3072xbf16>{3072,1})
          duration: -1
138524 2024-12-10 17:48:25.694041 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n39,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2124:tuple{%1388:<232x5120xbf16>{5120, 1}+15872000})
          outputs: (%1998:tuple{%771:<232x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2004:<232x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2004:<232x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2004:<232x1536xbf16>{3072, 1})
          outputs: (%2178:<232x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2004:<232x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2178:<232x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2178:<232x1536xbf16>{1536, 1}, %2177:<232x1536xbf16>{3072, 1}+1536)
          outputs: (%2179:<232x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2170:tuple{%2179:<232x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2170:tuple{%2179:<232x1536xbf16>{1536,1}}))
          duration: -1
138668 2024-12-10 17:48:25.705548 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n32,rank6)
        - aten::mm:
          inputs: (%2181:<232x1536xbf16>{1536, 1}, %2184:<1536x5120xbf16>{1, 1536})
          outputs: (%2185:<232x5120xbf16>{5120,1})
          duration: -1
138786 2024-12-10 17:48:25.710950 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n32,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2170:tuple{%2179:<232x1536xbf16>{1536, 1}})
          outputs: (%1968:tuple{%2184:<232x5120xbf16>{5120,1},None:NoneType})
          duration: -1
138842 2024-12-10 17:48:25.713400 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n30,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2105:<232x5120xbf16>{5120, 1}+15872000})
          outputs: (%1998:tuple{%1532:<232x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2160:<232x5120xbf16>{5120, 1}+15872000, %1532:<232x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2160:<232x5120xbf16>{5120,1}+15872000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%1570:<107x5120xbf16>{5120, 1}+17059840})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%1570:<107x5120xbf16>{5120,1}+17059840}))
          duration: -1
139054 2024-12-10 17:48:25.725146 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n31,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2144:tuple{%1388:<107x5120xbf16>{5120, 1}+17059840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2144:tuple{%1388:<107x5120xbf16>{5120,1}+17059840}))
          duration: -1
139114 2024-12-10 17:48:25.727538 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n40,rank6)
        - aten::mm:
          inputs: (%2195:<107x5120xbf16>{5120, 1}+17059840, %2198:<5120x3072xbf16>{1, 5120})
          outputs: (%2199:<107x3072xbf16>{3072,1})
          duration: -1
139240 2024-12-10 17:48:25.732967 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n40,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2144:tuple{%1388:<107x5120xbf16>{5120, 1}+17059840})
          outputs: (%1951:tuple{%2202:<107x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2198:<107x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2198:<107x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2198:<107x1536xbf16>{3072, 1})
          outputs: (%2207:<107x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2198:<107x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2207:<107x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2207:<107x1536xbf16>{1536, 1}, %2206:<107x1536xbf16>{3072, 1}+1536)
          outputs: (%2208:<107x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2196:tuple{%2208:<107x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2196:tuple{%2208:<107x1536xbf16>{1536,1}}))
          duration: -1
139383 2024-12-10 17:48:25.744439 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n33,rank6)
        - aten::mm:
          inputs: (%2210:<107x1536xbf16>{1536, 1}, %2213:<1536x5120xbf16>{1, 1536})
          outputs: (%1618:<107x5120xbf16>{5120,1})
          duration: -1
139502 2024-12-10 17:48:25.749883 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n33,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2196:tuple{%2208:<107x1536xbf16>{1536, 1}})
          outputs: (%1931:tuple{%2216:<107x5120xbf16>{5120,1},None:NoneType})
          duration: -1
139558 2024-12-10 17:48:25.752294 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n31,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%1570:<107x5120xbf16>{5120, 1}+17059840})
          outputs: (%1951:tuple{%2220:<107x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2222:<107x5120xbf16>{5120, 1}+17059840, %2220:<107x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2222:<107x5120xbf16>{5120,1}+17059840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2160:<183x5120xbf16>{5120, 1}+17607680})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2160:<183x5120xbf16>{5120,1}+17607680}))
          duration: -1
139771 2024-12-10 17:48:25.764097 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n32,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2167:tuple{%1388:<183x5120xbf16>{5120, 1}+17607680})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2167:tuple{%1388:<183x5120xbf16>{5120,1}+17607680}))
          duration: -1
139829 2024-12-10 17:48:25.766536 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n41,rank6)
        - aten::mm:
          inputs: (%2225:<183x5120xbf16>{5120, 1}+17607680, %2228:<5120x3072xbf16>{1, 5120})
          outputs: (%2229:<183x3072xbf16>{3072,1})
          duration: -1
139960 2024-12-10 17:48:25.772323 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n41,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2167:tuple{%1388:<183x5120xbf16>{5120, 1}+17607680})
          outputs: (%1968:tuple{%1866:<183x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2234:<183x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2234:<183x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2234:<183x1536xbf16>{3072, 1})
          outputs: (%1845:<183x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2234:<183x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1845:<183x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1845:<183x1536xbf16>{1536, 1}, %1238:<183x1536xbf16>{3072, 1}+1536)
          outputs: (%1424:<183x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2226:tuple{%1424:<183x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2226:tuple{%1424:<183x1536xbf16>{1536,1}}))
          duration: -1
140102 2024-12-10 17:48:25.783896 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n34,rank6)
        - aten::mm:
          inputs: (%2236:<183x1536xbf16>{1536, 1}, %2185:<1536x5120xbf16>{1, 1536})
          outputs: (%2240:<183x5120xbf16>{5120,1})
          duration: -1
140222 2024-12-10 17:48:25.789437 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n34,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2226:tuple{%1424:<183x1536xbf16>{1536, 1}})
          outputs: (%2239:tuple{%2009:<183x5120xbf16>{5120,1},None:NoneType})
          duration: -1
140278 2024-12-10 17:48:25.791895 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n32,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2160:<183x5120xbf16>{5120, 1}+17607680})
          outputs: (%1968:tuple{%2246:<183x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1663:<183x5120xbf16>{5120, 1}+17607680, %2246:<183x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1663:<183x5120xbf16>{5120,1}+17607680)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%765:<377x5120xbf16>{5120, 1}+18544640})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%765:<377x5120xbf16>{5120,1}+18544640}))
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2193:tuple{%1928:<377x5120xbf16>{5120, 1}+18544640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2193:tuple{%1928:<377x5120xbf16>{5120,1}+18544640}))
          duration: -1
140550 2024-12-10 17:48:25.806173 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n42,rank6)
        - aten::mm:
          inputs: (%1618:<377x5120xbf16>{5120, 1}+18544640, %2252:<5120x3072xbf16>{1, 5120})
          outputs: (%2253:<377x3072xbf16>{3072,1})
          duration: -1
140677 2024-12-10 17:48:25.811594 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n42,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2193:tuple{%1928:<377x5120xbf16>{5120, 1}+18544640})
          outputs: (%1931:tuple{%2256:<377x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1720:<377x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1720:<377x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1720:<377x1536xbf16>{3072, 1})
          outputs: (%2260:<377x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1720:<377x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2260:<377x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2260:<377x1536xbf16>{1536, 1}, %2185:<377x1536xbf16>{3072, 1}+1536)
          outputs: (%2261:<377x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2250:tuple{%2261:<377x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2250:tuple{%2261:<377x1536xbf16>{1536,1}}))
          duration: -1
140820 2024-12-10 17:48:25.823021 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n35,rank6)
        - aten::mm:
          inputs: (%2263:<377x1536xbf16>{1536, 1}, %2267:<1536x5120xbf16>{1, 1536})
          outputs: (%2256:<377x5120xbf16>{5120,1})
          duration: -1
140932 2024-12-10 17:48:25.828455 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n35,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2250:tuple{%2261:<377x1536xbf16>{1536, 1}})
          outputs: (%2266:tuple{%2267:<377x5120xbf16>{5120,1},None:NoneType})
          duration: -1
140989 2024-12-10 17:48:25.830902 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n33,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%765:<377x5120xbf16>{5120, 1}+18544640})
          outputs: (%1931:tuple{%2252:<377x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1532:<377x5120xbf16>{5120, 1}+18544640, %2252:<377x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1532:<377x5120xbf16>{5120,1}+18544640)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2216:<575x5120xbf16>{5120, 1}+20474880})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2216:<575x5120xbf16>{5120,1}+20474880}))
          duration: -1
141201 2024-12-10 17:48:25.842681 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n34,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2223:tuple{%1388:<575x5120xbf16>{5120, 1}+20474880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2223:tuple{%1388:<575x5120xbf16>{5120,1}+20474880}))
          duration: -1
141259 2024-12-10 17:48:25.845113 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n43,rank6)
        - aten::mm:
          inputs: (%2278:<575x5120xbf16>{5120, 1}+20474880, %2281:<5120x3072xbf16>{1, 5120})
          outputs: (%2282:<575x3072xbf16>{3072,1})
          duration: -1
141388 2024-12-10 17:48:25.850572 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n43,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2223:tuple{%1388:<575x5120xbf16>{5120, 1}+20474880})
          outputs: (%2239:tuple{%2267:<575x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1711:<575x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1711:<575x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1711:<575x1536xbf16>{3072, 1})
          outputs: (%2288:<575x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1711:<575x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2288:<575x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2288:<575x1536xbf16>{1536, 1}, %2202:<575x1536xbf16>{3072, 1}+1536)
          outputs: (%2289:<575x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2279:tuple{%2289:<575x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2279:tuple{%2289:<575x1536xbf16>{1536,1}}))
          duration: -1
141538 2024-12-10 17:48:25.862126 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n36,rank6)
        - aten::mm:
          inputs: (%2281:<575x1536xbf16>{1536, 1}, %2293:<1536x5120xbf16>{1, 1536})
          outputs: (%2294:<575x5120xbf16>{5120,1})
          duration: -1
141650 2024-12-10 17:48:25.867550 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n36,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2279:tuple{%2289:<575x1536xbf16>{1536, 1}})
          outputs: (%1968:tuple{%2009:<575x5120xbf16>{5120,1},None:NoneType})
          duration: -1
141703 2024-12-10 17:48:25.870013 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n34,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2216:<575x5120xbf16>{5120, 1}+20474880})
          outputs: (%2239:tuple{%2300:<575x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<575x5120xbf16>{5120, 1}+20474880, %2300:<575x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<575x5120xbf16>{5120,1}+20474880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2303:<344x5120xbf16>{5120, 1}+23418880})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2303:<344x5120xbf16>{5120,1}+23418880}))
          duration: -1
141920 2024-12-10 17:48:25.881771 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n35,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2249:tuple{%1388:<344x5120xbf16>{5120, 1}+23418880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2249:tuple{%1388:<344x5120xbf16>{5120,1}+23418880}))
          duration: -1
141974 2024-12-10 17:48:25.884164 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n44,rank6)
        - aten::mm:
          inputs: (%2307:<344x5120xbf16>{5120, 1}+23418880, %2310:<5120x3072xbf16>{1, 5120})
          outputs: (%2311:<344x3072xbf16>{3072,1})
          duration: -1
142103 2024-12-10 17:48:25.889576 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n44,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2249:tuple{%1388:<344x5120xbf16>{5120, 1}+23418880})
          outputs: (%2266:tuple{%2314:<344x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2310:<344x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2310:<344x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2310:<344x1536xbf16>{3072, 1})
          outputs: (%2252:<344x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2310:<344x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2252:<344x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2252:<344x1536xbf16>{1536, 1}, %2317:<344x1536xbf16>{3072, 1}+1536)
          outputs: (%2318:<344x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2308:tuple{%2318:<344x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2308:tuple{%2318:<344x1536xbf16>{1536,1}}))
          duration: -1
142256 2024-12-10 17:48:25.901058 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n37,rank6)
        - aten::mm:
          inputs: (%2320:<344x1536xbf16>{1536, 1}, %2323:<1536x5120xbf16>{1, 1536})
          outputs: (%2324:<344x5120xbf16>{5120,1})
          duration: -1
142362 2024-12-10 17:48:25.906443 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n37,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2308:tuple{%2318:<344x1536xbf16>{1536, 1}})
          outputs: (%1931:tuple{%2327:<344x5120xbf16>{5120,1},None:NoneType})
          duration: -1
142415 2024-12-10 17:48:25.908860 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n35,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2303:<344x5120xbf16>{5120, 1}+23418880})
          outputs: (%2266:tuple{%1954:<344x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<344x5120xbf16>{5120, 1}+23418880, %1954:<344x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<344x5120xbf16>{5120,1}+23418880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2300:<406x5120xbf16>{5120, 1}+25180160})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2300:<406x5120xbf16>{5120,1}+25180160}))
          duration: -1
142634 2024-12-10 17:48:25.920636 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n36,rank6)
142688 2024-12-10 17:48:25.923044 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n45,rank6)
        - aten::mm:
          inputs: (%2337:<406x5120xbf16>{5120, 1}+25180160, %2340:<5120x3072xbf16>{1, 5120})
          outputs: (%2341:<406x3072xbf16>{3072,1})
          duration: -1
142818 2024-12-10 17:48:25.928504 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n45,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2276:tuple{%1388:<406x5120xbf16>{5120, 1}+25180160})
          outputs: (%1968:tuple{%2344:<406x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2348:<406x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2348:<406x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2348:<406x1536xbf16>{3072, 1})
          outputs: (%2350:<406x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2348:<406x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2350:<406x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2350:<406x1536xbf16>{1536, 1}, %2349:<406x1536xbf16>{3072, 1}+1536)
          outputs: (%2351:<406x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2338:tuple{%2351:<406x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2338:tuple{%2351:<406x1536xbf16>{1536,1}}))
          duration: -1
142974 2024-12-10 17:48:25.940008 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n38,rank6)
        - aten::mm:
          inputs: (%2303:<406x1536xbf16>{1536, 1}, %2355:<1536x5120xbf16>{1, 1536})
          outputs: (%2356:<406x5120xbf16>{5120,1})
          duration: -1
143081 2024-12-10 17:48:25.945473 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n38,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2338:tuple{%2351:<406x1536xbf16>{1536, 1}})
          outputs: (%2239:tuple{%2359:<406x5120xbf16>{5120,1},None:NoneType})
          duration: -1
143132 2024-12-10 17:48:25.947881 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n36,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2300:<406x5120xbf16>{5120, 1}+25180160})
          outputs: (%1968:tuple{%2355:<406x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<406x5120xbf16>{5120, 1}+25180160, %2355:<406x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<406x5120xbf16>{5120,1}+25180160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2365:<569x5120xbf16>{5120, 1}+27258880})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2365:<569x5120xbf16>{5120,1}+27258880}))
          duration: -1
143354 2024-12-10 17:48:25.959747 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n37,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2305:tuple{%1388:<569x5120xbf16>{5120, 1}+27258880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2305:tuple{%1388:<569x5120xbf16>{5120,1}+27258880}))
          duration: -1
143405 2024-12-10 17:48:25.962156 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n46,rank6)
        - aten::mm:
          inputs: (%2369:<569x5120xbf16>{5120, 1}+27258880, %2372:<5120x3072xbf16>{1, 5120})
          outputs: (%2373:<569x3072xbf16>{3072,1})
          duration: -1
143535 2024-12-10 17:48:25.967561 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n46,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2305:tuple{%1388:<569x5120xbf16>{5120, 1}+27258880})
          outputs: (%1931:tuple{%2376:<569x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2380:<569x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2380:<569x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2380:<569x1536xbf16>{3072, 1})
          outputs: (%2382:<569x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2380:<569x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2382:<569x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2382:<569x1536xbf16>{1536, 1}, %2381:<569x1536xbf16>{3072, 1}+1536)
          outputs: (%2383:<569x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2370:tuple{%2383:<569x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2370:tuple{%2383:<569x1536xbf16>{1536,1}}))
          duration: -1
143692 2024-12-10 17:48:25.979118 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n39,rank6)
        - aten::mm:
          inputs: (%2385:<569x1536xbf16>{1536, 1}, %2388:<1536x5120xbf16>{1, 1536})
          outputs: (%2389:<569x5120xbf16>{5120,1})
          duration: -1
143801 2024-12-10 17:48:25.984611 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n39,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2370:tuple{%2383:<569x1536xbf16>{1536, 1}})
          outputs: (%2266:tuple{%2392:<569x5120xbf16>{5120,1},None:NoneType})
          duration: -1
143848 2024-12-10 17:48:25.987041 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n37,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2365:<569x5120xbf16>{5120, 1}+27258880})
          outputs: (%1931:tuple{%2388:<569x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1663:<569x5120xbf16>{5120, 1}+27258880, %2388:<569x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1663:<569x5120xbf16>{5120,1}+27258880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2356:<264x5120xbf16>{5120, 1}+30172160})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2356:<264x5120xbf16>{5120,1}+30172160}))
          duration: -1
144070 2024-12-10 17:48:25.998856 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n38,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2335:tuple{%2399:<264x5120xbf16>{5120, 1}+30172160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2335:tuple{%2399:<264x5120xbf16>{5120,1}+30172160}))
          duration: -1
144121 2024-12-10 17:48:26.001261 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n47,rank6)
        - aten::mm:
          inputs: (%2365:<264x5120xbf16>{5120, 1}+30172160, %2404:<5120x3072xbf16>{1, 5120})
          outputs: (%2405:<264x3072xbf16>{3072,1})
          duration: -1
144252 2024-12-10 17:48:26.006731 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n47,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2335:tuple{%2399:<264x5120xbf16>{5120, 1}+30172160})
          outputs: (%2239:tuple{%2404:<264x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1389:<264x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1389:<264x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1389:<264x1536xbf16>{3072, 1})
          outputs: (%2412:<264x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1389:<264x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2412:<264x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2412:<264x1536xbf16>{1536, 1}, %2411:<264x1536xbf16>{3072, 1}+1536)
          outputs: (%1709:<264x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2402:tuple{%1709:<264x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2402:tuple{%1709:<264x1536xbf16>{1536,1}}))
          duration: -1
144414 2024-12-10 17:48:26.018241 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n40,rank6)
        - aten::mm:
          inputs: (%2413:<264x1536xbf16>{1536, 1}, %2416:<1536x5120xbf16>{1, 1536})
          outputs: (%1643:<264x5120xbf16>{5120,1})
          duration: -1
144518 2024-12-10 17:48:26.023656 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n40,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2402:tuple{%1709:<264x1536xbf16>{1536, 1}})
          outputs: (%1968:tuple{%2419:<264x5120xbf16>{5120,1},None:NoneType})
          duration: -1
144568 2024-12-10 17:48:26.026125 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n38,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2356:<264x5120xbf16>{5120, 1}+30172160})
          outputs: (%2239:tuple{%2416:<264x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2389:<264x5120xbf16>{5120, 1}+30172160, %2416:<264x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2389:<264x5120xbf16>{5120,1}+30172160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2388:<398x5120xbf16>{5120, 1}+31523840})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2388:<398x5120xbf16>{5120,1}+31523840}))
          duration: -1
144790 2024-12-10 17:48:26.037962 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n39,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2367:tuple{%1388:<398x5120xbf16>{5120, 1}+31523840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2367:tuple{%1388:<398x5120xbf16>{5120,1}+31523840}))
          duration: -1
144842 2024-12-10 17:48:26.040374 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n48,rank6)
        - aten::mm:
          inputs: (%2429:<398x5120xbf16>{5120, 1}+31523840, %2432:<5120x3072xbf16>{1, 5120})
          outputs: (%2433:<398x3072xbf16>{3072,1})
          duration: -1
144969 2024-12-10 17:48:26.045834 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n48,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2367:tuple{%1388:<398x5120xbf16>{5120, 1}+31523840})
          outputs: (%2266:tuple{%2435:<398x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2439:<398x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2439:<398x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2439:<398x1536xbf16>{3072, 1})
          outputs: (%2441:<398x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2439:<398x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2441:<398x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2441:<398x1536xbf16>{1536, 1}, %2440:<398x1536xbf16>{3072, 1}+1536)
          outputs: (%2274:<398x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2430:tuple{%2274:<398x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2430:tuple{%2274:<398x1536xbf16>{1536,1}}))
          duration: -1
145135 2024-12-10 17:48:26.057294 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n41,rank6)
        - aten::mm:
          inputs: (%772:<398x1536xbf16>{1536, 1}, %2445:<1536x5120xbf16>{1, 1536})
          outputs: (%2446:<398x5120xbf16>{5120,1})
          duration: -1
145236 2024-12-10 17:48:26.062721 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n41,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2430:tuple{%2274:<398x1536xbf16>{1536, 1}})
          outputs: (%1931:tuple{%2449:<398x5120xbf16>{5120,1},None:NoneType})
          duration: -1
145281 2024-12-10 17:48:26.065168 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n39,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2388:<398x5120xbf16>{5120, 1}+31523840})
          outputs: (%2266:tuple{%1663:<398x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2449:<398x5120xbf16>{5120, 1}+31523840, %1663:<398x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2449:<398x5120xbf16>{5120,1}+31523840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%1971:tuple{%2455:<400x5120xbf16>{5120, 1}+33561600})
          outputs: (torch.2_3_0.MLP(%1971:tuple{%2455:<400x5120xbf16>{5120,1}+33561600}))
          duration: -1
145508 2024-12-10 17:48:26.076943 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n40,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2400:tuple{%1388:<400x5120xbf16>{5120, 1}+33561600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2400:tuple{%1388:<400x5120xbf16>{5120,1}+33561600}))
          duration: -1
145561 2024-12-10 17:48:26.079349 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n49,rank6)
        - aten::mm:
          inputs: (%2445:<400x5120xbf16>{5120, 1}+33561600, %2416:<5120x3072xbf16>{1, 5120})
          outputs: (%2461:<400x3072xbf16>{3072,1})
          duration: -1
145684 2024-12-10 17:48:26.084768 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n49,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2400:tuple{%1388:<400x5120xbf16>{5120, 1}+33561600})
          outputs: (%1968:tuple{%2432:<400x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2467:<400x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2467:<400x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2467:<400x1536xbf16>{3072, 1})
          outputs: (%2469:<400x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2467:<400x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2469:<400x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2469:<400x1536xbf16>{1536, 1}, %2468:<400x1536xbf16>{3072, 1}+1536)
          outputs: (%2470:<400x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2459:tuple{%2470:<400x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2459:tuple{%2470:<400x1536xbf16>{1536,1}}))
          duration: -1
145851 2024-12-10 17:48:26.096268 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n42,rank6)
        - aten::mm:
          inputs: (%2359:<400x1536xbf16>{1536, 1}, %2388:<1536x5120xbf16>{1, 1536})
          outputs: (%2425:<400x5120xbf16>{5120,1})
          duration: -1
145954 2024-12-10 17:48:26.101715 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n42,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2459:tuple{%2470:<400x1536xbf16>{1536, 1}})
          outputs: (%2239:tuple{%2475:<400x5120xbf16>{5120,1},None:NoneType})
          duration: -1
145999 2024-12-10 17:48:26.104168 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n40,rank6)
        - ----------->api::MLP return:
          inputs: (%1971:tuple{%2455:<400x5120xbf16>{5120, 1}+33561600})
          outputs: (%1968:tuple{%2479:<400x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1388:<400x5120xbf16>{5120, 1}+33561600, %2479:<400x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1388:<400x5120xbf16>{5120,1}+33561600)
          duration: -1
146132 2024-12-10 17:48:26.110715 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n1,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%1963:tuple{%1966:<6955x5120xbf16>{5120, 1}, %1850:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2239:tuple{%1388:<6955x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%1540:<6955x5120xbf16>{5120, 1}, 0:int, %1964:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, %1388:<6955x5120xbf16>{5120, 1})
          outputs: (%1967:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%1967:<6955x5120xbf16>{5120, 1}, %1944:<6955x1xbf16>{1, 1})
          outputs: (%1930:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%2478:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2449:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%2449:<8192x5120xbf16>{5120, 1}, 0:int, %243:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, %1930:<6955x5120xbf16>{5120, 1})
          outputs: (%2483:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%2449:<1024x5120xbf16>{5120, 1}, %2483:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%2449:<1024x5120xbf16>{5120,1},%2483:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%2449:<1024x5120xbf16>{5120, 1}, %2483:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%2449:<1024x5120xbf16>{5120,1},%2483:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%2449:<1024x5120xbf16>{5120, 1}, %2483:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%2484:tuple{%2449:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2480:tuple{%1914:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%2480:tuple{%1914:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
146451 2024-12-10 17:48:26.163329 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n41,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2457:tuple{%2404:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2457:tuple{%2404:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
        - aten::mm:
          inputs: (%2496:<1024x5120xbf16>{5120, 1}, %2494:<5120x6144xbf16>{1, 5120})
          outputs: (%2497:<1024x6144xbf16>{6144,1})
          duration: -1
146706 2024-12-10 17:48:26.175283 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n50,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2457:tuple{%2404:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%522:tuple{%2479:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2494:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2494:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2494:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%2505:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2494:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%2505:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%2505:<1024x1x3072xbf16>{3072, 3072, 1}, %2504:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%2388:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2491:tuple{%2388:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2491:tuple{%2388:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
146848 2024-12-10 17:48:26.186846 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n43,rank6)
        - aten::mm:
          inputs: (%2511:<1024x3072xbf16>{3072, 1}, %2510:<3072x5120xbf16>{1, 3072})
          outputs: (%2512:<1024x5120xbf16>{5120,1})
          duration: -1
147040 2024-12-10 17:48:26.196399 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n43,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2491:tuple{%2388:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%522:tuple{%2355:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
147096 2024-12-10 17:48:26.198921 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n41,rank6)
        - ----------->api::MLP return:
          inputs: (%2480:tuple{%1914:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2239:tuple{%2520:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%2446:<1024x1x5120xbf16>{5120, 5120, 1}, %1943:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2404:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
147184 2024-12-10 17:48:26.203789 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n1,rank6)
        - ----------->api::MoELayer return:
          inputs: (%805:tuple{%855:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2485:tuple{%1615:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2525:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2525:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,True:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, True:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2526:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2526:tuple{None:NoneType,)
          duration: -1
        - ----------->api::grad call:
          inputs: (%2527:list{NotSurpot:FunctionalTensor}, %2528:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %2529:list{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, None:NoneType)
          outputs: (torch.2_3_0.grad(%2527:list{NotSurpot:FunctionalTensor},%2528:list{NotSurpot:FunctionalTensor},True:bool,False:bool,False:bool,True:bool,False:bool,%2529:list{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},None:NoneType))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple call:
          inputs: (%2528:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (torch.2_3_0._tensor_or_tensors_to_tuple(%2528:list{NotSurpot:FunctionalTensor},1:int))
          duration: -1
        - ----------->api::_tensor_or_tensors_to_tuple return:
          inputs: (%2528:list{NotSurpot:FunctionalTensor}, 1:int)
          outputs: (%2530:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_make_grads call:
          inputs: (%2531:tuple{NotSurpot:FunctionalTensor}, %2530:tuple{NotSurpot:FunctionalTensor}, False:bool)
          outputs: (torch.2_3_0._make_grads(%2531:tuple{NotSurpot:FunctionalTensor},%2530:tuple{NotSurpot:FunctionalTensor},False:bool))
          duration: -1
        - ----------->api::_make_grads return:
          inputs: (%2531:tuple{NotSurpot:FunctionalTensor}, %2530:tuple{NotSurpot:FunctionalTensor}, False:bool, %2532:list{NotSurpot:FunctionalTensor}, _function_e_pect_true_at_0_7f5666049750_:function, _function_sym_eq_at_0_7f5666049480_:function, True:bool)
          outputs: (%2533:tuple{NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::_engine_run_backward call:
          inputs: (%2531:tuple{NotSurpot:FunctionalTensor}, %735:tuple{%2533:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %2534:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict)
          outputs: (torch.2_3_0._engine_run_backward(%2531:tuple{NotSurpot:FunctionalTensor},%735:tuple{%2533:tuple{NotSurpot:FunctionalTensor},False:bool,False:bool,%2534:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor},True:bool},_'accumulate_grad'__False_:dict))
          duration: -1
        - ----------->api::_engine_run_backward return:
          inputs: (%2531:tuple{NotSurpot:FunctionalTensor}, %735:tuple{%2533:tuple{NotSurpot:FunctionalTensor}, False:bool, False:bool, %2534:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, True:bool}, _'accumulate_grad'__False_:dict, False:bool)
          outputs: (%2535:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::grad return:
          inputs: (%2527:list{NotSurpot:FunctionalTensor}, %2528:list{NotSurpot:FunctionalTensor}, True:bool, False:bool, False:bool, True:bool, False:bool, %2534:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, False:bool, %2531:tuple{NotSurpot:FunctionalTensor}, %2536:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %2537:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor}, %2533:tuple{NotSurpot:FunctionalTensor}, %2535:tuple{NotSurpot:FunctionalTensor, NotSurpot:FunctionalTensor})
          outputs: (%2535:tuple{NotSurpot:FunctionalTensor,NotSurpot:FunctionalTensor})
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2537:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2537:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55dc36d330_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55dc36d330_:_InferenceMode)
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (True:bool)
          outputs: (torch.2_3_0._force_original_view_tracking(True:bool))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: ()
          outputs: (torch.2_3_0._force_original_view_tracking())
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::CompiledFunctionBackward call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.CompiledFunctionBackward(%455:tuple{}))
          duration: -1
        - ----------->api::_force_original_view_tracking call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0._force_original_view_tracking(None:NoneType,)
          duration: -1
149965 2024-12-10 17:48:26.920120 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n1,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%2546:tuple{%2543:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2545:tuple{%2543:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2545:tuple{%2543:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
150037 2024-12-10 17:48:26.928702 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n5,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%770:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%770:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%770:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%770:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%2551:tuple{%770:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%2551:tuple{%770:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%2551:tuple{%770:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2551:tuple{%770:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%2555:tuple{%770:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %2554:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%2555:tuple{%770:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%2554:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%770:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %2551:tuple{%770:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2552:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%770:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%2552:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
150295 2024-12-10 17:48:26.974310 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n5,rank6)
150313 2024-12-10 17:48:26.976575 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n1,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%284:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2559:tuple{%284:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%2559:tuple{%284:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
150407 2024-12-10 17:48:26.984627 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n51,rank6)
        - aten::mm:
          inputs: (%1765:<1024x5120xbf16>{5120, 1}, %2569:<5120x102400xbf16>{1, 5120})
          outputs: (%2570:<1024x102400xbf16>{102400,1})
          duration: -1
150577 2024-12-10 17:48:26.998598 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n51,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2559:tuple{%284:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%2568:tuple{%758:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%2569:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%2546:tuple{%2575:<1024x1xf32>{1,1},%2576:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2575:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2575:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2577:list{%2575:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2578:tuple{%2563:list{%2575:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%2569:<1024x1x102400xf32>{102400, 102400, 1}, %2579:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%2569:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%1765:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%2570:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%1765:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%2580:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%2570:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %2580:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%2581:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%2576:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%2582:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%2582:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %2583:list{%2581:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %2576:<i32>, False:bool)
          outputs: (%2582:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2584:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2222:<1024x102400xf32>{102400, 1}, %2563:list{%2584:<1024xCUSTOM_DATA_TYPE>{1}, %1662:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1388:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%2585:<1024x1xf32>{1, 1}, %2563:list{%2581:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1388:<i32>, False:bool)
          outputs: (%2585:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%2569:<1024x1x102400xf32>{102400, 102400, 1}, out=%2569:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%2569:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%2569:<1024x1x102400xf32>{102400, 102400, 1}, %2583:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%2586:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2585:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2585:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2583:list{%2585:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2568:tuple{%2556:list{%2585:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2586:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2586:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2587:list{%2586:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2588:tuple{%2589:list{%2586:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%2586:<1024x1xf32>{1, 1})
          outputs: (%1388:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1388:<1024x1xf32>{1, 1}, %2585:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%1926:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%2569:<1024x1x102400xf32>{102400, 102400, 1}, %2590:<1024x1x1xf32>{1, 1, 1})
          outputs: (%2569:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%2591:tuple{%2569:<1024x1x102400xf32>{102400, 102400, 1}, %2581:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %1662:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%2591:tuple{%2569:<1024x1x102400xf32>{102400,102400,1},%2581:<1024x1xCUSTOM_DATA_TYPE>{1,1},%1662:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%1735:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device='cuda_6')_:dict)
          outputs: (%2576:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2576:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2576:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2576:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%1734:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device='cuda_6')_:dict)
          outputs: (%2576:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%1733:tuple{%1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1726:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1262:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[21118,___279,__2764,_____,__4345,_12143,_23872]],_device='cuda_6')_:dict)
          outputs: (%2576:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%2571:<1024xf32>{1}, %1736:<1024xf32>{1})
          outputs: (%2593:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2593:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1751:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%1736:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1520:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1751:<i32>, %1520:<i32>)
          outputs: (%2595:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%2595:<i32>)
          outputs: (%1615:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%2548:list{%2597:<1xf32>{1}}, 0:int)
          outputs: (%1388:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1388:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1388:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2598:list{%1388:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2599:tuple{%2600:list{%1388:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%1388:<1xf32>{1}, 8:int)
          outputs: (%2596:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%2595:<i32>, 1:int)
          outputs: (%1928:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%1928:<i32>, 1:int)
          outputs: (%1928:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%2601:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1926:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%1926:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%1926:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %1926:<1xf32>{1})
          outputs: (%2557:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%2557:<1xf32>{1}, 1:int)
          outputs: (%2602:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%237:<i32>, 0:int, alpha=1:int)
          outputs: (%237:<i32>)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%1928:<i32>)
          outputs: (torch.2_3_0.ChainedOptimizer(%1928:<i32>))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %1928:<i32>)
          outputs: (%1926:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%2603:<1xf32>{1}, %164:<1xf32>{1})
          outputs: (%2543:<1xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2543:<1xf32>{1}, dtype=None:NoneType)
          outputs: (%2604:<i32>)
          duration: -1
        - aten::div:
          inputs: (%2604:<i32>, 1:int)
          outputs: (%2605:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%2605:<i32>, 1:int)
          outputs: (%2576:<i32>)
          duration: -1
        - aten::div:
          inputs: (%2576:<i32>, %1520:<i32>)
          outputs: (%2605:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1388:<1024xf32>{0}, %1736:<1024xf32>{1})
          outputs: (%2576:<1024xf32>{1})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2604:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::rsub:
          inputs: (%2404:<1024xf32>{1}, 1_0:float, 1:int)
          outputs: (%2608:<1024xf32>{1})
          duration: -1
        - aten::index:
          inputs: (%1388:<1024x102400xf32>{102400, 1}, %2606:list{%2604:<1024xCUSTOM_DATA_TYPE>{1}, %1662:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2479:<1024xf32>{1})
          duration: -1
        - aten::sub_:
          inputs: (%2479:<1024xf32>{1}, %2608:<1024xf32>{1}, alpha=1:int)
          outputs: (%2479:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%1388:<1024x102400xf32>{102400, 1}, %1981:list{%2604:<1024xCUSTOM_DATA_TYPE>{1}, %1662:<1024xCUSTOM_DATA_TYPE>{1}}, %2479:<1024xf32>{1}, False:bool)
          outputs: (%1388:<1024x102400xf32>{102400,1})
          duration: -1
        - aten::mul_:
          inputs: (%2569:<1024x1x102400xf32>{102400, 102400, 1}, %2609:<1024x1x1xf32>{1, 1024, 1})
          outputs: (%2569:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
152209 2024-12-10 17:48:27.134270 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.output_layer,n51,rank6)
        - aten::mm:
          inputs: (%2569:<1024x102400xbf16>{102400, 1}, %152:<102400x5120xbf16>{5120, 1})
          outputs: (%2582:<1024x5120xbf16>{5120,1})
          duration: -1
152315 2024-12-10 17:48:27.152101 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.output_layer,n51,rank6)
152317 2024-12-10 17:48:27.152167 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Backward:deepseek_v2.decoder,n1,rank6)
152318 2024-12-10 17:48:27.152214 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Backward:deepseek_v2.decoder,n1,rank6)
152320 2024-12-10 17:48:27.152265 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Backward:deepseek_v2.decoder.final_layernorm,n5,rank6)
        - aten::add_:
          inputs: (%175:<5120xf32>{1}+524288000, %747:<5120xbf16>{1}, alpha=1:int)
          outputs: (%175:<5120xf32>{1}+524288000)
          duration: -1
152405 2024-12-10 17:48:27.154295 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Backward:deepseek_v2.decoder.final_layernorm,n5,rank6)
152408 2024-12-10 17:48:27.154361 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Backward:deepseek_v2.decoder.layers.0,n1,rank6)
152410 2024-12-10 17:48:27.154404 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Backward:deepseek_v2.decoder.layers.0,n1,rank6)
        - profiler::_record_function_enter_new:
          inputs: (compile_f___locals__bw_compiler_(dynamo_timed):str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
152463 2024-12-10 17:48:27.155438 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Backward:deepseek_v2.decoder.layers.0.mlp,n1,rank6)
152485 2024-12-10 17:48:27.155854 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n41,rank6)
152490 2024-12-10 17:48:27.155921 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n43,rank6)
        - aten::mm:
          inputs: (%2151:<1024x5120xbf16>{5120, 1}, %67:<5120x3072xbf16>{3072, 1})
          outputs: (%2569:<1024x3072xbf16>{3072,1})
          duration: -1
152617 2024-12-10 17:48:27.158306 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n43,rank6)
        - aten::mul:
          inputs: (%2576:<1024x1x3072xbf16>{3072, 3072, 1}, %2505:<1024x1x3072xbf16>{3072, 3072, 1})
          outputs: (%2507:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%2576:<1024x1x3072xbf16>{3072, 3072, 1}, %2504:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%2566:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2566:<1024x1x3072xbf16>{3072, 3072, 1}, %2494:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%2504:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::cat:
          inputs: (%2563:list{%2504:<1024x1x3072xbf16>{3072, 3072, 1}, %2507:<1024x1x3072xbf16>{3072, 3072, 1}}, 2:int)
          outputs: (%2151:<1024x1x6144xbf16>{6144,6144,1})
          duration: -1
152678 2024-12-10 17:48:27.159446 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n50,rank6)
        - aten::mm:
          inputs: (%2499:<1024x6144xbf16>{6144, 1}, %70:<6144x5120xbf16>{5120, 1})
          outputs: (%2494:<1024x5120xbf16>{5120,1})
          duration: -1
152812 2024-12-10 17:48:27.162141 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n50,rank6)
152814 2024-12-10 17:48:27.162204 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n41,rank6)
        - c10d::_allgather_base_:
          inputs: (%2593:<8192x5120xbf16>{5120, 1}, %2151:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2588:tuple{%2593:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gather:
          inputs: (%2593:<8192x5120xbf16>{5120, 1}, 0:int, %243:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%2605:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%2605:<6955x5120xbf16>{5120, 1}, %1967:<6955x5120xbf16>{5120, 1})
          outputs: (%2576:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%2605:<6955x5120xbf16>{5120, 1}, %1944:<6955x1xbf16>{1, 1})
          outputs: (%2569:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::sum:
          inputs: (%2576:<6955x5120xbf16>{5120, 1}, %2610:list{1:int}, True:bool, dtype=None:NoneType)
          outputs: (%2613:<6955x1xbf16>{1,1})
          duration: -1
        - aten::gather:
          inputs: (%2569:<6955x5120xbf16>{5120, 1}, 0:int, %1964:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%2605:<6955x5120xbf16>{5120,1})
          duration: -1
153067 2024-12-10 17:48:27.172473 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts,n1,rank6)
        - aten::copy_:
          inputs: (%2151:<6955x5120xbf16>{5120, 1}, %2605:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2151:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2569:<400x5120xbf16>{5120, 1}+33561600, %2494:<400x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2569:<400x5120xbf16>{5120,1}+33561600)
          duration: -1
153175 2024-12-10 17:48:27.174714 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n40,rank6)
153178 2024-12-10 17:48:27.174777 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n42,rank6)
        - aten::mm:
          inputs: (%2576:<400x5120xbf16>{5120, 1}, %145:<5120x1536xbf16>{1536, 1})
          outputs: (%2388:<400x1536xbf16>{1536,1})
          duration: -1
153249 2024-12-10 17:48:27.175991 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n42,rank6)
        - aten::mul:
          inputs: (%2388:<400x1536xbf16>{1536, 1}, %2469:<400x1536xbf16>{1536, 1})
          outputs: (%2593:<400x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2388:<400x1536xbf16>{1536, 1}, %2468:<400x1536xbf16>{3072, 1}+1536)
          outputs: (%2507:<400x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2507:<400x1536xbf16>{1536, 1}, %2467:<400x1536xbf16>{3072, 1})
          outputs: (%2425:<400x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2598:list{%2425:<400x1536xbf16>{1536, 1}, %2593:<400x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2461:<400x3072xbf16>{3072,1})
          duration: -1
153324 2024-12-10 17:48:27.177130 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n49,rank6)
        - aten::mm:
          inputs: (%2461:<400x3072xbf16>{3072, 1}, %147:<3072x5120xbf16>{5120, 1})
          outputs: (%2569:<400x5120xbf16>{5120,1})
          duration: -1
153393 2024-12-10 17:48:27.178356 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n49,rank6)
153399 2024-12-10 17:48:27.178417 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n40,rank6)
        - aten::copy_:
          inputs: (%2593:<6955x5120xbf16>{5120, 1}, %2151:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2593:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2507:<398x5120xbf16>{5120, 1}+31523840, %2499:<398x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2507:<398x5120xbf16>{5120,1}+31523840)
          duration: -1
153526 2024-12-10 17:48:27.181064 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n39,rank6)
153530 2024-12-10 17:48:27.181129 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n41,rank6)
        - aten::mm:
          inputs: (%2615:<398x5120xbf16>{5120, 1}, %141:<5120x1536xbf16>{1536, 1})
          outputs: (%2388:<398x1536xbf16>{1536,1})
          duration: -1
153595 2024-12-10 17:48:27.182337 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n41,rank6)
        - aten::mul:
          inputs: (%2388:<398x1536xbf16>{1536, 1}, %2441:<398x1536xbf16>{1536, 1})
          outputs: (%2151:<398x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2388:<398x1536xbf16>{1536, 1}, %2440:<398x1536xbf16>{3072, 1}+1536)
          outputs: (%2609:<398x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2609:<398x1536xbf16>{1536, 1}, %2439:<398x1536xbf16>{3072, 1})
          outputs: (%2388:<398x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2563:list{%2388:<398x1536xbf16>{1536, 1}, %2151:<398x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2440:<398x3072xbf16>{3072,1})
          duration: -1
153654 2024-12-10 17:48:27.183425 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n48,rank6)
        - aten::mm:
          inputs: (%2440:<398x3072xbf16>{3072, 1}, %143:<3072x5120xbf16>{5120, 1})
          outputs: (%2439:<398x5120xbf16>{5120,1})
          duration: -1
153713 2024-12-10 17:48:27.184621 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n48,rank6)
153717 2024-12-10 17:48:27.184681 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n39,rank6)
        - aten::add:
          inputs: (%2576:<6955x5120xbf16>{5120, 1}, %2440:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2151:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2576:<6955x5120xbf16>{5120, 1}, %2593:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2576:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2388:<264x5120xbf16>{5120, 1}+30172160, %2617:<264x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2388:<264x5120xbf16>{5120,1}+30172160)
          duration: -1
153845 2024-12-10 17:48:27.187684 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n38,rank6)
153849 2024-12-10 17:48:27.187751 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n40,rank6)
        - aten::mm:
          inputs: (%2609:<264x5120xbf16>{5120, 1}, %137:<5120x1536xbf16>{1536, 1})
          outputs: (%2593:<264x1536xbf16>{1536,1})
          duration: -1
153911 2024-12-10 17:48:27.188943 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n40,rank6)
        - aten::mul:
          inputs: (%2593:<264x1536xbf16>{1536, 1}, %2412:<264x1536xbf16>{1536, 1})
          outputs: (%2388:<264x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2593:<264x1536xbf16>{1536, 1}, %2411:<264x1536xbf16>{3072, 1}+1536)
          outputs: (%2616:<264x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2616:<264x1536xbf16>{1536, 1}, %1389:<264x1536xbf16>{3072, 1})
          outputs: (%2507:<264x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2598:list{%2507:<264x1536xbf16>{1536, 1}, %2388:<264x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2569:<264x3072xbf16>{3072,1})
          duration: -1
153978 2024-12-10 17:48:27.190066 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n47,rank6)
        - aten::mm:
          inputs: (%2569:<264x3072xbf16>{3072, 1}, %139:<3072x5120xbf16>{5120, 1})
          outputs: (%2411:<264x5120xbf16>{5120,1})
          duration: -1
154032 2024-12-10 17:48:27.191269 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n47,rank6)
154038 2024-12-10 17:48:27.191341 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n38,rank6)
        - aten::add:
          inputs: (%2151:<6955x5120xbf16>{5120, 1}, %2580:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2388:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2593:<6955x5120xbf16>{5120, 1}, %2576:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2593:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2580:<569x5120xbf16>{5120, 1}+27258880, %2507:<569x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2580:<569x5120xbf16>{5120,1}+27258880)
          duration: -1
154170 2024-12-10 17:48:27.194294 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n37,rank6)
154172 2024-12-10 17:48:27.194356 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n39,rank6)
        - aten::mm:
          inputs: (%2151:<569x5120xbf16>{5120, 1}, %133:<5120x1536xbf16>{1536, 1})
          outputs: (%2576:<569x1536xbf16>{1536,1})
          duration: -1
154242 2024-12-10 17:48:27.195517 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n39,rank6)
        - aten::mul:
          inputs: (%2576:<569x1536xbf16>{1536, 1}, %2382:<569x1536xbf16>{1536, 1})
          outputs: (%2383:<569x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2576:<569x1536xbf16>{1536, 1}, %2381:<569x1536xbf16>{3072, 1}+1536)
          outputs: (%2615:<569x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2615:<569x1536xbf16>{1536, 1}, %2380:<569x1536xbf16>{3072, 1})
          outputs: (%2381:<569x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1981:list{%2381:<569x1536xbf16>{1536, 1}, %2383:<569x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2576:<569x3072xbf16>{3072,1})
          duration: -1
154315 2024-12-10 17:48:27.196635 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n46,rank6)
        - aten::mm:
          inputs: (%2576:<569x3072xbf16>{3072, 1}, %135:<3072x5120xbf16>{5120, 1})
          outputs: (%2380:<569x5120xbf16>{5120,1})
          duration: -1
154381 2024-12-10 17:48:27.197863 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n46,rank6)
154384 2024-12-10 17:48:27.197928 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n37,rank6)
        - aten::add:
          inputs: (%2388:<6955x5120xbf16>{5120, 1}, %1954:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2576:<6955x5120xbf16>{5120, 1}, %2593:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2576:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2380:<406x5120xbf16>{5120, 1}+25180160, %2425:<406x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2380:<406x5120xbf16>{5120,1}+25180160)
          duration: -1
154532 2024-12-10 17:48:27.200932 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n36,rank6)
154536 2024-12-10 17:48:27.200993 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n38,rank6)
        - aten::mm:
          inputs: (%2388:<406x5120xbf16>{5120, 1}, %129:<5120x1536xbf16>{1536, 1})
          outputs: (%2507:<406x1536xbf16>{1536,1})
          duration: -1
154610 2024-12-10 17:48:27.202178 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n38,rank6)
        - aten::mul:
          inputs: (%2507:<406x1536xbf16>{1536, 1}, %2350:<406x1536xbf16>{1536, 1})
          outputs: (%2593:<406x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2507:<406x1536xbf16>{1536, 1}, %2349:<406x1536xbf16>{3072, 1}+1536)
          outputs: (%2425:<406x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2425:<406x1536xbf16>{1536, 1}, %2348:<406x1536xbf16>{3072, 1})
          outputs: (%2373:<406x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2556:list{%2373:<406x1536xbf16>{1536, 1}, %2593:<406x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2348:<406x3072xbf16>{3072,1})
          duration: -1
154681 2024-12-10 17:48:27.203299 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n45,rank6)
        - aten::mm:
          inputs: (%2348:<406x3072xbf16>{3072, 1}, %131:<3072x5120xbf16>{5120, 1})
          outputs: (%2350:<406x5120xbf16>{5120,1})
          duration: -1
154751 2024-12-10 17:48:27.204495 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n45,rank6)
154755 2024-12-10 17:48:27.204564 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n36,rank6)
        - aten::add:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2348:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2593:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2576:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2388:<344x5120xbf16>{5120, 1}+23418880, %2381:<344x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2388:<344x5120xbf16>{5120,1}+23418880)
          duration: -1
154900 2024-12-10 17:48:27.207489 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n35,rank6)
154902 2024-12-10 17:48:27.207549 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n37,rank6)
        - aten::mm:
          inputs: (%2373:<344x5120xbf16>{5120, 1}, %125:<5120x1536xbf16>{1536, 1})
          outputs: (%2381:<344x1536xbf16>{1536,1})
          duration: -1
154980 2024-12-10 17:48:27.208781 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n37,rank6)
        - aten::mul:
          inputs: (%2381:<344x1536xbf16>{1536, 1}, %2252:<344x1536xbf16>{1536, 1})
          outputs: (%2576:<344x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2381:<344x1536xbf16>{1536, 1}, %2317:<344x1536xbf16>{3072, 1}+1536)
          outputs: (%2320:<344x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2320:<344x1536xbf16>{1536, 1}, %2310:<344x1536xbf16>{3072, 1})
          outputs: (%2381:<344x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2610:list{%2381:<344x1536xbf16>{1536, 1}, %2576:<344x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2310:<344x3072xbf16>{3072,1})
          duration: -1
155048 2024-12-10 17:48:27.209929 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n44,rank6)
        - aten::mm:
          inputs: (%2310:<344x3072xbf16>{3072, 1}, %127:<3072x5120xbf16>{5120, 1})
          outputs: (%2252:<344x5120xbf16>{5120,1})
          duration: -1
155117 2024-12-10 17:48:27.211137 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n44,rank6)
155123 2024-12-10 17:48:27.211197 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n35,rank6)
        - aten::add:
          inputs: (%2593:<6955x5120xbf16>{5120, 1}, %2576:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2310:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2593:<6955x5120xbf16>{5120, 1}, %2566:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2593:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2576:<575x5120xbf16>{5120, 1}+20474880, %2350:<575x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2576:<575x5120xbf16>{5120,1}+20474880)
          duration: -1
155270 2024-12-10 17:48:27.214127 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n34,rank6)
155272 2024-12-10 17:48:27.214186 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n36,rank6)
        - aten::mm:
          inputs: (%2586:<575x5120xbf16>{5120, 1}, %122:<5120x1536xbf16>{1536, 1})
          outputs: (%2566:<575x1536xbf16>{1536,1})
          duration: -1
155347 2024-12-10 17:48:27.215377 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n36,rank6)
        - aten::mul:
          inputs: (%2566:<575x1536xbf16>{1536, 1}, %2288:<575x1536xbf16>{1536, 1})
          outputs: (%2576:<575x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2566:<575x1536xbf16>{1536, 1}, %2202:<575x1536xbf16>{3072, 1}+1536)
          outputs: (%2469:<575x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2469:<575x1536xbf16>{1536, 1}, %1711:<575x1536xbf16>{3072, 1})
          outputs: (%2151:<575x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2563:list{%2151:<575x1536xbf16>{1536, 1}, %2576:<575x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%276:<575x3072xbf16>{3072,1})
          duration: -1
155418 2024-12-10 17:48:27.216487 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n43,rank6)
        - aten::mm:
          inputs: (%276:<575x3072xbf16>{3072, 1}, %123:<3072x5120xbf16>{5120, 1})
          outputs: (%2151:<575x5120xbf16>{5120,1})
          duration: -1
155486 2024-12-10 17:48:27.217704 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n43,rank6)
155492 2024-12-10 17:48:27.217768 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n34,rank6)
        - aten::add:
          inputs: (%2310:<6955x5120xbf16>{5120, 1}, %276:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2620:<6955x5120xbf16>{5120, 1}, %2593:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2620:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2576:<377x5120xbf16>{5120, 1}+18544640, %2365:<377x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2576:<377x5120xbf16>{5120,1}+18544640)
          duration: -1
155634 2024-12-10 17:48:27.220683 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n33,rank6)
155636 2024-12-10 17:48:27.220743 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n35,rank6)
        - aten::mm:
          inputs: (%2202:<377x5120xbf16>{5120, 1}, %80:<5120x1536xbf16>{1536, 1})
          outputs: (%2569:<377x1536xbf16>{1536,1})
          duration: -1
155712 2024-12-10 17:48:27.221992 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n35,rank6)
        - aten::mul:
          inputs: (%2569:<377x1536xbf16>{1536, 1}, %2260:<377x1536xbf16>{1536, 1})
          outputs: (%2576:<377x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2569:<377x1536xbf16>{1536, 1}, %2185:<377x1536xbf16>{3072, 1}+1536)
          outputs: (%2404:<377x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2404:<377x1536xbf16>{1536, 1}, %1720:<377x1536xbf16>{3072, 1})
          outputs: (%2593:<377x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2598:list{%2593:<377x1536xbf16>{1536, 1}, %2576:<377x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2253:<377x3072xbf16>{3072,1})
          duration: -1
155780 2024-12-10 17:48:27.223128 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n42,rank6)
        - aten::mm:
          inputs: (%2253:<377x3072xbf16>{3072, 1}, %120:<3072x5120xbf16>{5120, 1})
          outputs: (%2569:<377x5120xbf16>{5120,1})
          duration: -1
155855 2024-12-10 17:48:27.224339 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n42,rank6)
155859 2024-12-10 17:48:27.224400 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n33,rank6)
        - aten::add:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2576:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2593:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2620:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2576:<183x5120xbf16>{5120, 1}+17607680, %2261:<183x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2576:<183x5120xbf16>{5120,1}+17607680)
          duration: -1
156004 2024-12-10 17:48:27.227394 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n32,rank6)
156008 2024-12-10 17:48:27.227463 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n34,rank6)
        - aten::mm:
          inputs: (%2388:<183x5120xbf16>{5120, 1}, %114:<5120x1536xbf16>{1536, 1})
          outputs: (%2282:<183x1536xbf16>{1536,1})
          duration: -1
156083 2024-12-10 17:48:27.228628 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n34,rank6)
        - aten::mul:
          inputs: (%2282:<183x1536xbf16>{1536, 1}, %1845:<183x1536xbf16>{1536, 1})
          outputs: (%2236:<183x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2282:<183x1536xbf16>{1536, 1}, %1238:<183x1536xbf16>{3072, 1}+1536)
          outputs: (%2576:<183x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2576:<183x1536xbf16>{1536, 1}, %2234:<183x1536xbf16>{3072, 1})
          outputs: (%1238:<183x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1981:list{%1238:<183x1536xbf16>{1536, 1}, %2236:<183x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%276:<183x3072xbf16>{3072,1})
          duration: -1
156150 2024-12-10 17:48:27.229760 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n41,rank6)
        - aten::mm:
          inputs: (%276:<183x3072xbf16>{3072, 1}, %116:<3072x5120xbf16>{5120, 1})
          outputs: (%2234:<183x5120xbf16>{5120,1})
          duration: -1
156223 2024-12-10 17:48:27.230964 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n41,rank6)
156226 2024-12-10 17:48:27.231025 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n32,rank6)
        - aten::add:
          inputs: (%2593:<6955x5120xbf16>{5120, 1}, %276:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1424:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2229:<6955x5120xbf16>{5120, 1}, %2566:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2229:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<107x5120xbf16>{5120, 1}+17059840, %2576:<107x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<107x5120xbf16>{5120,1}+17059840)
          duration: -1
156368 2024-12-10 17:48:27.233990 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n31,rank6)
156372 2024-12-10 17:48:27.234050 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n33,rank6)
        - aten::mm:
          inputs: (%2593:<107x5120xbf16>{5120, 1}, %110:<5120x1536xbf16>{1536, 1})
          outputs: (%2566:<107x1536xbf16>{1536,1})
          duration: -1
156448 2024-12-10 17:48:27.235280 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n33,rank6)
        - aten::mul:
          inputs: (%2566:<107x1536xbf16>{1536, 1}, %2207:<107x1536xbf16>{1536, 1})
          outputs: (%2576:<107x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2566:<107x1536xbf16>{1536, 1}, %2206:<107x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<107x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1238:<107x1536xbf16>{1536, 1}, %2198:<107x1536xbf16>{3072, 1})
          outputs: (%2225:<107x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2556:list{%2225:<107x1536xbf16>{1536, 1}, %2576:<107x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2151:<107x3072xbf16>{3072,1})
          duration: -1
156515 2024-12-10 17:48:27.236368 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n40,rank6)
        - aten::mm:
          inputs: (%2151:<107x3072xbf16>{3072, 1}, %112:<3072x5120xbf16>{5120, 1})
          outputs: (%1238:<107x5120xbf16>{5120,1})
          duration: -1
156585 2024-12-10 17:48:27.237595 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n40,rank6)
156590 2024-12-10 17:48:27.237656 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n31,rank6)
        - aten::add:
          inputs: (%1424:<6955x5120xbf16>{5120, 1}, %2576:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1238:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1751:<6955x5120xbf16>{5120, 1}, %2229:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1751:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2593:<232x5120xbf16>{5120, 1}+15872000, %2576:<232x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2593:<232x5120xbf16>{5120,1}+15872000)
          duration: -1
156732 2024-12-10 17:48:27.240619 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n30,rank6)
156735 2024-12-10 17:48:27.240679 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n32,rank6)
        - aten::mm:
          inputs: (%2225:<232x5120xbf16>{5120, 1}, %106:<5120x1536xbf16>{1536, 1})
          outputs: (%2151:<232x1536xbf16>{1536,1})
          duration: -1
156802 2024-12-10 17:48:27.241869 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n32,rank6)
        - aten::mul:
          inputs: (%2151:<232x1536xbf16>{1536, 1}, %2178:<232x1536xbf16>{1536, 1})
          outputs: (%2569:<232x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2151:<232x1536xbf16>{1536, 1}, %2177:<232x1536xbf16>{3072, 1}+1536)
          outputs: (%2576:<232x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2576:<232x1536xbf16>{1536, 1}, %2004:<232x1536xbf16>{3072, 1})
          outputs: (%2151:<232x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2610:list{%2151:<232x1536xbf16>{1536, 1}, %2569:<232x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2004:<232x3072xbf16>{3072,1})
          duration: -1
156861 2024-12-10 17:48:27.242975 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n39,rank6)
        - aten::mm:
          inputs: (%2004:<232x3072xbf16>{3072, 1}, %108:<3072x5120xbf16>{5120, 1})
          outputs: (%2177:<232x5120xbf16>{5120,1})
          duration: -1
156922 2024-12-10 17:48:27.244156 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n39,rank6)
156926 2024-12-10 17:48:27.244216 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n30,rank6)
        - aten::add:
          inputs: (%1238:<6955x5120xbf16>{5120, 1}, %2169:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<6955x5120xbf16>{5120, 1}, %1751:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2004:<115x5120xbf16>{5120, 1}+15283200, %2177:<115x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2004:<115x5120xbf16>{5120,1}+15283200)
          duration: -1
157057 2024-12-10 17:48:27.247215 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n29,rank6)
157059 2024-12-10 17:48:27.247279 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n31,rank6)
        - aten::mm:
          inputs: (%2225:<115x5120xbf16>{5120, 1}, %102:<5120x1536xbf16>{1536, 1})
          outputs: (%2576:<115x1536xbf16>{1536,1})
          duration: -1
157122 2024-12-10 17:48:27.248471 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n31,rank6)
        - aten::mul:
          inputs: (%2576:<115x1536xbf16>{1536, 1}, %2155:<115x1536xbf16>{1536, 1})
          outputs: (%2177:<115x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2576:<115x1536xbf16>{1536, 1}, %1986:<115x1536xbf16>{3072, 1}+1536)
          outputs: (%276:<115x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%276:<115x1536xbf16>{1536, 1}, %1670:<115x1536xbf16>{3072, 1})
          outputs: (%1986:<115x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2563:list{%1986:<115x1536xbf16>{1536, 1}, %2177:<115x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%276:<115x3072xbf16>{3072,1})
          duration: -1
157181 2024-12-10 17:48:27.249601 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n38,rank6)
        - aten::mm:
          inputs: (%276:<115x3072xbf16>{3072, 1}, %104:<3072x5120xbf16>{5120, 1})
          outputs: (%2576:<115x5120xbf16>{5120,1})
          duration: -1
157243 2024-12-10 17:48:27.250780 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n38,rank6)
157246 2024-12-10 17:48:27.250841 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n29,rank6)
        - aten::add:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %1751:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1670:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2177:<6955x5120xbf16>{5120, 1}, %1238:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2177:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%478:<279x5120xbf16>{5120, 1}+13854720, %2566:<279x5120xbf16>{5120, 1}, False:bool)
          outputs: (%478:<279x5120xbf16>{5120,1}+13854720)
          duration: -1
157385 2024-12-10 17:48:27.253809 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n28,rank6)
157388 2024-12-10 17:48:27.253870 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n30,rank6)
        - aten::mm:
          inputs: (%276:<279x5120xbf16>{5120, 1}, %99:<5120x1536xbf16>{1536, 1})
          outputs: (%2566:<279x1536xbf16>{1536,1})
          duration: -1
157459 2024-12-10 17:48:27.255058 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n30,rank6)
        - aten::mul:
          inputs: (%2566:<279x1536xbf16>{1536, 1}, %1710:<279x1536xbf16>{1536, 1})
          outputs: (%2349:<279x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2566:<279x1536xbf16>{1536, 1}, %2082:<279x1536xbf16>{3072, 1}+1536)
          outputs: (%1662:<279x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%1662:<279x1536xbf16>{1536, 1}, %558:<279x1536xbf16>{3072, 1})
          outputs: (%1710:<279x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2598:list{%1710:<279x1536xbf16>{1536, 1}, %2349:<279x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%1707:<279x3072xbf16>{3072,1})
          duration: -1
157524 2024-12-10 17:48:27.256164 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n37,rank6)
        - aten::mm:
          inputs: (%1707:<279x3072xbf16>{3072, 1}, %100:<3072x5120xbf16>{5120, 1})
          outputs: (%2133:<279x5120xbf16>{5120,1})
          duration: -1
157594 2024-12-10 17:48:27.257391 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n37,rank6)
157598 2024-12-10 17:48:27.257456 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n28,rank6)
        - aten::add:
          inputs: (%1670:<6955x5120xbf16>{5120, 1}, %2225:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1707:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2177:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<733x5120xbf16>{5120, 1}+10101760, %2225:<733x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<733x5120xbf16>{5120,1}+10101760)
          duration: -1
157751 2024-12-10 17:48:27.260449 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n27,rank6)
157755 2024-12-10 17:48:27.260515 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n29,rank6)
        - aten::mm:
          inputs: (%1751:<733x5120xbf16>{5120, 1}, %95:<5120x1536xbf16>{1536, 1})
          outputs: (%1238:<733x1536xbf16>{1536,1})
          duration: -1
157818 2024-12-10 17:48:27.261721 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n29,rank6)
        - aten::mul:
          inputs: (%1238:<733x1536xbf16>{1536, 1}, %2114:<733x1536xbf16>{1536, 1})
          outputs: (%2177:<733x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<733x1536xbf16>{1536, 1}, %2113:<733x1536xbf16>{3072, 1}+1536)
          outputs: (%558:<733x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%558:<733x1536xbf16>{1536, 1}, %1362:<733x1536xbf16>{3072, 1})
          outputs: (%1238:<733x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1981:list{%1238:<733x1536xbf16>{1536, 1}, %2177:<733x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2225:<733x3072xbf16>{3072,1})
          duration: -1
157887 2024-12-10 17:48:27.262840 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n36,rank6)
        - aten::mm:
          inputs: (%2225:<733x3072xbf16>{3072, 1}, %97:<3072x5120xbf16>{5120, 1})
          outputs: (%1238:<733x5120xbf16>{5120,1})
          duration: -1
157960 2024-12-10 17:48:27.264143 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n36,rank6)
157964 2024-12-10 17:48:27.264204 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n27,rank6)
        - aten::add:
          inputs: (%1707:<6955x5120xbf16>{5120, 1}, %2229:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1238:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2177:<6955x5120xbf16>{5120, 1}, %2566:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2177:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2229:<48x5120xbf16>{5120, 1}+9856000, %1362:<48x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2229:<48x5120xbf16>{5120,1}+9856000)
          duration: -1
158120 2024-12-10 17:48:27.267144 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n26,rank6)
158121 2024-12-10 17:48:27.267206 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n28,rank6)
        - aten::mm:
          inputs: (%2225:<48x5120xbf16>{5120, 1}, %91:<5120x1536xbf16>{1536, 1})
          outputs: (%2151:<48x1536xbf16>{1536,1})
          duration: -1
158190 2024-12-10 17:48:27.268397 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n28,rank6)
        - aten::mul:
          inputs: (%2151:<48x1536xbf16>{1536, 1}, %1351:<48x1536xbf16>{1536, 1})
          outputs: (%2229:<48x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2151:<48x1536xbf16>{1536, 1}, %1334:<48x1536xbf16>{3072, 1}+1536)
          outputs: (%2096:<48x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2096:<48x1536xbf16>{1536, 1}, %2095:<48x1536xbf16>{3072, 1})
          outputs: (%2566:<48x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2556:list{%2566:<48x1536xbf16>{1536, 1}, %2229:<48x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2053:<48x3072xbf16>{3072,1})
          duration: -1
158255 2024-12-10 17:48:27.269579 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n35,rank6)
        - aten::mm:
          inputs: (%2053:<48x3072xbf16>{3072, 1}, %93:<3072x5120xbf16>{5120, 1})
          outputs: (%2151:<48x5120xbf16>{5120,1})
          duration: -1
158320 2024-12-10 17:48:27.270753 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n35,rank6)
158327 2024-12-10 17:48:27.270818 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n26,rank6)
        - aten::add:
          inputs: (%1238:<6955x5120xbf16>{5120, 1}, %2349:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2229:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2177:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2349:<462x5120xbf16>{5120, 1}+7490560, %2114:<462x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2349:<462x5120xbf16>{5120,1}+7490560)
          duration: -1
158516 2024-12-10 17:48:27.274342 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n25,rank6)
158520 2024-12-10 17:48:27.274403 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n27,rank6)
        - aten::mm:
          inputs: (%1238:<462x5120xbf16>{5120, 1}, %88:<5120x1536xbf16>{1536, 1})
          outputs: (%2177:<462x1536xbf16>{1536,1})
          duration: -1
158586 2024-12-10 17:48:27.275576 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n27,rank6)
        - aten::mul:
          inputs: (%2177:<462x1536xbf16>{1536, 1}, %2070:<462x1536xbf16>{1536, 1})
          outputs: (%2349:<462x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2177:<462x1536xbf16>{1536, 1}, %2069:<462x1536xbf16>{3072, 1}+1536)
          outputs: (%2114:<462x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2114:<462x1536xbf16>{1536, 1}, %2068:<462x1536xbf16>{3072, 1})
          outputs: (%2177:<462x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2610:list{%2177:<462x1536xbf16>{1536, 1}, %2349:<462x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2068:<462x3072xbf16>{3072,1})
          duration: -1
158655 2024-12-10 17:48:27.276686 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n34,rank6)
        - aten::mm:
          inputs: (%2068:<462x3072xbf16>{3072, 1}, %89:<3072x5120xbf16>{5120, 1})
          outputs: (%2177:<462x5120xbf16>{5120,1})
          duration: -1
158719 2024-12-10 17:48:27.277909 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n34,rank6)
158721 2024-12-10 17:48:27.277969 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n25,rank6)
        - aten::add:
          inputs: (%2229:<6955x5120xbf16>{5120, 1}, %2225:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%1238:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2177:<6955x5120xbf16>{5120, 1}, %2566:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2177:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2225:<294x5120xbf16>{5120, 1}+5985280, %1351:<294x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2225:<294x5120xbf16>{5120,1}+5985280)
          duration: -1
158885 2024-12-10 17:48:27.280914 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n24,rank6)
158888 2024-12-10 17:48:27.280984 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n26,rank6)
        - aten::mm:
          inputs: (%2229:<294x5120xbf16>{5120, 1}, %84:<5120x1536xbf16>{1536, 1})
          outputs: (%2151:<294x1536xbf16>{1536,1})
          duration: -1
158954 2024-12-10 17:48:27.282171 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n26,rank6)
        - aten::mul:
          inputs: (%2151:<294x1536xbf16>{1536, 1}, %2043:<294x1536xbf16>{1536, 1})
          outputs: (%2349:<294x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2151:<294x1536xbf16>{1536, 1}, %2025:<294x1536xbf16>{3072, 1}+1536)
          outputs: (%2236:<294x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2236:<294x1536xbf16>{1536, 1}, %488:<294x1536xbf16>{3072, 1})
          outputs: (%2566:<294x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2563:list{%2566:<294x1536xbf16>{1536, 1}, %2349:<294x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2037:<294x3072xbf16>{3072,1})
          duration: -1
159023 2024-12-10 17:48:27.283313 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n33,rank6)
        - aten::mm:
          inputs: (%2037:<294x3072xbf16>{3072, 1}, %86:<3072x5120xbf16>{5120, 1})
          outputs: (%2151:<294x5120xbf16>{5120,1})
          duration: -1
159085 2024-12-10 17:48:27.284513 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n33,rank6)
159089 2024-12-10 17:48:27.284574 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n24,rank6)
        - aten::add:
          inputs: (%1238:<6955x5120xbf16>{5120, 1}, %2349:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%276:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2566:<6955x5120xbf16>{5120, 1}, %2177:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2566:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2349:<308x5120xbf16>{5120, 1}+4408320, %2030:<308x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2349:<308x5120xbf16>{5120,1}+4408320)
          duration: -1
159252 2024-12-10 17:48:27.287549 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n23,rank6)
159256 2024-12-10 17:48:27.287620 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n25,rank6)
        - aten::mm:
          inputs: (%1238:<308x5120xbf16>{5120, 1}, %81:<5120x1536xbf16>{1536, 1})
          outputs: (%2177:<308x1536xbf16>{1536,1})
          duration: -1
159318 2024-12-10 17:48:27.288821 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n25,rank6)
        - aten::mul:
          inputs: (%2177:<308x1536xbf16>{1536, 1}, %1346:<308x1536xbf16>{1536, 1})
          outputs: (%2225:<308x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2177:<308x1536xbf16>{1536, 1}, %726:<308x1536xbf16>{3072, 1}+1536)
          outputs: (%2034:<308x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2034:<308x1536xbf16>{1536, 1}, %487:<308x1536xbf16>{3072, 1})
          outputs: (%2177:<308x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2598:list{%2177:<308x1536xbf16>{1536, 1}, %2225:<308x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%487:<308x3072xbf16>{3072,1})
          duration: -1
159384 2024-12-10 17:48:27.289939 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n32,rank6)
        - aten::mm:
          inputs: (%487:<308x3072xbf16>{3072, 1}, %82:<3072x5120xbf16>{5120, 1})
          outputs: (%2177:<308x5120xbf16>{5120,1})
          duration: -1
159446 2024-12-10 17:48:27.291163 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n32,rank6)
159449 2024-12-10 17:48:27.291236 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n23,rank6)
        - aten::add:
          inputs: (%276:<6955x5120xbf16>{5120, 1}, %967:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2177:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2229:<6955x5120xbf16>{5120, 1}, %2566:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2229:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<535x5120xbf16>{5120, 1}+1669120, %2068:<535x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<535x5120xbf16>{5120,1}+1669120)
          duration: -1
159646 2024-12-10 17:48:27.294914 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n22,rank6)
159649 2024-12-10 17:48:27.294978 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n24,rank6)
        - aten::mm:
          inputs: (%2225:<535x5120xbf16>{5120, 1}, %77:<5120x1536xbf16>{1536, 1})
          outputs: (%1238:<535x1536xbf16>{1536,1})
          duration: -1
159719 2024-12-10 17:48:27.296144 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n24,rank6)
        - aten::mul:
          inputs: (%1238:<535x1536xbf16>{1536, 1}, %600:<535x1536xbf16>{1536, 1})
          outputs: (%2566:<535x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<535x1536xbf16>{1536, 1}, %2001:<535x1536xbf16>{3072, 1}+1536)
          outputs: (%2030:<535x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2030:<535x1536xbf16>{1536, 1}, %430:<535x1536xbf16>{3072, 1})
          outputs: (%2001:<535x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%1981:list{%2001:<535x1536xbf16>{1536, 1}, %2566:<535x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%430:<535x3072xbf16>{3072,1})
          duration: -1
159779 2024-12-10 17:48:27.297258 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n31,rank6)
        - aten::mm:
          inputs: (%430:<535x3072xbf16>{3072, 1}, %78:<3072x5120xbf16>{5120, 1})
          outputs: (%2151:<535x5120xbf16>{5120,1})
          duration: -1
159847 2024-12-10 17:48:27.298467 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n31,rank6)
159851 2024-12-10 17:48:27.298533 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n22,rank6)
        - aten::add:
          inputs: (%2177:<6955x5120xbf16>{5120, 1}, %2095:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2155:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2177:<6955x5120xbf16>{5120, 1}, %2229:<6955x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2177:<6955x5120xbf16>{5120,1})
          duration: -1
159983 2024-12-10 17:48:27.300871 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n21,rank6)
159988 2024-12-10 17:48:27.300932 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n23,rank6)
        - aten::mm:
          inputs: (%2001:<326x5120xbf16>{5120, 1}, %73:<5120x1536xbf16>{1536, 1})
          outputs: (%2349:<326x1536xbf16>{1536,1})
          duration: -1
160052 2024-12-10 17:48:27.302138 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n23,rank6)
        - aten::mul:
          inputs: (%2349:<326x1536xbf16>{1536, 1}, %1309:<326x1536xbf16>{1536, 1})
          outputs: (%574:<326x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2349:<326x1536xbf16>{1536, 1}, %1200:<326x1536xbf16>{3072, 1}+1536)
          outputs: (%2177:<326x1536xbf16>{1536,1})
          duration: -1
        - aten::silu_backward:
          inputs: (%2177:<326x1536xbf16>{1536, 1}, %1552:<326x1536xbf16>{3072, 1})
          outputs: (%2229:<326x1536xbf16>{1536,1})
          duration: -1
        - aten::cat:
          inputs: (%2556:list{%2229:<326x1536xbf16>{1536, 1}, %574:<326x1536xbf16>{1536, 1}}, 1:int)
          outputs: (%2225:<326x3072xbf16>{3072,1})
          duration: -1
160117 2024-12-10 17:48:27.303248 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n30,rank6)
        - aten::mm:
          inputs: (%2225:<326x3072xbf16>{3072, 1}, %76:<3072x5120xbf16>{5120, 1})
          outputs: (%574:<326x5120xbf16>{5120,1})
          duration: -1
160174 2024-12-10 17:48:27.304452 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n30,rank6)
160178 2024-12-10 17:48:27.304514 module_return: A37503:None:0 torch.2_3_0.MLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n21,rank6)
        - aten::add:
          inputs: (%2155:<6955x5120xbf16>{5120, 1}, %2225:<6955x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%2177:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%2556:list{20:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cpu:device, pin_memory=None:NoneType)
          outputs: (%276:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
160227 2024-12-10 17:48:27.305767 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Backward:deepseek_v2.decoder.layers.0.mlp.experts,n1,rank6)
        - aten::new_zeros:
          inputs: (%2177:<6955x5120xbf16>{5120, 1}, %2556:list{6955:int, 5120:int}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType)
          outputs: (%1977:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%1977:<6955x5120xbf16>{5120, 1}, 0:int, %1964:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, %2177:<6955x5120xbf16>{5120, 1})
          outputs: (%2349:<6955x5120xbf16>{5120,1})
          duration: -1
        - aten::new_zeros:
          inputs: (%2349:<6955x5120xbf16>{5120, 1}, %2610:list{8192:int, 5120:int}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType)
          outputs: (%984:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%984:<8192x5120xbf16>{5120, 1}, 0:int, %243:<6955x5120xCUSTOM_DATA_TYPE>{1, 0}, %2349:<6955x5120xbf16>{5120, 1})
          outputs: (%2177:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::masked_scatter:
          inputs: (%1853:<8192x6xbf16>{6, 1}, %1868:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %1944:<6955xbf16>{1})
          outputs: (%2225:<8192x6xbf16>{6,1})
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%1868:<1024x6xbf16>{6, 1}, %2225:<8192x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%2546:tuple{%1868:<1024x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%1956:<1024x5120xbf16>{5120, 1}, %2177:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%2588:tuple{%1956:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::add:
          inputs: (%2612:<1024x1x5120xbf16>{5120, 5120, 1}, %2229:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%1853:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%2556:list{1024:int, 6:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%2177:<1024x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
160631 2024-12-10 17:48:27.326380 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Backward:deepseek_v2.decoder.layers.0.mlp.router,n1,rank6)
        - aten::mul:
          inputs: (%2349:<i32>, %2602:<1xf32>{1})
          outputs: (%2229:<1xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2229:<1xf32>{1}, dtype=None:NoneType)
          outputs: (%1552:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1552:<i32>, 2_5431315104166666e-07:float)
          outputs: (%2229:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1940:<1024x160xf32>{0, 0}, %1878:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%2225:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%2225:<1024x160xf32>{160, 1}, %2563:list{0:int}, True:bool, dtype=None:NoneType)
          outputs: (%1462:<1x160xf32>{160,1})
          duration: -1
        - aten::_softmax_backward_data:
          inputs: (%2349:<1024x160xf32>{0, 1}, %1940:<1024x160xf32>{160, 1}, -1:int, torch_float32:dtype)
          outputs: (%2177:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%2563:list{1024:int, 160:int}, dtype=torch_bfloat16:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%2229:<1024x160xbf16>{160,1})
          duration: -1
        - aten::scatter:
          inputs: (%2229:<1024x160xbf16>{160, 1}, -1:int, %1483:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, %1868:<1024x6xbf16>{6, 1})
          outputs: (%1462:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax_backward_data:
          inputs: (%2229:<1024x160xf32>{160, 1}, %1934:<1024x160xf32>{160, 1}, 1:int, torch_float32:dtype)
          outputs: (%2621:<1024x160xf32>{160,1})
          duration: -1
        - aten::add:
          inputs: (%2349:<1024x160xbf16>{160, 1}, %2177:<1024x160xbf16>{160, 1}, alpha=1:int)
          outputs: (%2155:<1024x160xbf16>{160,1})
          duration: -1
        - aten::mm:
          inputs: (%2225:<160x1024xbf16>{1, 160}, %1922:<1024x5120xbf16>{5120, 1})
          outputs: (%1462:<160x5120xbf16>{5120,1})
          duration: -1
        - aten::mm:
          inputs: (%1934:<1024x160xbf16>{160, 1}, %1878:<160x5120xbf16>{5120, 1})
          outputs: (%2349:<1024x5120xbf16>{5120,1})
          duration: -1
        - aten::add_:
          inputs: (%182:<160x5120xf32>{5120, 1}+571479040, %1934:<160x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%182:<160x5120xf32>{5120,1}+571479040)
          duration: -1
161181 2024-12-10 17:48:27.336198 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Backward:deepseek_v2.decoder.layers.0.mlp.router,n1,rank6)
        - aten::add:
          inputs: (%1853:<1024x1x5120xbf16>{5120, 5120, 1}, %2155:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%1878:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
161207 2024-12-10 17:48:27.336573 module_return: A37503:None:0 torch.2_3_0.MoELayer(Backward:deepseek_v2.decoder.layers.0.mlp,n1,rank6)
161212 2024-12-10 17:48:27.336650 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Backward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n4,rank6)
        - aten::add_:
          inputs: (%184:<5120xf32>{1}+572298240, %1904:<5120xbf16>{1}, alpha=1:int)
          outputs: (%184:<5120xf32>{1}+572298240)
          duration: -1
161333 2024-12-10 17:48:27.338741 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Backward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n4,rank6)
        - aten::add:
          inputs: (%2475:<1024x1x5120xbf16>{5120, 5120, 1}, %2621:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%1904:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (compile_f___locals__bw_compiler_(dynamo_timed):str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
161390 2024-12-10 17:48:27.339997 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Backward:deepseek_v2.decoder.layers.0.self_attention,n1,rank6)
161395 2024-12-10 17:48:27.340063 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n22,rank6)
        - aten::mm:
          inputs: (%2621:<1024x5120xbf16>{5120, 1}, %36:<5120x16384xbf16>{16384, 1})
          outputs: (%1854:<1024x16384xbf16>{16384,1})
          duration: -1
161528 2024-12-10 17:48:27.343762 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n22,rank6)
        - aten::bmm:
          inputs: (%2225:<128x1024x1024xbf16>{1048576, 1, 1024}, %1462:<128x1024x128xbf16>{128, 16384, 1})
          outputs: (%1483:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - aten::bmm:
          inputs: (%1462:<128x1024x128xbf16>{128, 16384, 1}, %2349:<128x128x1024xbf16>{256, 1, 32768}+128)
          outputs: (%1934:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::_softmax_backward_data:
          inputs: (%2621:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, %2349:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, torch_float32:dtype)
          outputs: (%1868:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%1921:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%2349:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%1864:<128x192x1024xbf16>{196608, 1, 192}, %2621:<128x1024x1024xbf16>{1048576, 1024, 1})
          outputs: (%1868:<128x192x1024xbf16>{196608,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%2621:<128x1024x1024xbf16>{1048576, 1024, 1}, %1921:<128x1024x192xbf16>{196608, 192, 1})
          outputs: (%2195:<128x1024x192xbf16>{196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1462:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %1861:<1x128x1024x192xbf16>{25165824, 196608, 1, 1024}, False:bool)
          outputs: (%1462:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1792:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %1864:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%1792:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::sum:
          inputs: (%2349:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %2563:list{0:int, 1:int}, True:bool, dtype=None:NoneType)
          outputs: (%1864:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%2349:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %1462:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, False:bool)
          outputs: (%2349:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1868:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %1862:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, False:bool)
          outputs: (%1868:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1934:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %2229:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%1934:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%1934:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, %1868:<1x128x1024x192xbf16>{25165824, 196608, 192, 1}, False:bool)
          outputs: (%1934:<1x128x1024x192xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::mul:
          inputs: (%1922:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1849:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1934:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%271:<1x1x1024x32xbf16>{65536, 65536, 64, 1})
          outputs: (%2151:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::add:
          inputs: (%1301:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %271:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%1934:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1922:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1840:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%2151:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%1934:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %2151:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%1921:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2225:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1849:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1922:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%1849:<1x128x1024x32xbf16>{8388608, 65536, 64, 1})
          outputs: (%1934:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::add:
          inputs: (%1809:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1934:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%271:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2225:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1840:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1934:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%271:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1934:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%2349:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::cat:
          inputs: (%2563:list{%2621:<1024x1x128x128xbf16>{128, 16777216, 131072, 1}, %1836:<1024x1x128x128xbf16>{128, 16777216, 131072, 1}}, 3:int)
          outputs: (%2225:<1024x1x128x256xbf16>{32768,32768,256,1})
          duration: -1
162777 2024-12-10 17:48:27.382363 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n29,rank6)
        - aten::mm:
          inputs: (%1809:<1024x32768xbf16>{32768, 1}, %60:<32768x512xbf16>{512, 1})
          outputs: (%1792:<1024x512xbf16>{512,1})
          duration: -1
162910 2024-12-10 17:48:27.384780 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n29,rank6)
        - aten::cat:
          inputs: (%2598:list{%1836:<1024x1x512xbf16>{512, 512, 1}, %1934:<1024x1x64xbf16>{64, 65536, 1}}, 2:int)
          outputs: (%1483:<1024x1x576xbf16>{576,576,1})
          duration: -1
162931 2024-12-10 17:48:27.385198 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n28,rank6)
        - aten::mm:
          inputs: (%2225:<1024x576xbf16>{576, 1}, %41:<576x5120xbf16>{5120, 1})
          outputs: (%2621:<1024x5120xbf16>{5120,1})
          duration: -1
163062 2024-12-10 17:48:27.387554 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n28,rank6)
        - aten::cat:
          inputs: (%1981:list{%1868:<1024x1x128x128xbf16>{128, 16777216, 131072, 1}, %1864:<1024x1x128x64xbf16>{64, 8388608, 65536, 1}}, 3:int)
          outputs: (%2225:<1024x1x128x192xbf16>{24576,24576,192,1})
          duration: -1
163104 2024-12-10 17:48:27.388428 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n27,rank6)
        - aten::mm:
          inputs: (%1462:<1024x24576xbf16>{24576, 1}, %47:<24576x1536xbf16>{1536, 1})
          outputs: (%1864:<1024x1536xbf16>{1536,1})
          duration: -1
163249 2024-12-10 17:48:27.391242 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n27,rank6)
163255 2024-12-10 17:48:27.391304 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n26,rank6)
        - aten::mm:
          inputs: (%2349:<1024x1536xbf16>{1536, 1}, %38:<1536x5120xbf16>{5120, 1})
          outputs: (%1483:<1024x5120xbf16>{5120,1})
          duration: -1
163384 2024-12-10 17:48:27.393611 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Backward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n26,rank6)
        - aten::add:
          inputs: (%1934:<1024x1x5120xbf16>{5120, 5120, 1}, %2621:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2349:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
163408 2024-12-10 17:48:27.393966 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Backward:deepseek_v2.decoder.layers.0.self_attention,n1,rank6)
163412 2024-12-10 17:48:27.394029 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Backward:deepseek_v2.decoder.layers.0.input_layernorm,n3,rank6)
        - aten::add_:
          inputs: (%189:<5120xf32>{1}+667002880, %1934:<5120xbf16>{1}, alpha=1:int)
          outputs: (%189:<5120xf32>{1}+667002880)
          duration: -1
163534 2024-12-10 17:48:27.396049 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Backward:deepseek_v2.decoder.layers.0.input_layernorm,n3,rank6)
        - aten::add:
          inputs: (%1904:<1024x1x5120xbf16>{5120, 5120, 1}, %2621:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2225:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
163557 2024-12-10 17:48:27.396404 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Backward:deepseek_v2.embedding,n1,rank6)
163560 2024-12-10 17:48:27.396450 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Backward:deepseek_v2.embedding,n1,rank6)
163568 2024-12-10 17:48:27.396506 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Backward:deepseek_v2.embedding.embedding_dropout,n1,rank6)
163572 2024-12-10 17:48:27.396567 module_return: A37503:None:0 torch.2_3_0.Dropout(Backward:deepseek_v2.embedding.embedding_dropout,n1,rank6)
163593 2024-12-10 17:48:27.396895 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Backward:deepseek_v2.embedding.word_embeddings,n1,rank6)
163594 2024-12-10 17:48:27.396938 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Backward:deepseek_v2.embedding.word_embeddings,n1,rank6)
        - aten::embedding_dense_backward:
          inputs: (%2621:<1x1024x5120xbf16>{5120, 5120, 1}, %1721:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 102400:int, -1:int, False:bool)
          outputs: (%271:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::add_:
          inputs: (%186:<102400x5120xf32>{5120, 1}+667008000, %2225:<102400x5120xbf16>{5120, 1}, alpha=1:int)
          outputs: (%186:<102400x5120xf32>{5120,1}+667008000)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (False:bool, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (False:bool)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%1960:<1xi32>{1}, %2623:list{%2624:list{%186:<102400x5120xf32>{5120, 1}+667008000, %167:<5120x16384xf32>{16384, 1}+583116800, %187:<1536x5120xf32>{5120, 1}+575252480, %185:<576x5120xf32>{5120, 1}+572303360, %182:<160x5120xf32>{5120, 1}+571479040, %179:<5120x3072xf32>{3072, 1}+555750400, %176:<6144x5120xf32>{5120, 1}+524293120, %173:<102400x5120xf32>{5120, 1}, %189:<5120xf32>{1}+667002880, %184:<5120xf32>{1}+572298240, %175:<5120xf32>{1}+524288000}}, %469:tuple{False:bool})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_l2norm_of_PyCapsule_object_at_0_7f5650c631e0_:builtin_function_or_method,%1960:<1xi32>{1},%2623:list{%2624:list{%186:<102400x5120xf32>{5120,1}+667008000,%167:<5120x16384xf32>{16384,1}+583116800,%187:<1536x5120xf32>{5120,1}+575252480,%185:<576x5120xf32>{5120,1}+572303360,%182:<160x5120xf32>{5120,1}+571479040,%179:<5120x3072xf32>{3072,1}+555750400,%176:<6144x5120xf32>{5120,1}+524293120,%173:<102400x5120xf32>{5120,1},%189:<5120xf32>{1}+667002880,%184:<5120xf32>{1}+572298240,%175:<5120xf32>{1}+524288000}},%469:tuple{False:bool}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - aten::zeros:
          inputs: (%2598:list{320:int}, dtype=torch_float32:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%2625:<320xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%1164:<1xf32>{1}, 2_0:float)
          outputs: (%2626:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2626:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0fb0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2626:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0fb0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2627:list{%2626:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2628:tuple{%2629:list{%2626:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%1960:<1xi32>{1}, %2630:list{%2631:list{%229:<24576x1536xf32>{1536, 1}+488636416, %228:<32768x512xf32>{512, 1}+471859200, %227:<5120x1536xf32>{1536, 1}+463994880, %226:<3072x5120xf32>{5120, 1}+448266240, %225:<5120x1536xf32>{1536, 1}+440401920, %224:<3072x5120xf32>{5120, 1}+424673280, %223:<5120x1536xf32>{1536, 1}+416808960, %222:<3072x5120xf32>{5120, 1}+401080320, %221:<5120x1536xf32>{1536, 1}+393216000, %163:<3072x5120xf32>{5120, 1}+377487360, %220:<5120x1536xf32>{1536, 1}+369623040, %154:<3072x5120xf32>{5120, 1}+353894400, %219:<5120x1536xf32>{1536, 1}+346030080, %157:<3072x5120xf32>{5120, 1}+330301440, %218:<5120x1536xf32>{1536, 1}+322437120, %217:<3072x5120xf32>{5120, 1}+306708480, %158:<5120x1536xf32>{1536, 1}+298844160, %216:<3072x5120xf32>{5120, 1}+283115520, %215:<5120x1536xf32>{1536, 1}+275251200, %155:<3072x5120xf32>{5120, 1}+259522560, %153:<5120x1536xf32>{1536, 1}+251658240, %214:<3072x5120xf32>{5120, 1}+235929600, %213:<5120x1536xf32>{1536, 1}+228065280, %174:<3072x5120xf32>{5120, 1}+212336640, %171:<5120x1536xf32>{1536, 1}+204472320, %212:<3072x5120xf32>{5120, 1}+188743680, %156:<5120x1536xf32>{1536, 1}+180879360, %211:<3072x5120xf32>{5120, 1}+165150720, %210:<5120x1536xf32>{1536, 1}+157286400, %193:<3072x5120xf32>{5120, 1}+141557760, %209:<5120x1536xf32>{1536, 1}+133693440, %208:<3072x5120xf32>{5120, 1}+117964800, %207:<5120x1536xf32>{1536, 1}+110100480, %206:<3072x5120xf32>{5120, 1}+94371840, %183:<5120x1536xf32>{1536, 1}+86507520, %205:<3072x5120xf32>{5120, 1}+70778880, %180:<5120x1536xf32>{1536, 1}+62914560, %204:<3072x5120xf32>{5120, 1}+47185920, %177:<5120x1536xf32>{1536, 1}+39321600, %201:<3072x5120xf32>{5120, 1}+23592960, %165:<5120x1536xf32>{1536, 1}+15728640, %198:<3072x5120xf32>{5120, 1}}}, %469:tuple{False:bool})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_l2norm_of_PyCapsule_object_at_0_7f5650c631e0_:builtin_function_or_method,%1960:<1xi32>{1},%2630:list{%2631:list{%229:<24576x1536xf32>{1536,1}+488636416,%228:<32768x512xf32>{512,1}+471859200,%227:<5120x1536xf32>{1536,1}+463994880,%226:<3072x5120xf32>{5120,1}+448266240,%225:<5120x1536xf32>{1536,1}+440401920,%224:<3072x5120xf32>{5120,1}+424673280,%223:<5120x1536xf32>{1536,1}+416808960,%222:<3072x5120xf32>{5120,1}+401080320,%221:<5120x1536xf32>{1536,1}+393216000,%163:<3072x5120xf32>{5120,1}+377487360,%220:<5120x1536xf32>{1536,1}+369623040,%154:<3072x5120xf32>{5120,1}+353894400,%219:<5120x1536xf32>{1536,1}+346030080,%157:<3072x5120xf32>{5120,1}+330301440,%218:<5120x1536xf32>{1536,1}+322437120,%217:<3072x5120xf32>{5120,1}+306708480,%158:<5120x1536xf32>{1536,1}+298844160,%216:<3072x5120xf32>{5120,1}+283115520,%215:<5120x1536xf32>{1536,1}+275251200,%155:<3072x5120xf32>{5120,1}+259522560,%153:<5120x1536xf32>{1536,1}+251658240,%214:<3072x5120xf32>{5120,1}+235929600,%213:<5120x1536xf32>{1536,1}+228065280,%174:<3072x5120xf32>{5120,1}+212336640,%171:<5120x1536xf32>{1536,1}+204472320,%212:<3072x5120xf32>{5120,1}+188743680,%156:<5120x1536xf32>{1536,1}+180879360,%211:<3072x5120xf32>{5120,1}+165150720,%210:<5120x1536xf32>{1536,1}+157286400,%193:<3072x5120xf32>{5120,1}+141557760,%209:<5120x1536xf32>{1536,1}+133693440,%208:<3072x5120xf32>{5120,1}+117964800,%207:<5120x1536xf32>{1536,1}+110100480,%206:<3072x5120xf32>{5120,1}+94371840,%183:<5120x1536xf32>{1536,1}+86507520,%205:<3072x5120xf32>{5120,1}+70778880,%180:<5120x1536xf32>{1536,1}+62914560,%204:<3072x5120xf32>{5120,1}+47185920,%177:<5120x1536xf32>{1536,1}+39321600,%201:<3072x5120xf32>{5120,1}+23592960,%165:<5120x1536xf32>{1536,1}+15728640,%198:<3072x5120xf32>{5120,1}}},%469:tuple{False:bool}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - aten::zeros:
          inputs: (%2627:list{320:int}, dtype=torch_float32:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%1720:<320xf32>{1})
          duration: -1
        - aten::pow:
          inputs: (%915:<1xf32>{1}, 2_0:float)
          outputs: (%574:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%574:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0af0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%574:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa0af0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%809:list{%574:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%1503:tuple{%2610:list{%574:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%2634:<1xi32>{1}, %1715:list{%2630:list{%915:<102400x5120xf32>{5120, 1}+667008000, %471:<5120x16384xf32>{16384, 1}+583116800, %2011:<1536x5120xf32>{5120, 1}+575252480, %550:<576x5120xf32>{5120, 1}+572303360, %332:<160x5120xf32>{5120, 1}+571479040, %271:<5120x3072xf32>{3072, 1}+555750400, %771:<6144x5120xf32>{5120, 1}+524293120, %1914:<102400x5120xf32>{5120, 1}, %2349:<5120xf32>{1}+667002880, %2633:<5120xf32>{1}+572298240, %237:<5120xf32>{1}+524288000}, %2630:list{%915:<102400x5120xf32>{5120, 1}+667008000, %471:<5120x16384xf32>{16384, 1}+583116800, %2011:<1536x5120xf32>{5120, 1}+575252480, %550:<576x5120xf32>{5120, 1}+572303360, %332:<160x5120xf32>{5120, 1}+571479040, %271:<5120x3072xf32>{3072, 1}+555750400, %771:<6144x5120xf32>{5120, 1}+524293120, %1914:<102400x5120xf32>{5120, 1}, %2349:<5120xf32>{1}+667002880, %2633:<5120xf32>{1}+572298240, %237:<5120xf32>{1}+524288000}}, %470:tuple{0_06491489871897513:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_scale_of_PyCapsule_object_at_0_7f5650c63300_:builtin_function_or_method,%2634:<1xi32>{1},%1715:list{%2630:list{%915:<102400x5120xf32>{5120,1}+667008000,%471:<5120x16384xf32>{16384,1}+583116800,%2011:<1536x5120xf32>{5120,1}+575252480,%550:<576x5120xf32>{5120,1}+572303360,%332:<160x5120xf32>{5120,1}+571479040,%271:<5120x3072xf32>{3072,1}+555750400,%771:<6144x5120xf32>{5120,1}+524293120,%1914:<102400x5120xf32>{5120,1},%2349:<5120xf32>{1}+667002880,%2633:<5120xf32>{1}+572298240,%237:<5120xf32>{1}+524288000},%2630:list{%915:<102400x5120xf32>{5120,1}+667008000,%471:<5120x16384xf32>{16384,1}+583116800,%2011:<1536x5120xf32>{5120,1}+575252480,%550:<576x5120xf32>{5120,1}+572303360,%332:<160x5120xf32>{5120,1}+571479040,%271:<5120x3072xf32>{3072,1}+555750400,%771:<6144x5120xf32>{5120,1}+524293120,%1914:<102400x5120xf32>{5120,1},%2349:<5120xf32>{1}+667002880,%2633:<5120xf32>{1}+572298240,%237:<5120xf32>{1}+524288000}},%470:tuple{0_06491489871897513:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%1715:list{%315:<24576x1536xf32>{1536, 1}, %316:<32768x512xf32>{512, 1}, %317:<5120x1536xf32>{1536, 1}, %320:<3072x5120xf32>{5120, 1}, %321:<5120x1536xf32>{1536, 1}, %257:<3072x5120xf32>{5120, 1}, %323:<5120x1536xf32>{1536, 1}, %248:<3072x5120xf32>{5120, 1}, %328:<5120x1536xf32>{1536, 1}, %330:<3072x5120xf32>{5120, 1}, %307:<5120x1536xf32>{1536, 1}, %298:<3072x5120xf32>{5120, 1}, %251:<5120x1536xf32>{1536, 1}, %333:<3072x5120xf32>{5120, 1}, %272:<5120x1536xf32>{1536, 1}, %335:<3072x5120xf32>{5120, 1}, %338:<5120x1536xf32>{1536, 1}, %267:<3072x5120xf32>{5120, 1}, %341:<5120x1536xf32>{1536, 1}, %342:<3072x5120xf32>{5120, 1}, %343:<5120x1536xf32>{1536, 1}, %346:<3072x5120xf32>{5120, 1}, %348:<5120x1536xf32>{1536, 1}, %327:<3072x5120xf32>{5120, 1}, %305:<5120x1536xf32>{1536, 1}, %352:<3072x5120xf32>{5120, 1}, %355:<5120x1536xf32>{1536, 1}, %324:<3072x5120xf32>{5120, 1}, %357:<5120x1536xf32>{1536, 1}, %349:<3072x5120xf32>{5120, 1}, %268:<5120x1536xf32>{1536, 1}, %354:<3072x5120xf32>{5120, 1}, %366:<5120x1536xf32>{1536, 1}, %368:<3072x5120xf32>{5120, 1}, %371:<5120x1536xf32>{1536, 1}, %373:<3072x5120xf32>{5120, 1}, %375:<5120x1536xf32>{1536, 1}, %379:<3072x5120xf32>{5120, 1}, %383:<5120x1536xf32>{1536, 1}, %381:<3072x5120xf32>{5120, 1}, %255:<5120x1536xf32>{1536, 1}, %360:<3072x5120xf32>{5120, 1}}, %360:<3072x5120xf32>{5120, 1})
          outputs: (%1715:list{%315:<24576x1536xf32>{1536,1},%316:<32768x512xf32>{512,1},%317:<5120x1536xf32>{1536,1},%320:<3072x5120xf32>{5120,1},%321:<5120x1536xf32>{1536,1},%257:<3072x5120xf32>{5120,1},%323:<5120x1536xf32>{1536,1},%248:<3072x5120xf32>{5120,1},%328:<5120x1536xf32>{1536,1},%330:<3072x5120xf32>{5120,1},%307:<5120x1536xf32>{1536,1},%298:<3072x5120xf32>{5120,1},%251:<5120x1536xf32>{1536,1},%333:<3072x5120xf32>{5120,1},%272:<5120x1536xf32>{1536,1},%335:<3072x5120xf32>{5120,1},%338:<5120x1536xf32>{1536,1},%267:<3072x5120xf32>{5120,1},%341:<5120x1536xf32>{1536,1},%342:<3072x5120xf32>{5120,1},%343:<5120x1536xf32>{1536,1},%346:<3072x5120xf32>{5120,1},%348:<5120x1536xf32>{1536,1},%327:<3072x5120xf32>{5120,1},%305:<5120x1536xf32>{1536,1},%352:<3072x5120xf32>{5120,1},%355:<5120x1536xf32>{1536,1},%324:<3072x5120xf32>{5120,1},%357:<5120x1536xf32>{1536,1},%349:<3072x5120xf32>{5120,1},%268:<5120x1536xf32>{1536,1},%354:<3072x5120xf32>{5120,1},%366:<5120x1536xf32>{1536,1},%368:<3072x5120xf32>{5120,1},%371:<5120x1536xf32>{1536,1},%373:<3072x5120xf32>{5120,1},%375:<5120x1536xf32>{1536,1},%379:<3072x5120xf32>{5120,1},%383:<5120x1536xf32>{1536,1},%381:<3072x5120xf32>{5120,1},%255:<5120x1536xf32>{1536,1},%360:<3072x5120xf32>{5120,1}})
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%2621:<1xi32>{1}, %2640:list{%2631:list{%2633:<24576x1536xf32>{1536, 1}+488636416, %1914:<32768x512xf32>{512, 1}+471859200, %271:<5120x1536xf32>{1536, 1}+463994880, %332:<3072x5120xf32>{5120, 1}+448266240, %1600:<5120x1536xf32>{1536, 1}+440401920, %2160:<3072x5120xf32>{5120, 1}+424673280, %2011:<5120x1536xf32>{1536, 1}+416808960, %471:<3072x5120xf32>{5120, 1}+401080320, %2349:<5120x1536xf32>{1536, 1}+393216000, %1618:<3072x5120xf32>{5120, 1}+377487360, %2635:<5120x1536xf32>{1536, 1}+369623040, %2636:<3072x5120xf32>{5120, 1}+353894400, %1928:<5120x1536xf32>{1536, 1}+346030080, %2603:<3072x5120xf32>{5120, 1}+330301440, %1721:<5120x1536xf32>{1536, 1}+322437120, %1483:<3072x5120xf32>{5120, 1}+306708480, %1930:<5120x1536xf32>{1536, 1}+298844160, %1552:<3072x5120xf32>{5120, 1}+283115520, %1934:<5120x1536xf32>{1536, 1}+275251200, %284:<3072x5120xf32>{5120, 1}+259522560, %1986:<5120x1536xf32>{1536, 1}+251658240, %915:<3072x5120xf32>{5120, 1}+235929600, %1794:<5120x1536xf32>{1536, 1}+228065280, %2229:<3072x5120xf32>{5120, 1}+212336640, %1760:<5120x1536xf32>{1536, 1}+204472320, %1756:<3072x5120xf32>{5120, 1}+188743680, %1811:<5120x1536xf32>{1536, 1}+180879360, %2637:<3072x5120xf32>{5120, 1}+165150720, %1762:<5120x1536xf32>{1536, 1}+157286400, %1809:<3072x5120xf32>{5120, 1}+141557760, %1765:<5120x1536xf32>{1536, 1}+133693440, %1788:<3072x5120xf32>{5120, 1}+117964800, %1792:<5120x1536xf32>{1536, 1}+110100480, %1836:<3072x5120xf32>{5120, 1}+94371840, %1774:<5120x1536xf32>{1536, 1}+86507520, %1864:<3072x5120xf32>{5120, 1}+70778880, %1854:<5120x1536xf32>{1536, 1}+62914560, %1569:<3072x5120xf32>{5120, 1}+47185920, %1861:<5120x1536xf32>{1536, 1}+39321600, %1970:<3072x5120xf32>{5120, 1}+23592960, %557:<5120x1536xf32>{1536, 1}+15728640, %2639:<3072x5120xf32>{5120, 1}}, %2631:list{%2633:<24576x1536xf32>{1536, 1}+488636416, %1914:<32768x512xf32>{512, 1}+471859200, %271:<5120x1536xf32>{1536, 1}+463994880, %332:<3072x5120xf32>{5120, 1}+448266240, %1600:<5120x1536xf32>{1536, 1}+440401920, %2160:<3072x5120xf32>{5120, 1}+424673280, %2011:<5120x1536xf32>{1536, 1}+416808960, %471:<3072x5120xf32>{5120, 1}+401080320, %2349:<5120x1536xf32>{1536, 1}+393216000, %1618:<3072x5120xf32>{5120, 1}+377487360, %2635:<5120x1536xf32>{1536, 1}+369623040, %2636:<3072x5120xf32>{5120, 1}+353894400, %1928:<5120x1536xf32>{1536, 1}+346030080, %2603:<3072x5120xf32>{5120, 1}+330301440, %1721:<5120x1536xf32>{1536, 1}+322437120, %1483:<3072x5120xf32>{5120, 1}+306708480, %1930:<5120x1536xf32>{1536, 1}+298844160, %1552:<3072x5120xf32>{5120, 1}+283115520, %1934:<5120x1536xf32>{1536, 1}+275251200, %284:<3072x5120xf32>{5120, 1}+259522560, %1986:<5120x1536xf32>{1536, 1}+251658240, %915:<3072x5120xf32>{5120, 1}+235929600, %1794:<5120x1536xf32>{1536, 1}+228065280, %2229:<3072x5120xf32>{5120, 1}+212336640, %1760:<5120x1536xf32>{1536, 1}+204472320, %1756:<3072x5120xf32>{5120, 1}+188743680, %1811:<5120x1536xf32>{1536, 1}+180879360, %2637:<3072x5120xf32>{5120, 1}+165150720, %1762:<5120x1536xf32>{1536, 1}+157286400, %1809:<3072x5120xf32>{5120, 1}+141557760, %1765:<5120x1536xf32>{1536, 1}+133693440, %1788:<3072x5120xf32>{5120, 1}+117964800, %1792:<5120x1536xf32>{1536, 1}+110100480, %1836:<3072x5120xf32>{5120, 1}+94371840, %1774:<5120x1536xf32>{1536, 1}+86507520, %1864:<3072x5120xf32>{5120, 1}+70778880, %1854:<5120x1536xf32>{1536, 1}+62914560, %1569:<3072x5120xf32>{5120, 1}+47185920, %1861:<5120x1536xf32>{1536, 1}+39321600, %1970:<3072x5120xf32>{5120, 1}+23592960, %557:<5120x1536xf32>{1536, 1}+15728640, %2639:<3072x5120xf32>{5120, 1}}}, %470:tuple{0_06491489871897513:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_scale_of_PyCapsule_object_at_0_7f5650c63300_:builtin_function_or_method,%2621:<1xi32>{1},%2640:list{%2631:list{%2633:<24576x1536xf32>{1536,1}+488636416,%1914:<32768x512xf32>{512,1}+471859200,%271:<5120x1536xf32>{1536,1}+463994880,%332:<3072x5120xf32>{5120,1}+448266240,%1600:<5120x1536xf32>{1536,1}+440401920,%2160:<3072x5120xf32>{5120,1}+424673280,%2011:<5120x1536xf32>{1536,1}+416808960,%471:<3072x5120xf32>{5120,1}+401080320,%2349:<5120x1536xf32>{1536,1}+393216000,%1618:<3072x5120xf32>{5120,1}+377487360,%2635:<5120x1536xf32>{1536,1}+369623040,%2636:<3072x5120xf32>{5120,1}+353894400,%1928:<5120x1536xf32>{1536,1}+346030080,%2603:<3072x5120xf32>{5120,1}+330301440,%1721:<5120x1536xf32>{1536,1}+322437120,%1483:<3072x5120xf32>{5120,1}+306708480,%1930:<5120x1536xf32>{1536,1}+298844160,%1552:<3072x5120xf32>{5120,1}+283115520,%1934:<5120x1536xf32>{1536,1}+275251200,%284:<3072x5120xf32>{5120,1}+259522560,%1986:<5120x1536xf32>{1536,1}+251658240,%915:<3072x5120xf32>{5120,1}+235929600,%1794:<5120x1536xf32>{1536,1}+228065280,%2229:<3072x5120xf32>{5120,1}+212336640,%1760:<5120x1536xf32>{1536,1}+204472320,%1756:<3072x5120xf32>{5120,1}+188743680,%1811:<5120x1536xf32>{1536,1}+180879360,%2637:<3072x5120xf32>{5120,1}+165150720,%1762:<5120x1536xf32>{1536,1}+157286400,%1809:<3072x5120xf32>{5120,1}+141557760,%1765:<5120x1536xf32>{1536,1}+133693440,%1788:<3072x5120xf32>{5120,1}+117964800,%1792:<5120x1536xf32>{1536,1}+110100480,%1836:<3072x5120xf32>{5120,1}+94371840,%1774:<5120x1536xf32>{1536,1}+86507520,%1864:<3072x5120xf32>{5120,1}+70778880,%1854:<5120x1536xf32>{1536,1}+62914560,%1569:<3072x5120xf32>{5120,1}+47185920,%1861:<5120x1536xf32>{1536,1}+39321600,%1970:<3072x5120xf32>{5120,1}+23592960,%557:<5120x1536xf32>{1536,1}+15728640,%2639:<3072x5120xf32>{5120,1}},%2631:list{%2633:<24576x1536xf32>{1536,1}+488636416,%1914:<32768x512xf32>{512,1}+471859200,%271:<5120x1536xf32>{1536,1}+463994880,%332:<3072x5120xf32>{5120,1}+448266240,%1600:<5120x1536xf32>{1536,1}+440401920,%2160:<3072x5120xf32>{5120,1}+424673280,%2011:<5120x1536xf32>{1536,1}+416808960,%471:<3072x5120xf32>{5120,1}+401080320,%2349:<5120x1536xf32>{1536,1}+393216000,%1618:<3072x5120xf32>{5120,1}+377487360,%2635:<5120x1536xf32>{1536,1}+369623040,%2636:<3072x5120xf32>{5120,1}+353894400,%1928:<5120x1536xf32>{1536,1}+346030080,%2603:<3072x5120xf32>{5120,1}+330301440,%1721:<5120x1536xf32>{1536,1}+322437120,%1483:<3072x5120xf32>{5120,1}+306708480,%1930:<5120x1536xf32>{1536,1}+298844160,%1552:<3072x5120xf32>{5120,1}+283115520,%1934:<5120x1536xf32>{1536,1}+275251200,%284:<3072x5120xf32>{5120,1}+259522560,%1986:<5120x1536xf32>{1536,1}+251658240,%915:<3072x5120xf32>{5120,1}+235929600,%1794:<5120x1536xf32>{1536,1}+228065280,%2229:<3072x5120xf32>{5120,1}+212336640,%1760:<5120x1536xf32>{1536,1}+204472320,%1756:<3072x5120xf32>{5120,1}+188743680,%1811:<5120x1536xf32>{1536,1}+180879360,%2637:<3072x5120xf32>{5120,1}+165150720,%1762:<5120x1536xf32>{1536,1}+157286400,%1809:<3072x5120xf32>{5120,1}+141557760,%1765:<5120x1536xf32>{1536,1}+133693440,%1788:<3072x5120xf32>{5120,1}+117964800,%1792:<5120x1536xf32>{1536,1}+110100480,%1836:<3072x5120xf32>{5120,1}+94371840,%1774:<5120x1536xf32>{1536,1}+86507520,%1864:<3072x5120xf32>{5120,1}+70778880,%1854:<5120x1536xf32>{1536,1}+62914560,%1569:<3072x5120xf32>{5120,1}+47185920,%1861:<5120x1536xf32>{1536,1}+39321600,%1970:<3072x5120xf32>{5120,1}+23592960,%557:<5120x1536xf32>{1536,1}+15728640,%2639:<3072x5120xf32>{5120,1}}},%470:tuple{0_06491489871897513:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::wrapper call:
          inputs: (%458:tuple{NotSurpot:FusedAdam}, _function_FusedAdam_step_at_0_7f5650cb89d0_:function)
          outputs: (torch.2_3_0.wrapper(%458:tuple{NotSurpot:FusedAdam},_function_FusedAdam_step_at_0_7f5650cb89d0_:function))
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (Optimizer_step#FusedAdam_step:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.FusedAdam(None:NoneType,)
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%285:<1xi32>{1}, %2642:list{%2643:list{%2626:<102400x5120xf32>{5120, 1}+667008000, %1600:<5120x16384xf32>{16384, 1}+583116800, %2349:<1536x5120xf32>{5120, 1}+575252480, %2636:<576x5120xf32>{5120, 1}+572303360, %1618:<160x5120xf32>{5120, 1}+571479040, %1483:<5120x3072xf32>{3072, 1}+555750400, %1934:<6144x5120xf32>{5120, 1}+524293120, %2151:<102400x5120xf32>{5120, 1}}, %1961:list{%2160:<102400x5120xf32>{5120, 1}, %2554:<5120x16384xf32>{16384, 1}, %2635:<1536x5120xf32>{5120, 1}, %1928:<576x5120xf32>{5120, 1}, %1552:<160x5120xf32>{5120, 1}, %284:<5120x3072xf32>{3072, 1}, %332:<6144x5120xf32>{5120, 1}, %1720:<102400x5120xf32>{5120, 1}}, %2556:list{%706:<102400x5120xf32>{5120, 1}, %606:<5120x16384xf32>{16384, 1}, %1611:<1536x5120xf32>{5120, 1}, %601:<576x5120xf32>{5120, 1}, %1621:<160x5120xf32>{5120, 1}, %1527:<5120x3072xf32>{3072, 1}, %1558:<6144x5120xf32>{5120, 1}, %1537:<102400x5120xf32>{5120, 1}}, %2644:list{%740:<102400x5120xf32>{5120, 1}, %496:<5120x16384xf32>{16384, 1}, %457:<1536x5120xf32>{5120, 1}, %1344:<576x5120xf32>{5120, 1}, %1487:<160x5120xf32>{5120, 1}, %1613:<5120x3072xf32>{3072, 1}, %1576:<6144x5120xf32>{5120, 1}, %711:<102400x5120xf32>{5120, 1}}}, %514:tuple{5_5e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_adam_of_PyCapsule_object_at_0_7f5650c63030_:builtin_function_or_method,%285:<1xi32>{1},%2642:list{%2643:list{%2626:<102400x5120xf32>{5120,1}+667008000,%1600:<5120x16384xf32>{16384,1}+583116800,%2349:<1536x5120xf32>{5120,1}+575252480,%2636:<576x5120xf32>{5120,1}+572303360,%1618:<160x5120xf32>{5120,1}+571479040,%1483:<5120x3072xf32>{3072,1}+555750400,%1934:<6144x5120xf32>{5120,1}+524293120,%2151:<102400x5120xf32>{5120,1}},%1961:list{%2160:<102400x5120xf32>{5120,1},%2554:<5120x16384xf32>{16384,1},%2635:<1536x5120xf32>{5120,1},%1928:<576x5120xf32>{5120,1},%1552:<160x5120xf32>{5120,1},%284:<5120x3072xf32>{3072,1},%332:<6144x5120xf32>{5120,1},%1720:<102400x5120xf32>{5120,1}},%2556:list{%706:<102400x5120xf32>{5120,1},%606:<5120x16384xf32>{16384,1},%1611:<1536x5120xf32>{5120,1},%601:<576x5120xf32>{5120,1},%1621:<160x5120xf32>{5120,1},%1527:<5120x3072xf32>{3072,1},%1558:<6144x5120xf32>{5120,1},%1537:<102400x5120xf32>{5120,1}},%2644:list{%740:<102400x5120xf32>{5120,1},%496:<5120x16384xf32>{16384,1},%457:<1536x5120xf32>{5120,1},%1344:<576x5120xf32>{5120,1},%1487:<160x5120xf32>{5120,1},%1613:<5120x3072xf32>{3072,1},%1576:<6144x5120xf32>{5120,1},%711:<102400x5120xf32>{5120,1}}},%514:tuple{5_5e-06:float,0_9:float,0_95:float,1e-08:float,2:int,1:int,1:int,0_1:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%285:<1xi32>{1}, %1961:list{%2627:list{%332:<5120xf32>{1}+667002880, %1483:<5120xf32>{1}+572298240, %1934:<5120xf32>{1}+524288000}, %2645:list{%2349:<5120xf32>{1}, %1552:<5120xf32>{1}, %2636:<5120xf32>{1}}, %613:list{%761:<5120xf32>{1}, %1194:<5120xf32>{1}, %1572:<5120xf32>{1}}, %2646:list{%519:<5120xf32>{1}, %1513:<5120xf32>{1}, %1622:<5120xf32>{1}}}, %514:tuple{5_5e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_0:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_adam_of_PyCapsule_object_at_0_7f5650c63030_:builtin_function_or_method,%285:<1xi32>{1},%1961:list{%2627:list{%332:<5120xf32>{1}+667002880,%1483:<5120xf32>{1}+572298240,%1934:<5120xf32>{1}+524288000},%2645:list{%2349:<5120xf32>{1},%1552:<5120xf32>{1},%2636:<5120xf32>{1}},%613:list{%761:<5120xf32>{1},%1194:<5120xf32>{1},%1572:<5120xf32>{1}},%2646:list{%519:<5120xf32>{1},%1513:<5120xf32>{1},%1622:<5120xf32>{1}}},%514:tuple{5_5e-06:float,0_9:float,0_95:float,1e-08:float,2:int,1:int,1:int,0_0:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%2652:list{%1929:<102400x5120xf32>{5120, 1}, %1552:<5120x16384xf32>{16384, 1}, %1928:<1536x5120xf32>{5120, 1}, %2635:<576x5120xf32>{5120, 1}, %2641:<160x5120xf32>{5120, 1}, %2626:<5120x3072xf32>{3072, 1}, %970:<6144x5120xf32>{5120, 1}, %557:<102400x5120xf32>{5120, 1}, %2011:<5120xf32>{1}, %574:<5120xf32>{1}, %1720:<5120xf32>{1}}, %1589:list{%306:<5120xf32>{1}, %301:<5120xf32>{1}, %247:<5120xf32>{1}}, %247:<5120xf32>{1})
          outputs: (%2654:tuple{%2655:list{%2225:<102400x5120xbf16>{5120,1},%1483:<5120x16384xbf16>{16384,1},%2636:<1536x5120xbf16>{5120,1},%1930:<576x5120xbf16>{5120,1},%1934:<160x5120xbf16>{5120,1},%237:<5120x3072xbf16>{3072,1},%2151:<6144x5120xbf16>{5120,1},%2160:<102400x5120xbf16>{5120,1},%1986:<5120xbf16>{1},%471:<5120xbf16>{1},%488:<5120xbf16>{1}},%2652:list{%1929:<102400x5120xf32>{5120,1},%1552:<5120x16384xf32>{16384,1},%1928:<1536x5120xf32>{5120,1},%2635:<576x5120xf32>{5120,1},%2641:<160x5120xf32>{5120,1},%2626:<5120x3072xf32>{3072,1},%970:<6144x5120xf32>{5120,1},%557:<102400x5120xf32>{5120,1},%2011:<5120xf32>{1},%574:<5120xf32>{1},%1720:<5120xf32>{1}}})
          duration: -1
        - ----------->api::_multi_tensor_copy_this_to_that call:
          inputs: (%2652:list{%1929:<102400x5120xf32>{5120, 1}, %1552:<5120x16384xf32>{16384, 1}, %1928:<1536x5120xf32>{5120, 1}, %2635:<576x5120xf32>{5120, 1}, %2641:<160x5120xf32>{5120, 1}, %2626:<5120x3072xf32>{3072, 1}, %970:<6144x5120xf32>{5120, 1}, %557:<102400x5120xf32>{5120, 1}, %2011:<5120xf32>{1}, %574:<5120xf32>{1}, %1720:<5120xf32>{1}}, %2655:list{%2225:<102400x5120xbf16>{5120, 1}, %1483:<5120x16384xbf16>{16384, 1}, %2636:<1536x5120xbf16>{5120, 1}, %1930:<576x5120xbf16>{5120, 1}, %1934:<160x5120xbf16>{5120, 1}, %237:<5120x3072xbf16>{3072, 1}, %2151:<6144x5120xbf16>{5120, 1}, %2160:<102400x5120xbf16>{5120, 1}, %1986:<5120xbf16>{1}, %471:<5120xbf16>{1}, %488:<5120xbf16>{1}}, None:NoneType)
          outputs: (torch.2_3_0._multi_tensor_copy_this_to_that(%2652:list{%1929:<102400x5120xf32>{5120,1},%1552:<5120x16384xf32>{16384,1},%1928:<1536x5120xf32>{5120,1},%2635:<576x5120xf32>{5120,1},%2641:<160x5120xf32>{5120,1},%2626:<5120x3072xf32>{3072,1},%970:<6144x5120xf32>{5120,1},%557:<102400x5120xf32>{5120,1},%2011:<5120xf32>{1},%574:<5120xf32>{1},%1720:<5120xf32>{1}},%2655:list{%2225:<102400x5120xbf16>{5120,1},%1483:<5120x16384xbf16>{16384,1},%2636:<1536x5120xbf16>{5120,1},%1930:<576x5120xbf16>{5120,1},%1934:<160x5120xbf16>{5120,1},%237:<5120x3072xbf16>{3072,1},%2151:<6144x5120xbf16>{5120,1},%2160:<102400x5120xbf16>{5120,1},%1986:<5120xbf16>{1},%471:<5120xbf16>{1},%488:<5120xbf16>{1}},None:NoneType))
          duration: -1
        - aten::copy_:
          inputs: (%2225:<102400x5120xbf16>{5120, 1}, %1929:<102400x5120xf32>{5120, 1}, False:bool)
          outputs: (%2225:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1483:<5120x16384xbf16>{16384, 1}, %1552:<5120x16384xf32>{16384, 1}, False:bool)
          outputs: (%1483:<5120x16384xbf16>{16384,1})
          duration: -1
        - aten::copy_:
          inputs: (%2636:<1536x5120xbf16>{5120, 1}, %1928:<1536x5120xf32>{5120, 1}, False:bool)
          outputs: (%2636:<1536x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1930:<576x5120xbf16>{5120, 1}, %2635:<576x5120xf32>{5120, 1}, False:bool)
          outputs: (%1930:<576x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1934:<160x5120xbf16>{5120, 1}, %2641:<160x5120xf32>{5120, 1}, False:bool)
          outputs: (%1934:<160x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%237:<5120x3072xbf16>{3072, 1}, %2626:<5120x3072xf32>{3072, 1}, False:bool)
          outputs: (%237:<5120x3072xbf16>{3072,1})
          duration: -1
        - aten::copy_:
          inputs: (%2151:<6144x5120xbf16>{5120, 1}, %970:<6144x5120xf32>{5120, 1}, False:bool)
          outputs: (%2151:<6144x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2160:<102400x5120xbf16>{5120, 1}, %557:<102400x5120xf32>{5120, 1}, False:bool)
          outputs: (%2160:<102400x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1986:<5120xbf16>{1}, %2011:<5120xf32>{1}, False:bool)
          outputs: (%1986:<5120xbf16>{1})
          duration: -1
        - aten::copy_:
          inputs: (%471:<5120xbf16>{1}, %574:<5120xf32>{1}, False:bool)
          outputs: (%471:<5120xbf16>{1})
          duration: -1
        - aten::copy_:
          inputs: (%488:<5120xbf16>{1}, %1720:<5120xf32>{1}, False:bool)
          outputs: (%488:<5120xbf16>{1})
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::wrapper call:
          inputs: (%458:tuple{NotSurpot:FusedAdam}, _function_FusedAdam_step_at_0_7f5650cb89d0_:function)
          outputs: (torch.2_3_0.wrapper(%458:tuple{NotSurpot:FusedAdam},_function_FusedAdam_step_at_0_7f5650cb89d0_:function))
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (Optimizer_step#FusedAdam_step:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: (None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.FusedAdam(None:NoneType,)
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: (%42:<1xi32>{1}, %1545:list{%2648:list{%2641:<24576x1536xf32>{1536, 1}+488636416, %1483:<32768x512xf32>{512, 1}+471859200, %1928:<5120x1536xf32>{1536, 1}+463994880, %1934:<3072x5120xf32>{5120, 1}+448266240, %271:<5120x1536xf32>{1536, 1}+440401920, %2626:<3072x5120xf32>{5120, 1}+424673280, %2621:<5120x1536xf32>{1536, 1}+416808960, %488:<3072x5120xf32>{5120, 1}+401080320, %915:<5120x1536xf32>{1536, 1}+393216000, %1811:<3072x5120xf32>{5120, 1}+377487360, %1861:<5120x1536xf32>{1536, 1}+369623040, %1774:<3072x5120xf32>{5120, 1}+353894400, %600:<5120x1536xf32>{1536, 1}+346030080, %1851:<3072x5120xf32>{5120, 1}+330301440, %1970:<5120x1536xf32>{1536, 1}+322437120, %1854:<3072x5120xf32>{5120, 1}+306708480, %1548:<5120x1536xf32>{1536, 1}+298844160, %2229:<3072x5120xf32>{5120, 1}+283115520, %1760:<5120x1536xf32>{1536, 1}+275251200, %2566:<3072x5120xf32>{5120, 1}+259522560, %1234:<5120x1536xf32>{1536, 1}+251658240, %2625:<3072x5120xf32>{5120, 1}+235929600, %1806:<5120x1536xf32>{1536, 1}+228065280, %1720:<3072x5120xf32>{5120, 1}+212336640, %1275:<5120x1536xf32>{1536, 1}+204472320, %984:<3072x5120xf32>{5120, 1}+188743680, %1301:<5120x1536xf32>{1536, 1}+180879360, %967:<3072x5120xf32>{5120, 1}+165150720, %1309:<5120x1536xf32>{1536, 1}+157286400, %2637:<3072x5120xf32>{5120, 1}+141557760, %1389:<5120x1536xf32>{1536, 1}+133693440, %491:<3072x5120xf32>{5120, 1}+117964800, %2130:<5120x1536xf32>{1536, 1}+110100480, %1762:<3072x5120xf32>{5120, 1}+94371840, %2569:<5120x1536xf32>{1536, 1}+86507520, %2661:<3072x5120xf32>{5120, 1}+70778880, %1814:<5120x1536xf32>{1536, 1}+62914560, %1065:<3072x5120xf32>{5120, 1}+47185920, %1765:<5120x1536xf32>{1536, 1}+39321600, %1162:<3072x5120xf32>{5120, 1}+23592960, %2036:<5120x1536xf32>{1536, 1}+15728640, %2025:<3072x5120xf32>{5120, 1}}, %2553:list{%1930:<24576x1536xf32>{1536, 1}, %237:<32768x512xf32>{512, 1}, %2635:<5120x1536xf32>{1536, 1}, %2160:<3072x5120xf32>{5120, 1}, %1986:<5120x1536xf32>{1536, 1}, %2011:<3072x5120xf32>{5120, 1}, %557:<5120x1536xf32>{1536, 1}, %574:<3072x5120xf32>{5120, 1}, %2639:<5120x1536xf32>{1536, 1}, %1809:<3072x5120xf32>{5120, 1}, %1836:<5120x1536xf32>{1536, 1}, %1864:<3072x5120xf32>{5120, 1}, %2656:<5120x1536xf32>{1536, 1}, %1788:<3072x5120xf32>{5120, 1}, %2638:<5120x1536xf32>{1536, 1}, %2082:<3072x5120xf32>{5120, 1}, %1792:<5120x1536xf32>{1536, 1}, %1756:<3072x5120xf32>{5120, 1}, %1262:<5120x1536xf32>{1536, 1}, %1089:<3072x5120xf32>{5120, 1}, %1979:<5120x1536xf32>{1536, 1}, %1318:<3072x5120xf32>{5120, 1}, %2105:<5120x1536xf32>{1536, 1}, %908:<3072x5120xf32>{5120, 1}, %1687:<5120x1536xf32>{1536, 1}, %1391:<3072x5120xf32>{5120, 1}, %1469:<5120x1536xf32>{1536, 1}, %1238:<3072x5120xf32>{5120, 1}, %1423:<5120x1536xf32>{1536, 1}, %2657:<3072x5120xf32>{5120, 1}, %2037:<5120x1536xf32>{1536, 1}, %2133:<3072x5120xf32>{5120, 1}, %2076:<5120x1536xf32>{1536, 1}, %1539:<3072x5120xf32>{5120, 1}, %2660:<5120x1536xf32>{1536, 1}, %2662:<3072x5120xf32>{5120, 1}, %1532:<5120x1536xf32>{1536, 1}, %2405:<3072x5120xf32>{5120, 1}, %2234:<5120x1536xf32>{1536, 1}, %1370:<3072x5120xf32>{5120, 1}, %1668:<5120x1536xf32>{1536, 1}, %2222:<3072x5120xf32>{5120, 1}}, %2663:list{%1607:<24576x1536xf32>{1536, 1}, %282:<32768x512xf32>{512, 1}, %1619:<5120x1536xf32>{1536, 1}, %1616:<3072x5120xf32>{5120, 1}, %1617:<5120x1536xf32>{1536, 1}, %885:<3072x5120xf32>{5120, 1}, %1303:<5120x1536xf32>{1536, 1}, %813:<3072x5120xf32>{5120, 1}, %1644:<5120x1536xf32>{1536, 1}, %823:<3072x5120xf32>{5120, 1}, %1649:<5120x1536xf32>{1536, 1}, %838:<3072x5120xf32>{5120, 1}, %810:<5120x1536xf32>{1536, 1}, %801:<3072x5120xf32>{5120, 1}, %828:<5120x1536xf32>{1536, 1}, %1358:<3072x5120xf32>{5120, 1}, %1654:<5120x1536xf32>{1536, 1}, %1557:<3072x5120xf32>{5120, 1}, %1660:<5120x1536xf32>{1536, 1}, %1656:<3072x5120xf32>{5120, 1}, %1542:<5120x1536xf32>{1536, 1}, %1669:<3072x5120xf32>{5120, 1}, %1671:<5120x1536xf32>{1536, 1}, %1372:<3072x5120xf32>{5120, 1}, %1114:<5120x1536xf32>{1536, 1}, %876:<3072x5120xf32>{5120, 1}, %1131:<5120x1536xf32>{1536, 1}, %1036:<3072x5120xf32>{5120, 1}, %1680:<5120x1536xf32>{1536, 1}, %1684:<3072x5120xf32>{5120, 1}, %888:<5120x1536xf32>{1536, 1}, %1686:<3072x5120xf32>{5120, 1}, %1153:<5120x1536xf32>{1536, 1}, %1225:<3072x5120xf32>{5120, 1}, %1173:<5120x1536xf32>{1536, 1}, %1013:<3072x5120xf32>{5120, 1}, %1254:<5120x1536xf32>{1536, 1}, %1142:<3072x5120xf32>{5120, 1}, %1699:<5120x1536xf32>{1536, 1}, %891:<3072x5120xf32>{5120, 1}, %904:<5120x1536xf32>{1536, 1}, %1209:<3072x5120xf32>{5120, 1}}, %2664:list{%325:<24576x1536xf32>{1536, 1}, %252:<32768x512xf32>{512, 1}, %340:<5120x1536xf32>{1536, 1}, %999:<3072x5120xf32>{5120, 1}, %1579:<5120x1536xf32>{1536, 1}, %1514:<3072x5120xf32>{5120, 1}, %1588:<5120x1536xf32>{1536, 1}, %1642:<3072x5120xf32>{5120, 1}, %917:<5120x1536xf32>{1536, 1}, %1646:<3072x5120xf32>{5120, 1}, %839:<5120x1536xf32>{1536, 1}, %869:<3072x5120xf32>{5120, 1}, %1650:<5120x1536xf32>{1536, 1}, %784:<3072x5120xf32>{5120, 1}, %1270:<5120x1536xf32>{1536, 1}, %1306:<3072x5120xf32>{5120, 1}, %1533:<5120x1536xf32>{1536, 1}, %1657:<3072x5120xf32>{5120, 1}, %1661:<5120x1536xf32>{1536, 1}, %1665:<3072x5120xf32>{5120, 1}, %1331:<5120x1536xf32>{1536, 1}, %1352:<3072x5120xf32>{5120, 1}, %1329:<5120x1536xf32>{1536, 1}, %1087:<3072x5120xf32>{5120, 1}, %1208:<5120x1536xf32>{1536, 1}, %1674:<3072x5120xf32>{5120, 1}, %898:<5120x1536xf32>{1536, 1}, %1677:<3072x5120xf32>{5120, 1}, %1681:<5120x1536xf32>{1536, 1}, %1683:<3072x5120xf32>{5120, 1}, %1099:<5120x1536xf32>{1536, 1}, %1032:<3072x5120xf32>{5120, 1}, %1121:<5120x1536xf32>{1536, 1}, %1184:<3072x5120xf32>{5120, 1}, %1042:<5120x1536xf32>{1536, 1}, %1697:<3072x5120xf32>{5120, 1}, %1295:<5120x1536xf32>{1536, 1}, %1247:<3072x5120xf32>{5120, 1}, %938:<5120x1536xf32>{1536, 1}, %873:<3072x5120xf32>{5120, 1}, %918:<5120x1536xf32>{1536, 1}, %1243:<3072x5120xf32>{5120, 1}}}, %514:tuple{5_5e-06:float, 0_9:float, 0_95:float, 1e-08:float, 2:int, 1:int, 1:int, 0_1:float})
          outputs: (torch.2_3_0.MultiTensorApply(_built-in_method_multi_tensor_adam_of_PyCapsule_object_at_0_7f5650c63030_:builtin_function_or_method,%42:<1xi32>{1},%1545:list{%2648:list{%2641:<24576x1536xf32>{1536,1}+488636416,%1483:<32768x512xf32>{512,1}+471859200,%1928:<5120x1536xf32>{1536,1}+463994880,%1934:<3072x5120xf32>{5120,1}+448266240,%271:<5120x1536xf32>{1536,1}+440401920,%2626:<3072x5120xf32>{5120,1}+424673280,%2621:<5120x1536xf32>{1536,1}+416808960,%488:<3072x5120xf32>{5120,1}+401080320,%915:<5120x1536xf32>{1536,1}+393216000,%1811:<3072x5120xf32>{5120,1}+377487360,%1861:<5120x1536xf32>{1536,1}+369623040,%1774:<3072x5120xf32>{5120,1}+353894400,%600:<5120x1536xf32>{1536,1}+346030080,%1851:<3072x5120xf32>{5120,1}+330301440,%1970:<5120x1536xf32>{1536,1}+322437120,%1854:<3072x5120xf32>{5120,1}+306708480,%1548:<5120x1536xf32>{1536,1}+298844160,%2229:<3072x5120xf32>{5120,1}+283115520,%1760:<5120x1536xf32>{1536,1}+275251200,%2566:<3072x5120xf32>{5120,1}+259522560,%1234:<5120x1536xf32>{1536,1}+251658240,%2625:<3072x5120xf32>{5120,1}+235929600,%1806:<5120x1536xf32>{1536,1}+228065280,%1720:<3072x5120xf32>{5120,1}+212336640,%1275:<5120x1536xf32>{1536,1}+204472320,%984:<3072x5120xf32>{5120,1}+188743680,%1301:<5120x1536xf32>{1536,1}+180879360,%967:<3072x5120xf32>{5120,1}+165150720,%1309:<5120x1536xf32>{1536,1}+157286400,%2637:<3072x5120xf32>{5120,1}+141557760,%1389:<5120x1536xf32>{1536,1}+133693440,%491:<3072x5120xf32>{5120,1}+117964800,%2130:<5120x1536xf32>{1536,1}+110100480,%1762:<3072x5120xf32>{5120,1}+94371840,%2569:<5120x1536xf32>{1536,1}+86507520,%2661:<3072x5120xf32>{5120,1}+70778880,%1814:<5120x1536xf32>{1536,1}+62914560,%1065:<3072x5120xf32>{5120,1}+47185920,%1765:<5120x1536xf32>{1536,1}+39321600,%1162:<3072x5120xf32>{5120,1}+23592960,%2036:<5120x1536xf32>{1536,1}+15728640,%2025:<3072x5120xf32>{5120,1}},%2553:list{%1930:<24576x1536xf32>{1536,1},%237:<32768x512xf32>{512,1},%2635:<5120x1536xf32>{1536,1},%2160:<3072x5120xf32>{5120,1},%1986:<5120x1536xf32>{1536,1},%2011:<3072x5120xf32>{5120,1},%557:<5120x1536xf32>{1536,1},%574:<3072x5120xf32>{5120,1},%2639:<5120x1536xf32>{1536,1},%1809:<3072x5120xf32>{5120,1},%1836:<5120x1536xf32>{1536,1},%1864:<3072x5120xf32>{5120,1},%2656:<5120x1536xf32>{1536,1},%1788:<3072x5120xf32>{5120,1},%2638:<5120x1536xf32>{1536,1},%2082:<3072x5120xf32>{5120,1},%1792:<5120x1536xf32>{1536,1},%1756:<3072x5120xf32>{5120,1},%1262:<5120x1536xf32>{1536,1},%1089:<3072x5120xf32>{5120,1},%1979:<5120x1536xf32>{1536,1},%1318:<3072x5120xf32>{5120,1},%2105:<5120x1536xf32>{1536,1},%908:<3072x5120xf32>{5120,1},%1687:<5120x1536xf32>{1536,1},%1391:<3072x5120xf32>{5120,1},%1469:<5120x1536xf32>{1536,1},%1238:<3072x5120xf32>{5120,1},%1423:<5120x1536xf32>{1536,1},%2657:<3072x5120xf32>{5120,1},%2037:<5120x1536xf32>{1536,1},%2133:<3072x5120xf32>{5120,1},%2076:<5120x1536xf32>{1536,1},%1539:<3072x5120xf32>{5120,1},%2660:<5120x1536xf32>{1536,1},%2662:<3072x5120xf32>{5120,1},%1532:<5120x1536xf32>{1536,1},%2405:<3072x5120xf32>{5120,1},%2234:<5120x1536xf32>{1536,1},%1370:<3072x5120xf32>{5120,1},%1668:<5120x1536xf32>{1536,1},%2222:<3072x5120xf32>{5120,1}},%2663:list{%1607:<24576x1536xf32>{1536,1},%282:<32768x512xf32>{512,1},%1619:<5120x1536xf32>{1536,1},%1616:<3072x5120xf32>{5120,1},%1617:<5120x1536xf32>{1536,1},%885:<3072x5120xf32>{5120,1},%1303:<5120x1536xf32>{1536,1},%813:<3072x5120xf32>{5120,1},%1644:<5120x1536xf32>{1536,1},%823:<3072x5120xf32>{5120,1},%1649:<5120x1536xf32>{1536,1},%838:<3072x5120xf32>{5120,1},%810:<5120x1536xf32>{1536,1},%801:<3072x5120xf32>{5120,1},%828:<5120x1536xf32>{1536,1},%1358:<3072x5120xf32>{5120,1},%1654:<5120x1536xf32>{1536,1},%1557:<3072x5120xf32>{5120,1},%1660:<5120x1536xf32>{1536,1},%1656:<3072x5120xf32>{5120,1},%1542:<5120x1536xf32>{1536,1},%1669:<3072x5120xf32>{5120,1},%1671:<5120x1536xf32>{1536,1},%1372:<3072x5120xf32>{5120,1},%1114:<5120x1536xf32>{1536,1},%876:<3072x5120xf32>{5120,1},%1131:<5120x1536xf32>{1536,1},%1036:<3072x5120xf32>{5120,1},%1680:<5120x1536xf32>{1536,1},%1684:<3072x5120xf32>{5120,1},%888:<5120x1536xf32>{1536,1},%1686:<3072x5120xf32>{5120,1},%1153:<5120x1536xf32>{1536,1},%1225:<3072x5120xf32>{5120,1},%1173:<5120x1536xf32>{1536,1},%1013:<3072x5120xf32>{5120,1},%1254:<5120x1536xf32>{1536,1},%1142:<3072x5120xf32>{5120,1},%1699:<5120x1536xf32>{1536,1},%891:<3072x5120xf32>{5120,1},%904:<5120x1536xf32>{1536,1},%1209:<3072x5120xf32>{5120,1}},%2664:list{%325:<24576x1536xf32>{1536,1},%252:<32768x512xf32>{512,1},%340:<5120x1536xf32>{1536,1},%999:<3072x5120xf32>{5120,1},%1579:<5120x1536xf32>{1536,1},%1514:<3072x5120xf32>{5120,1},%1588:<5120x1536xf32>{1536,1},%1642:<3072x5120xf32>{5120,1},%917:<5120x1536xf32>{1536,1},%1646:<3072x5120xf32>{5120,1},%839:<5120x1536xf32>{1536,1},%869:<3072x5120xf32>{5120,1},%1650:<5120x1536xf32>{1536,1},%784:<3072x5120xf32>{5120,1},%1270:<5120x1536xf32>{1536,1},%1306:<3072x5120xf32>{5120,1},%1533:<5120x1536xf32>{1536,1},%1657:<3072x5120xf32>{5120,1},%1661:<5120x1536xf32>{1536,1},%1665:<3072x5120xf32>{5120,1},%1331:<5120x1536xf32>{1536,1},%1352:<3072x5120xf32>{5120,1},%1329:<5120x1536xf32>{1536,1},%1087:<3072x5120xf32>{5120,1},%1208:<5120x1536xf32>{1536,1},%1674:<3072x5120xf32>{5120,1},%898:<5120x1536xf32>{1536,1},%1677:<3072x5120xf32>{5120,1},%1681:<5120x1536xf32>{1536,1},%1683:<3072x5120xf32>{5120,1},%1099:<5120x1536xf32>{1536,1},%1032:<3072x5120xf32>{5120,1},%1121:<5120x1536xf32>{1536,1},%1184:<3072x5120xf32>{5120,1},%1042:<5120x1536xf32>{1536,1},%1697:<3072x5120xf32>{5120,1},%1295:<5120x1536xf32>{1536,1},%1247:<3072x5120xf32>{5120,1},%938:<5120x1536xf32>{1536,1},%873:<3072x5120xf32>{5120,1},%918:<5120x1536xf32>{1536,1},%1243:<3072x5120xf32>{5120,1}}},%514:tuple{5_5e-06:float,0_9:float,0_95:float,1e-08:float,2:int,1:int,1:int,0_1:float}))
          duration: -1
        - ----------->api::MultiTensorApply call:
          inputs: ()
          outputs: (torch.2_3_0.MultiTensorApply())
          duration: -1
        - ----------->api::FusedAdam call:
          inputs: ()
          outputs: (torch.2_3_0.FusedAdam())
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params return:
          inputs: (%2648:list{%271:<24576x1536xf32>{1536, 1}, %2349:<32768x512xf32>{512, 1}, %237:<5120x1536xf32>{1536, 1}, %1262:<3072x5120xf32>{5120, 1}, %1116:<5120x1536xf32>{1536, 1}, %1979:<3072x5120xf32>{5120, 1}, %2105:<5120x1536xf32>{1536, 1}, %1687:<3072x5120xf32>{5120, 1}, %1469:<5120x1536xf32>{1536, 1}, %2566:<3072x5120xf32>{5120, 1}, %1275:<5120x1536xf32>{1536, 1}, %1309:<3072x5120xf32>{5120, 1}, %1301:<5120x1536xf32>{1536, 1}, %2656:<3072x5120xf32>{5120, 1}, %2639:<5120x1536xf32>{1536, 1}, %1866:<3072x5120xf32>{5120, 1}, %2638:<5120x1536xf32>{1536, 1}, %1792:<3072x5120xf32>{5120, 1}, %1811:<5120x1536xf32>{1536, 1}, %1774:<3072x5120xf32>{5120, 1}, %1970:<5120x1536xf32>{1536, 1}, %1548:<3072x5120xf32>{5120, 1}, %1760:<5120x1536xf32>{1536, 1}, %2637:<3072x5120xf32>{5120, 1}, %1814:<5120x1536xf32>{1536, 1}, %1762:<3072x5120xf32>{5120, 1}, %2133:<5120x1536xf32>{1536, 1}, %1363:<3072x5120xf32>{5120, 1}, %2068:<5120x1536xf32>{1536, 1}, %1539:<3072x5120xf32>{5120, 1}, %2662:<5120x1536xf32>{1536, 1}, %2405:<3072x5120xf32>{5120, 1}, %1370:<5120x1536xf32>{1536, 1}, %2222:<3072x5120xf32>{5120, 1}, %2661:<5120x1536xf32>{1536, 1}, %2025:<3072x5120xf32>{5120, 1}, %2055:<5120x1536xf32>{1536, 1}, %2113:<3072x5120xf32>{5120, 1}, %2043:<5120x1536xf32>{1536, 1}, %2236:<3072x5120xf32>{5120, 1}, %1710:<5120x1536xf32>{1536, 1}, %1662:<3072x5120xf32>{5120, 1}}, %1590:list{%315:<24576x1536xf32>{1536, 1}, %316:<32768x512xf32>{512, 1}, %317:<5120x1536xf32>{1536, 1}, %320:<3072x5120xf32>{5120, 1}, %321:<5120x1536xf32>{1536, 1}, %257:<3072x5120xf32>{5120, 1}, %323:<5120x1536xf32>{1536, 1}, %248:<3072x5120xf32>{5120, 1}, %328:<5120x1536xf32>{1536, 1}, %330:<3072x5120xf32>{5120, 1}, %307:<5120x1536xf32>{1536, 1}, %298:<3072x5120xf32>{5120, 1}, %251:<5120x1536xf32>{1536, 1}, %333:<3072x5120xf32>{5120, 1}, %272:<5120x1536xf32>{1536, 1}, %335:<3072x5120xf32>{5120, 1}, %338:<5120x1536xf32>{1536, 1}, %267:<3072x5120xf32>{5120, 1}, %341:<5120x1536xf32>{1536, 1}, %342:<3072x5120xf32>{5120, 1}, %343:<5120x1536xf32>{1536, 1}, %346:<3072x5120xf32>{5120, 1}, %348:<5120x1536xf32>{1536, 1}, %327:<3072x5120xf32>{5120, 1}, %305:<5120x1536xf32>{1536, 1}, %352:<3072x5120xf32>{5120, 1}, %355:<5120x1536xf32>{1536, 1}, %324:<3072x5120xf32>{5120, 1}, %357:<5120x1536xf32>{1536, 1}, %349:<3072x5120xf32>{5120, 1}, %268:<5120x1536xf32>{1536, 1}, %354:<3072x5120xf32>{5120, 1}, %366:<5120x1536xf32>{1536, 1}, %368:<3072x5120xf32>{5120, 1}, %371:<5120x1536xf32>{1536, 1}, %373:<3072x5120xf32>{5120, 1}, %375:<5120x1536xf32>{1536, 1}, %379:<3072x5120xf32>{5120, 1}, %383:<5120x1536xf32>{1536, 1}, %381:<3072x5120xf32>{5120, 1}, %255:<5120x1536xf32>{1536, 1}, %360:<3072x5120xf32>{5120, 1}}, %360:<3072x5120xf32>{5120, 1})
          outputs: (%1963:tuple{%2669:list{%2160:<24576x1536xbf16>{1536,1},%984:<32768x512xbf16>{512,1},%2621:<5120x1536xbf16>{1536,1},%1676:<3072x5120xbf16>{5120,1},%1992:<5120x1536xbf16>{1536,1},%1089:<3072x5120xbf16>{5120,1},%1318:<5120x1536xbf16>{1536,1},%908:<3072x5120xbf16>{5120,1},%1391:<5120x1536xbf16>{1536,1},%1238:<3072x5120xbf16>{5120,1},%1234:<5120x1536xbf16>{1536,1},%1065:<3072x5120xbf16>{5120,1},%967:<5120x1536xbf16>{1536,1},%1836:<3072x5120xbf16>{5120,1},%1809:<5120x1536xbf16>{1536,1},%1864:<3072x5120xbf16>{5120,1},%1788:<5120x1536xbf16>{1536,1},%2082:<3072x5120xbf16>{5120,1},%1756:<5120x1536xbf16>{1536,1},%1861:<3072x5120xbf16>{5120,1},%1851:<5120x1536xbf16>{1536,1},%1854:<3072x5120xbf16>{5120,1},%2229:<5120x1536xbf16>{1536,1},%1806:<3072x5120xbf16>{5120,1},%1765:<5120x1536xbf16>{1536,1},%2569:<3072x5120xbf16>{5120,1},%2657:<5120x1536xbf16>{1536,1},%1423:<3072x5120xbf16>{5120,1},%2037:<5120x1536xbf16>{1536,1},%2076:<3072x5120xbf16>{5120,1},%2660:<5120x1536xbf16>{1536,1},%1532:<3072x5120xbf16>{5120,1},%2234:<5120x1536xbf16>{1536,1},%1668:<3072x5120xbf16>{5120,1},%1389:<5120x1536xbf16>{1536,1},%2036:<3072x5120xbf16>{5120,1},%1670:<5120x1536xbf16>{1536,1},%1362:<3072x5120xbf16>{5120,1},%2668:<5120x1536xbf16>{1536,1},%1707:<3072x5120xbf16>{5120,1},%2115:<5120x1536xbf16>{1536,1},%1667:<3072x5120xbf16>{5120,1}},%2648:list{%271:<24576x1536xf32>{1536,1},%2349:<32768x512xf32>{512,1},%237:<5120x1536xf32>{1536,1},%1262:<3072x5120xf32>{5120,1},%1116:<5120x1536xf32>{1536,1},%1979:<3072x5120xf32>{5120,1},%2105:<5120x1536xf32>{1536,1},%1687:<3072x5120xf32>{5120,1},%1469:<5120x1536xf32>{1536,1},%2566:<3072x5120xf32>{5120,1},%1275:<5120x1536xf32>{1536,1},%1309:<3072x5120xf32>{5120,1},%1301:<5120x1536xf32>{1536,1},%2656:<3072x5120xf32>{5120,1},%2639:<5120x1536xf32>{1536,1},%1866:<3072x5120xf32>{5120,1},%2638:<5120x1536xf32>{1536,1},%1792:<3072x5120xf32>{5120,1},%1811:<5120x1536xf32>{1536,1},%1774:<3072x5120xf32>{5120,1},%1970:<5120x1536xf32>{1536,1},%1548:<3072x5120xf32>{5120,1},%1760:<5120x1536xf32>{1536,1},%2637:<3072x5120xf32>{5120,1},%1814:<5120x1536xf32>{1536,1},%1762:<3072x5120xf32>{5120,1},%2133:<5120x1536xf32>{1536,1},%1363:<3072x5120xf32>{5120,1},%2068:<5120x1536xf32>{1536,1},%1539:<3072x5120xf32>{5120,1},%2662:<5120x1536xf32>{1536,1},%2405:<3072x5120xf32>{5120,1},%1370:<5120x1536xf32>{1536,1},%2222:<3072x5120xf32>{5120,1},%2661:<5120x1536xf32>{1536,1},%2025:<3072x5120xf32>{5120,1},%2055:<5120x1536xf32>{1536,1},%2113:<3072x5120xf32>{5120,1},%2043:<5120x1536xf32>{1536,1},%2236:<3072x5120xf32>{5120,1},%1710:<5120x1536xf32>{1536,1},%1662:<3072x5120xf32>{5120,1}}})
          duration: -1
        - ----------->api::_multi_tensor_copy_this_to_that call:
          inputs: (%2648:list{%271:<24576x1536xf32>{1536, 1}, %2349:<32768x512xf32>{512, 1}, %237:<5120x1536xf32>{1536, 1}, %1262:<3072x5120xf32>{5120, 1}, %1116:<5120x1536xf32>{1536, 1}, %1979:<3072x5120xf32>{5120, 1}, %2105:<5120x1536xf32>{1536, 1}, %1687:<3072x5120xf32>{5120, 1}, %1469:<5120x1536xf32>{1536, 1}, %2566:<3072x5120xf32>{5120, 1}, %1275:<5120x1536xf32>{1536, 1}, %1309:<3072x5120xf32>{5120, 1}, %1301:<5120x1536xf32>{1536, 1}, %2656:<3072x5120xf32>{5120, 1}, %2639:<5120x1536xf32>{1536, 1}, %1866:<3072x5120xf32>{5120, 1}, %2638:<5120x1536xf32>{1536, 1}, %1792:<3072x5120xf32>{5120, 1}, %1811:<5120x1536xf32>{1536, 1}, %1774:<3072x5120xf32>{5120, 1}, %1970:<5120x1536xf32>{1536, 1}, %1548:<3072x5120xf32>{5120, 1}, %1760:<5120x1536xf32>{1536, 1}, %2637:<3072x5120xf32>{5120, 1}, %1814:<5120x1536xf32>{1536, 1}, %1762:<3072x5120xf32>{5120, 1}, %2133:<5120x1536xf32>{1536, 1}, %1363:<3072x5120xf32>{5120, 1}, %2068:<5120x1536xf32>{1536, 1}, %1539:<3072x5120xf32>{5120, 1}, %2662:<5120x1536xf32>{1536, 1}, %2405:<3072x5120xf32>{5120, 1}, %1370:<5120x1536xf32>{1536, 1}, %2222:<3072x5120xf32>{5120, 1}, %2661:<5120x1536xf32>{1536, 1}, %2025:<3072x5120xf32>{5120, 1}, %2055:<5120x1536xf32>{1536, 1}, %2113:<3072x5120xf32>{5120, 1}, %2043:<5120x1536xf32>{1536, 1}, %2236:<3072x5120xf32>{5120, 1}, %1710:<5120x1536xf32>{1536, 1}, %1662:<3072x5120xf32>{5120, 1}}, %2669:list{%2160:<24576x1536xbf16>{1536, 1}, %984:<32768x512xbf16>{512, 1}, %2621:<5120x1536xbf16>{1536, 1}, %1676:<3072x5120xbf16>{5120, 1}, %1992:<5120x1536xbf16>{1536, 1}, %1089:<3072x5120xbf16>{5120, 1}, %1318:<5120x1536xbf16>{1536, 1}, %908:<3072x5120xbf16>{5120, 1}, %1391:<5120x1536xbf16>{1536, 1}, %1238:<3072x5120xbf16>{5120, 1}, %1234:<5120x1536xbf16>{1536, 1}, %1065:<3072x5120xbf16>{5120, 1}, %967:<5120x1536xbf16>{1536, 1}, %1836:<3072x5120xbf16>{5120, 1}, %1809:<5120x1536xbf16>{1536, 1}, %1864:<3072x5120xbf16>{5120, 1}, %1788:<5120x1536xbf16>{1536, 1}, %2082:<3072x5120xbf16>{5120, 1}, %1756:<5120x1536xbf16>{1536, 1}, %1861:<3072x5120xbf16>{5120, 1}, %1851:<5120x1536xbf16>{1536, 1}, %1854:<3072x5120xbf16>{5120, 1}, %2229:<5120x1536xbf16>{1536, 1}, %1806:<3072x5120xbf16>{5120, 1}, %1765:<5120x1536xbf16>{1536, 1}, %2569:<3072x5120xbf16>{5120, 1}, %2657:<5120x1536xbf16>{1536, 1}, %1423:<3072x5120xbf16>{5120, 1}, %2037:<5120x1536xbf16>{1536, 1}, %2076:<3072x5120xbf16>{5120, 1}, %2660:<5120x1536xbf16>{1536, 1}, %1532:<3072x5120xbf16>{5120, 1}, %2234:<5120x1536xbf16>{1536, 1}, %1668:<3072x5120xbf16>{5120, 1}, %1389:<5120x1536xbf16>{1536, 1}, %2036:<3072x5120xbf16>{5120, 1}, %1670:<5120x1536xbf16>{1536, 1}, %1362:<3072x5120xbf16>{5120, 1}, %2668:<5120x1536xbf16>{1536, 1}, %1707:<3072x5120xbf16>{5120, 1}, %2115:<5120x1536xbf16>{1536, 1}, %1667:<3072x5120xbf16>{5120, 1}}, None:NoneType)
          outputs: (torch.2_3_0._multi_tensor_copy_this_to_that(%2648:list{%271:<24576x1536xf32>{1536,1},%2349:<32768x512xf32>{512,1},%237:<5120x1536xf32>{1536,1},%1262:<3072x5120xf32>{5120,1},%1116:<5120x1536xf32>{1536,1},%1979:<3072x5120xf32>{5120,1},%2105:<5120x1536xf32>{1536,1},%1687:<3072x5120xf32>{5120,1},%1469:<5120x1536xf32>{1536,1},%2566:<3072x5120xf32>{5120,1},%1275:<5120x1536xf32>{1536,1},%1309:<3072x5120xf32>{5120,1},%1301:<5120x1536xf32>{1536,1},%2656:<3072x5120xf32>{5120,1},%2639:<5120x1536xf32>{1536,1},%1866:<3072x5120xf32>{5120,1},%2638:<5120x1536xf32>{1536,1},%1792:<3072x5120xf32>{5120,1},%1811:<5120x1536xf32>{1536,1},%1774:<3072x5120xf32>{5120,1},%1970:<5120x1536xf32>{1536,1},%1548:<3072x5120xf32>{5120,1},%1760:<5120x1536xf32>{1536,1},%2637:<3072x5120xf32>{5120,1},%1814:<5120x1536xf32>{1536,1},%1762:<3072x5120xf32>{5120,1},%2133:<5120x1536xf32>{1536,1},%1363:<3072x5120xf32>{5120,1},%2068:<5120x1536xf32>{1536,1},%1539:<3072x5120xf32>{5120,1},%2662:<5120x1536xf32>{1536,1},%2405:<3072x5120xf32>{5120,1},%1370:<5120x1536xf32>{1536,1},%2222:<3072x5120xf32>{5120,1},%2661:<5120x1536xf32>{1536,1},%2025:<3072x5120xf32>{5120,1},%2055:<5120x1536xf32>{1536,1},%2113:<3072x5120xf32>{5120,1},%2043:<5120x1536xf32>{1536,1},%2236:<3072x5120xf32>{5120,1},%1710:<5120x1536xf32>{1536,1},%1662:<3072x5120xf32>{5120,1}},%2669:list{%2160:<24576x1536xbf16>{1536,1},%984:<32768x512xbf16>{512,1},%2621:<5120x1536xbf16>{1536,1},%1676:<3072x5120xbf16>{5120,1},%1992:<5120x1536xbf16>{1536,1},%1089:<3072x5120xbf16>{5120,1},%1318:<5120x1536xbf16>{1536,1},%908:<3072x5120xbf16>{5120,1},%1391:<5120x1536xbf16>{1536,1},%1238:<3072x5120xbf16>{5120,1},%1234:<5120x1536xbf16>{1536,1},%1065:<3072x5120xbf16>{5120,1},%967:<5120x1536xbf16>{1536,1},%1836:<3072x5120xbf16>{5120,1},%1809:<5120x1536xbf16>{1536,1},%1864:<3072x5120xbf16>{5120,1},%1788:<5120x1536xbf16>{1536,1},%2082:<3072x5120xbf16>{5120,1},%1756:<5120x1536xbf16>{1536,1},%1861:<3072x5120xbf16>{5120,1},%1851:<5120x1536xbf16>{1536,1},%1854:<3072x5120xbf16>{5120,1},%2229:<5120x1536xbf16>{1536,1},%1806:<3072x5120xbf16>{5120,1},%1765:<5120x1536xbf16>{1536,1},%2569:<3072x5120xbf16>{5120,1},%2657:<5120x1536xbf16>{1536,1},%1423:<3072x5120xbf16>{5120,1},%2037:<5120x1536xbf16>{1536,1},%2076:<3072x5120xbf16>{5120,1},%2660:<5120x1536xbf16>{1536,1},%1532:<3072x5120xbf16>{5120,1},%2234:<5120x1536xbf16>{1536,1},%1668:<3072x5120xbf16>{5120,1},%1389:<5120x1536xbf16>{1536,1},%2036:<3072x5120xbf16>{5120,1},%1670:<5120x1536xbf16>{1536,1},%1362:<3072x5120xbf16>{5120,1},%2668:<5120x1536xbf16>{1536,1},%1707:<3072x5120xbf16>{5120,1},%2115:<5120x1536xbf16>{1536,1},%1667:<3072x5120xbf16>{5120,1}},None:NoneType))
          duration: -1
        - aten::copy_:
          inputs: (%2160:<24576x1536xbf16>{1536, 1}, %271:<24576x1536xf32>{1536, 1}, False:bool)
          outputs: (%2160:<24576x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%984:<32768x512xbf16>{512, 1}, %2349:<32768x512xf32>{512, 1}, False:bool)
          outputs: (%984:<32768x512xbf16>{512,1})
          duration: -1
        - aten::copy_:
          inputs: (%2621:<5120x1536xbf16>{1536, 1}, %237:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2621:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1676:<3072x5120xbf16>{5120, 1}, %1262:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1676:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1992:<5120x1536xbf16>{1536, 1}, %1116:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1992:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1089:<3072x5120xbf16>{5120, 1}, %1979:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1089:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1318:<5120x1536xbf16>{1536, 1}, %2105:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1318:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%908:<3072x5120xbf16>{5120, 1}, %1687:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%908:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1391:<5120x1536xbf16>{1536, 1}, %1469:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1391:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<3072x5120xbf16>{5120, 1}, %2566:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1238:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1234:<5120x1536xbf16>{1536, 1}, %1275:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1234:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1065:<3072x5120xbf16>{5120, 1}, %1309:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1065:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%967:<5120x1536xbf16>{1536, 1}, %1301:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%967:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1836:<3072x5120xbf16>{5120, 1}, %2656:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1836:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1809:<5120x1536xbf16>{1536, 1}, %2639:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1809:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1864:<3072x5120xbf16>{5120, 1}, %1866:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1864:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1788:<5120x1536xbf16>{1536, 1}, %2638:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1788:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%2082:<3072x5120xbf16>{5120, 1}, %1792:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%2082:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1756:<5120x1536xbf16>{1536, 1}, %1811:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1756:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1861:<3072x5120xbf16>{5120, 1}, %1774:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1861:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1851:<5120x1536xbf16>{1536, 1}, %1970:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1851:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1854:<3072x5120xbf16>{5120, 1}, %1548:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1854:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2229:<5120x1536xbf16>{1536, 1}, %1760:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2229:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1806:<3072x5120xbf16>{5120, 1}, %2637:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1806:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1765:<5120x1536xbf16>{1536, 1}, %1814:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1765:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%2569:<3072x5120xbf16>{5120, 1}, %1762:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%2569:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2657:<5120x1536xbf16>{1536, 1}, %2133:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2657:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1423:<3072x5120xbf16>{5120, 1}, %1363:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1423:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2037:<5120x1536xbf16>{1536, 1}, %2068:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2037:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%2076:<3072x5120xbf16>{5120, 1}, %1539:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%2076:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2660:<5120x1536xbf16>{1536, 1}, %2662:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2660:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1532:<3072x5120xbf16>{5120, 1}, %2405:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1532:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2234:<5120x1536xbf16>{1536, 1}, %1370:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2234:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1668:<3072x5120xbf16>{5120, 1}, %2222:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1668:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1389:<5120x1536xbf16>{1536, 1}, %2661:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1389:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%2036:<3072x5120xbf16>{5120, 1}, %2025:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%2036:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%1670:<5120x1536xbf16>{1536, 1}, %2055:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%1670:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1362:<3072x5120xbf16>{5120, 1}, %2113:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1362:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2668:<5120x1536xbf16>{1536, 1}, %2043:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2668:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1707:<3072x5120xbf16>{5120, 1}, %2236:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1707:<3072x5120xbf16>{5120,1})
          duration: -1
        - aten::copy_:
          inputs: (%2115:<5120x1536xbf16>{1536, 1}, %1710:<5120x1536xf32>{1536, 1}, False:bool)
          outputs: (%2115:<5120x1536xbf16>{1536,1})
          duration: -1
        - aten::copy_:
          inputs: (%1667:<3072x5120xbf16>{5120, 1}, %1662:<3072x5120xf32>{5120, 1}, False:bool)
          outputs: (%1667:<3072x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (True:bool, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (True:bool)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (%2563:list{NotSurpot:dict, _'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6')],_'wd_mult'__0_0,_'lr_mult'__1_0,_'is_e_pert_parallel'__False,_'is_decoupled_lr'__False,_'ma__lr'__1e-05,_'min_lr'__1e-06,_'lr'__5_5e-06,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_0,_'step'__2_:dict, NotSurpot:dict}, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (%2563:list{NotSurpot:dict,_'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device)
          duration: -1
        - aten::add:
          inputs: (%775:<i32>, 0:int, alpha=1:int)
          outputs: (%2633:<i32>)
          duration: -1
        - aten::div:
          inputs: (%2633:<i32>, 1:int)
          outputs: (%2225:<i32>)
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: (%2648:list{NotSurpot:dict, _'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6'),_tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device='cuda_6')],_'wd_mult'__0_0,_'lr_mult'__1_0,_'is_e_pert_parallel'__False,_'is_decoupled_lr'__False,_'ma__lr'__1e-05,_'min_lr'__1e-06,_'lr'__1e-06,_'bias_correction'__True,_'betas'__(0_9,_0_95),_'eps'__1e-08,_'weight_decay'__0_0,_'step'__2_:dict, NotSurpot:dict}, _megatron_core_optimizer_optimizer_Float16OptimizerWithFloat16Params_object_at_0_7f55e83bfa30_:Float16OptimizerWithFloat16Params)
          outputs: (%2648:list{NotSurpot:dict,_'params'__[tensor([1_0000,_1_0000,_1_0000,_____,_1_0000,_1_0000,_1_0000],_device)
          duration: -1
        - aten::add:
          inputs: (%1584:<1xf32>{1}, %2225:<i32>, alpha=1:int)
          outputs: (%1977:<1xf32>{1})
          duration: -1
        - aten::zeros:
          inputs: (%2649:list{8:int, 24:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%762:<8x24xf32>{24,1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%908:<192xf32>{1}, %1318:<24xf32>{1}+144, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%908:<192xf32>{1},%1318:<24xf32>{1}+144,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%908:<192xf32>{1}, %1318:<24xf32>{1}+144, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2578:tuple{%908:<192xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gt:
          inputs: (%1089:<8xf32>{24}, 0_0:float)
          outputs: (%1462:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1089:<8xf32>{24}, %2653:list{%1462:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%984:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+1, 0_0:float)
          outputs: (%908:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+1, %2670:list{%908:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1462:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%39:<8xf32>{24}+2, 0_0:float)
          outputs: (%2621:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%39:<8xf32>{24}+2, %2653:list{%2621:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1687:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1992:<8xf32>{24}+3, 0_0:float)
          outputs: (%1301:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1992:<8xf32>{24}+3, %2670:list{%1301:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%908:<8xf32>{24}+4, 0_0:float)
          outputs: (%1462:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%908:<8xf32>{24}+4, %2653:list{%1462:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+5, 0_0:float)
          outputs: (%1469:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+5, %2670:list{%1469:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+6, 0_0:float)
          outputs: (%1992:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+6, %2653:list{%1992:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+7, 0_0:float)
          outputs: (%942:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+7, %2670:list{%942:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+8, 0_0:float)
          outputs: (%908:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+8, %2653:list{%908:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+9, 0_0:float)
          outputs: (%1469:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+9, %2670:list{%1469:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+10, 0_0:float)
          outputs: (%1992:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+10, %2653:list{%1992:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+11, 0_0:float)
          outputs: (%942:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+11, %2670:list{%942:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+12, 0_0:float)
          outputs: (%908:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+12, %2653:list{%908:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+13, 0_0:float)
          outputs: (%1469:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+13, %2670:list{%1469:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+14, 0_0:float)
          outputs: (%1992:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+14, %2653:list{%1992:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+15, 0_0:float)
          outputs: (%942:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+15, %2670:list{%942:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+16, 0_0:float)
          outputs: (%908:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+16, %2653:list{%908:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+17, 0_0:float)
          outputs: (%1469:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+17, %2670:list{%1469:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+18, 0_0:float)
          outputs: (%1992:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+18, %2653:list{%1992:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+19, 0_0:float)
          outputs: (%942:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+19, %2670:list{%942:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+20, 0_0:float)
          outputs: (%908:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+20, %2653:list{%908:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+21, 0_0:float)
          outputs: (%1469:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+21, %2670:list{%1469:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+22, 0_0:float)
          outputs: (%1992:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+22, %2653:list{%1992:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2151:<8xf32>{24}+23, 0_0:float)
          outputs: (%942:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2151:<8xf32>{24}+23, %2670:list{%942:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2621:<0xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%746:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2530_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%746:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2530_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2670:list{%746:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%1503:tuple{%2667:list{%746:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::zero_:
          inputs: (%746:<1xf32>{1})
          outputs: (%746:<1xf32>{1})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%766:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %2671:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%39:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %2672:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::zeros:
          inputs: (%2673:list{8:int, 24:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2151:<8x24xf32>{24,1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%1301:<192xf32>{1}, %1992:<24xf32>{1}+144, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%1301:<192xf32>{1},%1992:<24xf32>{1}+144,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%1301:<192xf32>{1}, %1992:<24xf32>{1}+144, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2628:tuple{%1301:<192xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::gt:
          inputs: (%1084:<8xf32>{24}, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1084:<8xf32>{24}, %1731:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%908:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1992:<8xf32>{24}+1, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1992:<8xf32>{24}+1, %2674:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1084:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2160:<8xf32>{24}+2, 0_0:float)
          outputs: (%1065:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2160:<8xf32>{24}+2, %1731:list{%1065:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1301:<8xf32>{24}+3, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1301:<8xf32>{24}+3, %2674:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1065:<8xf32>{24}+4, 0_0:float)
          outputs: (%1301:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1065:<8xf32>{24}+4, %1731:list{%1301:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1084:<8xf32>{24}+5, 0_0:float)
          outputs: (%967:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1084:<8xf32>{24}+5, %2674:list{%967:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+6, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+6, %1731:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1234:<8xf32>{24}+7, 0_0:float)
          outputs: (%1309:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1234:<8xf32>{24}+7, %2674:list{%1309:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2160:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2625:<8xf32>{24}+8, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2625:<8xf32>{24}+8, %1731:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1234:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+9, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+9, %2674:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1234:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1992:<8xf32>{24}+10, 0_0:float)
          outputs: (%1309:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1992:<8xf32>{24}+10, %1731:list{%1309:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2160:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1084:<8xf32>{24}+11, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1084:<8xf32>{24}+11, %2674:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+12, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+12, %1731:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1234:<8xf32>{24}+13, 0_0:float)
          outputs: (%1309:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1234:<8xf32>{24}+13, %2674:list{%1309:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2160:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2625:<8xf32>{24}+14, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2625:<8xf32>{24}+14, %1731:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1234:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+15, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+15, %2674:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1234:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1992:<8xf32>{24}+16, 0_0:float)
          outputs: (%1309:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1992:<8xf32>{24}+16, %1731:list{%1309:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2160:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1084:<8xf32>{24}+17, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1084:<8xf32>{24}+17, %2674:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+18, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+18, %1731:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1234:<8xf32>{24}+19, 0_0:float)
          outputs: (%1309:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1234:<8xf32>{24}+19, %2674:list{%1309:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2160:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%2625:<8xf32>{24}+20, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2625:<8xf32>{24}+20, %1731:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1234:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1687:<8xf32>{24}+21, 0_0:float)
          outputs: (%1238:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1687:<8xf32>{24}+21, %2674:list{%1238:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1234:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1992:<8xf32>{24}+22, 0_0:float)
          outputs: (%1309:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1992:<8xf32>{24}+22, %1731:list{%1309:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%2160:<0xf32>{1})
          duration: -1
        - aten::gt:
          inputs: (%1084:<8xf32>{24}+23, 0_0:float)
          outputs: (%2105:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1084:<8xf32>{24}+23, %2674:list{%2105:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1992:<0xf32>{1})
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%2151:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %2674:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - ----------->api::barrier call:
          inputs: (None:NoneType, False:bool, None:NoneType)
          outputs: (torch.2_3_0.barrier(None:NoneType,)
          duration: -1
        - ----------->api::_get_pg_default_device call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0._get_pg_default_device(None:NoneType))
          duration: -1
        - ----------->api::_get_pg_default_device return:
          inputs: (_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82f7e30_:ProcessGroup)
          outputs: (cuda:device)
          duration: -1
        - c10d::barrier:
          inputs: (%1992:<1xu8>{1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, %1155:list{}, -1:int)
          outputs: (ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject)
          duration: -1
        - aten::zeros:
          inputs: (%1731:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%1904:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %415:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::eq:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}, %415:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%2679:list{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2540:tuple{%2653:list{%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::index_put_:
          inputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1, %415:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2676:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2676:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2680:list{%2676:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1503:tuple{%1731:list{%2676:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1272:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1272:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2679:list{%1272:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2628:tuple{%2120:list{%1272:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2680:list{%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2540:tuple{%1630:list{%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1696:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2681:list{%1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%1503:tuple{%2653:list{%1696:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1696:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1696:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1696:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__11,_3779,___66,_____,__754,__340,_1099]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[__11,_3779,___66,_____,__754,__340,_1099]],_device)
          duration: -1
177555 2024-12-10 17:48:28.986908 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n2,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%473:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%473:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
177573 2024-12-10 17:48:28.987656 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n2,rank6)
        - ----------->api::embedding call:
          inputs: (%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%771:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%807:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%807:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
177672 2024-12-10 17:48:29.002355 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n2,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%473:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%967:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%473:tuple{%1655:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%473:tuple{%1655:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
177708 2024-12-10 17:48:29.004424 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n2,rank6)
        - ----------->api::dropout call:
          inputs: (%1655:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%1655:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%1655:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%1655:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
177755 2024-12-10 17:48:29.010654 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n2,rank6)
        - ----------->api::Dropout return:
          inputs: (%473:tuple{%1655:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%1655:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
177770 2024-12-10 17:48:29.011360 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n2,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__11,_3779,___66,_____,__754,__340,_1099]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%1655:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%473:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%473:tuple{1024:int}))
          duration: -1
177796 2024-12-10 17:48:29.012968 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n2,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%766:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%766:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%1084:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%762:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%2566:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%2688:list{%2566:<1024x20xf32>{20, 1}, %2566:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%766:<1024x40xf32>{40,1})
          duration: -1
177942 2024-12-10 17:48:29.023838 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n2,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%473:tuple{1024:int})
          outputs: (%2613:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
177987 2024-12-10 17:48:29.030009 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n2,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
178073 2024-12-10 17:48:29.038876 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n2,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%473:tuple{%1238:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%473:tuple{%1238:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
178090 2024-12-10 17:48:29.039612 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n6,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1238:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1238:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%1907:tuple{%1238:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1907:tuple{%1238:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%2691:tuple{%1238:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %908:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%2691:tuple{%1238:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%908:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%413:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%428:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%428:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %1907:tuple{%1238:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%583:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%583:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
178339 2024-12-10 17:48:29.083243 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n6,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%473:tuple{%583:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%473:tuple{%583:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
178357 2024-12-10 17:48:29.087382 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n2,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1982:tuple{%583:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1982:tuple{%583:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
178375 2024-12-10 17:48:29.088132 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n52,rank6)
        - aten::mm:
          inputs: (%908:<1024x5120xbf16>{5120, 1}, %942:<5120x1536xbf16>{1, 5120})
          outputs: (%2105:<1024x1536xbf16>{1536,1})
          duration: -1
178513 2024-12-10 17:48:29.095986 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n52,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1982:tuple{%583:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2694:tuple{%1301:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1982:tuple{%1301:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1982:tuple{%1301:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
178538 2024-12-10 17:48:29.097000 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n53,rank6)
        - aten::mm:
          inputs: (%1309:<1024x1536xbf16>{1536, 1}, %2566:<1536x24576xbf16>{1, 1536})
          outputs: (%332:<1024x24576xbf16>{24576,1})
          duration: -1
178655 2024-12-10 17:48:29.103056 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n53,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1982:tuple{%1301:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%2697:tuple{%2160:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%276:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %2695:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%276:<1024x1x128x192xbf16>{24576,24576,192,1},%2695:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%276:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %2695:list{128:int, 64:int}, -1:int)
          outputs: (%1963:tuple{%332:<1024x1x128x128xbf16>{24576,24576,192,1},%2677:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1982:tuple{%583:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1982:tuple{%583:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
178760 2024-12-10 17:48:29.112493 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n54,rank6)
        - aten::mm:
          inputs: (%1309:<1024x5120xbf16>{5120, 1}, %1938:<5120x576xbf16>{1, 5120})
          outputs: (%1164:<1024x576xbf16>{576,1})
          duration: -1
178894 2024-12-10 17:48:29.120264 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n54,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1982:tuple{%583:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%1963:tuple{%1234:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%1234:<1024x1x576xbf16>{576, 576, 1}, %2703:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%1234:<1024x1x576xbf16>{576,576,1},%2703:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%1234:<1024x1x576xbf16>{576, 576, 1}, %2703:list{512:int, 64:int}, -1:int)
          outputs: (%2697:tuple{%1301:<1024x1x512xbf16>{576,576,1},%2566:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%1982:tuple{%1301:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%1982:tuple{%1301:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
178982 2024-12-10 17:48:29.128437 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n55,rank6)
        - aten::mm:
          inputs: (%1848:<1024x512xbf16>{576, 1}, %1809:<512x32768xbf16>{1, 512})
          outputs: (%1853:<1024x32768xbf16>{32768,1})
          duration: -1
179080 2024-12-10 17:48:29.134497 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n55,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%1982:tuple{%1301:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%1963:tuple{%1792:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%2105:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %2698:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%2105:<1024x1x128x256xbf16>{32768,32768,256,1},%2698:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%2105:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %2698:list{128:int, 128:int}, -1:int)
          outputs: (%2694:tuple{%1309:<1024x1x128x128xbf16>{32768,32768,256,1},%942:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%1976:tuple{%1177:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%1976:tuple{%1177:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
179198 2024-12-10 17:48:29.146116 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n2,rank6)
179232 2024-12-10 17:48:29.149176 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n2,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%1976:tuple{%1177:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%2628:tuple{%1164:<1024x64xbf16>{64,1},%1809:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%1164:<1024x64xbf16>{64, 1}, %2707:list{%1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%984:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%1809:<1024x64xbf16>{64, 1}, %2706:list{%1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%271:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1866:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1871:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1849:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%1760:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%1811:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%2707:list{%1811:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %1116:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%1836:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1836:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1566:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1760:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%1849:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %1760:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%1862:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2638:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1871:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1849:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%1811:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%1854:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%2707:list{%1854:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %1116:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%1836:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1836:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1566:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1811:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%427:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%403:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%403:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::add:
          inputs: (%1849:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1811:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%1760:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%271:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %1782:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%271:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%332:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %1862:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%332:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%908:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %1853:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%908:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1309:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %1811:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%1309:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%1301:<128x1024x192xbf16>{196608, 192, 1}, %1234:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%1687:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%276:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%1234:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%2105:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%908:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%2105:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %2707:list{%908:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %1687:<i32>, False:bool)
          outputs: (%2105:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%1234:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %2105:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%332:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%332:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%332:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%1301:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%1309:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%332:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %1309:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%1309:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%2160:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%2160:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%2160:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%2160:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%908:<128x1024x1024xbf16>{1048576, 1024, 1}, %1309:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%1301:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%1982:tuple{%1301:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%1982:tuple{%1301:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
180714 2024-12-10 17:48:29.265192 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n44,rank6)
        - aten::mm:
          inputs: (%1853:<1024x16384xbf16>{16384, 1}, %1862:<16384x5120xbf16>{1, 16384})
          outputs: (%1262:<1024x5120xbf16>{5120,1})
          duration: -1
180857 2024-12-10 17:48:29.273079 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n44,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%1982:tuple{%1301:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%2599:tuple{%1861:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
180870 2024-12-10 17:48:29.273849 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n2,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%473:tuple{%583:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2599:tuple{%1861:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2713:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2713:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2555:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2555:tuple{None:NoneType,)
          duration: -1
        - aten::stack:
          inputs: (%441:list{%414:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%244:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%244:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%443:list{%412:<1024xf32>{1}}, 0:int, out=%262:<1x1024xf32>{1024, 1})
          outputs: (%262:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%444:list{%413:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%280:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%280:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b860e230_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b860e230_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1972:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1972:tuple{%434:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
183500 2024-12-10 17:48:30.267981 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n7,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%434:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%434:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%2714:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%2714:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%2714:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2714:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%2717:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %726:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%2717:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%726:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %2714:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2030:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%2030:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
183733 2024-12-10 17:48:30.311839 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n7,rank6)
        - ----------->api::MoELayer call:
          inputs: (%1972:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%1972:tuple{%2030:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
183744 2024-12-10 17:48:30.312582 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n2,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%2718:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%2718:tuple{%2030:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
183755 2024-12-10 17:48:30.313330 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n2,rank6)
        - aten::mm:
          inputs: (%2720:<1024x5120xbf16>{5120, 1}, %1574:<5120x160xbf16>{1, 5120})
          outputs: (%2169:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%1612:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%2576:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%2593:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%2722:tuple{%2723:<1024x6xbf16>{6,1},%2576:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%2575:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%2584:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%1468:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%1584:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%1584:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %1200:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%1584:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%1584:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %1468:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%1687:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%2584:<1024x160xf32>{160, 1}, %1468:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%1234:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%1234:<160xf32>{1}, %1687:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%1201:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%1201:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%1861:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%1861:<i32>, 2_5431315104166666e-07:float)
          outputs: (%2724:<i32>)
          duration: -1
        - aten::div:
          inputs: (%2724:<i32>, 0_01:float)
          outputs: (%2725:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%1861:<i32>, %1774:<i32>, alpha=1:int)
          outputs: (%1861:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%1569:<i32>, %1861:<i32>, False:bool)
          outputs: (%1569:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%2726:tuple{%2724:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%2726:tuple{%2724:<i32>}))
          duration: -1
184290 2024-12-10 17:48:30.348223 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n2,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%2718:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2728:tuple{%2483:<1024x6xbf16>{6,1},%2576:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%1687:<8192x5120xbf16>{5120, 1}, %2151:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%1687:<8192x5120xbf16>{5120,1},%2151:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%1687:<8192x5120xbf16>{5120, 1}, %2151:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2729:tuple{%1687:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%2724:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2576:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%2724:<8192x6xCUSTOM_DATA_TYPE>{6,1},%2576:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%2724:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2576:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2722:tuple{%2724:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%2724:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%2656:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%2724:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%1970:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%2656:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %1970:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2380:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%2724:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2380:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2733:<6121xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%2735:<8192x6xbf16>{6, 1}, %2483:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%2735:<8192x6xbf16>{6,1},%2483:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%2735:<8192x6xbf16>{6, 1}, %2483:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2588:tuple{%2735:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%2735:<8192x6xbf16>{6, 1}, %2380:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2381:<6121xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%2380:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%856:<6121x2xCUSTOM_DATA_TYPE>{1,6121})
          duration: -1
        - aten::gather:
          inputs: (%1687:<8192x5120xbf16>{5120, 1}, 0:int, %2739:<6121x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%1958:<6121x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%2733:<6121xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%1808:tuple{%426:<6121xCUSTOM_DATA_TYPE>{1},%840:<6121xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%2740:<6121xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%2741:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%1958:<6121x5120xbf16>{5120, 1}, 0:int, %2743:<6121x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%1780:<6121x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%1808:tuple{%1780:<6121x5120xbf16>{5120, 1}, %2742:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%1808:tuple{%1780:<6121x5120xbf16>{5120,1},%2742:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
185106 2024-12-10 17:48:30.468300 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n2,rank6)
        - aten::cumsum:
          inputs: (%2742:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%2169:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%2737:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%2160:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%2737:list{%2160:<1xCUSTOM_DATA_TYPE>{1}, %2169:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%2151:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%1116:<119x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%793:tuple{%1116:<119x5120xbf16>{5120,1}}))
          duration: -1
185273 2024-12-10 17:48:30.479058 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n42,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%1116:<119x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%1116:<119x5120xbf16>{5120,1}}))
          duration: -1
185295 2024-12-10 17:48:30.479786 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n56,rank6)
        - aten::mm:
          inputs: (%1116:<119x5120xbf16>{5120, 1}, %2327:<5120x3072xbf16>{1, 5120})
          outputs: (%2744:<119x3072xbf16>{3072,1})
          duration: -1
185379 2024-12-10 17:48:30.483621 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n56,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%1116:<119x5120xbf16>{5120, 1}})
          outputs: (%2746:tuple{%2744:<119x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2747:<119x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2747:<119x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2747:<119x1536xbf16>{3072, 1})
          outputs: (%2507:<119x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2747:<119x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2507:<119x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2507:<119x1536xbf16>{1536, 1}, %2267:<119x1536xbf16>{3072, 1}+1536)
          outputs: (%2748:<119x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2748:<119x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2748:<119x1536xbf16>{1536,1}}))
          duration: -1
185482 2024-12-10 17:48:30.493517 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n45,rank6)
        - aten::mm:
          inputs: (%2748:<119x1536xbf16>{1536, 1}, %2735:<1536x5120xbf16>{1, 1536})
          outputs: (%2607:<119x5120xbf16>{5120,1})
          duration: -1
185562 2024-12-10 17:48:30.497275 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n45,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2748:<119x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2607:<119x5120xbf16>{5120,1},None:NoneType})
          duration: -1
185578 2024-12-10 17:48:30.498031 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n42,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%1116:<119x5120xbf16>{5120, 1}})
          outputs: (%1948:tuple{%2607:<119x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1562:<119x5120xbf16>{5120, 1}, %2607:<119x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1562:<119x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%358:<712x5120xbf16>{5120, 1}+609280})
          outputs: (torch.2_3_0.MLP(%793:tuple{%358:<712x5120xbf16>{5120,1}+609280}))
          duration: -1
185755 2024-12-10 17:48:30.508198 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n43,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%358:<712x5120xbf16>{5120, 1}+609280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%358:<712x5120xbf16>{5120,1}+609280}))
          duration: -1
185775 2024-12-10 17:48:30.508919 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n57,rank6)
        - aten::mm:
          inputs: (%358:<712x5120xbf16>{5120, 1}+609280, %2507:<5120x3072xbf16>{1, 5120})
          outputs: (%2748:<712x3072xbf16>{3072,1})
          duration: -1
185859 2024-12-10 17:48:30.512664 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n57,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%358:<712x5120xbf16>{5120, 1}+609280})
          outputs: (%2516:tuple{%2748:<712x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1970:<712x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1970:<712x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1970:<712x1536xbf16>{3072, 1})
          outputs: (%2744:<712x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1970:<712x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2744:<712x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2744:<712x1536xbf16>{1536, 1}, %1817:<712x1536xbf16>{3072, 1}+1536)
          outputs: (%2179:<712x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2179:<712x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2179:<712x1536xbf16>{1536,1}}))
          duration: -1
185960 2024-12-10 17:48:30.522444 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n46,rank6)
        - aten::mm:
          inputs: (%2179:<712x1536xbf16>{1536, 1}, %2198:<1536x5120xbf16>{1, 1536})
          outputs: (%2267:<712x5120xbf16>{5120,1})
          duration: -1
186042 2024-12-10 17:48:30.526189 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n46,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2179:<712x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2267:<712x5120xbf16>{5120,1},None:NoneType})
          duration: -1
186058 2024-12-10 17:48:30.526938 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n43,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%358:<712x5120xbf16>{5120, 1}+609280})
          outputs: (%1948:tuple{%2267:<712x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1817:<712x5120xbf16>{5120, 1}+609280, %2267:<712x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1817:<712x5120xbf16>{5120,1}+609280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2747:<369x5120xbf16>{5120, 1}+4254720})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2747:<369x5120xbf16>{5120,1}+4254720}))
          duration: -1
186234 2024-12-10 17:48:30.537111 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n44,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2747:<369x5120xbf16>{5120, 1}+4254720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2747:<369x5120xbf16>{5120,1}+4254720}))
          duration: -1
186253 2024-12-10 17:48:30.537843 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n58,rank6)
        - aten::mm:
          inputs: (%2747:<369x5120xbf16>{5120, 1}+4254720, %2744:<5120x3072xbf16>{1, 5120})
          outputs: (%2615:<369x3072xbf16>{3072,1})
          duration: -1
186336 2024-12-10 17:48:30.541581 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n58,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2747:<369x5120xbf16>{5120, 1}+4254720})
          outputs: (%2736:tuple{%2615:<369x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%358:<369x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%358:<369x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%358:<369x1536xbf16>{3072, 1})
          outputs: (%2744:<369x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%358:<369x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2744:<369x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2744:<369x1536xbf16>{1536, 1}, %2380:<369x1536xbf16>{3072, 1}+1536)
          outputs: (%2607:<369x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2607:<369x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2607:<369x1536xbf16>{1536,1}}))
          duration: -1
186435 2024-12-10 17:48:30.551388 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n47,rank6)
        - aten::mm:
          inputs: (%2607:<369x1536xbf16>{1536, 1}, %2469:<1536x5120xbf16>{1, 1536})
          outputs: (%2507:<369x5120xbf16>{5120,1})
          duration: -1
186519 2024-12-10 17:48:30.555132 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n47,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2607:<369x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2507:<369x5120xbf16>{5120,1},None:NoneType})
          duration: -1
186534 2024-12-10 17:48:30.555881 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n44,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2747:<369x5120xbf16>{5120, 1}+4254720})
          outputs: (%1948:tuple{%2507:<369x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%243:<369x5120xbf16>{5120, 1}+4254720, %2507:<369x5120xbf16>{5120, 1}, False:bool)
          outputs: (%243:<369x5120xbf16>{5120,1}+4254720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2607:<456x5120xbf16>{5120, 1}+6144000})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2607:<456x5120xbf16>{5120,1}+6144000}))
          duration: -1
186709 2024-12-10 17:48:30.566078 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n45,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2607:<456x5120xbf16>{5120, 1}+6144000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2607:<456x5120xbf16>{5120,1}+6144000}))
          duration: -1
186729 2024-12-10 17:48:30.566820 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n59,rank6)
        - aten::mm:
          inputs: (%2607:<456x5120xbf16>{5120, 1}+6144000, %2179:<5120x3072xbf16>{1, 5120})
          outputs: (%2380:<456x3072xbf16>{3072,1})
          duration: -1
186814 2024-12-10 17:48:30.570616 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n59,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2607:<456x5120xbf16>{5120, 1}+6144000})
          outputs: (%2746:tuple{%2380:<456x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<456x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<456x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<456x1536xbf16>{3072, 1})
          outputs: (%2198:<456x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<456x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2198:<456x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2198:<456x1536xbf16>{1536, 1}, %426:<456x1536xbf16>{3072, 1}+1536)
          outputs: (%776:<456x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%776:<456x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%776:<456x1536xbf16>{1536,1}}))
          duration: -1
186917 2024-12-10 17:48:30.580506 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n48,rank6)
        - aten::mm:
          inputs: (%776:<456x1536xbf16>{1536, 1}, %2181:<1536x5120xbf16>{1, 1536})
          outputs: (%724:<456x5120xbf16>{5120,1})
          duration: -1
187000 2024-12-10 17:48:30.584316 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n48,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%776:<456x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%724:<456x5120xbf16>{5120,1},None:NoneType})
          duration: -1
187017 2024-12-10 17:48:30.585086 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n45,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2607:<456x5120xbf16>{5120, 1}+6144000})
          outputs: (%1948:tuple{%724:<456x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1798:<456x5120xbf16>{5120, 1}+6144000, %724:<456x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1798:<456x5120xbf16>{5120,1}+6144000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%358:<142x5120xbf16>{5120, 1}+8478720})
          outputs: (torch.2_3_0.MLP(%793:tuple{%358:<142x5120xbf16>{5120,1}+8478720}))
          duration: -1
187194 2024-12-10 17:48:30.595357 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n46,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%358:<142x5120xbf16>{5120, 1}+8478720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%358:<142x5120xbf16>{5120,1}+8478720}))
          duration: -1
187218 2024-12-10 17:48:30.596082 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n60,rank6)
        - aten::mm:
          inputs: (%358:<142x5120xbf16>{5120, 1}+8478720, %2758:<5120x3072xbf16>{1, 5120})
          outputs: (%1645:<142x3072xbf16>{3072,1})
          duration: -1
187296 2024-12-10 17:48:30.599849 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n60,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%358:<142x5120xbf16>{5120, 1}+8478720})
          outputs: (%2516:tuple{%1645:<142x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1116:<142x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1116:<142x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1116:<142x1536xbf16>{3072, 1})
          outputs: (%2607:<142x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1116:<142x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2607:<142x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2607:<142x1536xbf16>{1536, 1}, %2758:<142x1536xbf16>{3072, 1}+1536)
          outputs: (%2181:<142x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2181:<142x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2181:<142x1536xbf16>{1536,1}}))
          duration: -1
187401 2024-12-10 17:48:30.609685 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n49,rank6)
        - aten::mm:
          inputs: (%2181:<142x1536xbf16>{1536, 1}, %2198:<1536x5120xbf16>{1, 1536})
          outputs: (%2735:<142x5120xbf16>{5120,1})
          duration: -1
187480 2024-12-10 17:48:30.613478 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n49,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2181:<142x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2735:<142x5120xbf16>{5120,1},None:NoneType})
          duration: -1
187495 2024-12-10 17:48:30.614225 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n46,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%358:<142x5120xbf16>{5120, 1}+8478720})
          outputs: (%1948:tuple{%2735:<142x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1798:<142x5120xbf16>{5120, 1}+8478720, %2735:<142x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1798:<142x5120xbf16>{5120,1}+8478720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2747:<573x5120xbf16>{5120, 1}+9205760})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2747:<573x5120xbf16>{5120,1}+9205760}))
          duration: -1
187671 2024-12-10 17:48:30.624406 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n47,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2747:<573x5120xbf16>{5120, 1}+9205760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2747:<573x5120xbf16>{5120,1}+9205760}))
          duration: -1
187698 2024-12-10 17:48:30.625174 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n61,rank6)
        - aten::mm:
          inputs: (%2747:<573x5120xbf16>{5120, 1}+9205760, %2762:<5120x3072xbf16>{1, 5120})
          outputs: (%2608:<573x3072xbf16>{3072,1})
          duration: -1
187777 2024-12-10 17:48:30.628956 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n61,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2747:<573x5120xbf16>{5120, 1}+9205760})
          outputs: (%2736:tuple{%2608:<573x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1817:<573x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1817:<573x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1817:<573x1536xbf16>{3072, 1})
          outputs: (%2762:<573x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1817:<573x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2762:<573x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2762:<573x1536xbf16>{1536, 1}, %2615:<573x1536xbf16>{3072, 1}+1536)
          outputs: (%2469:<573x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2469:<573x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2469:<573x1536xbf16>{1536,1}}))
          duration: -1
187884 2024-12-10 17:48:30.638782 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n50,rank6)
        - aten::mm:
          inputs: (%2469:<573x1536xbf16>{1536, 1}, %2365:<1536x5120xbf16>{1, 1536})
          outputs: (%2607:<573x5120xbf16>{5120,1})
          duration: -1
187963 2024-12-10 17:48:30.642549 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n50,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2469:<573x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2607:<573x5120xbf16>{5120,1},None:NoneType})
          duration: -1
187977 2024-12-10 17:48:30.643303 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n47,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2747:<573x5120xbf16>{5120, 1}+9205760})
          outputs: (%1948:tuple{%2607:<573x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%243:<573x5120xbf16>{5120, 1}+9205760, %2607:<573x5120xbf16>{5120, 1}, False:bool)
          outputs: (%243:<573x5120xbf16>{5120,1}+9205760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2615:<120x5120xbf16>{5120, 1}+12139520})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2615:<120x5120xbf16>{5120,1}+12139520}))
          duration: -1
188154 2024-12-10 17:48:30.653524 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n48,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2615:<120x5120xbf16>{5120, 1}+12139520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2615:<120x5120xbf16>{5120,1}+12139520}))
          duration: -1
188179 2024-12-10 17:48:30.654253 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n62,rank6)
        - aten::mm:
          inputs: (%2615:<120x5120xbf16>{5120, 1}+12139520, %2198:<5120x3072xbf16>{1, 5120})
          outputs: (%776:<120x3072xbf16>{3072,1})
          duration: -1
188262 2024-12-10 17:48:30.658022 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n62,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2615:<120x5120xbf16>{5120, 1}+12139520})
          outputs: (%2746:tuple{%776:<120x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<120x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<120x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<120x1536xbf16>{3072, 1})
          outputs: (%2507:<120x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<120x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2507:<120x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2507:<120x1536xbf16>{1536, 1}, %426:<120x1536xbf16>{3072, 1}+1536)
          outputs: (%2738:<120x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2738:<120x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2738:<120x1536xbf16>{1536,1}}))
          duration: -1
188370 2024-12-10 17:48:30.667853 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n51,rank6)
        - aten::mm:
          inputs: (%2738:<120x1536xbf16>{1536, 1}, %2310:<1536x5120xbf16>{1, 1536})
          outputs: (%2365:<120x5120xbf16>{5120,1})
          duration: -1
188447 2024-12-10 17:48:30.671609 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n51,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2738:<120x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2365:<120x5120xbf16>{5120,1},None:NoneType})
          duration: -1
188463 2024-12-10 17:48:30.672357 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n48,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2615:<120x5120xbf16>{5120, 1}+12139520})
          outputs: (%1948:tuple{%2365:<120x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1798:<120x5120xbf16>{5120, 1}+12139520, %2365:<120x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1798:<120x5120xbf16>{5120,1}+12139520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%358:<120x5120xbf16>{5120, 1}+12753920})
          outputs: (torch.2_3_0.MLP(%793:tuple{%358:<120x5120xbf16>{5120,1}+12753920}))
          duration: -1
188637 2024-12-10 17:48:30.682512 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n49,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%358:<120x5120xbf16>{5120, 1}+12753920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%358:<120x5120xbf16>{5120,1}+12753920}))
          duration: -1
188661 2024-12-10 17:48:30.683239 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n63,rank6)
        - aten::mm:
          inputs: (%358:<120x5120xbf16>{5120, 1}+12753920, %2253:<5120x3072xbf16>{1, 5120})
          outputs: (%2469:<120x3072xbf16>{3072,1})
          duration: -1
188742 2024-12-10 17:48:30.686988 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n63,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%358:<120x5120xbf16>{5120, 1}+12753920})
          outputs: (%2516:tuple{%2469:<120x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1116:<120x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1116:<120x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1116:<120x1536xbf16>{3072, 1})
          outputs: (%2253:<120x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1116:<120x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2253:<120x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2253:<120x1536xbf16>{1536, 1}, %426:<120x1536xbf16>{3072, 1}+1536)
          outputs: (%2432:<120x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2432:<120x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2432:<120x1536xbf16>{1536,1}}))
          duration: -1
188850 2024-12-10 17:48:30.696797 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n52,rank6)
        - aten::mm:
          inputs: (%2432:<120x1536xbf16>{1536, 1}, %2615:<1536x5120xbf16>{1, 1536})
          outputs: (%2748:<120x5120xbf16>{5120,1})
          duration: -1
188923 2024-12-10 17:48:30.700512 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n52,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2432:<120x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2748:<120x5120xbf16>{5120,1},None:NoneType})
          duration: -1
188938 2024-12-10 17:48:30.701276 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n49,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%358:<120x5120xbf16>{5120, 1}+12753920})
          outputs: (%1948:tuple{%2748:<120x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2747:<120x5120xbf16>{5120, 1}+12753920, %2748:<120x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2747:<120x5120xbf16>{5120,1}+12753920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2365:<75x5120xbf16>{5120, 1}+13368320})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2365:<75x5120xbf16>{5120,1}+13368320}))
          duration: -1
189112 2024-12-10 17:48:30.711440 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n50,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2365:<75x5120xbf16>{5120, 1}+13368320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2365:<75x5120xbf16>{5120,1}+13368320}))
          duration: -1
189138 2024-12-10 17:48:30.712176 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n64,rank6)
        - aten::mm:
          inputs: (%2365:<75x5120xbf16>{5120, 1}+13368320, %2179:<5120x3072xbf16>{1, 5120})
          outputs: (%2432:<75x3072xbf16>{3072,1})
          duration: -1
189216 2024-12-10 17:48:30.715962 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n64,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2365:<75x5120xbf16>{5120, 1}+13368320})
          outputs: (%2736:tuple{%2432:<75x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<75x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<75x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<75x1536xbf16>{3072, 1})
          outputs: (%2507:<75x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<75x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2507:<75x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2507:<75x1536xbf16>{1536, 1}, %2740:<75x1536xbf16>{3072, 1}+1536)
          outputs: (%776:<75x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%776:<75x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%776:<75x1536xbf16>{1536,1}}))
          duration: -1
189336 2024-12-10 17:48:30.725774 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n53,rank6)
        - aten::mm:
          inputs: (%776:<75x1536xbf16>{1536, 1}, %2350:<1536x5120xbf16>{1, 1536})
          outputs: (%2744:<75x5120xbf16>{5120,1})
          duration: -1
189409 2024-12-10 17:48:30.729501 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n53,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%776:<75x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2744:<75x5120xbf16>{5120,1},None:NoneType})
          duration: -1
189425 2024-12-10 17:48:30.730264 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n50,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2365:<75x5120xbf16>{5120, 1}+13368320})
          outputs: (%1948:tuple{%2744:<75x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1798:<75x5120xbf16>{5120, 1}+13368320, %2744:<75x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1798:<75x5120xbf16>{5120,1}+13368320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%1116:<311x5120xbf16>{5120, 1}+13752320})
          outputs: (torch.2_3_0.MLP(%793:tuple{%1116:<311x5120xbf16>{5120,1}+13752320}))
          duration: -1
189600 2024-12-10 17:48:30.740387 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n51,rank6)
189622 2024-12-10 17:48:30.741128 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n65,rank6)
        - aten::mm:
          inputs: (%1116:<311x5120xbf16>{5120, 1}+13752320, %2253:<5120x3072xbf16>{1, 5120})
          outputs: (%2310:<311x3072xbf16>{3072,1})
          duration: -1
189702 2024-12-10 17:48:30.744882 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n65,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%1116:<311x5120xbf16>{5120, 1}+13752320})
          outputs: (%2746:tuple{%2310:<311x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2747:<311x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2747:<311x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2747:<311x1536xbf16>{3072, 1})
          outputs: (%2365:<311x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2747:<311x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2365:<311x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2365:<311x1536xbf16>{1536, 1}, %2253:<311x1536xbf16>{3072, 1}+1536)
          outputs: (%2608:<311x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2608:<311x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2608:<311x1536xbf16>{1536,1}}))
          duration: -1
189814 2024-12-10 17:48:30.754707 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n54,rank6)
        - aten::mm:
          inputs: (%2608:<311x1536xbf16>{1536, 1}, %2432:<1536x5120xbf16>{1, 1536})
          outputs: (%2469:<311x5120xbf16>{5120,1})
          duration: -1
189887 2024-12-10 17:48:30.758465 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n54,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2608:<311x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2469:<311x5120xbf16>{5120,1},None:NoneType})
          duration: -1
189903 2024-12-10 17:48:30.759221 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n51,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%1116:<311x5120xbf16>{5120, 1}+13752320})
          outputs: (%1948:tuple{%2469:<311x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2747:<311x5120xbf16>{5120, 1}+13752320, %2469:<311x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2747:<311x5120xbf16>{5120,1}+13752320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%1954:<165x5120xbf16>{5120, 1}+15344640})
          outputs: (torch.2_3_0.MLP(%793:tuple{%1954:<165x5120xbf16>{5120,1}+15344640}))
          duration: -1
190080 2024-12-10 17:48:30.769385 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n52,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%1954:<165x5120xbf16>{5120, 1}+15344640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%1954:<165x5120xbf16>{5120,1}+15344640}))
          duration: -1
190102 2024-12-10 17:48:30.770108 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n66,rank6)
        - aten::mm:
          inputs: (%1954:<165x5120xbf16>{5120, 1}+15344640, %2350:<5120x3072xbf16>{1, 5120})
          outputs: (%2179:<165x3072xbf16>{3072,1})
          duration: -1
190182 2024-12-10 17:48:30.773852 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n66,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%1954:<165x5120xbf16>{5120, 1}+15344640})
          outputs: (%2516:tuple{%2179:<165x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1970:<165x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1970:<165x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1970:<165x1536xbf16>{3072, 1})
          outputs: (%2425:<165x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1970:<165x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2425:<165x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2350:<165x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2350:<165x1536xbf16>{1536,1}}))
          duration: -1
190294 2024-12-10 17:48:30.783654 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n55,rank6)
        - aten::mm:
          inputs: (%2350:<165x1536xbf16>{1536, 1}, %2748:<1536x5120xbf16>{1, 1536})
          outputs: (%2738:<165x5120xbf16>{5120,1})
          duration: -1
190367 2024-12-10 17:48:30.787412 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n55,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2350:<165x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2738:<165x5120xbf16>{5120,1},None:NoneType})
          duration: -1
190383 2024-12-10 17:48:30.788165 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n52,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%1954:<165x5120xbf16>{5120, 1}+15344640})
          outputs: (%1948:tuple{%2738:<165x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1798:<165x5120xbf16>{5120, 1}+15344640, %2738:<165x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1798:<165x5120xbf16>{5120,1}+15344640)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2747:<233x5120xbf16>{5120, 1}+16189440})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2747:<233x5120xbf16>{5120,1}+16189440}))
          duration: -1
190561 2024-12-10 17:48:30.798293 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n53,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2747:<233x5120xbf16>{5120, 1}+16189440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2747:<233x5120xbf16>{5120,1}+16189440}))
          duration: -1
190579 2024-12-10 17:48:30.799014 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n67,rank6)
        - aten::mm:
          inputs: (%2747:<233x5120xbf16>{5120, 1}+16189440, %2253:<5120x3072xbf16>{1, 5120})
          outputs: (%2310:<233x3072xbf16>{3072,1})
          duration: -1
190661 2024-12-10 17:48:30.802907 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n67,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2747:<233x5120xbf16>{5120, 1}+16189440})
          outputs: (%2736:tuple{%2310:<233x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%358:<233x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%358:<233x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%358:<233x1536xbf16>{3072, 1})
          outputs: (%2253:<233x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%358:<233x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2253:<233x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2253:<233x1536xbf16>{1536, 1}, %1798:<233x1536xbf16>{3072, 1}+1536)
          outputs: (%2469:<233x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2469:<233x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2469:<233x1536xbf16>{1536,1}}))
          duration: -1
190772 2024-12-10 17:48:30.812683 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n56,rank6)
        - aten::mm:
          inputs: (%2469:<233x1536xbf16>{1536, 1}, %2425:<1536x5120xbf16>{1, 1536})
          outputs: (%2365:<233x5120xbf16>{5120,1})
          duration: -1
190846 2024-12-10 17:48:30.816430 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n56,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2469:<233x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2365:<233x5120xbf16>{5120,1},None:NoneType})
          duration: -1
190859 2024-12-10 17:48:30.817206 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n53,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2747:<233x5120xbf16>{5120, 1}+16189440})
          outputs: (%1948:tuple{%2365:<233x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%243:<233x5120xbf16>{5120, 1}+16189440, %2365:<233x5120xbf16>{5120, 1}, False:bool)
          outputs: (%243:<233x5120xbf16>{5120,1}+16189440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%1116:<100x5120xbf16>{5120, 1}+17382400})
          outputs: (torch.2_3_0.MLP(%793:tuple{%1116:<100x5120xbf16>{5120,1}+17382400}))
          duration: -1
191042 2024-12-10 17:48:30.827411 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n54,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%1116:<100x5120xbf16>{5120, 1}+17382400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%1116:<100x5120xbf16>{5120,1}+17382400}))
          duration: -1
191058 2024-12-10 17:48:30.828134 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n68,rank6)
        - aten::mm:
          inputs: (%1116:<100x5120xbf16>{5120, 1}+17382400, %2179:<5120x3072xbf16>{1, 5120})
          outputs: (%2780:<100x3072xbf16>{3072,1})
          duration: -1
191140 2024-12-10 17:48:30.831894 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n68,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%1116:<100x5120xbf16>{5120, 1}+17382400})
          outputs: (%2746:tuple{%2780:<100x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<100x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<100x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<100x1536xbf16>{3072, 1})
          outputs: (%2469:<100x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<100x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2469:<100x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2469:<100x1536xbf16>{1536, 1}, %426:<100x1536xbf16>{3072, 1}+1536)
          outputs: (%2179:<100x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2179:<100x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2179:<100x1536xbf16>{1536,1}}))
          duration: -1
191251 2024-12-10 17:48:30.841776 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n57,rank6)
        - aten::mm:
          inputs: (%2179:<100x1536xbf16>{1536, 1}, %2303:<1536x5120xbf16>{1, 1536})
          outputs: (%2738:<100x5120xbf16>{5120,1})
          duration: -1
191326 2024-12-10 17:48:30.845521 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n57,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2179:<100x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2738:<100x5120xbf16>{5120,1},None:NoneType})
          duration: -1
191338 2024-12-10 17:48:30.846272 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n54,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%1116:<100x5120xbf16>{5120, 1}+17382400})
          outputs: (%1948:tuple{%2738:<100x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1817:<100x5120xbf16>{5120, 1}+17382400, %2738:<100x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1817:<100x5120xbf16>{5120,1}+17382400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2740:<396x5120xbf16>{5120, 1}+17894400})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2740:<396x5120xbf16>{5120,1}+17894400}))
          duration: -1
191526 2024-12-10 17:48:30.856421 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n55,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2740:<396x5120xbf16>{5120, 1}+17894400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2740:<396x5120xbf16>{5120,1}+17894400}))
          duration: -1
191541 2024-12-10 17:48:30.857162 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n69,rank6)
        - aten::mm:
          inputs: (%2740:<396x5120xbf16>{5120, 1}+17894400, %2469:<5120x3072xbf16>{1, 5120})
          outputs: (%2350:<396x3072xbf16>{3072,1})
          duration: -1
191623 2024-12-10 17:48:30.860886 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n69,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2740:<396x5120xbf16>{5120, 1}+17894400})
          outputs: (%2516:tuple{%2350:<396x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1116:<396x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1116:<396x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1116:<396x1536xbf16>{3072, 1})
          outputs: (%2253:<396x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1116:<396x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2253:<396x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2253:<396x1536xbf16>{1536, 1}, %2747:<396x1536xbf16>{3072, 1}+1536)
          outputs: (%736:<396x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%736:<396x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%736:<396x1536xbf16>{1536,1}}))
          duration: -1
191733 2024-12-10 17:48:30.870736 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n58,rank6)
        - aten::mm:
          inputs: (%736:<396x1536xbf16>{1536, 1}, %2365:<1536x5120xbf16>{1, 1536})
          outputs: (%2786:<396x5120xbf16>{5120,1})
          duration: -1
191808 2024-12-10 17:48:30.874479 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n58,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%736:<396x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2786:<396x5120xbf16>{5120,1},None:NoneType})
          duration: -1
191822 2024-12-10 17:48:30.875237 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n55,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2740:<396x5120xbf16>{5120, 1}+17894400})
          outputs: (%1948:tuple{%2786:<396x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1562:<396x5120xbf16>{5120, 1}+17894400, %2786:<396x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1562:<396x5120xbf16>{5120,1}+17894400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2748:<158x5120xbf16>{5120, 1}+19921920})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2748:<158x5120xbf16>{5120,1}+19921920}))
          duration: -1
192000 2024-12-10 17:48:30.885440 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n56,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2748:<158x5120xbf16>{5120, 1}+19921920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2748:<158x5120xbf16>{5120,1}+19921920}))
          duration: -1
192014 2024-12-10 17:48:30.886172 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n70,rank6)
        - aten::mm:
          inputs: (%2748:<158x5120xbf16>{5120, 1}+19921920, %2350:<5120x3072xbf16>{1, 5120})
          outputs: (%2738:<158x3072xbf16>{3072,1})
          duration: -1
192098 2024-12-10 17:48:30.889954 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n70,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2748:<158x5120xbf16>{5120, 1}+19921920})
          outputs: (%2736:tuple{%2738:<158x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<158x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<158x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<158x1536xbf16>{3072, 1})
          outputs: (%2425:<158x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<158x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2425:<158x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2425:<158x1536xbf16>{1536, 1}, %358:<158x1536xbf16>{3072, 1}+1536)
          outputs: (%2310:<158x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2310:<158x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2310:<158x1536xbf16>{1536,1}}))
          duration: -1
192212 2024-12-10 17:48:30.899737 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n59,rank6)
        - aten::mm:
          inputs: (%2310:<158x1536xbf16>{1536, 1}, %2365:<1536x5120xbf16>{1, 1536})
          outputs: (%2607:<158x5120xbf16>{5120,1})
          duration: -1
192288 2024-12-10 17:48:30.903477 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n59,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2310:<158x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2607:<158x5120xbf16>{5120,1},None:NoneType})
          duration: -1
192302 2024-12-10 17:48:30.904232 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n56,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2748:<158x5120xbf16>{5120, 1}+19921920})
          outputs: (%1948:tuple{%2607:<158x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1817:<158x5120xbf16>{5120, 1}+19921920, %2607:<158x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1817:<158x5120xbf16>{5120,1}+19921920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%426:<186x5120xbf16>{5120, 1}+20730880})
          outputs: (torch.2_3_0.MLP(%793:tuple{%426:<186x5120xbf16>{5120,1}+20730880}))
          duration: -1
192479 2024-12-10 17:48:30.914377 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n57,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%426:<186x5120xbf16>{5120, 1}+20730880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%426:<186x5120xbf16>{5120,1}+20730880}))
          duration: -1
192494 2024-12-10 17:48:30.915101 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n71,rank6)
        - aten::mm:
          inputs: (%426:<186x5120xbf16>{5120, 1}+20730880, %2365:<5120x3072xbf16>{1, 5120})
          outputs: (%2310:<186x3072xbf16>{3072,1})
          duration: -1
192576 2024-12-10 17:48:30.918847 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n71,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%426:<186x5120xbf16>{5120, 1}+20730880})
          outputs: (%2746:tuple{%2310:<186x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1116:<186x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1116:<186x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1116:<186x1536xbf16>{3072, 1})
          outputs: (%2365:<186x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1116:<186x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2365:<186x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2365:<186x1536xbf16>{1536, 1}, %2303:<186x1536xbf16>{3072, 1}+1536)
          outputs: (%2469:<186x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2469:<186x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2469:<186x1536xbf16>{1536,1}}))
          duration: -1
192692 2024-12-10 17:48:30.928691 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n60,rank6)
        - aten::mm:
          inputs: (%2469:<186x1536xbf16>{1536, 1}, %2748:<1536x5120xbf16>{1, 1536})
          outputs: (%2425:<186x5120xbf16>{5120,1})
          duration: -1
192764 2024-12-10 17:48:30.932443 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n60,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2469:<186x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2425:<186x5120xbf16>{5120,1},None:NoneType})
          duration: -1
192781 2024-12-10 17:48:30.933209 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n57,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%426:<186x5120xbf16>{5120, 1}+20730880})
          outputs: (%1948:tuple{%2425:<186x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1116:<186x5120xbf16>{5120, 1}+20730880, %2425:<186x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1116:<186x5120xbf16>{5120,1}+20730880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%358:<652x5120xbf16>{5120, 1}+21683200})
          outputs: (torch.2_3_0.MLP(%793:tuple{%358:<652x5120xbf16>{5120,1}+21683200}))
          duration: -1
192958 2024-12-10 17:48:30.943382 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n58,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%358:<652x5120xbf16>{5120, 1}+21683200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%358:<652x5120xbf16>{5120,1}+21683200}))
          duration: -1
192972 2024-12-10 17:48:30.944105 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n72,rank6)
        - aten::mm:
          inputs: (%358:<652x5120xbf16>{5120, 1}+21683200, %2432:<5120x3072xbf16>{1, 5120})
          outputs: (%776:<652x3072xbf16>{3072,1})
          duration: -1
193054 2024-12-10 17:48:30.947886 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n72,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%358:<652x5120xbf16>{5120, 1}+21683200})
          outputs: (%2516:tuple{%776:<652x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<652x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<652x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<652x1536xbf16>{3072, 1})
          outputs: (%2303:<652x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<652x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2303:<652x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2303:<652x1536xbf16>{1536, 1}, %2747:<652x1536xbf16>{3072, 1}+1536)
          outputs: (%1645:<652x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%1645:<652x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%1645:<652x1536xbf16>{1536,1}}))
          duration: -1
193171 2024-12-10 17:48:30.957698 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n61,rank6)
        - aten::mm:
          inputs: (%1645:<652x1536xbf16>{1536, 1}, %2253:<1536x5120xbf16>{1, 1536})
          outputs: (%2607:<652x5120xbf16>{5120,1})
          duration: -1
193246 2024-12-10 17:48:30.961431 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n61,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%1645:<652x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2607:<652x5120xbf16>{5120,1},None:NoneType})
          duration: -1
193260 2024-12-10 17:48:30.962179 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n58,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%358:<652x5120xbf16>{5120, 1}+21683200})
          outputs: (%1948:tuple{%2607:<652x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%776:<652x5120xbf16>{5120, 1}+21683200, %2607:<652x5120xbf16>{5120, 1}, False:bool)
          outputs: (%776:<652x5120xbf16>{5120,1}+21683200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%1116:<131x5120xbf16>{5120, 1}+25021440})
          outputs: (torch.2_3_0.MLP(%793:tuple{%1116:<131x5120xbf16>{5120,1}+25021440}))
          duration: -1
193438 2024-12-10 17:48:30.972407 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n59,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%1116:<131x5120xbf16>{5120, 1}+25021440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%1116:<131x5120xbf16>{5120,1}+25021440}))
          duration: -1
193451 2024-12-10 17:48:30.973146 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n73,rank6)
        - aten::mm:
          inputs: (%1116:<131x5120xbf16>{5120, 1}+25021440, %2318:<5120x3072xbf16>{1, 5120})
          outputs: (%2281:<131x3072xbf16>{3072,1})
          duration: -1
193533 2024-12-10 17:48:30.976880 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n73,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%1116:<131x5120xbf16>{5120, 1}+25021440})
          outputs: (%2736:tuple{%2281:<131x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2740:<131x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2740:<131x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2740:<131x1536xbf16>{3072, 1})
          outputs: (%2179:<131x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2740:<131x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2179:<131x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2179:<131x1536xbf16>{1536, 1}, %1798:<131x1536xbf16>{3072, 1}+1536)
          outputs: (%2738:<131x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2738:<131x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2738:<131x1536xbf16>{1536,1}}))
          duration: -1
193650 2024-12-10 17:48:30.986695 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n62,rank6)
        - aten::mm:
          inputs: (%2738:<131x1536xbf16>{1536, 1}, %2383:<1536x5120xbf16>{1, 1536})
          outputs: (%724:<131x5120xbf16>{5120,1})
          duration: -1
193726 2024-12-10 17:48:30.990443 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n62,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2738:<131x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%724:<131x5120xbf16>{5120,1},None:NoneType})
          duration: -1
193741 2024-12-10 17:48:30.991201 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n59,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%1116:<131x5120xbf16>{5120, 1}+25021440})
          outputs: (%1948:tuple{%724:<131x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2740:<131x5120xbf16>{5120, 1}+25021440, %724:<131x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2740:<131x5120xbf16>{5120,1}+25021440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%426:<439x5120xbf16>{5120, 1}+25692160})
          outputs: (torch.2_3_0.MLP(%793:tuple{%426:<439x5120xbf16>{5120,1}+25692160}))
          duration: -1
193917 2024-12-10 17:48:31.001459 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n60,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%426:<439x5120xbf16>{5120, 1}+25692160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%426:<439x5120xbf16>{5120,1}+25692160}))
          duration: -1
193934 2024-12-10 17:48:31.002187 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n74,rank6)
        - aten::mm:
          inputs: (%426:<439x5120xbf16>{5120, 1}+25692160, %2281:<5120x3072xbf16>{1, 5120})
          outputs: (%2469:<439x3072xbf16>{3072,1})
          duration: -1
194013 2024-12-10 17:48:31.005952 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n74,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%426:<439x5120xbf16>{5120, 1}+25692160})
          outputs: (%2746:tuple{%2469:<439x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1116:<439x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1116:<439x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1116:<439x1536xbf16>{3072, 1})
          outputs: (%2310:<439x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1116:<439x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2310:<439x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2310:<439x1536xbf16>{1536, 1}, %2365:<439x1536xbf16>{3072, 1}+1536)
          outputs: (%2318:<439x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2318:<439x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2318:<439x1536xbf16>{1536,1}}))
          duration: -1
194133 2024-12-10 17:48:31.015745 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n63,rank6)
        - aten::mm:
          inputs: (%2318:<439x1536xbf16>{1536, 1}, %2350:<1536x5120xbf16>{1, 1536})
          outputs: (%2748:<439x5120xbf16>{5120,1})
          duration: -1
194207 2024-12-10 17:48:31.019498 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n63,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2318:<439x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2748:<439x5120xbf16>{5120,1},None:NoneType})
          duration: -1
194219 2024-12-10 17:48:31.020248 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n60,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%426:<439x5120xbf16>{5120, 1}+25692160})
          outputs: (%1948:tuple{%2748:<439x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1116:<439x5120xbf16>{5120, 1}+25692160, %2748:<439x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1116:<439x5120xbf16>{5120,1}+25692160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2740:<664x5120xbf16>{5120, 1}+27939840})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2740:<664x5120xbf16>{5120,1}+27939840}))
          duration: -1
194399 2024-12-10 17:48:31.030481 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n61,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%836:tuple{%2740:<664x5120xbf16>{5120, 1}+27939840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%836:tuple{%2740:<664x5120xbf16>{5120,1}+27939840}))
          duration: -1
194415 2024-12-10 17:48:31.031210 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n75,rank6)
        - aten::mm:
          inputs: (%2740:<664x5120xbf16>{5120, 1}+27939840, %2253:<5120x3072xbf16>{1, 5120})
          outputs: (%1954:<664x3072xbf16>{3072,1})
          duration: -1
194494 2024-12-10 17:48:31.034990 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n75,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2740:<664x5120xbf16>{5120, 1}+27939840})
          outputs: (%2516:tuple{%1954:<664x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1798:<664x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1798:<664x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1798:<664x1536xbf16>{3072, 1})
          outputs: (%2383:<664x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1798:<664x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2383:<664x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2383:<664x1536xbf16>{1536, 1}, %2747:<664x1536xbf16>{3072, 1}+1536)
          outputs: (%2780:<664x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2780:<664x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2780:<664x1536xbf16>{1536,1}}))
          duration: -1
194614 2024-12-10 17:48:31.044813 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n64,rank6)
        - aten::mm:
          inputs: (%2780:<664x1536xbf16>{1536, 1}, %2507:<1536x5120xbf16>{1, 1536})
          outputs: (%2253:<664x5120xbf16>{5120,1})
          duration: -1
194689 2024-12-10 17:48:31.048567 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n64,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2780:<664x1536xbf16>{1536, 1}})
          outputs: (%1948:tuple{%2253:<664x5120xbf16>{5120,1},None:NoneType})
          duration: -1
194702 2024-12-10 17:48:31.049346 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n61,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2740:<664x5120xbf16>{5120, 1}+27939840})
          outputs: (%1948:tuple{%2253:<664x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1954:<664x5120xbf16>{5120, 1}+27939840, %2253:<664x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1954:<664x5120xbf16>{5120,1}+27939840)
          duration: -1
194775 2024-12-10 17:48:31.054224 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n2,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%1808:tuple{%1780:<6121x5120xbf16>{5120, 1}, %2742:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1345:tuple{%2625:<6121x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%1958:<6121x5120xbf16>{5120, 1}, 0:int, %2743:<6121x5120xCUSTOM_DATA_TYPE>{1, 0}, %2625:<6121x5120xbf16>{5120, 1})
          outputs: (%2151:<6121x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%2151:<6121x5120xbf16>{5120, 1}, %1954:<6121x1xbf16>{1, 1})
          outputs: (%2169:<6121x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%2803:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%243:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%243:<8192x5120xbf16>{5120, 1}, 0:int, %2739:<6121x5120xCUSTOM_DATA_TYPE>{1, 0}, %2169:<6121x5120xbf16>{5120, 1})
          outputs: (%1954:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%426:<1024x5120xbf16>{5120, 1}, %1954:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%426:<1024x5120xbf16>{5120,1},%1954:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%426:<1024x5120xbf16>{5120, 1}, %1954:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%426:<1024x5120xbf16>{5120,1},%1954:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%426:<1024x5120xbf16>{5120, 1}, %1954:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%2804:tuple{%426:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%793:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%793:tuple{%2030:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
195033 2024-12-10 17:48:31.101208 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n62,rank6)
195057 2024-12-10 17:48:31.101958 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n76,rank6)
        - aten::mm:
          inputs: (%2169:<1024x5120xbf16>{5120, 1}, %2160:<5120x6144xbf16>{1, 5120})
          outputs: (%2151:<1024x6144xbf16>{6144,1})
          duration: -1
195212 2024-12-10 17:48:31.109863 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n76,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%836:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%1948:tuple{%2740:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2656:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2656:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2656:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%1817:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2656:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%1817:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%1817:<1024x1x3072xbf16>{3072, 3072, 1}, %1798:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%2151:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%812:tuple{%2151:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%812:tuple{%2151:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
195316 2024-12-10 17:48:31.119775 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n65,rank6)
        - aten::mm:
          inputs: (%1954:<1024x3072xbf16>{3072, 1}, %2318:<3072x5120xbf16>{1, 3072})
          outputs: (%2740:<1024x5120xbf16>{5120,1})
          duration: -1
195467 2024-12-10 17:48:31.127591 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n65,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%812:tuple{%2151:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%2805:tuple{%1798:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
195484 2024-12-10 17:48:31.128362 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n62,rank6)
        - ----------->api::MLP return:
          inputs: (%793:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2805:tuple{%1798:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%1116:<1024x1x5120xbf16>{5120, 5120, 1}, %2151:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2160:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
195531 2024-12-10 17:48:31.131486 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n2,rank6)
        - ----------->api::MoELayer return:
          inputs: (%1972:tuple{%2030:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2805:tuple{%2160:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2815:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2815:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2816:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2816:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55bb15ea30_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55bb15ea30_:_InferenceMode)
          duration: -1
197678 2024-12-10 17:48:32.004671 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n2,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%2736:tuple{%2818:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1972:tuple{%2818:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1972:tuple{%2818:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
197734 2024-12-10 17:48:32.011516 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n8,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2818:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%2818:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%2714:tuple{%2818:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%2714:tuple{%2818:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%2714:tuple{%2818:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2714:tuple{%2818:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%2821:tuple{%2818:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %1612:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%2821:tuple{%2818:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%1612:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %2714:tuple{%2818:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1586:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1586:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
197930 2024-12-10 17:48:32.055371 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n8,rank6)
197932 2024-12-10 17:48:32.055885 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n2,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%1586:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2822:tuple{%1586:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%2822:tuple{%1586:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
197983 2024-12-10 17:48:32.062183 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n77,rank6)
        - aten::mm:
          inputs: (%2341:<1024x5120xbf16>{5120, 1}, %1932:<5120x102400xbf16>{1, 5120})
          outputs: (%1562:<1024x102400xbf16>{102400,1})
          duration: -1
198122 2024-12-10 17:48:32.074458 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n77,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2822:tuple{%1586:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%2829:tuple{%2675:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%1932:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%2010:tuple{%1562:<1024x1xf32>{1,1},%2341:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1562:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1562:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2830:list{%1562:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2831:tuple{%2832:list{%1562:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%1932:<1024x1x102400xf32>{102400, 102400, 1}, %550:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%1932:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%39:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%1600:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%39:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%2234:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%1600:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %2234:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%1668:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1334:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%1385:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%1385:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %2833:list{%1668:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1600:<i32>, False:bool)
          outputs: (%1385:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2096:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%1710:<1024x102400xf32>{102400, 1}, %2832:list{%2096:<1024xCUSTOM_DATA_TYPE>{1}, %2030:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1600:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%1388:<1024x1xf32>{1, 1}, %2832:list{%1668:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1600:<i32>, False:bool)
          outputs: (%1388:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%1932:<1024x1x102400xf32>{102400, 102400, 1}, out=%1932:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%1932:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%1932:<1024x1x102400xf32>{102400, 102400, 1}, %2833:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%1238:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1388:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1388:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2833:list{%1388:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2829:tuple{%2827:list{%1388:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1238:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1238:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1166:list{%1238:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2835:tuple{%2836:list{%1238:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%1238:<1024x1xf32>{1, 1})
          outputs: (%237:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%237:<1024x1xf32>{1, 1}, %1388:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%2096:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%1932:<1024x1x102400xf32>{102400, 102400, 1}, %1162:<1024x1x1xf32>{1, 1, 1})
          outputs: (%1932:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%2837:tuple{%1932:<1024x1x102400xf32>{102400, 102400, 1}, %1668:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %2030:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%2837:tuple{%1932:<1024x1x102400xf32>{102400,102400,1},%1668:<1024x1xCUSTOM_DATA_TYPE>{1,1},%2030:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2684:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device='cuda_6')_:dict)
          outputs: (%1388:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1388:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1388:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1388:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device='cuda_6')_:dict)
          outputs: (%1388:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%771:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1696:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %1469:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[3779,___66,_2992,_____,__340,_1099,__245]],_device='cuda_6')_:dict)
          outputs: (%1388:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%1668:<1024xf32>{1}, %1562:<1024xf32>{1})
          outputs: (%1385:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%1385:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1696:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%1562:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1469:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1696:<i32>, %1469:<i32>)
          outputs: (%1238:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%1238:<i32>)
          outputs: (%2839:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%2840:list{%2818:<1xf32>{1}}, 0:int)
          outputs: (%1469:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1469:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1469:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2841:list{%1469:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%2290:tuple{%2842:list{%1469:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%1469:<1xf32>{1}, 8:int)
          outputs: (%1162:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<i32>, 1:int)
          outputs: (%2720:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%2720:<i32>, 1:int)
          outputs: (%2720:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%2843:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2839:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%2839:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%2839:<1xf32>{1}))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %2839:<1xf32>{1})
          outputs: (%1388:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%1388:<1xf32>{1}, 1:int)
          outputs: (%1238:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%1904:<i32>, 0:int, alpha=1:int)
          outputs: (%1904:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2839:<i32>, %237:<i32>, alpha=1:int)
          outputs: (%2839:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%39:<i32>, %2839:<i32>, False:bool)
          outputs: (%39:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%1469:<i32>, 1:int, alpha=1:int)
          outputs: (%1469:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%534:<i32>, %1469:<i32>, False:bool)
          outputs: (%534:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%2844:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%1469:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%426:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %434:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%433:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%433:<1024xCUSTOM_DATA_TYPE>{1}, %434:list{%420:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%433:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::eq:
          inputs: (%426:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%420:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%426:<1024xCUSTOM_DATA_TYPE>{1}+1, %434:list{%420:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%426:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - c10d::broadcast_:
          inputs: (%2655:list{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2831:tuple{%2853:list{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2839:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2839:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2854:list{%2839:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2855:tuple{%2844:list{%2839:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2848:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2848:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2856:list{%2848:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2835:tuple{%2832:list{%2848:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2857:list{%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2831:tuple{%1155:list{%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2852:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2858:list{%2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%2855:tuple{%2853:list{%2852:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2852:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2852:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2852:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2859:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2859:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2852:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__588,__7756,_71415,_____,___786,_20378,____13]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[__588,__7756,_71415,_____,___786,_20378,____13]],_device)
          duration: -1
200141 2024-12-10 17:48:32.273287 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n3,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%2822:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%2822:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
200163 2024-12-10 17:48:32.274026 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n3,rank6)
        - ----------->api::embedding call:
          inputs: (%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%2851:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%2851:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
200257 2024-12-10 17:48:32.288713 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n3,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%2822:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%1615:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%2822:tuple{%2845:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%2822:tuple{%2845:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
200300 2024-12-10 17:48:32.290787 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n3,rank6)
        - ----------->api::dropout call:
          inputs: (%2845:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%2845:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%2845:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%2845:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
200340 2024-12-10 17:48:32.296969 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n3,rank6)
        - ----------->api::Dropout return:
          inputs: (%2822:tuple{%2845:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2845:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
200350 2024-12-10 17:48:32.297696 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n3,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__588,__7756,_71415,_____,___786,_20378,____13]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%2845:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%2822:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%2822:tuple{1024:int}))
          duration: -1
200380 2024-12-10 17:48:32.299340 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n3,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1615:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%1615:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%1562:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%2847:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%2864:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%2865:list{%2864:<1024x20xf32>{20, 1}, %2864:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%583:<1024x40xf32>{40,1})
          duration: -1
200538 2024-12-10 17:48:32.310240 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n3,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%2822:tuple{1024:int})
          outputs: (%1655:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
200567 2024-12-10 17:48:32.316544 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n3,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
200647 2024-12-10 17:48:32.325536 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n3,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2822:tuple{%2818:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2822:tuple{%2818:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
200663 2024-12-10 17:48:32.326265 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n9,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2818:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%2818:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%2714:tuple{%2818:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%2714:tuple{%2818:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%2714:tuple{%2818:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2714:tuple{%2818:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - aten::stack:
          inputs: (%429:list{%433:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%431:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%431:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%2838:tuple{%2818:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %2869:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%2838:tuple{%2818:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%2869:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %2714:tuple{%2818:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2475:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%2818:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%2475:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
200916 2024-12-10 17:48:32.369885 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n9,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%2822:tuple{%2475:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%2822:tuple{%2475:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
200935 2024-12-10 17:48:32.374109 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n3,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2870:tuple{%2475:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2870:tuple{%2475:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
200951 2024-12-10 17:48:32.374861 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n78,rank6)
        - aten::mm:
          inputs: (%1536:<1024x5120xbf16>{5120, 1}, %1866:<5120x1536xbf16>{1, 5120})
          outputs: (%1794:<1024x1536xbf16>{1536,1})
          duration: -1
201111 2024-12-10 17:48:32.382670 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n78,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2870:tuple{%2475:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2874:tuple{%846:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2870:tuple{%846:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2870:tuple{%846:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
201134 2024-12-10 17:48:32.383693 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n79,rank6)
        - aten::mm:
          inputs: (%1866:<1024x1536xbf16>{1536, 1}, %1794:<1536x24576xbf16>{1, 1536})
          outputs: (%2875:<1024x24576xbf16>{24576,1})
          duration: -1
201255 2024-12-10 17:48:32.389766 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n79,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2870:tuple{%846:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%2878:tuple{%1553:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%1860:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %2157:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%1860:<1024x1x128x192xbf16>{24576,24576,192,1},%2157:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%1860:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %2157:list{128:int, 64:int}, -1:int)
          outputs: (%2829:tuple{%1687:<1024x1x128x128xbf16>{24576,24576,192,1},%1866:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
201339 2024-12-10 17:48:32.399192 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n80,rank6)
        - aten::mm:
          inputs: (%2733:<1024x5120xbf16>{5120, 1}, %2303:<5120x576xbf16>{1, 5120})
          outputs: (%2253:<1024x576xbf16>{576,1})
          duration: -1
201496 2024-12-10 17:48:32.407012 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n80,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2870:tuple{%2475:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2829:tuple{%2213:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%2213:<1024x1x576xbf16>{576, 576, 1}, %2884:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%2213:<1024x1x576xbf16>{576,576,1},%2884:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%2213:<1024x1x576xbf16>{576, 576, 1}, %2884:list{512:int, 64:int}, -1:int)
          outputs: (%2878:tuple{%1798:<1024x1x512xbf16>{576,576,1},%358:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2870:tuple{%1798:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2870:tuple{%1798:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
201564 2024-12-10 17:48:32.415215 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n81,rank6)
        - aten::mm:
          inputs: (%2198:<1024x512xbf16>{576, 1}, %2885:<512x32768xbf16>{1, 512})
          outputs: (%2602:<1024x32768xbf16>{32768,1})
          duration: -1
201680 2024-12-10 17:48:32.421321 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n81,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2870:tuple{%1798:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%2829:tuple{%2887:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%846:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %2886:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%846:<1024x1x128x256xbf16>{32768,32768,256,1},%2886:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%846:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %2886:list{128:int, 128:int}, -1:int)
          outputs: (%2874:tuple{%2875:<1024x1x128x128xbf16>{32768,32768,256,1},%2602:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%2860:tuple{%2869:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%2860:tuple{%2869:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
201794 2024-12-10 17:48:32.432962 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n3,rank6)
201840 2024-12-10 17:48:32.436041 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n3,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%2860:tuple{%2869:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%2835:tuple{%2253:<1024x64xbf16>{64,1},%2890:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%2253:<1024x64xbf16>{64, 1}, %2623:list{%2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%2891:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%2890:<1024x64xbf16>{64, 1}, %2889:list{%2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%1979:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2895:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %2893:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%2898:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%2899:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%2900:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%2623:list{%2900:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %2234:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%2897:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2897:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %2892:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%2899:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%2898:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %2899:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%2260:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2894:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %2893:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%2899:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%2901:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%1794:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%2623:list{%1794:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %2234:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%2741:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%2741:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %2892:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%1794:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%2899:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %1794:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%2900:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%2066:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %2902:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%2066:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::stack:
          inputs: (%436:list{%426:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%435:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%435:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%358:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %2260:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%358:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%2740:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %1696:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%2740:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%358:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %2903:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%358:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%2260:<128x1024x192xbf16>{196608, 192, 1}, %846:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%358:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%2253:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%846:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%358:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%2260:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%358:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %2623:list{%2260:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %2253:<i32>, False:bool)
          outputs: (%358:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%846:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %358:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%2253:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%2253:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%2253:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%1860:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%2893:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%2253:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %2893:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%2893:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%2160:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%2160:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%2160:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%2160:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%2253:<128x1024x1024xbf16>{1048576, 1024, 1}, %2900:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%1979:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2870:tuple{%2253:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2870:tuple{%2253:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
203345 2024-12-10 17:48:32.553479 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n66,rank6)
        - aten::mm:
          inputs: (%2893:<1024x16384xbf16>{16384, 1}, %2905:<16384x5120xbf16>{1, 16384})
          outputs: (%1272:<1024x5120xbf16>{5120,1})
          duration: -1
203500 2024-12-10 17:48:32.561560 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n66,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2870:tuple{%2253:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%2805:tuple{%2902:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
203513 2024-12-10 17:48:32.562325 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n3,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%2822:tuple{%2475:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2805:tuple{%2902:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2362:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2362:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%2909:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%2909:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55bb021970_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55bb021970_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2911:tuple{%358:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2911:tuple{%358:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
206017 2024-12-10 17:48:33.102320 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n10,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%358:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%358:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%358:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%358:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%2912:tuple{%358:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%2912:tuple{%358:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%2912:tuple{%358:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2912:tuple{%358:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%2915:tuple{%358:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %2914:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%2915:tuple{%358:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%2914:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%358:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %2912:tuple{%358:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2904:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%358:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%2904:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
206229 2024-12-10 17:48:33.146535 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n10,rank6)
        - ----------->api::MoELayer call:
          inputs: (%2911:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%2911:tuple{%2904:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
206246 2024-12-10 17:48:33.147307 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n3,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%2916:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%2916:tuple{%2904:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
206264 2024-12-10 17:48:33.148060 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n3,rank6)
        - aten::mm:
          inputs: (%1600:<1024x5120xbf16>{5120, 1}, %2917:<5120x160xbf16>{1, 5120})
          outputs: (%2902:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%2905:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%2893:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%2921:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%2922:tuple{%2923:<1024x6xbf16>{6,1},%2893:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%2924:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%1600:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%2925:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%2926:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%2926:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %2160:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%2926:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%2926:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %2925:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%2741:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%1600:<1024x160xf32>{160, 1}, %2925:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%2740:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%2740:<160xf32>{1}, %2741:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%1860:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%1860:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%2263:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%2263:<i32>, 2_5431315104166666e-07:float)
          outputs: (%1864:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1864:<i32>, 0_01:float)
          outputs: (%1860:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2740:<i32>, %2928:<i32>, alpha=1:int)
          outputs: (%2740:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%2926:<i32>, %2740:<i32>, False:bool)
          outputs: (%2926:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%2929:tuple{%1864:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%2929:tuple{%1864:<i32>}))
          duration: -1
206813 2024-12-10 17:48:33.183063 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n3,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%2916:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2931:tuple{%2930:<1024x6xbf16>{6,1},%2893:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%2741:<8192x5120xbf16>{5120, 1}, %1864:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%2741:<8192x5120xbf16>{5120,1},%1864:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%2741:<8192x5120xbf16>{5120, 1}, %1864:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2933:tuple{%2741:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%2920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2893:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%2920:<8192x6xCUSTOM_DATA_TYPE>{6,1},%2893:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%2920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2893:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2922:tuple{%2920:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%2920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%2160:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%2920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%2936:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%2160:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2936:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2926:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%2920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2926:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1600:<5524xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%2937:<8192x6xbf16>{6, 1}, %2930:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%2937:<8192x6xbf16>{6,1},%2930:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%2937:<8192x6xbf16>{6, 1}, %2930:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%2938:tuple{%2937:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%2937:<8192x6xbf16>{6, 1}, %2926:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2928:<5524xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%2926:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2939:<5524x2xCUSTOM_DATA_TYPE>{1,5524})
          duration: -1
        - aten::gather:
          inputs: (%2741:<8192x5120xbf16>{5120, 1}, 0:int, %2940:<5524x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%2849:<5524x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%1600:<5524xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%1246:tuple{%1904:<5524xCUSTOM_DATA_TYPE>{1},%434:<5524xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%1904:<5524xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%2096:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%2849:<5524x5120xbf16>{5120, 1}, 0:int, %2066:<5524x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%2096:<5524x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%1246:tuple{%2096:<5524x5120xbf16>{5120, 1}, %2941:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%1246:tuple{%2096:<5524x5120xbf16>{5120,1},%2941:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
207589 2024-12-10 17:48:33.296821 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n3,rank6)
        - aten::cumsum:
          inputs: (%2941:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%2926:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%2428:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%1860:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%2428:list{%1860:<1xCUSTOM_DATA_TYPE>{1}, %2926:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%1864:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%1600:<69x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%1600:<69x5120xbf16>{5120,1}}))
          duration: -1
207759 2024-12-10 17:48:33.307705 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n63,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%1600:<69x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%1600:<69x5120xbf16>{5120,1}}))
          duration: -1
207781 2024-12-10 17:48:33.308443 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n82,rank6)
        - aten::mm:
          inputs: (%1600:<69x5120xbf16>{5120, 1}, %2945:<5120x3072xbf16>{1, 5120})
          outputs: (%2740:<69x3072xbf16>{3072,1})
          duration: -1
207861 2024-12-10 17:48:33.312304 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n82,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%1600:<69x5120xbf16>{5120, 1}})
          outputs: (%2931:tuple{%2740:<69x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2948:<69x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2948:<69x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2948:<69x1536xbf16>{3072, 1})
          outputs: (%2950:<69x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2948:<69x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2950:<69x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2950:<69x1536xbf16>{1536, 1}, %2949:<69x1536xbf16>{3072, 1}+1536)
          outputs: (%2951:<69x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2951:<69x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2951:<69x1536xbf16>{1536,1}}))
          duration: -1
207965 2024-12-10 17:48:33.322256 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n67,rank6)
        - aten::mm:
          inputs: (%2951:<69x1536xbf16>{1536, 1}, %2952:<1536x5120xbf16>{1, 1536})
          outputs: (%2953:<69x5120xbf16>{5120,1})
          duration: -1
208046 2024-12-10 17:48:33.326097 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n67,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2951:<69x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2953:<69x5120xbf16>{5120,1},None:NoneType})
          duration: -1
208062 2024-12-10 17:48:33.326863 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n63,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%1600:<69x5120xbf16>{5120, 1}})
          outputs: (%2925:tuple{%2953:<69x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2955:<69x5120xbf16>{5120, 1}, %2953:<69x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2955:<69x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2955:<543x5120xbf16>{5120, 1}+353280})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2955:<543x5120xbf16>{5120,1}+353280}))
          duration: -1
208241 2024-12-10 17:48:33.337077 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n64,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2955:<543x5120xbf16>{5120, 1}+353280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2955:<543x5120xbf16>{5120,1}+353280}))
          duration: -1
208261 2024-12-10 17:48:33.337814 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n83,rank6)
        - aten::mm:
          inputs: (%2955:<543x5120xbf16>{5120, 1}+353280, %2957:<5120x3072xbf16>{1, 5120})
          outputs: (%2958:<543x3072xbf16>{3072,1})
          duration: -1
208343 2024-12-10 17:48:33.341634 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n83,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2955:<543x5120xbf16>{5120, 1}+353280})
          outputs: (%2874:tuple{%2958:<543x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2945:<543x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2945:<543x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2945:<543x1536xbf16>{3072, 1})
          outputs: (%2950:<543x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2945:<543x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2950:<543x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2950:<543x1536xbf16>{1536, 1}, %2948:<543x1536xbf16>{3072, 1}+1536)
          outputs: (%2961:<543x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2961:<543x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2961:<543x1536xbf16>{1536,1}}))
          duration: -1
208445 2024-12-10 17:48:33.351511 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n68,rank6)
        - aten::mm:
          inputs: (%2961:<543x1536xbf16>{1536, 1}, %2957:<1536x5120xbf16>{1, 1536})
          outputs: (%2952:<543x5120xbf16>{5120,1})
          duration: -1
208525 2024-12-10 17:48:33.355298 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n68,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2961:<543x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2952:<543x5120xbf16>{5120,1},None:NoneType})
          duration: -1
208541 2024-12-10 17:48:33.356050 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n64,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2955:<543x5120xbf16>{5120, 1}+353280})
          outputs: (%2925:tuple{%2952:<543x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2936:<543x5120xbf16>{5120, 1}+353280, %2952:<543x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2936:<543x5120xbf16>{5120,1}+353280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2905:<145x5120xbf16>{5120, 1}+3133440})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2905:<145x5120xbf16>{5120,1}+3133440}))
          duration: -1
208721 2024-12-10 17:48:33.366288 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n65,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2905:<145x5120xbf16>{5120, 1}+3133440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2905:<145x5120xbf16>{5120,1}+3133440}))
          duration: -1
208741 2024-12-10 17:48:33.367008 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n84,rank6)
        - aten::mm:
          inputs: (%2905:<145x5120xbf16>{5120, 1}+3133440, %2963:<5120x3072xbf16>{1, 5120})
          outputs: (%2950:<145x3072xbf16>{3072,1})
          duration: -1
208822 2024-12-10 17:48:33.370816 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n84,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2905:<145x5120xbf16>{5120, 1}+3133440})
          outputs: (%2944:tuple{%2950:<145x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2740:<145x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2740:<145x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2740:<145x1536xbf16>{3072, 1})
          outputs: (%2963:<145x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2740:<145x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2963:<145x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2963:<145x1536xbf16>{1536, 1}, %2945:<145x1536xbf16>{3072, 1}+1536)
          outputs: (%2953:<145x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2953:<145x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2953:<145x1536xbf16>{1536,1}}))
          duration: -1
208925 2024-12-10 17:48:33.380684 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n69,rank6)
        - aten::mm:
          inputs: (%2953:<145x1536xbf16>{1536, 1}, %2965:<1536x5120xbf16>{1, 1536})
          outputs: (%2963:<145x5120xbf16>{5120,1})
          duration: -1
209006 2024-12-10 17:48:33.384483 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n69,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2953:<145x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2963:<145x5120xbf16>{5120,1},None:NoneType})
          duration: -1
209020 2024-12-10 17:48:33.385255 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n65,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2905:<145x5120xbf16>{5120, 1}+3133440})
          outputs: (%2925:tuple{%2963:<145x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2952:<145x5120xbf16>{5120, 1}+3133440, %2963:<145x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2952:<145x5120xbf16>{5120,1}+3133440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2956:<339x5120xbf16>{5120, 1}+3875840})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2956:<339x5120xbf16>{5120,1}+3875840}))
          duration: -1
209197 2024-12-10 17:48:33.395450 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n66,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2956:<339x5120xbf16>{5120, 1}+3875840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2956:<339x5120xbf16>{5120,1}+3875840}))
          duration: -1
209217 2024-12-10 17:48:33.396172 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n85,rank6)
        - aten::mm:
          inputs: (%2956:<339x5120xbf16>{5120, 1}+3875840, %2967:<5120x3072xbf16>{1, 5120})
          outputs: (%2968:<339x3072xbf16>{3072,1})
          duration: -1
209301 2024-12-10 17:48:33.399967 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n85,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2956:<339x5120xbf16>{5120, 1}+3875840})
          outputs: (%2931:tuple{%2968:<339x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2741:<339x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2741:<339x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2741:<339x1536xbf16>{3072, 1})
          outputs: (%2972:<339x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2741:<339x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2972:<339x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2972:<339x1536xbf16>{1536, 1}, %2971:<339x1536xbf16>{3072, 1}+1536)
          outputs: (%2740:<339x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2740:<339x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2740:<339x1536xbf16>{1536,1}}))
          duration: -1
209399 2024-12-10 17:48:33.409848 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n70,rank6)
        - aten::mm:
          inputs: (%2740:<339x1536xbf16>{1536, 1}, %2967:<1536x5120xbf16>{1, 1536})
          outputs: (%2612:<339x5120xbf16>{5120,1})
          duration: -1
209484 2024-12-10 17:48:33.413674 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n70,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2740:<339x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2612:<339x5120xbf16>{5120,1},None:NoneType})
          duration: -1
209499 2024-12-10 17:48:33.414433 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n66,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2956:<339x5120xbf16>{5120, 1}+3875840})
          outputs: (%2925:tuple{%2612:<339x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2740:<339x5120xbf16>{5120, 1}+3875840, %2612:<339x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2740:<339x5120xbf16>{5120,1}+3875840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2952:<11x5120xbf16>{5120, 1}+5611520})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2952:<11x5120xbf16>{5120,1}+5611520}))
          duration: -1
209675 2024-12-10 17:48:33.424634 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n67,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2952:<11x5120xbf16>{5120, 1}+5611520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2952:<11x5120xbf16>{5120,1}+5611520}))
          duration: -1
209695 2024-12-10 17:48:33.425375 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n86,rank6)
        - aten::mm:
          inputs: (%2952:<11x5120xbf16>{5120, 1}+5611520, %2974:<5120x3072xbf16>{1, 5120})
          outputs: (%2975:<11x3072xbf16>{3072,1})
          duration: -1
209781 2024-12-10 17:48:33.429195 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n86,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2952:<11x5120xbf16>{5120, 1}+5611520})
          outputs: (%2874:tuple{%2975:<11x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2956:<11x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2956:<11x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2956:<11x1536xbf16>{3072, 1})
          outputs: (%2978:<11x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2956:<11x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2978:<11x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2978:<11x1536xbf16>{1536, 1}, %2971:<11x1536xbf16>{3072, 1}+1536)
          outputs: (%2979:<11x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2979:<11x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2979:<11x1536xbf16>{1536,1}}))
          duration: -1
209879 2024-12-10 17:48:33.439028 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n71,rank6)
        - aten::mm:
          inputs: (%2979:<11x1536xbf16>{1536, 1}, %2980:<1536x5120xbf16>{1, 1536})
          outputs: (%2968:<11x5120xbf16>{5120,1})
          duration: -1
209964 2024-12-10 17:48:33.442865 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n71,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2979:<11x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2968:<11x5120xbf16>{5120,1},None:NoneType})
          duration: -1
209979 2024-12-10 17:48:33.443624 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n67,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2952:<11x5120xbf16>{5120, 1}+5611520})
          outputs: (%2925:tuple{%2968:<11x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1600:<11x5120xbf16>{5120, 1}+5611520, %2968:<11x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1600:<11x5120xbf16>{5120,1}+5611520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2740:<258x5120xbf16>{5120, 1}+5667840})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2740:<258x5120xbf16>{5120,1}+5667840}))
          duration: -1
210156 2024-12-10 17:48:33.454002 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n68,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2740:<258x5120xbf16>{5120, 1}+5667840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2740:<258x5120xbf16>{5120,1}+5667840}))
          duration: -1
210182 2024-12-10 17:48:33.454742 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n87,rank6)
        - aten::mm:
          inputs: (%2740:<258x5120xbf16>{5120, 1}+5667840, %2982:<5120x3072xbf16>{1, 5120})
          outputs: (%2612:<258x3072xbf16>{3072,1})
          duration: -1
210260 2024-12-10 17:48:33.458524 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n87,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2740:<258x5120xbf16>{5120, 1}+5667840})
          outputs: (%2944:tuple{%2612:<258x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2952:<258x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2952:<258x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2952:<258x1536xbf16>{3072, 1})
          outputs: (%2974:<258x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2952:<258x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2974:<258x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2974:<258x1536xbf16>{1536, 1}, %2936:<258x1536xbf16>{3072, 1}+1536)
          outputs: (%2982:<258x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2982:<258x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2982:<258x1536xbf16>{1536,1}}))
          duration: -1
210355 2024-12-10 17:48:33.468451 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n72,rank6)
        - aten::mm:
          inputs: (%2982:<258x1536xbf16>{1536, 1}, %2979:<1536x5120xbf16>{1, 1536})
          outputs: (%2917:<258x5120xbf16>{5120,1})
          duration: -1
210426 2024-12-10 17:48:33.472234 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n72,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2982:<258x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2917:<258x5120xbf16>{5120,1},None:NoneType})
          duration: -1
210441 2024-12-10 17:48:33.472986 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n68,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2740:<258x5120xbf16>{5120, 1}+5667840})
          outputs: (%2925:tuple{%2917:<258x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2936:<258x5120xbf16>{5120, 1}+5667840, %2917:<258x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2936:<258x5120xbf16>{5120,1}+5667840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2741:<79x5120xbf16>{5120, 1}+6988800})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2741:<79x5120xbf16>{5120,1}+6988800}))
          duration: -1
210611 2024-12-10 17:48:33.483196 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n69,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2741:<79x5120xbf16>{5120, 1}+6988800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2741:<79x5120xbf16>{5120,1}+6988800}))
          duration: -1
210635 2024-12-10 17:48:33.483922 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n88,rank6)
        - aten::mm:
          inputs: (%2741:<79x5120xbf16>{5120, 1}+6988800, %2980:<5120x3072xbf16>{1, 5120})
          outputs: (%2979:<79x3072xbf16>{3072,1})
          duration: -1
210711 2024-12-10 17:48:33.487698 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n88,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2741:<79x5120xbf16>{5120, 1}+6988800})
          outputs: (%2931:tuple{%2979:<79x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2952:<79x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2952:<79x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2952:<79x1536xbf16>{3072, 1})
          outputs: (%2988:<79x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2952:<79x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2988:<79x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2988:<79x1536xbf16>{1536, 1}, %2971:<79x1536xbf16>{3072, 1}+1536)
          outputs: (%2612:<79x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2612:<79x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2612:<79x1536xbf16>{1536,1}}))
          duration: -1
210824 2024-12-10 17:48:33.497572 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n73,rank6)
        - aten::mm:
          inputs: (%2612:<79x1536xbf16>{1536, 1}, %2989:<1536x5120xbf16>{1, 1536})
          outputs: (%2980:<79x5120xbf16>{5120,1})
          duration: -1
210904 2024-12-10 17:48:33.501345 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n73,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2612:<79x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2980:<79x5120xbf16>{5120,1},None:NoneType})
          duration: -1
210920 2024-12-10 17:48:33.502104 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n69,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2741:<79x5120xbf16>{5120, 1}+6988800})
          outputs: (%2925:tuple{%2980:<79x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2740:<79x5120xbf16>{5120, 1}+6988800, %2980:<79x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2740:<79x5120xbf16>{5120,1}+6988800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2952:<239x5120xbf16>{5120, 1}+7393280})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2952:<239x5120xbf16>{5120,1}+7393280}))
          duration: -1
211089 2024-12-10 17:48:33.512337 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n70,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2952:<239x5120xbf16>{5120, 1}+7393280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2952:<239x5120xbf16>{5120,1}+7393280}))
          duration: -1
211116 2024-12-10 17:48:33.513073 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n89,rank6)
        - aten::mm:
          inputs: (%2952:<239x5120xbf16>{5120, 1}+7393280, %2968:<5120x3072xbf16>{1, 5120})
          outputs: (%2612:<239x3072xbf16>{3072,1})
          duration: -1
211190 2024-12-10 17:48:33.516836 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n89,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2952:<239x5120xbf16>{5120, 1}+7393280})
          outputs: (%2874:tuple{%2612:<239x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1600:<239x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1600:<239x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1600:<239x1536xbf16>{3072, 1})
          outputs: (%2993:<239x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1600:<239x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2993:<239x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2993:<239x1536xbf16>{1536, 1}, %2741:<239x1536xbf16>{3072, 1}+1536)
          outputs: (%2917:<239x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2917:<239x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2917:<239x1536xbf16>{1536,1}}))
          duration: -1
211306 2024-12-10 17:48:33.526736 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n74,rank6)
        - aten::mm:
          inputs: (%2917:<239x1536xbf16>{1536, 1}, %2988:<1536x5120xbf16>{1, 1536})
          outputs: (%2982:<239x5120xbf16>{5120,1})
          duration: -1
211386 2024-12-10 17:48:33.530528 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n74,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2917:<239x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2982:<239x5120xbf16>{5120,1},None:NoneType})
          duration: -1
211401 2024-12-10 17:48:33.531318 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n70,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2952:<239x5120xbf16>{5120, 1}+7393280})
          outputs: (%2925:tuple{%2982:<239x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2936:<239x5120xbf16>{5120, 1}+7393280, %2982:<239x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2936:<239x5120xbf16>{5120,1}+7393280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2741:<98x5120xbf16>{5120, 1}+8616960})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2741:<98x5120xbf16>{5120,1}+8616960}))
          duration: -1
211572 2024-12-10 17:48:33.541498 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n71,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2741:<98x5120xbf16>{5120, 1}+8616960})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2741:<98x5120xbf16>{5120,1}+8616960}))
          duration: -1
211598 2024-12-10 17:48:33.542230 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n90,rank6)
        - aten::mm:
          inputs: (%2741:<98x5120xbf16>{5120, 1}+8616960, %2995:<5120x3072xbf16>{1, 5120})
          outputs: (%2612:<98x3072xbf16>{3072,1})
          duration: -1
211671 2024-12-10 17:48:33.546021 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n90,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2741:<98x5120xbf16>{5120, 1}+8616960})
          outputs: (%2944:tuple{%2612:<98x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2740:<98x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2740:<98x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2740:<98x1536xbf16>{3072, 1})
          outputs: (%2971:<98x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2740:<98x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2971:<98x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2971:<98x1536xbf16>{1536, 1}, %2952:<98x1536xbf16>{3072, 1}+1536)
          outputs: (%2956:<98x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2956:<98x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2956:<98x1536xbf16>{1536,1}}))
          duration: -1
211788 2024-12-10 17:48:33.555889 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n75,rank6)
        - aten::mm:
          inputs: (%2956:<98x1536xbf16>{1536, 1}, %2980:<1536x5120xbf16>{1, 1536})
          outputs: (%2995:<98x5120xbf16>{5120,1})
          duration: -1
211867 2024-12-10 17:48:33.559660 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n75,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2956:<98x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2995:<98x5120xbf16>{5120,1},None:NoneType})
          duration: -1
211882 2024-12-10 17:48:33.560418 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n71,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2741:<98x5120xbf16>{5120, 1}+8616960})
          outputs: (%2925:tuple{%2995:<98x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2998:<98x5120xbf16>{5120, 1}+8616960, %2995:<98x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2998:<98x5120xbf16>{5120,1}+8616960)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2955:<172x5120xbf16>{5120, 1}+9118720})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2955:<172x5120xbf16>{5120,1}+9118720}))
          duration: -1
212053 2024-12-10 17:48:33.570679 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n72,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2955:<172x5120xbf16>{5120, 1}+9118720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2955:<172x5120xbf16>{5120,1}+9118720}))
          duration: -1
212078 2024-12-10 17:48:33.571407 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n91,rank6)
        - aten::mm:
          inputs: (%2955:<172x5120xbf16>{5120, 1}+9118720, %2999:<5120x3072xbf16>{1, 5120})
          outputs: (%2917:<172x3072xbf16>{3072,1})
          duration: -1
212149 2024-12-10 17:48:33.575181 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n91,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2955:<172x5120xbf16>{5120, 1}+9118720})
          outputs: (%2931:tuple{%2917:<172x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2998:<172x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2998:<172x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2998:<172x1536xbf16>{3072, 1})
          outputs: (%3002:<172x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2998:<172x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3002:<172x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3002:<172x1536xbf16>{1536, 1}, %2905:<172x1536xbf16>{3072, 1}+1536)
          outputs: (%2999:<172x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2999:<172x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2999:<172x1536xbf16>{1536,1}}))
          duration: -1
212267 2024-12-10 17:48:33.585067 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n76,rank6)
        - aten::mm:
          inputs: (%2999:<172x1536xbf16>{1536, 1}, %2980:<1536x5120xbf16>{1, 1536})
          outputs: (%3002:<172x5120xbf16>{5120,1})
          duration: -1
212344 2024-12-10 17:48:33.588824 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n76,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2999:<172x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3002:<172x5120xbf16>{5120,1},None:NoneType})
          duration: -1
212360 2024-12-10 17:48:33.589592 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n72,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2955:<172x5120xbf16>{5120, 1}+9118720})
          outputs: (%2925:tuple{%3002:<172x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2905:<172x5120xbf16>{5120, 1}+9118720, %3002:<172x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2905:<172x5120xbf16>{5120,1}+9118720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2952:<105x5120xbf16>{5120, 1}+9999360})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2952:<105x5120xbf16>{5120,1}+9999360}))
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2952:<105x5120xbf16>{5120, 1}+9999360})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2952:<105x5120xbf16>{5120,1}+9999360}))
          duration: -1
212553 2024-12-10 17:48:33.600545 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n92,rank6)
        - aten::mm:
          inputs: (%2952:<105x5120xbf16>{5120, 1}+9999360, %3004:<5120x3072xbf16>{1, 5120})
          outputs: (%2917:<105x3072xbf16>{3072,1})
          duration: -1
212629 2024-12-10 17:48:33.604333 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n92,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2952:<105x5120xbf16>{5120, 1}+9999360})
          outputs: (%2874:tuple{%2917:<105x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2998:<105x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2998:<105x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2998:<105x1536xbf16>{3072, 1})
          outputs: (%3007:<105x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2998:<105x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3007:<105x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3007:<105x1536xbf16>{1536, 1}, %2740:<105x1536xbf16>{3072, 1}+1536)
          outputs: (%3004:<105x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%3004:<105x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%3004:<105x1536xbf16>{1536,1}}))
          duration: -1
212745 2024-12-10 17:48:33.614246 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n77,rank6)
        - aten::mm:
          inputs: (%3004:<105x1536xbf16>{1536, 1}, %2982:<1536x5120xbf16>{1, 1536})
          outputs: (%3008:<105x5120xbf16>{5120,1})
          duration: -1
212827 2024-12-10 17:48:33.618008 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n77,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%3004:<105x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3008:<105x5120xbf16>{5120,1},None:NoneType})
          duration: -1
212840 2024-12-10 17:48:33.618758 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n73,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2952:<105x5120xbf16>{5120, 1}+9999360})
          outputs: (%2925:tuple{%3008:<105x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2951:<105x5120xbf16>{5120, 1}+9999360, %3008:<105x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2951:<105x5120xbf16>{5120,1}+9999360)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%1600:<97x5120xbf16>{5120, 1}+10536960})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%1600:<97x5120xbf16>{5120,1}+10536960}))
          duration: -1
213014 2024-12-10 17:48:33.628952 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n74,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%1600:<97x5120xbf16>{5120, 1}+10536960})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%1600:<97x5120xbf16>{5120,1}+10536960}))
          duration: -1
213033 2024-12-10 17:48:33.629696 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n93,rank6)
        - aten::mm:
          inputs: (%1600:<97x5120xbf16>{5120, 1}+10536960, %2972:<5120x3072xbf16>{1, 5120})
          outputs: (%2620:<97x3072xbf16>{3072,1})
          duration: -1
213109 2024-12-10 17:48:33.633486 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n93,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%1600:<97x5120xbf16>{5120, 1}+10536960})
          outputs: (%2944:tuple{%2620:<97x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2905:<97x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2905:<97x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2905:<97x1536xbf16>{3072, 1})
          outputs: (%3012:<97x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2905:<97x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3012:<97x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3012:<97x1536xbf16>{1536, 1}, %2971:<97x1536xbf16>{3072, 1}+1536)
          outputs: (%2952:<97x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2952:<97x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2952:<97x1536xbf16>{1536,1}}))
          duration: -1
213224 2024-12-10 17:48:33.643335 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n78,rank6)
        - aten::mm:
          inputs: (%2952:<97x1536xbf16>{1536, 1}, %3013:<1536x5120xbf16>{1, 1536})
          outputs: (%2980:<97x5120xbf16>{5120,1})
          duration: -1
213306 2024-12-10 17:48:33.647115 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n78,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2952:<97x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2980:<97x5120xbf16>{5120,1},None:NoneType})
          duration: -1
213319 2024-12-10 17:48:33.647884 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n74,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%1600:<97x5120xbf16>{5120, 1}+10536960})
          outputs: (%2925:tuple{%2980:<97x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2741:<97x5120xbf16>{5120, 1}+10536960, %2980:<97x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2741:<97x5120xbf16>{5120,1}+10536960)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2952:<257x5120xbf16>{5120, 1}+11033600})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2952:<257x5120xbf16>{5120,1}+11033600}))
          duration: -1
213488 2024-12-10 17:48:33.658096 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n75,rank6)
213508 2024-12-10 17:48:33.658840 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n94,rank6)
        - aten::mm:
          inputs: (%2952:<257x5120xbf16>{5120, 1}+11033600, %3013:<5120x3072xbf16>{1, 5120})
          outputs: (%2914:<257x3072xbf16>{3072,1})
          duration: -1
213588 2024-12-10 17:48:33.662640 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n94,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2952:<257x5120xbf16>{5120, 1}+11033600})
          outputs: (%2931:tuple{%2914:<257x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2955:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2955:<257x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2955:<257x1536xbf16>{3072, 1})
          outputs: (%2995:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2955:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2995:<257x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2995:<257x1536xbf16>{1536, 1}, %2998:<257x1536xbf16>{3072, 1}+1536)
          outputs: (%2936:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2936:<257x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2936:<257x1536xbf16>{1536,1}}))
          duration: -1
213699 2024-12-10 17:48:33.672534 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n79,rank6)
        - aten::mm:
          inputs: (%2936:<257x1536xbf16>{1536, 1}, %2974:<1536x5120xbf16>{1, 1536})
          outputs: (%2972:<257x5120xbf16>{5120,1})
          duration: -1
213785 2024-12-10 17:48:33.676334 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n79,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2936:<257x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2972:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
213799 2024-12-10 17:48:33.677106 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n75,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2952:<257x5120xbf16>{5120, 1}+11033600})
          outputs: (%2925:tuple{%2972:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2914:<257x5120xbf16>{5120, 1}+11033600, %2972:<257x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2914:<257x5120xbf16>{5120,1}+11033600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2741:<551x5120xbf16>{5120, 1}+12349440})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2741:<551x5120xbf16>{5120,1}+12349440}))
          duration: -1
213968 2024-12-10 17:48:33.687319 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n76,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2741:<551x5120xbf16>{5120, 1}+12349440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2741:<551x5120xbf16>{5120,1}+12349440}))
          duration: -1
213989 2024-12-10 17:48:33.688046 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n95,rank6)
        - aten::mm:
          inputs: (%2741:<551x5120xbf16>{5120, 1}+12349440, %2974:<5120x3072xbf16>{1, 5120})
          outputs: (%2995:<551x3072xbf16>{3072,1})
          duration: -1
214067 2024-12-10 17:48:33.691826 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n95,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2741:<551x5120xbf16>{5120, 1}+12349440})
          outputs: (%2874:tuple{%2995:<551x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2956:<551x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2956:<551x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2956:<551x1536xbf16>{3072, 1})
          outputs: (%3008:<551x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2956:<551x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3008:<551x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3008:<551x1536xbf16>{1536, 1}, %2952:<551x1536xbf16>{3072, 1}+1536)
          outputs: (%3002:<551x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%3002:<551x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%3002:<551x1536xbf16>{1536,1}}))
          duration: -1
214180 2024-12-10 17:48:33.701704 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n80,rank6)
        - aten::mm:
          inputs: (%3002:<551x1536xbf16>{1536, 1}, %3008:<1536x5120xbf16>{1, 1536})
          outputs: (%3020:<551x5120xbf16>{5120,1})
          duration: -1
214264 2024-12-10 17:48:33.705492 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n80,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%3002:<551x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3020:<551x5120xbf16>{5120,1},None:NoneType})
          duration: -1
214278 2024-12-10 17:48:33.706247 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n76,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2741:<551x5120xbf16>{5120, 1}+12349440})
          outputs: (%2925:tuple{%3020:<551x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2956:<551x5120xbf16>{5120, 1}+12349440, %3020:<551x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2956:<551x5120xbf16>{5120,1}+12349440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2955:<458x5120xbf16>{5120, 1}+15170560})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2955:<458x5120xbf16>{5120,1}+15170560}))
          duration: -1
214446 2024-12-10 17:48:33.716428 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n77,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2955:<458x5120xbf16>{5120, 1}+15170560})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2955:<458x5120xbf16>{5120,1}+15170560}))
          duration: -1
214469 2024-12-10 17:48:33.717167 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n96,rank6)
        - aten::mm:
          inputs: (%2955:<458x5120xbf16>{5120, 1}+15170560, %3022:<5120x3072xbf16>{1, 5120})
          outputs: (%2972:<458x3072xbf16>{3072,1})
          duration: -1
214547 2024-12-10 17:48:33.720969 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n96,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2955:<458x5120xbf16>{5120, 1}+15170560})
          outputs: (%2944:tuple{%2972:<458x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2971:<458x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2971:<458x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2971:<458x1536xbf16>{3072, 1})
          outputs: (%3004:<458x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2971:<458x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3004:<458x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3004:<458x1536xbf16>{1536, 1}, %2741:<458x1536xbf16>{3072, 1}+1536)
          outputs: (%2982:<458x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2982:<458x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2982:<458x1536xbf16>{1536,1}}))
          duration: -1
214660 2024-12-10 17:48:33.730849 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n81,rank6)
        - aten::mm:
          inputs: (%2982:<458x1536xbf16>{1536, 1}, %3002:<1536x5120xbf16>{1, 1536})
          outputs: (%3004:<458x5120xbf16>{5120,1})
          duration: -1
214745 2024-12-10 17:48:33.734632 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n81,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2982:<458x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3004:<458x5120xbf16>{5120,1},None:NoneType})
          duration: -1
214759 2024-12-10 17:48:33.735387 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n77,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2955:<458x5120xbf16>{5120, 1}+15170560})
          outputs: (%2925:tuple{%3004:<458x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2971:<458x5120xbf16>{5120, 1}+15170560, %3004:<458x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2971:<458x5120xbf16>{5120,1}+15170560)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2936:<148x5120xbf16>{5120, 1}+17515520})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2936:<148x5120xbf16>{5120,1}+17515520}))
          duration: -1
214928 2024-12-10 17:48:33.745604 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n78,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2936:<148x5120xbf16>{5120, 1}+17515520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2936:<148x5120xbf16>{5120,1}+17515520}))
          duration: -1
214948 2024-12-10 17:48:33.746342 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n97,rank6)
        - aten::mm:
          inputs: (%2936:<148x5120xbf16>{5120, 1}+17515520, %3002:<5120x3072xbf16>{1, 5120})
          outputs: (%2982:<148x3072xbf16>{3072,1})
          duration: -1
215027 2024-12-10 17:48:33.750104 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n97,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2936:<148x5120xbf16>{5120, 1}+17515520})
          outputs: (%2931:tuple{%2982:<148x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2998:<148x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2998:<148x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2998:<148x1536xbf16>{3072, 1})
          outputs: (%3027:<148x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2998:<148x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3027:<148x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3027:<148x1536xbf16>{1536, 1}, %2955:<148x1536xbf16>{3072, 1}+1536)
          outputs: (%2612:<148x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2612:<148x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2612:<148x1536xbf16>{1536,1}}))
          duration: -1
215140 2024-12-10 17:48:33.760006 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n82,rank6)
        - aten::mm:
          inputs: (%2612:<148x1536xbf16>{1536, 1}, %3008:<1536x5120xbf16>{1, 1536})
          outputs: (%3013:<148x5120xbf16>{5120,1})
          duration: -1
215226 2024-12-10 17:48:33.763810 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n82,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2612:<148x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3013:<148x5120xbf16>{5120,1},None:NoneType})
          duration: -1
215240 2024-12-10 17:48:33.764572 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n78,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2936:<148x5120xbf16>{5120, 1}+17515520})
          outputs: (%2925:tuple{%3013:<148x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2952:<148x5120xbf16>{5120, 1}+17515520, %3013:<148x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2952:<148x5120xbf16>{5120,1}+17515520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2905:<682x5120xbf16>{5120, 1}+18273280})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2905:<682x5120xbf16>{5120,1}+18273280}))
          duration: -1
215407 2024-12-10 17:48:33.774777 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n79,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2905:<682x5120xbf16>{5120, 1}+18273280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2905:<682x5120xbf16>{5120,1}+18273280}))
          duration: -1
215428 2024-12-10 17:48:33.775509 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n98,rank6)
        - aten::mm:
          inputs: (%2905:<682x5120xbf16>{5120, 1}+18273280, %2982:<5120x3072xbf16>{1, 5120})
          outputs: (%3004:<682x3072xbf16>{3072,1})
          duration: -1
215507 2024-12-10 17:48:33.779298 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n98,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2905:<682x5120xbf16>{5120, 1}+18273280})
          outputs: (%2874:tuple{%3004:<682x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2998:<682x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2998:<682x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2998:<682x1536xbf16>{3072, 1})
          outputs: (%2995:<682x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2998:<682x1536xbf16>{3072, 1}, False:bool)
          outputs: (%2995:<682x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%2995:<682x1536xbf16>{1536, 1}, %1600:<682x1536xbf16>{3072, 1}+1536)
          outputs: (%3031:<682x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%3031:<682x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%3031:<682x1536xbf16>{1536,1}}))
          duration: -1
215619 2024-12-10 17:48:33.789218 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n83,rank6)
        - aten::mm:
          inputs: (%3031:<682x1536xbf16>{1536, 1}, %2968:<1536x5120xbf16>{1, 1536})
          outputs: (%2972:<682x5120xbf16>{5120,1})
          duration: -1
215706 2024-12-10 17:48:33.792981 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n83,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%3031:<682x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%2972:<682x5120xbf16>{5120,1},None:NoneType})
          duration: -1
215717 2024-12-10 17:48:33.793753 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n79,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2905:<682x5120xbf16>{5120, 1}+18273280})
          outputs: (%2925:tuple{%2972:<682x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2936:<682x5120xbf16>{5120, 1}+18273280, %2972:<682x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2936:<682x5120xbf16>{5120,1}+18273280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2956:<148x5120xbf16>{5120, 1}+21765120})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2956:<148x5120xbf16>{5120,1}+21765120}))
          duration: -1
215885 2024-12-10 17:48:33.803937 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n80,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2956:<148x5120xbf16>{5120, 1}+21765120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2956:<148x5120xbf16>{5120,1}+21765120}))
          duration: -1
215903 2024-12-10 17:48:33.804676 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n99,rank6)
        - aten::mm:
          inputs: (%2956:<148x5120xbf16>{5120, 1}+21765120, %3008:<5120x3072xbf16>{1, 5120})
          outputs: (%1539:<148x3072xbf16>{3072,1})
          duration: -1
215983 2024-12-10 17:48:33.808431 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n99,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2956:<148x5120xbf16>{5120, 1}+21765120})
          outputs: (%2944:tuple{%1539:<148x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2998:<148x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2998:<148x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2998:<148x1536xbf16>{3072, 1})
          outputs: (%3022:<148x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2998:<148x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3022:<148x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3022:<148x1536xbf16>{1536, 1}, %2741:<148x1536xbf16>{3072, 1}+1536)
          outputs: (%3031:<148x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%3031:<148x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%3031:<148x1536xbf16>{1536,1}}))
          duration: -1
216095 2024-12-10 17:48:33.818287 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n84,rank6)
        - aten::mm:
          inputs: (%3031:<148x1536xbf16>{1536, 1}, %3027:<1536x5120xbf16>{1, 1536})
          outputs: (%3008:<148x5120xbf16>{5120,1})
          duration: -1
216178 2024-12-10 17:48:33.822033 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n84,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%3031:<148x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3008:<148x5120xbf16>{5120,1},None:NoneType})
          duration: -1
216194 2024-12-10 17:48:33.822790 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n80,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2956:<148x5120xbf16>{5120, 1}+21765120})
          outputs: (%2925:tuple{%3008:<148x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2905:<148x5120xbf16>{5120, 1}+21765120, %3008:<148x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2905:<148x5120xbf16>{5120,1}+21765120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2741:<869x5120xbf16>{5120, 1}+22522880})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2741:<869x5120xbf16>{5120,1}+22522880}))
          duration: -1
216365 2024-12-10 17:48:33.833080 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n81,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2741:<869x5120xbf16>{5120, 1}+22522880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2741:<869x5120xbf16>{5120,1}+22522880}))
          duration: -1
216383 2024-12-10 17:48:33.833846 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n100,rank6)
        - aten::mm:
          inputs: (%2741:<869x5120xbf16>{5120, 1}+22522880, %2995:<5120x3072xbf16>{1, 5120})
          outputs: (%2612:<869x3072xbf16>{3072,1})
          duration: -1
216465 2024-12-10 17:48:33.837643 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n100,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2741:<869x5120xbf16>{5120, 1}+22522880})
          outputs: (%2931:tuple{%2612:<869x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2998:<869x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2998:<869x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2998:<869x1536xbf16>{3072, 1})
          outputs: (%3004:<869x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2998:<869x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3004:<869x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3004:<869x1536xbf16>{1536, 1}, %2951:<869x1536xbf16>{3072, 1}+1536)
          outputs: (%3031:<869x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%3031:<869x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%3031:<869x1536xbf16>{1536,1}}))
          duration: -1
216578 2024-12-10 17:48:33.847511 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n85,rank6)
        - aten::mm:
          inputs: (%3031:<869x1536xbf16>{1536, 1}, %3027:<1536x5120xbf16>{1, 1536})
          outputs: (%3017:<869x5120xbf16>{5120,1})
          duration: -1
216661 2024-12-10 17:48:33.851297 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n85,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%3031:<869x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3017:<869x5120xbf16>{5120,1},None:NoneType})
          duration: -1
216676 2024-12-10 17:48:33.852048 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n81,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2741:<869x5120xbf16>{5120, 1}+22522880})
          outputs: (%2925:tuple{%3017:<869x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2971:<869x5120xbf16>{5120, 1}+22522880, %3017:<869x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2971:<869x5120xbf16>{5120,1}+22522880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%1600:<256x5120xbf16>{5120, 1}+26972160})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%1600:<256x5120xbf16>{5120,1}+26972160}))
          duration: -1
216846 2024-12-10 17:48:33.862240 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n82,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%1600:<256x5120xbf16>{5120, 1}+26972160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%1600:<256x5120xbf16>{5120,1}+26972160}))
          duration: -1
216865 2024-12-10 17:48:33.862977 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n101,rank6)
        - aten::mm:
          inputs: (%1600:<256x5120xbf16>{5120, 1}+26972160, %3027:<5120x3072xbf16>{1, 5120})
          outputs: (%3008:<256x3072xbf16>{3072,1})
          duration: -1
216944 2024-12-10 17:48:33.866810 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n101,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%1600:<256x5120xbf16>{5120, 1}+26972160})
          outputs: (%2874:tuple{%3008:<256x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2971:<256x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2971:<256x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2971:<256x1536xbf16>{3072, 1})
          outputs: (%3027:<256x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2971:<256x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3027:<256x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3027:<256x1536xbf16>{1536, 1}, %2952:<256x1536xbf16>{3072, 1}+1536)
          outputs: (%2974:<256x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2974:<256x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2974:<256x1536xbf16>{1536,1}}))
          duration: -1
217058 2024-12-10 17:48:33.876701 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n86,rank6)
        - aten::mm:
          inputs: (%2974:<256x1536xbf16>{1536, 1}, %2972:<1536x5120xbf16>{1, 1536})
          outputs: (%3013:<256x5120xbf16>{5120,1})
          duration: -1
217141 2024-12-10 17:48:33.880516 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n86,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2974:<256x1536xbf16>{1536, 1}})
          outputs: (%2925:tuple{%3013:<256x5120xbf16>{5120,1},None:NoneType})
          duration: -1
217157 2024-12-10 17:48:33.881284 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n82,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%1600:<256x5120xbf16>{5120, 1}+26972160})
          outputs: (%2925:tuple{%3013:<256x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2741:<256x5120xbf16>{5120, 1}+26972160, %3013:<256x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2741:<256x5120xbf16>{5120,1}+26972160)
          duration: -1
217236 2024-12-10 17:48:33.886188 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n3,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%1246:tuple{%2096:<5524x5120xbf16>{5120, 1}, %2941:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3041:tuple{%2942:<5524x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%2951:<5524x5120xbf16>{5120, 1}, 0:int, %2066:<5524x5120xCUSTOM_DATA_TYPE>{1, 0}, %2942:<5524x5120xbf16>{5120, 1})
          outputs: (%1600:<5524x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%1600:<5524x5120xbf16>{5120, 1}, %2926:<5524x1xbf16>{1, 1})
          outputs: (%2741:<5524x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%3043:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2937:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%2937:<8192x5120xbf16>{5120, 1}, 0:int, %2940:<5524x5120xCUSTOM_DATA_TYPE>{1, 0}, %2741:<5524x5120xbf16>{5120, 1})
          outputs: (%1860:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%2936:<1024x5120xbf16>{5120, 1}, %1860:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%2936:<1024x5120xbf16>{5120,1},%1860:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%2936:<1024x5120xbf16>{5120, 1}, %1860:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%2936:<1024x5120xbf16>{5120,1},%1860:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%2936:<1024x5120xbf16>{5120, 1}, %1860:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%2935:tuple{%2936:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%2916:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%2916:tuple{%2904:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
217494 2024-12-10 17:48:33.940680 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n83,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%2943:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%2943:tuple{%2904:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
217518 2024-12-10 17:48:33.941438 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n102,rank6)
        - aten::mm:
          inputs: (%2955:<1024x5120xbf16>{5120, 1}, %2998:<5120x6144xbf16>{1, 5120})
          outputs: (%2956:<1024x6144xbf16>{6144,1})
          duration: -1
217670 2024-12-10 17:48:33.949264 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n102,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%2943:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%2925:tuple{%3017:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1860:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1860:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1860:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%2740:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1860:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%2740:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%2740:<1024x1x3072xbf16>{3072, 3072, 1}, %2741:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%2951:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%2929:tuple{%2951:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%2929:tuple{%2951:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
217774 2024-12-10 17:48:33.959165 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n87,rank6)
        - aten::mm:
          inputs: (%2971:<1024x3072xbf16>{3072, 1}, %2952:<3072x5120xbf16>{1, 3072})
          outputs: (%2849:<1024x5120xbf16>{5120,1})
          duration: -1
217926 2024-12-10 17:48:33.967062 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n87,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%2929:tuple{%2951:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%3044:tuple{%2971:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
217942 2024-12-10 17:48:33.967836 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n83,rank6)
        - ----------->api::MLP return:
          inputs: (%2916:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3044:tuple{%2971:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%2926:<1024x1x5120xbf16>{5120, 5120, 1}, %2951:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2952:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
217990 2024-12-10 17:48:33.970965 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n3,rank6)
        - ----------->api::MoELayer return:
          inputs: (%2911:tuple{%2904:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3044:tuple{%2952:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3054:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3054:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3055:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3055:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8615e30_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8615e30_:_InferenceMode)
          duration: -1
220135 2024-12-10 17:48:34.497787 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n3,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%2925:tuple{%471:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2911:tuple{%471:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2911:tuple{%471:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
220185 2024-12-10 17:48:34.504694 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n11,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%471:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%471:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%471:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%471:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3056:tuple{%471:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3056:tuple{%471:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3056:tuple{%471:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3056:tuple{%471:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3060:tuple{%471:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %775:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3060:tuple{%471:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%775:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%471:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3056:tuple{%471:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3058:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%471:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%3058:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
220361 2024-12-10 17:48:34.548911 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n11,rank6)
220362 2024-12-10 17:48:34.549452 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n3,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%3058:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3061:tuple{%3058:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%3061:tuple{%3058:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
220445 2024-12-10 17:48:34.555801 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n103,rank6)
        - aten::mm:
          inputs: (%3066:<1024x5120xbf16>{5120, 1}, %2475:<5120x102400xbf16>{1, 5120})
          outputs: (%3067:<1024x102400xbf16>{102400,1})
          duration: -1
220578 2024-12-10 17:48:34.568114 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n103,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3061:tuple{%3058:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%3071:tuple{%3069:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%2475:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%2855:tuple{%3066:<1024x1xf32>{1,1},%3073:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3066:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3066:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3074:list{%3066:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3075:tuple{%3076:list{%3066:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%2475:<1024x1x102400xf32>{102400, 102400, 1}, %3077:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%2475:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%3072:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3078:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%3072:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3073:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3078:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3073:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3079:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3080:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%3081:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%3081:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3082:list{%3079:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3080:<i32>, False:bool)
          outputs: (%3081:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2952:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3083:<1024x102400xf32>{102400, 1}, %3076:list{%2952:<1024xCUSTOM_DATA_TYPE>{1}, %2941:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3084:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%2971:<1024x1xf32>{1, 1}, %3076:list{%3079:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %840:<i32>, False:bool)
          outputs: (%2971:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%2475:<1024x1x102400xf32>{102400, 102400, 1}, out=%2475:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%2475:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%2475:<1024x1x102400xf32>{102400, 102400, 1}, %3082:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%2055:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2971:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2971:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3082:list{%2971:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3071:tuple{%3065:list{%2971:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2055:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2055:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3085:list{%2055:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3086:tuple{%3063:list{%2055:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%2055:<1024x1xf32>{1, 1})
          outputs: (%1385:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1385:<1024x1xf32>{1, 1}, %2971:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%2952:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%2475:<1024x1x102400xf32>{102400, 102400, 1}, %3083:<1024x1x1xf32>{1, 1, 1})
          outputs: (%2475:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%3087:tuple{%2475:<1024x1x102400xf32>{102400, 102400, 1}, %3079:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %2941:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%3087:tuple{%2475:<1024x1x102400xf32>{102400,102400,1},%3079:<1024x1xCUSTOM_DATA_TYPE>{1,1},%2941:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2859:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device='cuda_6')_:dict)
          outputs: (%3066:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3066:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3066:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3066:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device='cuda_6')_:dict)
          outputs: (%3066:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2852:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2850:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_7756,_71415,___768,_____,_20378,____13,_34150]],_device='cuda_6')_:dict)
          outputs: (%3066:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%2743:<1024xf32>{1}, %2852:<1024xf32>{1})
          outputs: (%1655:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%1655:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%2851:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%2852:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%2845:<i32>)
          duration: -1
        - aten::div:
          inputs: (%2851:<i32>, %2845:<i32>)
          outputs: (%471:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%471:<i32>)
          outputs: (%1553:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%1781:list{%3090:<1xf32>{1}}, 0:int)
          outputs: (%2475:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2475:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2475:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3091:list{%2475:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3041:tuple{%2844:list{%2475:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%2475:<1xf32>{1}, 8:int)
          outputs: (%1385:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%471:<i32>, 1:int)
          outputs: (%3079:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%3079:<i32>, 1:int)
          outputs: (%3079:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%3092:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2852:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%2852:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%2852:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %2852:<1xf32>{1})
          outputs: (%3072:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%3072:<1xf32>{1}, 1:int)
          outputs: (%2475:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%1469:<i32>, 0:int, alpha=1:int)
          outputs: (%1469:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%1469:<i32>, %2941:<i32>, alpha=1:int)
          outputs: (%1469:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%1238:<i32>, %1469:<i32>, False:bool)
          outputs: (%1238:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%1238:<i32>, 1:int, alpha=1:int)
          outputs: (%1238:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%2852:<i32>, %1238:<i32>, False:bool)
          outputs: (%2852:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%2844:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%2852:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%418:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %433:list{%431:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%245:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%423:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%245:<1024xCUSTOM_DATA_TYPE>{1}, %433:list{%423:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%245:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%418:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%418:<1024xCUSTOM_DATA_TYPE>{1}+1, %433:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %430:<i32>, False:bool)
          outputs: (%418:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - c10d::broadcast_:
          inputs: (%3097:list{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3075:tuple{%3082:list{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3098:list{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3099:tuple{%2844:list{%1388:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3094:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3094:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3100:list{%3094:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3086:tuple{%3076:list{%3094:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3101:list{%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3075:tuple{%3102:list{%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3096:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3103:list{%3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3099:tuple{%3082:list{%3096:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3096:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3096:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3096:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2684:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2684:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3096:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[___11,___372,___838,_____,__1378,_16304,___317]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[___11,___372,___838,_____,__1378,_16304,___317]],_device)
          duration: -1
222559 2024-12-10 17:48:34.753700 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n4,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%2911:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%2911:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
222581 2024-12-10 17:48:34.754461 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n4,rank6)
        - ----------->api::embedding call:
          inputs: (%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%2068:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%3095:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%3095:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
222671 2024-12-10 17:48:34.769164 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n4,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%2911:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3072:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%2911:tuple{%452:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%2911:tuple{%452:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
222717 2024-12-10 17:48:34.771233 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n4,rank6)
        - ----------->api::dropout call:
          inputs: (%452:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%452:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%452:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%452:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
222756 2024-12-10 17:48:34.777468 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n4,rank6)
        - ----------->api::Dropout return:
          inputs: (%2911:tuple{%452:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%452:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
222770 2024-12-10 17:48:34.778188 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n4,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[___11,___372,___838,_____,__1378,_16304,___317]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%452:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%2911:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%2911:tuple{1024:int}))
          duration: -1
222798 2024-12-10 17:48:34.779792 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n4,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1411:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%1411:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%2998:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%3079:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%3107:list{%3079:<1024x20xf32>{20, 1}, %3079:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%3072:<1024x40xf32>{40,1})
          duration: -1
222952 2024-12-10 17:48:34.790712 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n4,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%2911:tuple{1024:int})
          outputs: (%3109:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
222986 2024-12-10 17:48:34.796943 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n4,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
223064 2024-12-10 17:48:34.805909 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n4,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2911:tuple{%2584:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2911:tuple{%2584:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
223085 2024-12-10 17:48:34.806663 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n12,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2584:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2584:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%2584:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%2584:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3056:tuple{%2584:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3056:tuple{%2584:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3056:tuple{%2584:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3056:tuple{%2584:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - aten::stack:
          inputs: (%436:list{%245:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%435:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%435:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3088:tuple{%2584:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3090:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3088:tuple{%2584:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3090:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%2584:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3056:tuple{%2584:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3108:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%2584:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%3108:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
223329 2024-12-10 17:48:34.850533 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n12,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%2911:tuple{%3108:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%2911:tuple{%3108:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
223352 2024-12-10 17:48:34.854765 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n4,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3112:tuple{%3108:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3112:tuple{%3108:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
223373 2024-12-10 17:48:34.855528 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n104,rank6)
        - aten::mm:
          inputs: (%2998:<1024x5120xbf16>{5120, 1}, %2971:<5120x1536xbf16>{1, 5120})
          outputs: (%2743:<1024x1536xbf16>{1536,1})
          duration: -1
223524 2024-12-10 17:48:34.863448 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n104,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3112:tuple{%3108:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%439:tuple{%3079:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3112:tuple{%3079:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3112:tuple{%3079:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
223553 2024-12-10 17:48:34.864464 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n105,rank6)
        - aten::mm:
          inputs: (%2743:<1024x1536xbf16>{1536, 1}, %2998:<1536x24576xbf16>{1, 1536})
          outputs: (%2952:<1024x24576xbf16>{24576,1})
          duration: -1
223667 2024-12-10 17:48:34.870523 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n105,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3112:tuple{%3079:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%3120:tuple{%358:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%2743:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3117:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%2743:<1024x1x128x192xbf16>{24576,24576,192,1},%3117:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%2743:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3117:list{128:int, 64:int}, -1:int)
          outputs: (%3071:tuple{%3121:<1024x1x128x128xbf16>{24576,24576,192,1},%3122:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3112:tuple{%3108:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3112:tuple{%3108:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
223756 2024-12-10 17:48:34.880008 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n106,rank6)
        - aten::mm:
          inputs: (%550:<1024x5120xbf16>{5120, 1}, %3125:<5120x576xbf16>{1, 5120})
          outputs: (%2952:<1024x576xbf16>{576,1})
          duration: -1
223905 2024-12-10 17:48:34.887818 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n106,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3112:tuple{%3108:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3071:tuple{%2995:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%2995:<1024x1x576xbf16>{576, 576, 1}, %3115:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%2995:<1024x1x576xbf16>{576,576,1},%3115:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%2995:<1024x1x576xbf16>{576, 576, 1}, %3115:list{512:int, 64:int}, -1:int)
          outputs: (%3120:tuple{%550:<1024x1x512xbf16>{576,576,1},%1600:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3112:tuple{%550:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3112:tuple{%550:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
223971 2024-12-10 17:48:34.896025 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n107,rank6)
        - aten::mm:
          inputs: (%3130:<1024x512xbf16>{576, 1}, %2999:<512x32768xbf16>{1, 512})
          outputs: (%3131:<1024x32768xbf16>{32768,1})
          duration: -1
224071 2024-12-10 17:48:34.902136 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n107,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3112:tuple{%550:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%3071:tuple{%3069:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%2998:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3129:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%2998:<1024x1x128x256xbf16>{32768,32768,256,1},%3129:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%2998:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3129:list{128:int, 128:int}, -1:int)
          outputs: (%439:tuple{%2092:<1024x1x128x128xbf16>{32768,32768,256,1},%2952:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%3104:tuple{%3027:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%3104:tuple{%3027:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
224235 2024-12-10 17:48:34.916888 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n4,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%3104:tuple{%3027:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%3086:tuple{%2952:<1024x64xbf16>{64,1},%2999:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%2952:<1024x64xbf16>{64, 1}, %3115:list{%3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3079:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%2999:<1024x64xbf16>{64, 1}, %3133:list{%3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%237:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%1162:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3135:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3139:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3140:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%3141:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%3115:list{%3141:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %3008:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%3142:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3142:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3134:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3140:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3139:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3140:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%3143:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3137:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3135:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3136:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3145:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%3146:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::stack:
          inputs: (%438:list{%418:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%437:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%437:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::cat:
          inputs: (%3115:list{%3146:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %3144:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%3147:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3147:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3134:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3144:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3136:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3144:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%3146:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%1162:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3134:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%1162:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3121:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3143:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%3121:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%3144:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3137:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%3144:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3148:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3145:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%3148:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%3137:<128x1024x192xbf16>{196608, 192, 1}, %3121:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%3144:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%3146:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%3121:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%3144:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%3137:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%3144:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %3115:list{%3137:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3148:<i32>, False:bool)
          outputs: (%3144:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%3121:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %3144:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%2995:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%2995:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%2995:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%1162:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%358:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%2995:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %358:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%358:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%3148:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3148:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3148:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3148:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%2995:<128x1024x1024xbf16>{1048576, 1024, 1}, %3121:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%3090:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3112:tuple{%3137:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3112:tuple{%3137:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
225718 2024-12-10 17:48:35.034270 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n88,rank6)
        - aten::mm:
          inputs: (%3141:<1024x16384xbf16>{16384, 1}, %2998:<16384x5120xbf16>{1, 16384})
          outputs: (%3150:<1024x5120xbf16>{5120,1})
          duration: -1
225867 2024-12-10 17:48:35.042427 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n88,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3112:tuple{%3137:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%3044:tuple{%550:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
225881 2024-12-10 17:48:35.043199 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n4,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%2911:tuple{%3108:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3044:tuple{%550:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3153:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3153:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3154:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3154:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8631270_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8631270_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3061:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3061:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
228340 2024-12-10 17:48:35.584726 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n13,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1979:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1979:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1979:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1979:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3156:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3156:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3156:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3156:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3159:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %237:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3159:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%237:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1979:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3156:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1498:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1979:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1498:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
228599 2024-12-10 17:48:35.628909 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n13,rank6)
        - ----------->api::MoELayer call:
          inputs: (%3061:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%3061:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
228606 2024-12-10 17:48:35.629667 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n4,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%3160:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%3160:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
228614 2024-12-10 17:48:35.630415 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n4,rank6)
        - aten::mm:
          inputs: (%237:<1024x5120xbf16>{5120, 1}, %550:<5120x160xbf16>{1, 5120})
          outputs: (%3162:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%3163:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%3069:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%3164:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%3165:tuple{%3166:<1024x6xbf16>{6,1},%3163:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%3083:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%3167:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%3168:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%2055:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%2055:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %3169:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%2055:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%2055:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %3168:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3170:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3167:<1024x160xf32>{160, 1}, %3168:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%3171:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3171:<160xf32>{1}, %3170:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%3172:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3172:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%3173:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%3173:<i32>, 2_5431315104166666e-07:float)
          outputs: (%2899:<i32>)
          duration: -1
        - aten::div:
          inputs: (%2899:<i32>, 0_01:float)
          outputs: (%3142:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3174:<i32>, %3175:<i32>, alpha=1:int)
          outputs: (%3174:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3176:<i32>, %3174:<i32>, False:bool)
          outputs: (%3176:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%3177:tuple{%2899:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%3177:tuple{%2899:<i32>}))
          duration: -1
229106 2024-12-10 17:48:35.665557 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n4,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%3160:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3179:tuple{%3140:<1024x6xbf16>{6,1},%3163:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%237:<8192x5120xbf16>{5120, 1}, %1600:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%237:<8192x5120xbf16>{5120,1},%1600:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%237:<8192x5120xbf16>{5120, 1}, %1600:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3181:tuple{%237:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3162:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3163:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3162:<8192x6xCUSTOM_DATA_TYPE>{6,1},%3163:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3162:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3163:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3165:tuple{%3162:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%3162:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%3174:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%3162:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%1574:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%3174:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %1574:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3121:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%3162:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3121:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3172:<7869xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3186:<8192x6xbf16>{6, 1}, %3140:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3186:<8192x6xbf16>{6,1},%3140:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3186:<8192x6xbf16>{6, 1}, %3140:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3187:tuple{%3186:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%3186:<8192x6xbf16>{6, 1}, %3121:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%840:<7869xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%3121:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%771:<7869x2xCUSTOM_DATA_TYPE>{1,7869})
          duration: -1
        - aten::gather:
          inputs: (%237:<8192x5120xbf16>{5120, 1}, 0:int, %3189:<7869x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%2940:<7869x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%3172:<7869xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%3190:tuple{%3176:<7869xCUSTOM_DATA_TYPE>{1},%3191:<7869xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%3173:<7869xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%3176:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%2940:<7869x5120xbf16>{5120, 1}, 0:int, %3192:<7869x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3176:<7869x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%3190:tuple{%3176:<7869x5120xbf16>{5120, 1}, %3193:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%3190:tuple{%3176:<7869x5120xbf16>{5120,1},%3193:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
230001 2024-12-10 17:48:35.792607 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n4,rank6)
        - aten::cumsum:
          inputs: (%3193:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%3186:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%3188:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%3162:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%3188:list{%3162:<1xCUSTOM_DATA_TYPE>{1}, %3186:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%2066:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%237:<69x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%237:<69x5120xbf16>{5120,1}}))
          duration: -1
230169 2024-12-10 17:48:35.803528 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n84,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%237:<69x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%237:<69x5120xbf16>{5120,1}}))
          duration: -1
230192 2024-12-10 17:48:35.804283 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n108,rank6)
        - aten::mm:
          inputs: (%237:<69x5120xbf16>{5120, 1}, %3197:<5120x3072xbf16>{1, 5120})
          outputs: (%3198:<69x3072xbf16>{3072,1})
          duration: -1
230269 2024-12-10 17:48:35.808105 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n108,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%237:<69x5120xbf16>{5120, 1}})
          outputs: (%3179:tuple{%3198:<69x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3201:<69x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3201:<69x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3201:<69x1536xbf16>{3072, 1})
          outputs: (%3203:<69x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3201:<69x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3203:<69x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3203:<69x1536xbf16>{1536, 1}, %3202:<69x1536xbf16>{3072, 1}+1536)
          outputs: (%3204:<69x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3204:<69x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3204:<69x1536xbf16>{1536,1}}))
          duration: -1
230371 2024-12-10 17:48:35.818044 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n89,rank6)
        - aten::mm:
          inputs: (%3204:<69x1536xbf16>{1536, 1}, %3205:<1536x5120xbf16>{1, 1536})
          outputs: (%3197:<69x5120xbf16>{5120,1})
          duration: -1
230451 2024-12-10 17:48:35.821802 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n89,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3204:<69x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3197:<69x5120xbf16>{5120,1},None:NoneType})
          duration: -1
230467 2024-12-10 17:48:35.822562 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n84,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%237:<69x5120xbf16>{5120, 1}})
          outputs: (%3184:tuple{%3197:<69x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2940:<69x5120xbf16>{5120, 1}, %3197:<69x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2940:<69x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3167:<836x5120xbf16>{5120, 1}+353280})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3167:<836x5120xbf16>{5120,1}+353280}))
          duration: -1
230643 2024-12-10 17:48:35.832810 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n85,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3167:<836x5120xbf16>{5120, 1}+353280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3167:<836x5120xbf16>{5120,1}+353280}))
          duration: -1
230667 2024-12-10 17:48:35.833561 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n109,rank6)
        - aten::mm:
          inputs: (%3167:<836x5120xbf16>{5120, 1}+353280, %3207:<5120x3072xbf16>{1, 5120})
          outputs: (%3208:<836x3072xbf16>{3072,1})
          duration: -1
230747 2024-12-10 17:48:35.837368 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n109,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3167:<836x5120xbf16>{5120, 1}+353280})
          outputs: (%3120:tuple{%3208:<836x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3198:<836x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3198:<836x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3198:<836x1536xbf16>{3072, 1})
          outputs: (%3201:<836x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3198:<836x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3201:<836x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3201:<836x1536xbf16>{1536, 1}, %3211:<836x1536xbf16>{3072, 1}+1536)
          outputs: (%3212:<836x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3212:<836x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3212:<836x1536xbf16>{1536,1}}))
          duration: -1
230851 2024-12-10 17:48:35.847220 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n90,rank6)
        - aten::mm:
          inputs: (%3212:<836x1536xbf16>{1536, 1}, %3208:<1536x5120xbf16>{1, 1536})
          outputs: (%3204:<836x5120xbf16>{5120,1})
          duration: -1
230929 2024-12-10 17:48:35.851001 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n90,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3212:<836x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3204:<836x5120xbf16>{5120,1},None:NoneType})
          duration: -1
230945 2024-12-10 17:48:35.851751 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n85,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3167:<836x5120xbf16>{5120, 1}+353280})
          outputs: (%3184:tuple{%3204:<836x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%434:<836x5120xbf16>{5120, 1}+353280, %3204:<836x5120xbf16>{5120, 1}, False:bool)
          outputs: (%434:<836x5120xbf16>{5120,1}+353280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3121:<450x5120xbf16>{5120, 1}+4633600})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3121:<450x5120xbf16>{5120,1}+4633600}))
          duration: -1
231119 2024-12-10 17:48:35.861992 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n86,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3121:<450x5120xbf16>{5120, 1}+4633600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3121:<450x5120xbf16>{5120,1}+4633600}))
          duration: -1
231143 2024-12-10 17:48:35.862719 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n110,rank6)
        - aten::mm:
          inputs: (%3121:<450x5120xbf16>{5120, 1}+4633600, %3214:<5120x3072xbf16>{1, 5120})
          outputs: (%3198:<450x3072xbf16>{3072,1})
          duration: -1
231225 2024-12-10 17:48:35.866498 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n110,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3121:<450x5120xbf16>{5120, 1}+4633600})
          outputs: (%3196:tuple{%3198:<450x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3207:<450x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3207:<450x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3207:<450x1536xbf16>{3072, 1})
          outputs: (%3211:<450x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3207:<450x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3211:<450x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3211:<450x1536xbf16>{1536, 1}, %3214:<450x1536xbf16>{3072, 1}+1536)
          outputs: (%3208:<450x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3208:<450x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3208:<450x1536xbf16>{1536,1}}))
          duration: -1
231319 2024-12-10 17:48:35.876380 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n91,rank6)
        - aten::mm:
          inputs: (%3208:<450x1536xbf16>{1536, 1}, %3217:<1536x5120xbf16>{1, 1536})
          outputs: (%3201:<450x5120xbf16>{5120,1})
          duration: -1
231405 2024-12-10 17:48:35.880138 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n91,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3208:<450x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3201:<450x5120xbf16>{5120,1},None:NoneType})
          duration: -1
231420 2024-12-10 17:48:35.880896 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n86,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3121:<450x5120xbf16>{5120, 1}+4633600})
          outputs: (%3184:tuple{%3201:<450x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3167:<450x5120xbf16>{5120, 1}+4633600, %3201:<450x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3167:<450x5120xbf16>{5120,1}+4633600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%2940:<493x5120xbf16>{5120, 1}+6937600})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%2940:<493x5120xbf16>{5120,1}+6937600}))
          duration: -1
231590 2024-12-10 17:48:35.891158 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n87,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%2940:<493x5120xbf16>{5120, 1}+6937600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%2940:<493x5120xbf16>{5120,1}+6937600}))
          duration: -1
231615 2024-12-10 17:48:35.891888 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n111,rank6)
        - aten::mm:
          inputs: (%2940:<493x5120xbf16>{5120, 1}+6937600, %3204:<5120x3072xbf16>{1, 5120})
          outputs: (%1574:<493x3072xbf16>{3072,1})
          duration: -1
231700 2024-12-10 17:48:35.895667 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n111,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%2940:<493x5120xbf16>{5120, 1}+6937600})
          outputs: (%3179:tuple{%1574:<493x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<493x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<493x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<493x1536xbf16>{3072, 1})
          outputs: (%3220:<493x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<493x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3220:<493x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3220:<493x1536xbf16>{1536, 1}, %3208:<493x1536xbf16>{3072, 1}+1536)
          outputs: (%3221:<493x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3221:<493x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3221:<493x1536xbf16>{1536,1}}))
          duration: -1
231797 2024-12-10 17:48:35.905545 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n92,rank6)
        - aten::mm:
          inputs: (%3221:<493x1536xbf16>{1536, 1}, %3222:<1536x5120xbf16>{1, 1536})
          outputs: (%1612:<493x5120xbf16>{5120,1})
          duration: -1
231884 2024-12-10 17:48:35.909319 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n92,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3221:<493x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%1612:<493x5120xbf16>{5120,1},None:NoneType})
          duration: -1
231898 2024-12-10 17:48:35.910074 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n87,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%2940:<493x5120xbf16>{5120, 1}+6937600})
          outputs: (%3184:tuple{%1612:<493x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3224:<493x5120xbf16>{5120, 1}+6937600, %1612:<493x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3224:<493x5120xbf16>{5120,1}+6937600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%1600:<63x5120xbf16>{5120, 1}+9461760})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%1600:<63x5120xbf16>{5120,1}+9461760}))
          duration: -1
232069 2024-12-10 17:48:35.920288 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n88,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%1600:<63x5120xbf16>{5120, 1}+9461760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%1600:<63x5120xbf16>{5120,1}+9461760}))
          duration: -1
232091 2024-12-10 17:48:35.921031 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n112,rank6)
        - aten::mm:
          inputs: (%1600:<63x5120xbf16>{5120, 1}+9461760, %3220:<5120x3072xbf16>{1, 5120})
          outputs: (%1574:<63x3072xbf16>{3072,1})
          duration: -1
232179 2024-12-10 17:48:35.924807 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n112,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%1600:<63x5120xbf16>{5120, 1}+9461760})
          outputs: (%3120:tuple{%1574:<63x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<63x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<63x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<63x1536xbf16>{3072, 1})
          outputs: (%3227:<63x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<63x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3227:<63x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3227:<63x1536xbf16>{1536, 1}, %3205:<63x1536xbf16>{3072, 1}+1536)
          outputs: (%3228:<63x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3228:<63x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3228:<63x1536xbf16>{1536,1}}))
          duration: -1
232275 2024-12-10 17:48:35.934659 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n93,rank6)
        - aten::mm:
          inputs: (%3228:<63x1536xbf16>{1536, 1}, %3221:<1536x5120xbf16>{1, 1536})
          outputs: (%1539:<63x5120xbf16>{5120,1})
          duration: -1
232363 2024-12-10 17:48:35.938443 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n93,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3228:<63x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%1539:<63x5120xbf16>{5120,1},None:NoneType})
          duration: -1
232375 2024-12-10 17:48:35.939195 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n88,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%1600:<63x5120xbf16>{5120, 1}+9461760})
          outputs: (%3184:tuple{%1539:<63x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2940:<63x5120xbf16>{5120, 1}+9461760, %1539:<63x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2940:<63x5120xbf16>{5120,1}+9461760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3121:<813x5120xbf16>{5120, 1}+9784320})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3121:<813x5120xbf16>{5120,1}+9784320}))
          duration: -1
232546 2024-12-10 17:48:35.949449 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n89,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3121:<813x5120xbf16>{5120, 1}+9784320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3121:<813x5120xbf16>{5120,1}+9784320}))
          duration: -1
232568 2024-12-10 17:48:35.950180 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n113,rank6)
        - aten::mm:
          inputs: (%3121:<813x5120xbf16>{5120, 1}+9784320, %3230:<5120x3072xbf16>{1, 5120})
          outputs: (%1574:<813x3072xbf16>{3072,1})
          duration: -1
232659 2024-12-10 17:48:35.953958 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n113,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3121:<813x5120xbf16>{5120, 1}+9784320})
          outputs: (%3196:tuple{%1574:<813x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3208:<813x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3208:<813x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3208:<813x1536xbf16>{3072, 1})
          outputs: (%3222:<813x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3208:<813x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3222:<813x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3222:<813x1536xbf16>{1536, 1}, %1600:<813x1536xbf16>{3072, 1}+1536)
          outputs: (%3069:<813x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3069:<813x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3069:<813x1536xbf16>{1536,1}}))
          duration: -1
232753 2024-12-10 17:48:35.963803 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n94,rank6)
        - aten::mm:
          inputs: (%3069:<813x1536xbf16>{1536, 1}, %3221:<1536x5120xbf16>{1, 1536})
          outputs: (%1612:<813x5120xbf16>{5120,1})
          duration: -1
232842 2024-12-10 17:48:35.967579 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n94,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3069:<813x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%1612:<813x5120xbf16>{5120,1},None:NoneType})
          duration: -1
232854 2024-12-10 17:48:35.968342 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n89,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3121:<813x5120xbf16>{5120, 1}+9784320})
          outputs: (%3184:tuple{%1612:<813x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3069:<813x5120xbf16>{5120, 1}+9784320, %1612:<813x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3069:<813x5120xbf16>{5120,1}+9784320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%237:<230x5120xbf16>{5120, 1}+13946880})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%237:<230x5120xbf16>{5120,1}+13946880}))
          duration: -1
233025 2024-12-10 17:48:35.978667 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n90,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%237:<230x5120xbf16>{5120, 1}+13946880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%237:<230x5120xbf16>{5120,1}+13946880}))
          duration: -1
233045 2024-12-10 17:48:35.979398 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n114,rank6)
        - aten::mm:
          inputs: (%237:<230x5120xbf16>{5120, 1}+13946880, %3230:<5120x3072xbf16>{1, 5120})
          outputs: (%3121:<230x3072xbf16>{3072,1})
          duration: -1
233138 2024-12-10 17:48:35.983190 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n114,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%237:<230x5120xbf16>{5120, 1}+13946880})
          outputs: (%3179:tuple{%3121:<230x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<230x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<230x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<230x1536xbf16>{3072, 1})
          outputs: (%3220:<230x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<230x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3220:<230x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3220:<230x1536xbf16>{1536, 1}, %3214:<230x1536xbf16>{3072, 1}+1536)
          outputs: (%3230:<230x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3230:<230x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3230:<230x1536xbf16>{1536,1}}))
          duration: -1
233233 2024-12-10 17:48:35.993075 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n95,rank6)
        - aten::mm:
          inputs: (%3230:<230x1536xbf16>{1536, 1}, %3236:<1536x5120xbf16>{1, 1536})
          outputs: (%1574:<230x5120xbf16>{5120,1})
          duration: -1
233320 2024-12-10 17:48:35.996822 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n95,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3230:<230x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%1574:<230x5120xbf16>{5120,1},None:NoneType})
          duration: -1
233334 2024-12-10 17:48:35.997601 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n90,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%237:<230x5120xbf16>{5120, 1}+13946880})
          outputs: (%3184:tuple{%1574:<230x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3121:<230x5120xbf16>{5120, 1}+13946880, %1574:<230x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3121:<230x5120xbf16>{5120,1}+13946880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%1600:<699x5120xbf16>{5120, 1}+15124480})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%1600:<699x5120xbf16>{5120,1}+15124480}))
          duration: -1
233505 2024-12-10 17:48:36.007823 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n91,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%1600:<699x5120xbf16>{5120, 1}+15124480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%1600:<699x5120xbf16>{5120,1}+15124480}))
          duration: -1
233525 2024-12-10 17:48:36.008551 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n115,rank6)
        - aten::mm:
          inputs: (%1600:<699x5120xbf16>{5120, 1}+15124480, %3220:<5120x3072xbf16>{1, 5120})
          outputs: (%3222:<699x3072xbf16>{3072,1})
          duration: -1
233616 2024-12-10 17:48:36.012327 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n115,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%1600:<699x5120xbf16>{5120, 1}+15124480})
          outputs: (%3120:tuple{%3222:<699x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3205:<699x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3205:<699x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3205:<699x1536xbf16>{3072, 1})
          outputs: (%3240:<699x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3205:<699x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3240:<699x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3240:<699x1536xbf16>{1536, 1}, %1612:<699x1536xbf16>{3072, 1}+1536)
          outputs: (%3241:<699x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3241:<699x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3241:<699x1536xbf16>{1536,1}}))
          duration: -1
233712 2024-12-10 17:48:36.022201 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n96,rank6)
        - aten::mm:
          inputs: (%3241:<699x1536xbf16>{1536, 1}, %3242:<1536x5120xbf16>{1, 1536})
          outputs: (%3069:<699x5120xbf16>{5120,1})
          duration: -1
233797 2024-12-10 17:48:36.025952 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n96,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3241:<699x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3069:<699x5120xbf16>{5120,1},None:NoneType})
          duration: -1
233812 2024-12-10 17:48:36.026708 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n91,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%1600:<699x5120xbf16>{5120, 1}+15124480})
          outputs: (%3184:tuple{%3069:<699x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%237:<699x5120xbf16>{5120, 1}+15124480, %3069:<699x5120xbf16>{5120, 1}, False:bool)
          outputs: (%237:<699x5120xbf16>{5120,1}+15124480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3121:<573x5120xbf16>{5120, 1}+18703360})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3121:<573x5120xbf16>{5120,1}+18703360}))
          duration: -1
233983 2024-12-10 17:48:36.036924 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n92,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3121:<573x5120xbf16>{5120, 1}+18703360})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3121:<573x5120xbf16>{5120,1}+18703360}))
          duration: -1
234004 2024-12-10 17:48:36.037666 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n116,rank6)
        - aten::mm:
          inputs: (%3121:<573x5120xbf16>{5120, 1}+18703360, %3244:<5120x3072xbf16>{1, 5120})
          outputs: (%1539:<573x3072xbf16>{3072,1})
          duration: -1
234094 2024-12-10 17:48:36.041472 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n116,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3121:<573x5120xbf16>{5120, 1}+18703360})
          outputs: (%3196:tuple{%1539:<573x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3205:<573x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3205:<573x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3205:<573x1536xbf16>{3072, 1})
          outputs: (%3244:<573x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3205:<573x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3244:<573x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3244:<573x1536xbf16>{1536, 1}, %3214:<573x1536xbf16>{3072, 1}+1536)
          outputs: (%1600:<573x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%1600:<573x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%1600:<573x1536xbf16>{1536,1}}))
          duration: -1
234193 2024-12-10 17:48:36.051353 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n97,rank6)
        - aten::mm:
          inputs: (%1600:<573x1536xbf16>{1536, 1}, %3242:<1536x5120xbf16>{1, 1536})
          outputs: (%3067:<573x5120xbf16>{5120,1})
          duration: -1
234277 2024-12-10 17:48:36.055124 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n97,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%1600:<573x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3067:<573x5120xbf16>{5120,1},None:NoneType})
          duration: -1
234293 2024-12-10 17:48:36.055880 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n92,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3121:<573x5120xbf16>{5120, 1}+18703360})
          outputs: (%3184:tuple{%3067:<573x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%237:<573x5120xbf16>{5120, 1}+18703360, %3067:<573x5120xbf16>{5120, 1}, False:bool)
          outputs: (%237:<573x5120xbf16>{5120,1}+18703360)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%237:<309x5120xbf16>{5120, 1}+21637120})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%237:<309x5120xbf16>{5120,1}+21637120}))
          duration: -1
234463 2024-12-10 17:48:36.066129 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n93,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%237:<309x5120xbf16>{5120, 1}+21637120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%237:<309x5120xbf16>{5120,1}+21637120}))
          duration: -1
234484 2024-12-10 17:48:36.066857 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n117,rank6)
        - aten::mm:
          inputs: (%237:<309x5120xbf16>{5120, 1}+21637120, %3241:<5120x3072xbf16>{1, 5120})
          outputs: (%3222:<309x3072xbf16>{3072,1})
          duration: -1
234573 2024-12-10 17:48:36.070648 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n117,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%237:<309x5120xbf16>{5120, 1}+21637120})
          outputs: (%3179:tuple{%3222:<309x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<309x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<309x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<309x1536xbf16>{3072, 1})
          outputs: (%3241:<309x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<309x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3241:<309x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3241:<309x1536xbf16>{1536, 1}, %3214:<309x1536xbf16>{3072, 1}+1536)
          outputs: (%3208:<309x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3208:<309x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3208:<309x1536xbf16>{1536,1}}))
          duration: -1
234672 2024-12-10 17:48:36.080493 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n98,rank6)
        - aten::mm:
          inputs: (%3208:<309x1536xbf16>{1536, 1}, %3236:<1536x5120xbf16>{1, 1536})
          outputs: (%3250:<309x5120xbf16>{5120,1})
          duration: -1
234757 2024-12-10 17:48:36.084279 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n98,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3208:<309x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3250:<309x5120xbf16>{5120,1},None:NoneType})
          duration: -1
234771 2024-12-10 17:48:36.085045 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n93,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%237:<309x5120xbf16>{5120, 1}+21637120})
          outputs: (%3184:tuple{%3250:<309x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3121:<309x5120xbf16>{5120, 1}+21637120, %3250:<309x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3121:<309x5120xbf16>{5120,1}+21637120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3167:<211x5120xbf16>{5120, 1}+23219200})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3167:<211x5120xbf16>{5120,1}+23219200}))
          duration: -1
234944 2024-12-10 17:48:36.095278 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n94,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3167:<211x5120xbf16>{5120, 1}+23219200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3167:<211x5120xbf16>{5120,1}+23219200}))
          duration: -1
234962 2024-12-10 17:48:36.096010 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n118,rank6)
        - aten::mm:
          inputs: (%3167:<211x5120xbf16>{5120, 1}+23219200, %3236:<5120x3072xbf16>{1, 5120})
          outputs: (%3252:<211x3072xbf16>{3072,1})
          duration: -1
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3167:<211x5120xbf16>{5120, 1}+23219200})
          outputs: (%3120:tuple{%3252:<211x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3208:<211x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3208:<211x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3208:<211x1536xbf16>{3072, 1})
          outputs: (%3255:<211x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3208:<211x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3255:<211x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3255:<211x1536xbf16>{1536, 1}, %3205:<211x1536xbf16>{3072, 1}+1536)
          outputs: (%3214:<211x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3214:<211x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3214:<211x1536xbf16>{1536,1}}))
          duration: -1
235150 2024-12-10 17:48:36.109655 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n99,rank6)
        - aten::mm:
          inputs: (%3214:<211x1536xbf16>{1536, 1}, %3222:<1536x5120xbf16>{1, 1536})
          outputs: (%3240:<211x5120xbf16>{5120,1})
          duration: -1
235234 2024-12-10 17:48:36.113433 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n99,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3214:<211x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3240:<211x5120xbf16>{5120,1},None:NoneType})
          duration: -1
235249 2024-12-10 17:48:36.114185 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n94,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3167:<211x5120xbf16>{5120, 1}+23219200})
          outputs: (%3184:tuple{%3240:<211x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1600:<211x5120xbf16>{5120, 1}+23219200, %3240:<211x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1600:<211x5120xbf16>{5120,1}+23219200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3121:<286x5120xbf16>{5120, 1}+24299520})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3121:<286x5120xbf16>{5120,1}+24299520}))
          duration: -1
235422 2024-12-10 17:48:36.124415 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n95,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3121:<286x5120xbf16>{5120, 1}+24299520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3121:<286x5120xbf16>{5120,1}+24299520}))
          duration: -1
235437 2024-12-10 17:48:36.125152 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n119,rank6)
        - aten::mm:
          inputs: (%3121:<286x5120xbf16>{5120, 1}+24299520, %3222:<5120x3072xbf16>{1, 5120})
          outputs: (%1600:<286x3072xbf16>{3072,1})
          duration: -1
235530 2024-12-10 17:48:36.128934 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n119,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3121:<286x5120xbf16>{5120, 1}+24299520})
          outputs: (%3196:tuple{%1600:<286x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<286x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<286x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<286x1536xbf16>{3072, 1})
          outputs: (%3258:<286x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<286x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3258:<286x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3258:<286x1536xbf16>{1536, 1}, %3214:<286x1536xbf16>{3072, 1}+1536)
          outputs: (%3078:<286x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3078:<286x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3078:<286x1536xbf16>{1536,1}}))
          duration: -1
235629 2024-12-10 17:48:36.138815 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n100,rank6)
        - aten::mm:
          inputs: (%3078:<286x1536xbf16>{1536, 1}, %3259:<1536x5120xbf16>{1, 1536})
          outputs: (%3241:<286x5120xbf16>{5120,1})
          duration: -1
235711 2024-12-10 17:48:36.142587 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n100,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3078:<286x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3241:<286x5120xbf16>{5120,1},None:NoneType})
          duration: -1
235727 2024-12-10 17:48:36.143347 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n95,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3121:<286x5120xbf16>{5120, 1}+24299520})
          outputs: (%3184:tuple{%3241:<286x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2940:<286x5120xbf16>{5120, 1}+24299520, %3241:<286x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2940:<286x5120xbf16>{5120,1}+24299520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%1600:<433x5120xbf16>{5120, 1}+25763840})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%1600:<433x5120xbf16>{5120,1}+25763840}))
          duration: -1
235899 2024-12-10 17:48:36.153596 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n96,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%1600:<433x5120xbf16>{5120, 1}+25763840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%1600:<433x5120xbf16>{5120,1}+25763840}))
          duration: -1
235916 2024-12-10 17:48:36.154326 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n120,rank6)
        - aten::mm:
          inputs: (%1600:<433x5120xbf16>{5120, 1}+25763840, %3261:<5120x3072xbf16>{1, 5120})
          outputs: (%3069:<433x3072xbf16>{3072,1})
          duration: -1
236007 2024-12-10 17:48:36.158101 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n120,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%1600:<433x5120xbf16>{5120, 1}+25763840})
          outputs: (%3179:tuple{%3069:<433x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3205:<433x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3205:<433x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3205:<433x1536xbf16>{3072, 1})
          outputs: (%3261:<433x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3205:<433x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3261:<433x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3261:<433x1536xbf16>{1536, 1}, %3214:<433x1536xbf16>{3072, 1}+1536)
          outputs: (%3208:<433x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3208:<433x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3208:<433x1536xbf16>{1536,1}}))
          duration: -1
236111 2024-12-10 17:48:36.167977 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n101,rank6)
        - aten::mm:
          inputs: (%3208:<433x1536xbf16>{1536, 1}, %3236:<1536x5120xbf16>{1, 1536})
          outputs: (%3259:<433x5120xbf16>{5120,1})
          duration: -1
236191 2024-12-10 17:48:36.171746 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n101,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3208:<433x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3259:<433x5120xbf16>{5120,1},None:NoneType})
          duration: -1
236206 2024-12-10 17:48:36.172497 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n96,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%1600:<433x5120xbf16>{5120, 1}+25763840})
          outputs: (%3184:tuple{%3259:<433x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3121:<433x5120xbf16>{5120, 1}+25763840, %3259:<433x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3121:<433x5120xbf16>{5120,1}+25763840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%2940:<412x5120xbf16>{5120, 1}+27980800})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%2940:<412x5120xbf16>{5120,1}+27980800}))
          duration: -1
236379 2024-12-10 17:48:36.182772 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n97,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%2940:<412x5120xbf16>{5120, 1}+27980800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%2940:<412x5120xbf16>{5120,1}+27980800}))
          duration: -1
236395 2024-12-10 17:48:36.183504 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n121,rank6)
        - aten::mm:
          inputs: (%2940:<412x5120xbf16>{5120, 1}+27980800, %3264:<5120x3072xbf16>{1, 5120})
          outputs: (%3250:<412x3072xbf16>{3072,1})
          duration: -1
236487 2024-12-10 17:48:36.187284 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n121,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%2940:<412x5120xbf16>{5120, 1}+27980800})
          outputs: (%3120:tuple{%3250:<412x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3205:<412x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3205:<412x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3205:<412x1536xbf16>{3072, 1})
          outputs: (%3264:<412x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3205:<412x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3264:<412x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3264:<412x1536xbf16>{1536, 1}, %3214:<412x1536xbf16>{3072, 1}+1536)
          outputs: (%3208:<412x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3208:<412x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3208:<412x1536xbf16>{1536,1}}))
          duration: -1
236591 2024-12-10 17:48:36.197152 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n102,rank6)
        - aten::mm:
          inputs: (%3208:<412x1536xbf16>{1536, 1}, %3240:<1536x5120xbf16>{1, 1536})
          outputs: (%3236:<412x5120xbf16>{5120,1})
          duration: -1
236670 2024-12-10 17:48:36.200902 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n102,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3208:<412x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3236:<412x5120xbf16>{5120,1},None:NoneType})
          duration: -1
236683 2024-12-10 17:48:36.201683 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n97,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%2940:<412x5120xbf16>{5120, 1}+27980800})
          outputs: (%3184:tuple{%3236:<412x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1600:<412x5120xbf16>{5120, 1}+27980800, %3236:<412x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1600:<412x5120xbf16>{5120,1}+27980800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3121:<213x5120xbf16>{5120, 1}+30090240})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3121:<213x5120xbf16>{5120,1}+30090240}))
          duration: -1
236858 2024-12-10 17:48:36.212001 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n98,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3121:<213x5120xbf16>{5120, 1}+30090240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3121:<213x5120xbf16>{5120,1}+30090240}))
          duration: -1
236877 2024-12-10 17:48:36.212731 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n122,rank6)
        - aten::mm:
          inputs: (%3121:<213x5120xbf16>{5120, 1}+30090240, %3255:<5120x3072xbf16>{1, 5120})
          outputs: (%1574:<213x3072xbf16>{3072,1})
          duration: -1
236966 2024-12-10 17:48:36.216485 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n122,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3121:<213x5120xbf16>{5120, 1}+30090240})
          outputs: (%3196:tuple{%1574:<213x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<213x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<213x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<213x1536xbf16>{3072, 1})
          outputs: (%3255:<213x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<213x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3255:<213x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3255:<213x1536xbf16>{1536, 1}, %2940:<213x1536xbf16>{3072, 1}+1536)
          outputs: (%3208:<213x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3208:<213x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3208:<213x1536xbf16>{1536,1}}))
          duration: -1
        - aten::mm:
          inputs: (%3208:<213x1536xbf16>{1536, 1}, %3259:<1536x5120xbf16>{1, 1536})
          outputs: (%3240:<213x5120xbf16>{5120,1})
          duration: -1
237150 2024-12-10 17:48:36.230128 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n103,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3208:<213x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3240:<213x5120xbf16>{5120,1},None:NoneType})
          duration: -1
237164 2024-12-10 17:48:36.230883 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n98,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3121:<213x5120xbf16>{5120, 1}+30090240})
          outputs: (%3184:tuple{%3240:<213x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2940:<213x5120xbf16>{5120, 1}+30090240, %3240:<213x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2940:<213x5120xbf16>{5120,1}+30090240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%237:<95x5120xbf16>{5120, 1}+31180800})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%237:<95x5120xbf16>{5120,1}+31180800}))
          duration: -1
237339 2024-12-10 17:48:36.241159 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n99,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%237:<95x5120xbf16>{5120, 1}+31180800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%237:<95x5120xbf16>{5120,1}+31180800}))
          duration: -1
237359 2024-12-10 17:48:36.241903 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n123,rank6)
        - aten::mm:
          inputs: (%237:<95x5120xbf16>{5120, 1}+31180800, %3258:<5120x3072xbf16>{1, 5120})
          outputs: (%3078:<95x3072xbf16>{3072,1})
          duration: -1
237447 2024-12-10 17:48:36.245708 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n123,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%237:<95x5120xbf16>{5120, 1}+31180800})
          outputs: (%3179:tuple{%3078:<95x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1612:<95x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1612:<95x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1612:<95x1536xbf16>{3072, 1})
          outputs: (%3255:<95x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1612:<95x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3255:<95x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3255:<95x1536xbf16>{1536, 1}, %3214:<95x1536xbf16>{3072, 1}+1536)
          outputs: (%3258:<95x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3258:<95x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3258:<95x1536xbf16>{1536,1}}))
          duration: -1
237554 2024-12-10 17:48:36.255610 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n104,rank6)
        - aten::mm:
          inputs: (%3258:<95x1536xbf16>{1536, 1}, %3222:<1536x5120xbf16>{1, 1536})
          outputs: (%3255:<95x5120xbf16>{5120,1})
          duration: -1
237629 2024-12-10 17:48:36.259402 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n104,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3258:<95x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3255:<95x5120xbf16>{5120,1},None:NoneType})
          duration: -1
237645 2024-12-10 17:48:36.260164 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n99,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%237:<95x5120xbf16>{5120, 1}+31180800})
          outputs: (%3184:tuple{%3255:<95x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3078:<95x5120xbf16>{5120, 1}+31180800, %3255:<95x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3078:<95x5120xbf16>{5120,1}+31180800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%2940:<225x5120xbf16>{5120, 1}+31667200})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%2940:<225x5120xbf16>{5120,1}+31667200}))
          duration: -1
237820 2024-12-10 17:48:36.270486 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n100,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%2940:<225x5120xbf16>{5120, 1}+31667200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%2940:<225x5120xbf16>{5120,1}+31667200}))
          duration: -1
237841 2024-12-10 17:48:36.271221 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n124,rank6)
        - aten::mm:
          inputs: (%2940:<225x5120xbf16>{5120, 1}+31667200, %3264:<5120x3072xbf16>{1, 5120})
          outputs: (%3244:<225x3072xbf16>{3072,1})
          duration: -1
237925 2024-12-10 17:48:36.275010 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n124,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%2940:<225x5120xbf16>{5120, 1}+31667200})
          outputs: (%3120:tuple{%3244:<225x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2914:<225x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2914:<225x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2914:<225x1536xbf16>{3072, 1})
          outputs: (%3208:<225x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2914:<225x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3208:<225x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3208:<225x1536xbf16>{1536, 1}, %3205:<225x1536xbf16>{3072, 1}+1536)
          outputs: (%3214:<225x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3214:<225x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3214:<225x1536xbf16>{1536,1}}))
          duration: -1
238036 2024-12-10 17:48:36.284874 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n105,rank6)
        - aten::mm:
          inputs: (%3214:<225x1536xbf16>{1536, 1}, %3261:<1536x5120xbf16>{1, 1536})
          outputs: (%3067:<225x5120xbf16>{5120,1})
          duration: -1
238110 2024-12-10 17:48:36.288638 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n105,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3214:<225x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3067:<225x5120xbf16>{5120,1},None:NoneType})
          duration: -1
238124 2024-12-10 17:48:36.289410 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n100,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%2940:<225x5120xbf16>{5120, 1}+31667200})
          outputs: (%3184:tuple{%3067:<225x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%237:<225x5120xbf16>{5120, 1}+31667200, %3067:<225x5120xbf16>{5120, 1}, False:bool)
          outputs: (%237:<225x5120xbf16>{5120,1}+31667200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3121:<586x5120xbf16>{5120, 1}+32819200})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3121:<586x5120xbf16>{5120,1}+32819200}))
          duration: -1
238302 2024-12-10 17:48:36.299666 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n101,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3121:<586x5120xbf16>{5120, 1}+32819200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3121:<586x5120xbf16>{5120,1}+32819200}))
          duration: -1
238319 2024-12-10 17:48:36.300396 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n125,rank6)
        - aten::mm:
          inputs: (%3121:<586x5120xbf16>{5120, 1}+32819200, %3236:<5120x3072xbf16>{1, 5120})
          outputs: (%1612:<586x3072xbf16>{3072,1})
          duration: -1
238404 2024-12-10 17:48:36.304184 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n125,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3121:<586x5120xbf16>{5120, 1}+32819200})
          outputs: (%3196:tuple{%1612:<586x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3204:<586x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3204:<586x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3204:<586x1536xbf16>{3072, 1})
          outputs: (%3236:<586x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3204:<586x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3236:<586x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3236:<586x1536xbf16>{1536, 1}, %2940:<586x1536xbf16>{3072, 1}+1536)
          outputs: (%3214:<586x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3214:<586x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3214:<586x1536xbf16>{1536,1}}))
          duration: -1
238516 2024-12-10 17:48:36.314053 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n106,rank6)
        - aten::mm:
          inputs: (%3214:<586x1536xbf16>{1536, 1}, %3255:<1536x5120xbf16>{1, 1536})
          outputs: (%3261:<586x5120xbf16>{5120,1})
          duration: -1
238589 2024-12-10 17:48:36.317821 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n106,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3214:<586x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3261:<586x5120xbf16>{5120,1},None:NoneType})
          duration: -1
238604 2024-12-10 17:48:36.318583 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n101,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3121:<586x5120xbf16>{5120, 1}+32819200})
          outputs: (%3184:tuple{%3261:<586x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2940:<586x5120xbf16>{5120, 1}+32819200, %3261:<586x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2940:<586x5120xbf16>{5120,1}+32819200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%3008:<808x5120xbf16>{5120, 1}+35819520})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%3008:<808x5120xbf16>{5120,1}+35819520}))
          duration: -1
238781 2024-12-10 17:48:36.328813 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n102,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%3008:<808x5120xbf16>{5120, 1}+35819520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%3008:<808x5120xbf16>{5120,1}+35819520}))
          duration: -1
238799 2024-12-10 17:48:36.329567 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n126,rank6)
        - aten::mm:
          inputs: (%3008:<808x5120xbf16>{5120, 1}+35819520, %3255:<5120x3072xbf16>{1, 5120})
          outputs: (%1612:<808x3072xbf16>{3072,1})
          duration: -1
238882 2024-12-10 17:48:36.333356 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n126,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%3008:<808x5120xbf16>{5120, 1}+35819520})
          outputs: (%3179:tuple{%1612:<808x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3205:<808x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3205:<808x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3205:<808x1536xbf16>{3072, 1})
          outputs: (%3255:<808x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3205:<808x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3255:<808x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3255:<808x1536xbf16>{1536, 1}, %3121:<808x1536xbf16>{3072, 1}+1536)
          outputs: (%3214:<808x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3214:<808x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3214:<808x1536xbf16>{1536,1}}))
          duration: -1
238996 2024-12-10 17:48:36.343211 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n107,rank6)
        - aten::mm:
          inputs: (%3214:<808x1536xbf16>{1536, 1}, %3240:<1536x5120xbf16>{1, 1536})
          outputs: (%3236:<808x5120xbf16>{5120,1})
          duration: -1
239067 2024-12-10 17:48:36.346990 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n107,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3214:<808x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3236:<808x5120xbf16>{5120,1},None:NoneType})
          duration: -1
239085 2024-12-10 17:48:36.347747 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n102,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%3008:<808x5120xbf16>{5120, 1}+35819520})
          outputs: (%3184:tuple{%3236:<808x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3121:<808x5120xbf16>{5120, 1}+35819520, %3236:<808x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3121:<808x5120xbf16>{5120,1}+35819520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%1600:<65x5120xbf16>{5120, 1}+39956480})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%1600:<65x5120xbf16>{5120,1}+39956480}))
          duration: -1
239261 2024-12-10 17:48:36.357962 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n103,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%1600:<65x5120xbf16>{5120, 1}+39956480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%1600:<65x5120xbf16>{5120,1}+39956480}))
          duration: -1
239277 2024-12-10 17:48:36.358688 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n127,rank6)
        - aten::mm:
          inputs: (%1600:<65x5120xbf16>{5120, 1}+39956480, %3240:<5120x3072xbf16>{1, 5120})
          outputs: (%3281:<65x3072xbf16>{3072,1})
          duration: -1
239362 2024-12-10 17:48:36.362476 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n127,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%1600:<65x5120xbf16>{5120, 1}+39956480})
          outputs: (%3120:tuple{%3281:<65x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3214:<65x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3214:<65x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3214:<65x1536xbf16>{3072, 1})
          outputs: (%3240:<65x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3214:<65x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3240:<65x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3240:<65x1536xbf16>{1536, 1}, %3008:<65x1536xbf16>{3072, 1}+1536)
          outputs: (%3208:<65x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3208:<65x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3208:<65x1536xbf16>{1536,1}}))
          duration: -1
239476 2024-12-10 17:48:36.372353 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n108,rank6)
        - aten::mm:
          inputs: (%3208:<65x1536xbf16>{1536, 1}, %3261:<1536x5120xbf16>{1, 1536})
          outputs: (%3240:<65x5120xbf16>{5120,1})
          duration: -1
239546 2024-12-10 17:48:36.376164 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n108,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3208:<65x1536xbf16>{1536, 1}})
          outputs: (%3184:tuple{%3240:<65x5120xbf16>{5120,1},None:NoneType})
          duration: -1
239565 2024-12-10 17:48:36.376927 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n103,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%1600:<65x5120xbf16>{5120, 1}+39956480})
          outputs: (%3184:tuple{%3240:<65x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2914:<65x5120xbf16>{5120, 1}+39956480, %3240:<65x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2914:<65x5120xbf16>{5120,1}+39956480)
          duration: -1
239650 2024-12-10 17:48:36.381855 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n4,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%3190:tuple{%3176:<7869x5120xbf16>{5120, 1}, %3193:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3284:tuple{%3170:<7869x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%2066:<7869x5120xbf16>{5120, 1}, 0:int, %3192:<7869x5120xCUSTOM_DATA_TYPE>{1, 0}, %3170:<7869x5120xbf16>{5120, 1})
          outputs: (%3121:<7869x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%3121:<7869x5120xbf16>{5120, 1}, %1600:<7869x1xbf16>{1, 1})
          outputs: (%2914:<7869x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%3180:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3167:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%3167:<8192x5120xbf16>{5120, 1}, 0:int, %3189:<7869x5120xCUSTOM_DATA_TYPE>{1, 0}, %2914:<7869x5120xbf16>{5120, 1})
          outputs: (%3285:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%3141:<1024x5120xbf16>{5120, 1}, %3285:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%3141:<1024x5120xbf16>{5120,1},%3285:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%3141:<1024x5120xbf16>{5120, 1}, %3285:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%3141:<1024x5120xbf16>{5120,1},%3285:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%3141:<1024x5120xbf16>{5120, 1}, %3285:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%3183:tuple{%3141:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3160:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%3160:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
239900 2024-12-10 17:48:36.423093 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n104,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3195:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3195:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
239924 2024-12-10 17:48:36.423830 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n128,rank6)
        - aten::mm:
          inputs: (%2914:<1024x5120xbf16>{5120, 1}, %2940:<5120x6144xbf16>{1, 5120})
          outputs: (%3121:<1024x6144xbf16>{6144,1})
          duration: -1
240075 2024-12-10 17:48:36.431637 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n128,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3195:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3184:tuple{%3174:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3285:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3285:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3285:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%3186:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3285:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%3186:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%3186:<1024x1x3072xbf16>{3072, 3072, 1}, %2940:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%3167:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3177:tuple{%3167:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3177:tuple{%3167:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
240179 2024-12-10 17:48:36.441530 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n109,rank6)
        - aten::mm:
          inputs: (%3121:<1024x3072xbf16>{3072, 1}, %3214:<3072x5120xbf16>{1, 3072})
          outputs: (%2914:<1024x5120xbf16>{5120,1})
          duration: -1
240330 2024-12-10 17:48:36.449302 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n109,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3177:tuple{%3167:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%3286:tuple{%3121:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
240346 2024-12-10 17:48:36.450056 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n104,rank6)
        - ----------->api::MLP return:
          inputs: (%3160:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3286:tuple{%3121:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%3169:<1024x1x5120xbf16>{5120, 5120, 1}, %434:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2914:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
240394 2024-12-10 17:48:36.453179 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n4,rank6)
        - ----------->api::MoELayer return:
          inputs: (%3061:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3286:tuple{%2914:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3296:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3296:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3297:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3297:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55baf77e70_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55baf77e70_:_InferenceMode)
          duration: -1
242479 2024-12-10 17:48:37.050610 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n4,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%3183:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3061:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3061:tuple{%434:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
242553 2024-12-10 17:48:37.057396 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n14,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%434:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%434:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3299:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3299:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3299:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3299:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3301:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %237:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3301:tuple{%434:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%237:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3299:tuple{%434:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%2939:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%434:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%2939:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
242746 2024-12-10 17:48:37.101473 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n14,rank6)
242748 2024-12-10 17:48:37.101991 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n4,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%2939:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3303:tuple{%2939:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%3303:tuple{%2939:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
242774 2024-12-10 17:48:37.108251 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n129,rank6)
        - aten::mm:
          inputs: (%3163:<1024x5120xbf16>{5120, 1}, %3307:<5120x102400xbf16>{1, 5120})
          outputs: (%3166:<1024x102400xbf16>{102400,1})
          duration: -1
242917 2024-12-10 17:48:37.120548 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n129,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3303:tuple{%2939:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%3311:tuple{%3078:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%2584:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%3110:tuple{%3307:<1024x1xf32>{1,1},%3312:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3307:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3307:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3313:list{%3307:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3314:tuple{%3315:list{%3307:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%2584:<1024x1x102400xf32>{102400, 102400, 1}, %3108:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%2584:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%1498:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3163:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%1498:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3316:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3163:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3316:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3317:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3318:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%3319:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%3319:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3320:list{%3317:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3163:<i32>, False:bool)
          outputs: (%3319:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1695:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3321:<1024x102400xf32>{102400, 1}, %3315:list{%1695:<1024xCUSTOM_DATA_TYPE>{1}, %1979:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1238:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%3121:<1024x1xf32>{1, 1}, %3315:list{%3317:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1238:<i32>, False:bool)
          outputs: (%3121:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%2584:<1024x1x102400xf32>{102400, 102400, 1}, out=%2584:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%2584:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%2584:<1024x1x102400xf32>{102400, 102400, 1}, %3320:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3323:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3121:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3121:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3320:list{%3121:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3311:tuple{%3308:list{%3121:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3323:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3323:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3226:list{%3323:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3324:tuple{%2589:list{%3323:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%3323:<1024x1xf32>{1, 1})
          outputs: (%1162:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1162:<1024x1xf32>{1, 1}, %3121:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3166:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%2584:<1024x1x102400xf32>{102400, 102400, 1}, %1238:<1024x1x1xf32>{1, 1, 1})
          outputs: (%2584:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%3325:tuple{%2584:<1024x1x102400xf32>{102400, 102400, 1}, %3317:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %1979:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%3325:tuple{%2584:<1024x1x102400xf32>{102400,102400,1},%3317:<1024x1xCUSTOM_DATA_TYPE>{1,1},%1979:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2684:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device='cuda_6')_:dict)
          outputs: (%3317:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3317:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3317:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3317:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device='cuda_6')_:dict)
          outputs: (%3317:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%2068:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3096:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3080:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[__372,___838,_84914,_____,_16304,___317,__9697]],_device='cuda_6')_:dict)
          outputs: (%3317:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<1024xf32>{1}, %3095:<1024xf32>{1})
          outputs: (%1574:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%1574:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3096:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%3095:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3080:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3096:<i32>, %3080:<i32>)
          outputs: (%3319:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%3319:<i32>)
          outputs: (%3078:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%3327:list{%3072:<1xf32>{1}}, 0:int)
          outputs: (%452:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%452:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%452:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3328:list{%452:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3196:tuple{%3329:list{%452:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%452:<1xf32>{1}, 8:int)
          outputs: (%3330:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3319:<i32>, 1:int)
          outputs: (%3109:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%3109:<i32>, 1:int)
          outputs: (%3109:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%3331:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3096:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%3096:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%3096:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %3096:<1xf32>{1})
          outputs: (%3072:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%3072:<1xf32>{1}, 1:int)
          outputs: (%3322:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%2852:<i32>, 0:int, alpha=1:int)
          outputs: (%2852:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2475:<i32>, %2914:<i32>, alpha=1:int)
          outputs: (%2475:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3323:<i32>, %2475:<i32>, False:bool)
          outputs: (%3323:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3169:<i32>, 1:int, alpha=1:int)
          outputs: (%3169:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3121:<i32>, %3169:<i32>, False:bool)
          outputs: (%3121:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%3329:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%3169:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%408:<1024xf32>{1}, %431:list{%428:<1024xCUSTOM_DATA_TYPE>{1}}, %427:<i32>, False:bool)
          outputs: (%408:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%430:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%430:<1024xCUSTOM_DATA_TYPE>{1}, %431:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %427:<i32>, False:bool)
          outputs: (%430:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%419:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%3337:list{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3314:tuple{%3338:list{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::index_put_:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1, %431:list{%419:<1024xCUSTOM_DATA_TYPE>{1}}, %427:<i32>, False:bool)
          outputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1539:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1539:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3339:list{%1539:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3340:tuple{%3329:list{%1539:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3109:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3109:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3337:list{%3109:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3324:tuple{%3315:list{%3109:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3339:list{%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3314:tuple{%2918:list{%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2941:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3341:list{%2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3340:tuple{%3338:list{%2941:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2941:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2941:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2941:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2859:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2859:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2941:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__418,_32169,___410,_____,____30,___185,__4044]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[__418,_32169,___410,_____,____30,___185,__4044]],_device)
          duration: -1
244960 2024-12-10 17:48:37.313201 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n5,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%3303:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%3303:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
244992 2024-12-10 17:48:37.314160 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n5,rank6)
        - ----------->api::embedding call:
          inputs: (%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%3336:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%3336:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
245080 2024-12-10 17:48:37.328961 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n5,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%3303:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3332:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%3303:tuple{%3334:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%3303:tuple{%3334:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
245128 2024-12-10 17:48:37.331036 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n5,rank6)
        - ----------->api::dropout call:
          inputs: (%3334:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3334:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3334:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3334:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
245160 2024-12-10 17:48:37.337292 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n5,rank6)
        - ----------->api::Dropout return:
          inputs: (%3303:tuple{%3334:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3334:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
245176 2024-12-10 17:48:37.338004 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n5,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__418,_32169,___410,_____,____30,___185,__4044]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%3334:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%3303:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%3303:tuple{1024:int}))
          duration: -1
245208 2024-12-10 17:48:37.339626 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n5,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3078:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%3078:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%3095:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3096:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%433:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%3200:list{%433:<1024x20xf32>{20, 1}, %433:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%3332:<1024x40xf32>{40,1})
          duration: -1
245360 2024-12-10 17:48:37.350618 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n5,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%3303:tuple{1024:int})
          outputs: (%1385:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
245388 2024-12-10 17:48:37.356961 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n5,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
245466 2024-12-10 17:48:37.365971 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n5,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3303:tuple{%3095:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3303:tuple{%3095:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
245490 2024-12-10 17:48:37.366752 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n15,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3095:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3095:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3095:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3095:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3299:tuple{%3095:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3299:tuple{%3095:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3299:tuple{%3095:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3299:tuple{%3095:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - aten::stack:
          inputs: (%435:list{%430:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%434:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%434:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3102:tuple{%3095:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3347:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3102:tuple{%3095:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3347:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3095:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3299:tuple{%3095:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1162:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3095:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%1162:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
245733 2024-12-10 17:48:37.410861 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n15,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%3303:tuple{%1162:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%3303:tuple{%1162:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
245752 2024-12-10 17:48:37.415132 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n5,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3348:tuple{%1162:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3348:tuple{%1162:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
245774 2024-12-10 17:48:37.415898 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n130,rank6)
        - aten::mm:
          inputs: (%3261:<1024x5120xbf16>{5120, 1}, %3264:<5120x1536xbf16>{1, 5120})
          outputs: (%3351:<1024x1536xbf16>{1536,1})
          duration: -1
245932 2024-12-10 17:48:37.423900 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n130,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3348:tuple{%1162:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3354:tuple{%3352:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3348:tuple{%3352:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3348:tuple{%3352:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
245957 2024-12-10 17:48:37.424934 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n131,rank6)
        - aten::mm:
          inputs: (%3356:<1024x1536xbf16>{1536, 1}, %3351:<1536x24576xbf16>{1, 1536})
          outputs: (%3357:<1024x24576xbf16>{24576,1})
          duration: -1
246076 2024-12-10 17:48:37.431050 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n131,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3348:tuple{%3352:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%3360:tuple{%3108:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%1695:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3355:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%1695:<1024x1x128x192xbf16>{24576,24576,192,1},%3355:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%1695:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3355:list{128:int, 64:int}, -1:int)
          outputs: (%3311:tuple{%1238:<1024x1x128x128xbf16>{24576,24576,192,1},%433:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3348:tuple{%1162:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3348:tuple{%1162:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
246158 2024-12-10 17:48:37.440702 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n132,rank6)
        - aten::mm:
          inputs: (%3281:<1024x5120xbf16>{5120, 1}, %3241:<5120x576xbf16>{1, 5120})
          outputs: (%3365:<1024x576xbf16>{576,1})
          duration: -1
246316 2024-12-10 17:48:37.448598 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n132,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3348:tuple{%1162:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3311:tuple{%3366:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3366:<1024x1x576xbf16>{576, 576, 1}, %3368:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3366:<1024x1x576xbf16>{576,576,1},%3368:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3366:<1024x1x576xbf16>{576, 576, 1}, %3368:list{512:int, 64:int}, -1:int)
          outputs: (%3360:tuple{%1979:<1024x1x512xbf16>{576,576,1},%3352:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3348:tuple{%1979:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3348:tuple{%1979:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
246382 2024-12-10 17:48:37.456894 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n133,rank6)
        - aten::mm:
          inputs: (%3281:<1024x512xbf16>{576, 1}, %3369:<512x32768xbf16>{1, 512})
          outputs: (%3371:<1024x32768xbf16>{32768,1})
          duration: -1
246499 2024-12-10 17:48:37.463044 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n133,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3348:tuple{%1979:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%3311:tuple{%3255:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3351:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3370:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%3351:<1024x1x128x256xbf16>{32768,32768,256,1},%3370:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3351:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3370:list{128:int, 128:int}, -1:int)
          outputs: (%3354:tuple{%3375:<1024x1x128x128xbf16>{32768,32768,256,1},%3281:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%3342:tuple{%3369:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%3342:tuple{%3369:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
246614 2024-12-10 17:48:37.474993 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n5,rank6)
246658 2024-12-10 17:48:37.478123 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n5,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%3342:tuple{%3369:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%3324:tuple{%3281:<1024x64xbf16>{64,1},%3347:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%3281:<1024x64xbf16>{64, 1}, %3378:list{%2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3379:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%3347:<1024x64xbf16>{64, 1}, %3374:list{%2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3380:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3383:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3241:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3261:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3387:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%3388:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%3378:list{%3388:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %3386:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%3389:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3389:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %550:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3386:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3261:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3386:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%3388:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3382:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3241:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3386:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3008:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%3389:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::stack:
          inputs: (%437:list{%369:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%436:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::cat:
          inputs: (%3378:list{%3389:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %3387:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%3390:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3390:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %550:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3387:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3386:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3387:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%3008:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%3387:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3241:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%3387:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3387:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3388:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%3387:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%3386:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3387:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%3386:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3371:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3391:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%3371:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%3387:<128x1024x192xbf16>{196608, 192, 1}, %3108:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%1695:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%1979:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%1695:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%3108:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%1498:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%3108:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %3378:list{%1498:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3387:<i32>, False:bool)
          outputs: (%3108:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%1695:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %3108:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%3387:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%3387:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%3387:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%3371:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%3392:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%3387:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %3392:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%3392:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%3366:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3366:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3366:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3366:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%3387:<128x1024x1024xbf16>{1048576, 1024, 1}, %3386:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%3371:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3348:tuple{%3371:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3348:tuple{%3371:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
248150 2024-12-10 17:48:37.596234 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n110,rank6)
        - aten::mm:
          inputs: (%3008:<1024x16384xbf16>{16384, 1}, %3394:<16384x5120xbf16>{1, 16384})
          outputs: (%3241:<1024x5120xbf16>{5120,1})
          duration: -1
248307 2024-12-10 17:48:37.604487 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n110,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3348:tuple{%3371:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%3286:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
248321 2024-12-10 17:48:37.605280 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n5,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%3303:tuple{%1162:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3286:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3398:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3398:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3399:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3399:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55dc424630_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55dc424630_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3401:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3401:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
250835 2024-12-10 17:48:38.157296 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n16,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1498:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1498:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1498:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1498:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3402:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3402:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3402:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3402:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3405:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %3108:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3405:tuple{%1498:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%3108:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1498:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3402:tuple{%1498:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1979:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1498:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1979:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
251051 2024-12-10 17:48:38.201837 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n16,rank6)
        - ----------->api::MoELayer call:
          inputs: (%3401:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%3401:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
251061 2024-12-10 17:48:38.202590 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n5,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%3407:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%3407:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
251076 2024-12-10 17:48:38.203345 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n5,rank6)
        - aten::mm:
          inputs: (%3312:<1024x5120xbf16>{5120, 1}, %3408:<5120x160xbf16>{1, 5120})
          outputs: (%3166:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%2055:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%2848:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%3411:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%3412:tuple{%3317:<1024x6xbf16>{6,1},%2055:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%3413:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%3414:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%3415:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%3416:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%3416:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %3357:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%3416:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%3416:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %3415:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3417:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3414:<1024x160xf32>{160, 1}, %3415:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%3418:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3418:<160xf32>{1}, %3417:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%3416:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3416:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%3419:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%3419:<i32>, 2_5431315104166666e-07:float)
          outputs: (%3420:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3420:<i32>, 0_01:float)
          outputs: (%3416:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3421:<i32>, %3423:<i32>, alpha=1:int)
          outputs: (%3421:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3424:<i32>, %3421:<i32>, False:bool)
          outputs: (%3424:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%3425:tuple{%3420:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%3425:tuple{%3420:<i32>}))
          duration: -1
251641 2024-12-10 17:48:38.238715 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n5,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%3407:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3427:tuple{%3426:<1024x6xbf16>{6,1},%2055:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3420:<8192x5120xbf16>{5120, 1}, %3164:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3420:<8192x5120xbf16>{5120,1},%3164:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3420:<8192x5120xbf16>{5120, 1}, %3164:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3429:tuple{%3420:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3416:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2055:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3416:<8192x6xCUSTOM_DATA_TYPE>{6,1},%2055:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3416:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2055:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3412:tuple{%3416:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%3416:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%1238:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%3416:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%3357:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%1238:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3357:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3108:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%3416:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3108:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3414:<7248xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3433:<8192x6xbf16>{6, 1}, %3426:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3433:<8192x6xbf16>{6,1},%3426:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3433:<8192x6xbf16>{6, 1}, %3426:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3434:tuple{%3433:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%3433:<8192x6xbf16>{6, 1}, %3108:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3436:<7248xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%3108:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1238:<7248x2xCUSTOM_DATA_TYPE>{1,7248})
          duration: -1
        - aten::gather:
          inputs: (%3420:<8192x5120xbf16>{5120, 1}, 0:int, %3421:<7248x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3439:<7248x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%3414:<7248xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%3440:tuple{%840:<7248xCUSTOM_DATA_TYPE>{1},%771:<7248xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%3441:<7248xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%840:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%3439:<7248x5120xbf16>{5120, 1}, 0:int, %3444:<7248x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%840:<7248x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%3440:tuple{%840:<7248x5120xbf16>{5120, 1}, %3443:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%3440:tuple{%840:<7248x5120xbf16>{5120,1},%3443:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
252396 2024-12-10 17:48:38.347537 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n5,rank6)
        - aten::cumsum:
          inputs: (%3443:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%3164:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%3438:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%3108:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%3438:list{%3108:<1xCUSTOM_DATA_TYPE>{1}, %3164:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%3416:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<341x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<341x5120xbf16>{5120,1}}))
          duration: -1
252562 2024-12-10 17:48:38.358448 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n105,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<341x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<341x5120xbf16>{5120,1}}))
          duration: -1
252586 2024-12-10 17:48:38.359189 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n134,rank6)
        - aten::mm:
          inputs: (%3433:<341x5120xbf16>{5120, 1}, %3447:<5120x3072xbf16>{1, 5120})
          outputs: (%3448:<341x3072xbf16>{3072,1})
          duration: -1
252670 2024-12-10 17:48:38.363094 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n134,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<341x5120xbf16>{5120, 1}})
          outputs: (%3427:tuple{%3448:<341x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3451:<341x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3451:<341x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3451:<341x1536xbf16>{3072, 1})
          outputs: (%3453:<341x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3451:<341x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3453:<341x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3453:<341x1536xbf16>{1536, 1}, %3452:<341x1536xbf16>{3072, 1}+1536)
          outputs: (%3189:<341x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3189:<341x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3189:<341x1536xbf16>{1536,1}}))
          duration: -1
252771 2024-12-10 17:48:38.373099 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n111,rank6)
        - aten::mm:
          inputs: (%3189:<341x1536xbf16>{1536, 1}, %3454:<1536x5120xbf16>{1, 1536})
          outputs: (%3447:<341x5120xbf16>{5120,1})
          duration: -1
252854 2024-12-10 17:48:38.376899 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n111,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3189:<341x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3447:<341x5120xbf16>{5120,1},None:NoneType})
          duration: -1
252869 2024-12-10 17:48:38.377696 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n105,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<341x5120xbf16>{5120, 1}})
          outputs: (%3432:tuple{%3447:<341x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3189:<341x5120xbf16>{5120, 1}, %3447:<341x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3189:<341x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3441:<978x5120xbf16>{5120, 1}+1745920})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3441:<978x5120xbf16>{5120,1}+1745920}))
          duration: -1
253043 2024-12-10 17:48:38.387986 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n106,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3441:<978x5120xbf16>{5120, 1}+1745920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3441:<978x5120xbf16>{5120,1}+1745920}))
          duration: -1
253067 2024-12-10 17:48:38.388721 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n135,rank6)
        - aten::mm:
          inputs: (%3441:<978x5120xbf16>{5120, 1}+1745920, %3457:<5120x3072xbf16>{1, 5120})
          outputs: (%3454:<978x3072xbf16>{3072,1})
          duration: -1
253150 2024-12-10 17:48:38.392564 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n135,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3441:<978x5120xbf16>{5120, 1}+1745920})
          outputs: (%3324:tuple{%3454:<978x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3452:<978x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3452:<978x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3452:<978x1536xbf16>{3072, 1})
          outputs: (%3460:<978x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3452:<978x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3460:<978x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3460:<978x1536xbf16>{1536, 1}, %3457:<978x1536xbf16>{3072, 1}+1536)
          outputs: (%3461:<978x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3461:<978x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3461:<978x1536xbf16>{1536,1}}))
          duration: -1
253252 2024-12-10 17:48:38.402539 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n112,rank6)
        - aten::mm:
          inputs: (%3461:<978x1536xbf16>{1536, 1}, %3453:<1536x5120xbf16>{1, 1536})
          outputs: (%3462:<978x5120xbf16>{5120,1})
          duration: -1
253334 2024-12-10 17:48:38.406382 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n112,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3461:<978x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3462:<978x5120xbf16>{5120,1},None:NoneType})
          duration: -1
253349 2024-12-10 17:48:38.407148 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n106,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3441:<978x5120xbf16>{5120, 1}+1745920})
          outputs: (%3432:tuple{%3462:<978x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3464:<978x5120xbf16>{5120, 1}+1745920, %3462:<978x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3464:<978x5120xbf16>{5120,1}+1745920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<541x5120xbf16>{5120, 1}+6753280})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<541x5120xbf16>{5120,1}+6753280}))
          duration: -1
253521 2024-12-10 17:48:38.417506 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n107,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<541x5120xbf16>{5120, 1}+6753280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<541x5120xbf16>{5120,1}+6753280}))
          duration: -1
253546 2024-12-10 17:48:38.418236 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n136,rank6)
        - aten::mm:
          inputs: (%3433:<541x5120xbf16>{5120, 1}+6753280, %3460:<5120x3072xbf16>{1, 5120})
          outputs: (%3465:<541x3072xbf16>{3072,1})
          duration: -1
253629 2024-12-10 17:48:38.422103 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n136,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<541x5120xbf16>{5120, 1}+6753280})
          outputs: (%3446:tuple{%3465:<541x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3441:<541x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3441:<541x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3441:<541x1536xbf16>{3072, 1})
          outputs: (%3456:<541x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3441:<541x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3456:<541x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3456:<541x1536xbf16>{1536, 1}, %3447:<541x1536xbf16>{3072, 1}+1536)
          outputs: (%3460:<541x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3460:<541x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3460:<541x1536xbf16>{1536,1}}))
          duration: -1
253730 2024-12-10 17:48:38.432109 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n113,rank6)
        - aten::mm:
          inputs: (%3460:<541x1536xbf16>{1536, 1}, %3453:<1536x5120xbf16>{1, 1536})
          outputs: (%3461:<541x5120xbf16>{5120,1})
          duration: -1
253815 2024-12-10 17:48:38.435926 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n113,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3460:<541x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3461:<541x5120xbf16>{5120,1},None:NoneType})
          duration: -1
253830 2024-12-10 17:48:38.436690 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n107,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<541x5120xbf16>{5120, 1}+6753280})
          outputs: (%3432:tuple{%3461:<541x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3067:<541x5120xbf16>{5120, 1}+6753280, %3461:<541x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3067:<541x5120xbf16>{5120,1}+6753280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3465:<372x5120xbf16>{5120, 1}+9523200})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3465:<372x5120xbf16>{5120,1}+9523200}))
          duration: -1
254004 2024-12-10 17:48:38.447014 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n108,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3465:<372x5120xbf16>{5120, 1}+9523200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3465:<372x5120xbf16>{5120,1}+9523200}))
          duration: -1
254025 2024-12-10 17:48:38.447746 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n137,rank6)
        - aten::mm:
          inputs: (%3465:<372x5120xbf16>{5120, 1}+9523200, %3468:<5120x3072xbf16>{1, 5120})
          outputs: (%3469:<372x3072xbf16>{3072,1})
          duration: -1
254114 2024-12-10 17:48:38.451765 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n137,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3465:<372x5120xbf16>{5120, 1}+9523200})
          outputs: (%3427:tuple{%3469:<372x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3352:<372x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3352:<372x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3352:<372x1536xbf16>{3072, 1})
          outputs: (%3472:<372x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3352:<372x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3472:<372x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3472:<372x1536xbf16>{1536, 1}, %3447:<372x1536xbf16>{3072, 1}+1536)
          outputs: (%3189:<372x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3189:<372x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3189:<372x1536xbf16>{1536,1}}))
          duration: -1
254218 2024-12-10 17:48:38.461770 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n114,rank6)
        - aten::mm:
          inputs: (%3189:<372x1536xbf16>{1536, 1}, %3453:<1536x5120xbf16>{1, 1536})
          outputs: (%3473:<372x5120xbf16>{5120,1})
          duration: -1
254300 2024-12-10 17:48:38.465597 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n114,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3189:<372x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3473:<372x5120xbf16>{5120,1},None:NoneType})
          duration: -1
254315 2024-12-10 17:48:38.466374 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n108,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3465:<372x5120xbf16>{5120, 1}+9523200})
          outputs: (%3432:tuple{%3473:<372x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3475:<372x5120xbf16>{5120, 1}+9523200, %3473:<372x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3475:<372x5120xbf16>{5120,1}+9523200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<24x5120xbf16>{5120, 1}+11427840})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<24x5120xbf16>{5120,1}+11427840}))
          duration: -1
254491 2024-12-10 17:48:38.476707 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n109,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<24x5120xbf16>{5120, 1}+11427840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<24x5120xbf16>{5120,1}+11427840}))
          duration: -1
254515 2024-12-10 17:48:38.477488 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n138,rank6)
        - aten::mm:
          inputs: (%3433:<24x5120xbf16>{5120, 1}+11427840, %3453:<5120x3072xbf16>{1, 5120})
          outputs: (%3476:<24x3072xbf16>{3072,1})
          duration: -1
254596 2024-12-10 17:48:38.481350 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n138,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<24x5120xbf16>{5120, 1}+11427840})
          outputs: (%3324:tuple{%3476:<24x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3453:<24x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3453:<24x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3453:<24x1536xbf16>{3072, 1})
          outputs: (%3469:<24x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3453:<24x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3469:<24x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3469:<24x1536xbf16>{1536, 1}, %3465:<24x1536xbf16>{3072, 1}+1536)
          outputs: (%3479:<24x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3479:<24x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3479:<24x1536xbf16>{1536,1}}))
          duration: -1
254699 2024-12-10 17:48:38.491302 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n115,rank6)
        - aten::mm:
          inputs: (%3479:<24x1536xbf16>{1536, 1}, %3460:<1536x5120xbf16>{1, 1536})
          outputs: (%3480:<24x5120xbf16>{5120,1})
          duration: -1
254781 2024-12-10 17:48:38.495132 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n115,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3479:<24x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3480:<24x5120xbf16>{5120,1},None:NoneType})
          duration: -1
254795 2024-12-10 17:48:38.495896 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n109,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<24x5120xbf16>{5120, 1}+11427840})
          outputs: (%3432:tuple{%3480:<24x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3439:<24x5120xbf16>{5120, 1}+11427840, %3480:<24x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3439:<24x5120xbf16>{5120,1}+11427840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3441:<274x5120xbf16>{5120, 1}+11550720})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3441:<274x5120xbf16>{5120,1}+11550720}))
          duration: -1
254969 2024-12-10 17:48:38.506178 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n110,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3441:<274x5120xbf16>{5120, 1}+11550720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3441:<274x5120xbf16>{5120,1}+11550720}))
          duration: -1
254990 2024-12-10 17:48:38.506906 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n139,rank6)
        - aten::mm:
          inputs: (%3441:<274x5120xbf16>{5120, 1}+11550720, %3483:<5120x3072xbf16>{1, 5120})
          outputs: (%3484:<274x3072xbf16>{3072,1})
          duration: -1
255075 2024-12-10 17:48:38.510738 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n139,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3441:<274x5120xbf16>{5120, 1}+11550720})
          outputs: (%3446:tuple{%3484:<274x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3352:<274x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3352:<274x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3352:<274x1536xbf16>{3072, 1})
          outputs: (%3453:<274x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3352:<274x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3453:<274x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3453:<274x1536xbf16>{1536, 1}, %3460:<274x1536xbf16>{3072, 1}+1536)
          outputs: (%3461:<274x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3461:<274x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3461:<274x1536xbf16>{1536,1}}))
          duration: -1
255177 2024-12-10 17:48:38.520702 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n116,rank6)
        - aten::mm:
          inputs: (%3461:<274x1536xbf16>{1536, 1}, %3486:<1536x5120xbf16>{1, 1536})
          outputs: (%3487:<274x5120xbf16>{5120,1})
          duration: -1
255260 2024-12-10 17:48:38.524577 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n116,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3461:<274x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3487:<274x5120xbf16>{5120,1},None:NoneType})
          duration: -1
255275 2024-12-10 17:48:38.525357 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n110,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3441:<274x5120xbf16>{5120, 1}+11550720})
          outputs: (%3432:tuple{%3487:<274x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3464:<274x5120xbf16>{5120, 1}+11550720, %3487:<274x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3464:<274x5120xbf16>{5120,1}+11550720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<71x5120xbf16>{5120, 1}+12953600})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<71x5120xbf16>{5120,1}+12953600}))
          duration: -1
255452 2024-12-10 17:48:38.535693 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n111,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<71x5120xbf16>{5120, 1}+12953600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<71x5120xbf16>{5120,1}+12953600}))
          duration: -1
255475 2024-12-10 17:48:38.536438 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n140,rank6)
        - aten::mm:
          inputs: (%3433:<71x5120xbf16>{5120, 1}+12953600, %3456:<5120x3072xbf16>{1, 5120})
          outputs: (%3489:<71x3072xbf16>{3072,1})
          duration: -1
255554 2024-12-10 17:48:38.540255 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n140,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<71x5120xbf16>{5120, 1}+12953600})
          outputs: (%3427:tuple{%3489:<71x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3456:<71x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3456:<71x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3456:<71x1536xbf16>{3072, 1})
          outputs: (%3484:<71x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3456:<71x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3484:<71x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3484:<71x1536xbf16>{1536, 1}, %3461:<71x1536xbf16>{3072, 1}+1536)
          outputs: (%3482:<71x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3482:<71x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3482:<71x1536xbf16>{1536,1}}))
          duration: -1
255661 2024-12-10 17:48:38.550209 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n117,rank6)
        - aten::mm:
          inputs: (%3482:<71x1536xbf16>{1536, 1}, %3480:<1536x5120xbf16>{1, 1536})
          outputs: (%3483:<71x5120xbf16>{5120,1})
          duration: -1
255740 2024-12-10 17:48:38.554042 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n117,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3482:<71x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3483:<71x5120xbf16>{5120,1},None:NoneType})
          duration: -1
255756 2024-12-10 17:48:38.554810 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n111,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<71x5120xbf16>{5120, 1}+12953600})
          outputs: (%3432:tuple{%3483:<71x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3352:<71x5120xbf16>{5120, 1}+12953600, %3483:<71x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3352:<71x5120xbf16>{5120,1}+12953600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3067:<873x5120xbf16>{5120, 1}+13317120})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3067:<873x5120xbf16>{5120,1}+13317120}))
          duration: -1
255934 2024-12-10 17:48:38.565110 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n112,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3067:<873x5120xbf16>{5120, 1}+13317120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3067:<873x5120xbf16>{5120,1}+13317120}))
          duration: -1
255955 2024-12-10 17:48:38.565851 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n141,rank6)
        - aten::mm:
          inputs: (%3067:<873x5120xbf16>{5120, 1}+13317120, %3493:<5120x3072xbf16>{1, 5120})
          outputs: (%3489:<873x3072xbf16>{3072,1})
          duration: -1
256035 2024-12-10 17:48:38.569677 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n141,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3067:<873x5120xbf16>{5120, 1}+13317120})
          outputs: (%3324:tuple{%3489:<873x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3487:<873x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3487:<873x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3487:<873x1536xbf16>{3072, 1})
          outputs: (%3496:<873x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3487:<873x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3496:<873x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3496:<873x1536xbf16>{1536, 1}, %3493:<873x1536xbf16>{3072, 1}+1536)
          outputs: (%3461:<873x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3461:<873x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3461:<873x1536xbf16>{1536,1}}))
          duration: -1
256139 2024-12-10 17:48:38.579629 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n118,rank6)
        - aten::mm:
          inputs: (%3461:<873x1536xbf16>{1536, 1}, %3460:<1536x5120xbf16>{1, 1536})
          outputs: (%3482:<873x5120xbf16>{5120,1})
          duration: -1
256221 2024-12-10 17:48:38.583452 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n118,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3461:<873x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3482:<873x5120xbf16>{5120,1},None:NoneType})
          duration: -1
256235 2024-12-10 17:48:38.584213 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n112,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3067:<873x5120xbf16>{5120, 1}+13317120})
          outputs: (%3432:tuple{%3482:<873x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3464:<873x5120xbf16>{5120, 1}+13317120, %3482:<873x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3464:<873x5120xbf16>{5120,1}+13317120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<316x5120xbf16>{5120, 1}+17786880})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<316x5120xbf16>{5120,1}+17786880}))
          duration: -1
256412 2024-12-10 17:48:38.594623 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n113,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<316x5120xbf16>{5120, 1}+17786880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<316x5120xbf16>{5120,1}+17786880}))
          duration: -1
256435 2024-12-10 17:48:38.595366 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n142,rank6)
        - aten::mm:
          inputs: (%3433:<316x5120xbf16>{5120, 1}+17786880, %3460:<5120x3072xbf16>{1, 5120})
          outputs: (%3484:<316x3072xbf16>{3072,1})
          duration: -1
256515 2024-12-10 17:48:38.599222 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n142,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<316x5120xbf16>{5120, 1}+17786880})
          outputs: (%3446:tuple{%3484:<316x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3472:<316x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3472:<316x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3472:<316x1536xbf16>{3072, 1})
          outputs: (%3461:<316x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3472:<316x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3461:<316x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3461:<316x1536xbf16>{1536, 1}, %3460:<316x1536xbf16>{3072, 1}+1536)
          outputs: (%3456:<316x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3456:<316x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3456:<316x1536xbf16>{1536,1}}))
          duration: -1
256621 2024-12-10 17:48:38.609182 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n119,rank6)
        - aten::mm:
          inputs: (%3456:<316x1536xbf16>{1536, 1}, %3483:<1536x5120xbf16>{1, 1536})
          outputs: (%3453:<316x5120xbf16>{5120,1})
          duration: -1
256700 2024-12-10 17:48:38.612957 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n119,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3456:<316x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3453:<316x5120xbf16>{5120,1},None:NoneType})
          duration: -1
256715 2024-12-10 17:48:38.613737 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n113,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<316x5120xbf16>{5120, 1}+17786880})
          outputs: (%3432:tuple{%3453:<316x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3439:<316x5120xbf16>{5120, 1}+17786880, %3453:<316x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3439:<316x5120xbf16>{5120,1}+17786880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3441:<172x5120xbf16>{5120, 1}+19404800})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3441:<172x5120xbf16>{5120,1}+19404800}))
          duration: -1
256893 2024-12-10 17:48:38.624018 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n114,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3441:<172x5120xbf16>{5120, 1}+19404800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3441:<172x5120xbf16>{5120,1}+19404800}))
          duration: -1
256913 2024-12-10 17:48:38.624753 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n143,rank6)
        - aten::mm:
          inputs: (%3441:<172x5120xbf16>{5120, 1}+19404800, %3480:<5120x3072xbf16>{1, 5120})
          outputs: (%3501:<172x3072xbf16>{3072,1})
          duration: -1
256994 2024-12-10 17:48:38.628534 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n143,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3441:<172x5120xbf16>{5120, 1}+19404800})
          outputs: (%3427:tuple{%3501:<172x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3472:<172x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3472:<172x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3472:<172x1536xbf16>{3072, 1})
          outputs: (%3482:<172x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3472:<172x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3482:<172x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3482:<172x1536xbf16>{1536, 1}, %3480:<172x1536xbf16>{3072, 1}+1536)
          outputs: (%3461:<172x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3461:<172x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3461:<172x1536xbf16>{1536,1}}))
          duration: -1
257100 2024-12-10 17:48:38.638478 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n120,rank6)
        - aten::mm:
          inputs: (%3461:<172x1536xbf16>{1536, 1}, %3486:<1536x5120xbf16>{1, 1536})
          outputs: (%3460:<172x5120xbf16>{5120,1})
          duration: -1
257180 2024-12-10 17:48:38.642286 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n120,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3461:<172x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3460:<172x5120xbf16>{5120,1},None:NoneType})
          duration: -1
257193 2024-12-10 17:48:38.643047 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n114,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3441:<172x5120xbf16>{5120, 1}+19404800})
          outputs: (%3432:tuple{%3460:<172x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3464:<172x5120xbf16>{5120, 1}+19404800, %3460:<172x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3464:<172x5120xbf16>{5120,1}+19404800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<163x5120xbf16>{5120, 1}+20285440})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<163x5120xbf16>{5120,1}+20285440}))
          duration: -1
257371 2024-12-10 17:48:38.653367 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n115,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<163x5120xbf16>{5120, 1}+20285440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<163x5120xbf16>{5120,1}+20285440}))
          duration: -1
        - aten::mm:
          inputs: (%3433:<163x5120xbf16>{5120, 1}+20285440, %3482:<5120x3072xbf16>{1, 5120})
          outputs: (%3472:<163x3072xbf16>{3072,1})
          duration: -1
257472 2024-12-10 17:48:38.657955 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n144,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<163x5120xbf16>{5120, 1}+20285440})
          outputs: (%3324:tuple{%3472:<163x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3506:<163x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3506:<163x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3506:<163x1536xbf16>{3072, 1})
          outputs: (%3456:<163x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3506:<163x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3456:<163x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3456:<163x1536xbf16>{1536, 1}, %3482:<163x1536xbf16>{3072, 1}+1536)
          outputs: (%3480:<163x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3480:<163x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3480:<163x1536xbf16>{1536,1}}))
          duration: -1
        - aten::mm:
          inputs: (%3480:<163x1536xbf16>{1536, 1}, %3507:<1536x5120xbf16>{1, 1536})
          outputs: (%3501:<163x5120xbf16>{5120,1})
          duration: -1
257657 2024-12-10 17:48:38.671698 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n121,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3480:<163x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3501:<163x5120xbf16>{5120,1},None:NoneType})
          duration: -1
257672 2024-12-10 17:48:38.672464 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n115,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<163x5120xbf16>{5120, 1}+20285440})
          outputs: (%3432:tuple{%3501:<163x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3352:<163x5120xbf16>{5120, 1}+20285440, %3501:<163x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3352:<163x5120xbf16>{5120,1}+20285440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3067:<149x5120xbf16>{5120, 1}+21120000})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3067:<149x5120xbf16>{5120,1}+21120000}))
          duration: -1
257849 2024-12-10 17:48:38.682767 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n116,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3067:<149x5120xbf16>{5120, 1}+21120000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3067:<149x5120xbf16>{5120,1}+21120000}))
          duration: -1
257868 2024-12-10 17:48:38.683500 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n145,rank6)
        - aten::mm:
          inputs: (%3067:<149x5120xbf16>{5120, 1}+21120000, %3507:<5120x3072xbf16>{1, 5120})
          outputs: (%3480:<149x3072xbf16>{3072,1})
          duration: -1
257950 2024-12-10 17:48:38.687295 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n145,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3067:<149x5120xbf16>{5120, 1}+21120000})
          outputs: (%3446:tuple{%3480:<149x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3352:<149x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3352:<149x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3352:<149x1536xbf16>{3072, 1})
          outputs: (%3507:<149x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3352:<149x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3507:<149x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3507:<149x1536xbf16>{1536, 1}, %3482:<149x1536xbf16>{3072, 1}+1536)
          outputs: (%3461:<149x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3461:<149x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3461:<149x1536xbf16>{1536,1}}))
          duration: -1
258056 2024-12-10 17:48:38.697275 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n122,rank6)
        - aten::mm:
          inputs: (%3461:<149x1536xbf16>{1536, 1}, %3456:<1536x5120xbf16>{1, 1536})
          outputs: (%3453:<149x5120xbf16>{5120,1})
          duration: -1
258138 2024-12-10 17:48:38.701111 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n122,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3461:<149x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3453:<149x5120xbf16>{5120,1},None:NoneType})
          duration: -1
258153 2024-12-10 17:48:38.701883 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n116,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3067:<149x5120xbf16>{5120, 1}+21120000})
          outputs: (%3432:tuple{%3453:<149x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3464:<149x5120xbf16>{5120, 1}+21120000, %3453:<149x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3464:<149x5120xbf16>{5120,1}+21120000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3433:<155x5120xbf16>{5120, 1}+21882880})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3433:<155x5120xbf16>{5120,1}+21882880}))
          duration: -1
258331 2024-12-10 17:48:38.712208 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n117,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3433:<155x5120xbf16>{5120, 1}+21882880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3433:<155x5120xbf16>{5120,1}+21882880}))
          duration: -1
258348 2024-12-10 17:48:38.712949 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n146,rank6)
        - aten::mm:
          inputs: (%3433:<155x5120xbf16>{5120, 1}+21882880, %3512:<5120x3072xbf16>{1, 5120})
          outputs: (%3513:<155x3072xbf16>{3072,1})
          duration: -1
258431 2024-12-10 17:48:38.716772 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n146,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3433:<155x5120xbf16>{5120, 1}+21882880})
          outputs: (%3427:tuple{%3513:<155x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3461:<155x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3461:<155x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3461:<155x1536xbf16>{3072, 1})
          outputs: (%3506:<155x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3461:<155x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3506:<155x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3506:<155x1536xbf16>{1536, 1}, %3512:<155x1536xbf16>{3072, 1}+1536)
          outputs: (%3482:<155x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3482:<155x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3482:<155x1536xbf16>{1536,1}}))
          duration: -1
258538 2024-12-10 17:48:38.726722 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n123,rank6)
        - aten::mm:
          inputs: (%3482:<155x1536xbf16>{1536, 1}, %3472:<1536x5120xbf16>{1, 1536})
          outputs: (%3483:<155x5120xbf16>{5120,1})
          duration: -1
258618 2024-12-10 17:48:38.730529 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n123,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3482:<155x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3483:<155x5120xbf16>{5120,1},None:NoneType})
          duration: -1
258632 2024-12-10 17:48:38.731298 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n117,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3433:<155x5120xbf16>{5120, 1}+21882880})
          outputs: (%3432:tuple{%3483:<155x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3439:<155x5120xbf16>{5120, 1}+21882880, %3483:<155x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3439:<155x5120xbf16>{5120,1}+21882880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3518:<335x5120xbf16>{5120, 1}+22676480})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3518:<335x5120xbf16>{5120,1}+22676480}))
          duration: -1
258811 2024-12-10 17:48:38.741650 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n118,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3518:<335x5120xbf16>{5120, 1}+22676480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3518:<335x5120xbf16>{5120,1}+22676480}))
          duration: -1
258829 2024-12-10 17:48:38.742386 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n147,rank6)
        - aten::mm:
          inputs: (%3518:<335x5120xbf16>{5120, 1}+22676480, %3480:<5120x3072xbf16>{1, 5120})
          outputs: (%3519:<335x3072xbf16>{3072,1})
          duration: -1
258911 2024-12-10 17:48:38.746211 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n147,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3518:<335x5120xbf16>{5120, 1}+22676480})
          outputs: (%3324:tuple{%3519:<335x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3433:<335x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3433:<335x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3433:<335x1536xbf16>{3072, 1})
          outputs: (%3482:<335x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3433:<335x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3482:<335x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3482:<335x1536xbf16>{1536, 1}, %3453:<335x1536xbf16>{3072, 1}+1536)
          outputs: (%3480:<335x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3480:<335x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3480:<335x1536xbf16>{1536,1}}))
          duration: -1
259019 2024-12-10 17:48:38.756160 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n124,rank6)
        - aten::mm:
          inputs: (%3480:<335x1536xbf16>{1536, 1}, %3486:<1536x5120xbf16>{1, 1536})
          outputs: (%3482:<335x5120xbf16>{5120,1})
          duration: -1
259098 2024-12-10 17:48:38.759956 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n124,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3480:<335x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3482:<335x5120xbf16>{5120,1},None:NoneType})
          duration: -1
259111 2024-12-10 17:48:38.760726 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n118,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3518:<335x5120xbf16>{5120, 1}+22676480})
          outputs: (%3432:tuple{%3482:<335x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3441:<335x5120xbf16>{5120, 1}+22676480, %3482:<335x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3441:<335x5120xbf16>{5120,1}+22676480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3439:<321x5120xbf16>{5120, 1}+24391680})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3439:<321x5120xbf16>{5120,1}+24391680}))
          duration: -1
259292 2024-12-10 17:48:38.771092 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n119,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3439:<321x5120xbf16>{5120, 1}+24391680})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3439:<321x5120xbf16>{5120,1}+24391680}))
          duration: -1
259307 2024-12-10 17:48:38.771830 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n148,rank6)
        - aten::mm:
          inputs: (%3439:<321x5120xbf16>{5120, 1}+24391680, %3523:<5120x3072xbf16>{1, 5120})
          outputs: (%3517:<321x3072xbf16>{3072,1})
          duration: -1
259387 2024-12-10 17:48:38.775660 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n148,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3439:<321x5120xbf16>{5120, 1}+24391680})
          outputs: (%3446:tuple{%3517:<321x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3352:<321x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3352:<321x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3352:<321x1536xbf16>{3072, 1})
          outputs: (%3472:<321x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3352:<321x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3472:<321x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3472:<321x1536xbf16>{1536, 1}, %3526:<321x1536xbf16>{3072, 1}+1536)
          outputs: (%3523:<321x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3523:<321x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3523:<321x1536xbf16>{1536,1}}))
          duration: -1
259498 2024-12-10 17:48:38.785634 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n125,rank6)
        - aten::mm:
          inputs: (%3523:<321x1536xbf16>{1536, 1}, %3480:<1536x5120xbf16>{1, 1536})
          outputs: (%3486:<321x5120xbf16>{5120,1})
          duration: -1
259575 2024-12-10 17:48:38.789428 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n125,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3523:<321x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3486:<321x5120xbf16>{5120,1},None:NoneType})
          duration: -1
259590 2024-12-10 17:48:38.790191 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n119,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3439:<321x5120xbf16>{5120, 1}+24391680})
          outputs: (%3432:tuple{%3486:<321x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3518:<321x5120xbf16>{5120, 1}+24391680, %3486:<321x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3518:<321x5120xbf16>{5120,1}+24391680)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3441:<429x5120xbf16>{5120, 1}+26035200})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3441:<429x5120xbf16>{5120,1}+26035200}))
          duration: -1
259771 2024-12-10 17:48:38.800504 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n120,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3441:<429x5120xbf16>{5120, 1}+26035200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3441:<429x5120xbf16>{5120,1}+26035200}))
          duration: -1
259786 2024-12-10 17:48:38.801248 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n149,rank6)
        - aten::mm:
          inputs: (%3441:<429x5120xbf16>{5120, 1}+26035200, %3483:<5120x3072xbf16>{1, 5120})
          outputs: (%3189:<429x3072xbf16>{3072,1})
          duration: -1
259865 2024-12-10 17:48:38.805062 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n149,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3441:<429x5120xbf16>{5120, 1}+26035200})
          outputs: (%3427:tuple{%3189:<429x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3523:<429x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3523:<429x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3523:<429x1536xbf16>{3072, 1})
          outputs: (%3461:<429x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3523:<429x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3461:<429x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3461:<429x1536xbf16>{1536, 1}, %3483:<429x1536xbf16>{3072, 1}+1536)
          outputs: (%3460:<429x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3460:<429x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3460:<429x1536xbf16>{1536,1}}))
          duration: -1
259979 2024-12-10 17:48:38.815044 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n126,rank6)
        - aten::mm:
          inputs: (%3460:<429x1536xbf16>{1536, 1}, %3480:<1536x5120xbf16>{1, 1536})
          outputs: (%3482:<429x5120xbf16>{5120,1})
          duration: -1
260055 2024-12-10 17:48:38.818858 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n126,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3460:<429x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3482:<429x5120xbf16>{5120,1},None:NoneType})
          duration: -1
260071 2024-12-10 17:48:38.819629 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n120,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3441:<429x5120xbf16>{5120, 1}+26035200})
          outputs: (%3432:tuple{%3482:<429x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3439:<429x5120xbf16>{5120, 1}+26035200, %3482:<429x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3439:<429x5120xbf16>{5120,1}+26035200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3464:<346x5120xbf16>{5120, 1}+28231680})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3464:<346x5120xbf16>{5120,1}+28231680}))
          duration: -1
260250 2024-12-10 17:48:38.829938 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n121,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3464:<346x5120xbf16>{5120, 1}+28231680})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3464:<346x5120xbf16>{5120,1}+28231680}))
          duration: -1
260267 2024-12-10 17:48:38.830668 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n150,rank6)
        - aten::mm:
          inputs: (%3464:<346x5120xbf16>{5120, 1}+28231680, %3480:<5120x3072xbf16>{1, 5120})
          outputs: (%3518:<346x3072xbf16>{3072,1})
          duration: -1
260346 2024-12-10 17:48:38.834485 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n150,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3464:<346x5120xbf16>{5120, 1}+28231680})
          outputs: (%3324:tuple{%3518:<346x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3441:<346x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3441:<346x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3441:<346x1536xbf16>{3072, 1})
          outputs: (%3456:<346x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3441:<346x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3456:<346x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3456:<346x1536xbf16>{1536, 1}, %3523:<346x1536xbf16>{3072, 1}+1536)
          outputs: (%3480:<346x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3480:<346x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3480:<346x1536xbf16>{1536,1}}))
          duration: -1
260459 2024-12-10 17:48:38.844434 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n127,rank6)
        - aten::mm:
          inputs: (%3480:<346x1536xbf16>{1536, 1}, %3472:<1536x5120xbf16>{1, 1536})
          outputs: (%3456:<346x5120xbf16>{5120,1})
          duration: -1
260534 2024-12-10 17:48:38.848269 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n127,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3480:<346x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3456:<346x5120xbf16>{5120,1},None:NoneType})
          duration: -1
260549 2024-12-10 17:48:38.849040 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n121,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3464:<346x5120xbf16>{5120, 1}+28231680})
          outputs: (%3432:tuple{%3456:<346x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3441:<346x5120xbf16>{5120, 1}+28231680, %3456:<346x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3441:<346x5120xbf16>{5120,1}+28231680)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3439:<169x5120xbf16>{5120, 1}+30003200})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3439:<169x5120xbf16>{5120,1}+30003200}))
          duration: -1
260730 2024-12-10 17:48:38.859340 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n122,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3439:<169x5120xbf16>{5120, 1}+30003200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3439:<169x5120xbf16>{5120,1}+30003200}))
          duration: -1
260747 2024-12-10 17:48:38.860081 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n151,rank6)
        - aten::mm:
          inputs: (%3439:<169x5120xbf16>{5120, 1}+30003200, %3472:<5120x3072xbf16>{1, 5120})
          outputs: (%3518:<169x3072xbf16>{3072,1})
          duration: -1
260824 2024-12-10 17:48:38.863894 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n151,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3439:<169x5120xbf16>{5120, 1}+30003200})
          outputs: (%3446:tuple{%3518:<169x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3486:<169x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3486:<169x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3486:<169x1536xbf16>{3072, 1})
          outputs: (%3489:<169x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3486:<169x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3489:<169x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3489:<169x1536xbf16>{1536, 1}, %3472:<169x1536xbf16>{3072, 1}+1536)
          outputs: (%3480:<169x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3480:<169x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3480:<169x1536xbf16>{1536,1}}))
          duration: -1
260939 2024-12-10 17:48:38.873856 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n128,rank6)
        - aten::mm:
          inputs: (%3480:<169x1536xbf16>{1536, 1}, %3468:<1536x5120xbf16>{1, 1536})
          outputs: (%3461:<169x5120xbf16>{5120,1})
          duration: -1
261013 2024-12-10 17:48:38.877664 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n128,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3480:<169x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3461:<169x5120xbf16>{5120,1},None:NoneType})
          duration: -1
261026 2024-12-10 17:48:38.878429 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n122,rank6)
        - aten::copy_:
          inputs: (%3482:<169x5120xbf16>{5120, 1}+30003200, %3461:<169x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3482:<169x5120xbf16>{5120,1}+30003200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3441:<718x5120xbf16>{5120, 1}+30868480})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3441:<718x5120xbf16>{5120,1}+30868480}))
          duration: -1
261209 2024-12-10 17:48:38.888725 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n123,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3441:<718x5120xbf16>{5120, 1}+30868480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3441:<718x5120xbf16>{5120,1}+30868480}))
          duration: -1
261227 2024-12-10 17:48:38.889473 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n152,rank6)
        - aten::mm:
          inputs: (%3441:<718x5120xbf16>{5120, 1}+30868480, %3536:<5120x3072xbf16>{1, 5120})
          outputs: (%3537:<718x3072xbf16>{3072,1})
          duration: -1
261304 2024-12-10 17:48:38.893325 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n152,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3441:<718x5120xbf16>{5120, 1}+30868480})
          outputs: (%3427:tuple{%3537:<718x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3453:<718x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3453:<718x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3453:<718x1536xbf16>{3072, 1})
          outputs: (%3460:<718x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3453:<718x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3460:<718x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3460:<718x1536xbf16>{1536, 1}, %3536:<718x1536xbf16>{3072, 1}+1536)
          outputs: (%3480:<718x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3480:<718x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3480:<718x1536xbf16>{1536,1}}))
          duration: -1
261418 2024-12-10 17:48:38.903234 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n129,rank6)
        - aten::mm:
          inputs: (%3480:<718x1536xbf16>{1536, 1}, %3540:<1536x5120xbf16>{1, 1536})
          outputs: (%3453:<718x5120xbf16>{5120,1})
          duration: -1
261492 2024-12-10 17:48:38.907062 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n129,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3480:<718x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3453:<718x5120xbf16>{5120,1},None:NoneType})
          duration: -1
261506 2024-12-10 17:48:38.907832 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n123,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3441:<718x5120xbf16>{5120, 1}+30868480})
          outputs: (%3432:tuple{%3453:<718x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3352:<718x5120xbf16>{5120, 1}+30868480, %3453:<718x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3352:<718x5120xbf16>{5120,1}+30868480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%3439:<501x5120xbf16>{5120, 1}+34544640})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%3439:<501x5120xbf16>{5120,1}+34544640}))
          duration: -1
261687 2024-12-10 17:48:38.918171 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n124,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%3439:<501x5120xbf16>{5120, 1}+34544640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%3439:<501x5120xbf16>{5120,1}+34544640}))
          duration: -1
261703 2024-12-10 17:48:38.918917 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n153,rank6)
        - aten::mm:
          inputs: (%3439:<501x5120xbf16>{5120, 1}+34544640, %3542:<5120x3072xbf16>{1, 5120})
          outputs: (%3472:<501x3072xbf16>{3072,1})
          duration: -1
261781 2024-12-10 17:48:38.922752 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n153,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%3439:<501x5120xbf16>{5120, 1}+34544640})
          outputs: (%3324:tuple{%3472:<501x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3441:<501x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3441:<501x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3441:<501x1536xbf16>{3072, 1})
          outputs: (%3482:<501x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3441:<501x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3482:<501x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3482:<501x1536xbf16>{1536, 1}, %3544:<501x1536xbf16>{3072, 1}+1536)
          outputs: (%3518:<501x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3518:<501x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3518:<501x1536xbf16>{1536,1}}))
          duration: -1
261898 2024-12-10 17:48:38.932729 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n130,rank6)
        - aten::mm:
          inputs: (%3518:<501x1536xbf16>{1536, 1}, %3542:<1536x5120xbf16>{1, 1536})
          outputs: (%3456:<501x5120xbf16>{5120,1})
          duration: -1
261972 2024-12-10 17:48:38.936570 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n130,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3518:<501x1536xbf16>{1536, 1}})
          outputs: (%3432:tuple{%3456:<501x5120xbf16>{5120,1},None:NoneType})
          duration: -1
261986 2024-12-10 17:48:38.937347 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n124,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%3439:<501x5120xbf16>{5120, 1}+34544640})
          outputs: (%3432:tuple{%3456:<501x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3433:<501x5120xbf16>{5120, 1}+34544640, %3456:<501x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3433:<501x5120xbf16>{5120,1}+34544640)
          duration: -1
262068 2024-12-10 17:48:38.942423 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n5,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%3440:tuple{%840:<7248x5120xbf16>{5120, 1}, %3443:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3546:tuple{%3437:<7248x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%3414:<7248x5120xbf16>{5120, 1}, 0:int, %3444:<7248x5120xCUSTOM_DATA_TYPE>{1, 0}, %3437:<7248x5120xbf16>{5120, 1})
          outputs: (%3108:<7248x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%3108:<7248x5120xbf16>{5120, 1}, %3166:<7248x1xbf16>{1, 1})
          outputs: (%3414:<7248x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%3548:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3164:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%3164:<8192x5120xbf16>{5120, 1}, 0:int, %3421:<7248x5120xCUSTOM_DATA_TYPE>{1, 0}, %3414:<7248x5120xbf16>{5120, 1})
          outputs: (%3433:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%3441:<1024x5120xbf16>{5120, 1}, %3433:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%3441:<1024x5120xbf16>{5120,1},%3433:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%3441:<1024x5120xbf16>{5120, 1}, %3433:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%3441:<1024x5120xbf16>{5120,1},%3433:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%3441:<1024x5120xbf16>{5120, 1}, %3433:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%3435:tuple{%3441:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3407:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%3407:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
262305 2024-12-10 17:48:38.983986 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n125,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3445:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3445:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
262329 2024-12-10 17:48:38.984754 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n154,rank6)
        - aten::mm:
          inputs: (%3501:<1024x5120xbf16>{5120, 1}, %3442:<5120x6144xbf16>{1, 5120})
          outputs: (%3189:<1024x6144xbf16>{6144,1})
          duration: -1
262480 2024-12-10 17:48:38.992691 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n154,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3445:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3432:tuple{%3460:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3433:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3433:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3433:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%3352:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3433:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%3352:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%3352:<1024x1x3072xbf16>{3072, 3072, 1}, %3067:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%3416:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3425:tuple{%3416:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3425:tuple{%3416:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
262585 2024-12-10 17:48:39.002705 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n131,rank6)
        - aten::mm:
          inputs: (%3518:<1024x3072xbf16>{3072, 1}, %3453:<3072x5120xbf16>{1, 3072})
          outputs: (%3189:<1024x5120xbf16>{5120,1})
          duration: -1
262737 2024-12-10 17:48:39.010609 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n131,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3425:tuple{%3416:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%3409:tuple{%3518:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
262753 2024-12-10 17:48:39.011377 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n125,rank6)
        - ----------->api::MLP return:
          inputs: (%3407:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3409:tuple{%3518:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%3259:<1024x1x5120xbf16>{5120, 5120, 1}, %3067:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%3433:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
262801 2024-12-10 17:48:39.014546 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n5,rank6)
        - ----------->api::MoELayer return:
          inputs: (%3401:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3409:tuple{%3433:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3559:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3559:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3560:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3560:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8633330_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8633330_:_InferenceMode)
          duration: -1
264926 2024-12-10 17:48:39.547487 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n5,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%3427:tuple{%3078:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3401:tuple{%3078:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3401:tuple{%3078:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
264976 2024-12-10 17:48:39.554293 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n17,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3078:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3078:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3078:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3078:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3562:tuple{%3078:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3562:tuple{%3078:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3562:tuple{%3078:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3562:tuple{%3078:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3565:tuple{%3078:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %2160:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3565:tuple{%3078:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%2160:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3078:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3562:tuple{%3078:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1979:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3078:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1979:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
265158 2024-12-10 17:48:39.598800 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n17,rank6)
265162 2024-12-10 17:48:39.599323 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n5,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%1979:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3566:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%3566:tuple{%1979:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
265195 2024-12-10 17:48:39.605724 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n155,rank6)
        - aten::mm:
          inputs: (%3572:<1024x5120xbf16>{5120, 1}, %3570:<5120x102400xbf16>{1, 5120})
          outputs: (%3433:<1024x102400xbf16>{102400,1})
          duration: -1
265345 2024-12-10 17:48:39.618149 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n155,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3566:tuple{%1979:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%3575:tuple{%3573:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%3576:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%3577:tuple{%3570:<1024x1xf32>{1,1},%3426:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3570:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3570:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3578:list{%3570:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3579:tuple{%3580:list{%3570:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%3576:<1024x1x102400xf32>{102400, 102400, 1}, %3426:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%3576:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%2475:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3441:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%2475:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3581:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3441:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3581:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3433:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3582:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%3583:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%3583:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3584:list{%3433:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3582:<i32>, False:bool)
          outputs: (%3583:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2160:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3585:<1024x102400xf32>{102400, 1}, %3580:list{%2160:<1024xCUSTOM_DATA_TYPE>{1}, %3581:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%840:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%3441:<1024x1xf32>{1, 1}, %3580:list{%3433:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %840:<i32>, False:bool)
          outputs: (%3441:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%3576:<1024x1x102400xf32>{102400, 102400, 1}, out=%3576:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%3576:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%3576:<1024x1x102400xf32>{102400, 102400, 1}, %3584:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3439:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3441:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3441:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3584:list{%3441:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3575:tuple{%3571:list{%3441:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3439:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3439:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3586:list{%3439:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3587:tuple{%3588:list{%3439:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%3439:<1024x1xf32>{1, 1})
          outputs: (%3589:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3589:<1024x1xf32>{1, 1}, %3441:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3585:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%3576:<1024x1x102400xf32>{102400, 102400, 1}, %840:<1024x1x1xf32>{1, 1, 1})
          outputs: (%3576:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%3590:tuple{%3576:<1024x1x102400xf32>{102400, 102400, 1}, %3433:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %3581:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%3590:tuple{%3576:<1024x1x102400xf32>{102400,102400,1},%3433:<1024x1xCUSTOM_DATA_TYPE>{1,1},%3581:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2859:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device='cuda_6')_:dict)
          outputs: (%840:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%840:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%840:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%840:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device='cuda_6')_:dict)
          outputs: (%840:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2941:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3335:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[32169,___410,___430,_____,___185,__4044,_47278]],_device='cuda_6')_:dict)
          outputs: (%840:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%3336:<1024xf32>{1}, %3121:<1024xf32>{1})
          outputs: (%2475:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2475:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1162:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%3121:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1979:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1162:<i32>, %1979:<i32>)
          outputs: (%3332:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%3332:<i32>)
          outputs: (%2068:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%2627:list{%3581:<1xf32>{1}}, 0:int)
          outputs: (%3335:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3335:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3335:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3593:list{%3335:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3432:tuple{%3594:list{%3335:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%3335:<1xf32>{1}, 8:int)
          outputs: (%1385:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3332:<i32>, 1:int)
          outputs: (%1162:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%1162:<i32>, 1:int)
          outputs: (%1162:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%3595:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3121:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%3121:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%3121:<1xf32>{1}))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %3121:<1xf32>{1})
          outputs: (%1574:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%1574:<1xf32>{1}, 1:int)
          outputs: (%2068:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%3169:<i32>, 0:int, alpha=1:int)
          outputs: (%3169:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3109:<i32>, %3573:<i32>, alpha=1:int)
          outputs: (%3109:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%840:<i32>, %3109:<i32>, False:bool)
          outputs: (%840:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3121:<i32>, 1:int, alpha=1:int)
          outputs: (%3121:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%1574:<i32>, %3121:<i32>, False:bool)
          outputs: (%1574:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%3594:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%1574:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%427:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%262:<1024xf32>{1}, %431:list{%427:<1024xCUSTOM_DATA_TYPE>{1}}, %417:<i32>, False:bool)
          outputs: (%262:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%384:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%417:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%384:<1024xCUSTOM_DATA_TYPE>{1}, %431:list{%417:<1024xCUSTOM_DATA_TYPE>{1}}, %415:<i32>, False:bool)
          outputs: (%384:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%3601:list{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3579:tuple{%3043:list{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::eq:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1, %431:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %415:<i32>, False:bool)
          outputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%1816:list{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3602:tuple{%3594:list{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3433:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3433:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3603:list{%3433:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3587:tuple{%3580:list{%3433:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%2832:list{%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3579:tuple{%3604:list{%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3589:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3605:list{%3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3602:tuple{%3043:list{%3589:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3589:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3589:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3589:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2684:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2684:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3589:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__185,__3516,_19890,_____,__4441,____11,__2677]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[__185,__3516,_19890,_____,__4441,____11,__2677]],_device)
          duration: -1
267360 2024-12-10 17:48:39.810498 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n6,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%3566:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%3566:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
267380 2024-12-10 17:48:39.811254 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n6,rank6)
        - ----------->api::embedding call:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%3439:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%3439:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
267474 2024-12-10 17:48:39.826223 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n6,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%3566:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3608:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%3566:tuple{%3441:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%3566:tuple{%3441:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
267514 2024-12-10 17:48:39.828294 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n6,rank6)
        - ----------->api::dropout call:
          inputs: (%3441:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3441:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3441:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3441:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
267558 2024-12-10 17:48:39.834629 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n6,rank6)
        - ----------->api::Dropout return:
          inputs: (%3566:tuple{%3441:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3441:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
267571 2024-12-10 17:48:39.835358 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n6,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__185,__3516,_19890,_____,__4441,____11,__2677]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%3441:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%3566:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%3566:tuple{1024:int}))
          duration: -1
267599 2024-12-10 17:48:39.836968 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n6,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3597:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%3597:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%3448:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3570:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%3611:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%3612:list{%3611:<1024x20xf32>{20, 1}, %3611:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%3613:<1024x40xf32>{40,1})
          duration: -1
267753 2024-12-10 17:48:39.847989 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n6,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%3566:tuple{1024:int})
          outputs: (%3614:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
267786 2024-12-10 17:48:39.854330 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n6,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
267866 2024-12-10 17:48:39.863405 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n6,rank6)
267882 2024-12-10 17:48:39.864148 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n18,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1562:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3562:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3562:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3562:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3562:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3591:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3618:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3591:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3618:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%429:list{%384:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%417:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%417:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3562:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3616:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%3616:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
268132 2024-12-10 17:48:39.908479 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n18,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%3566:tuple{%3616:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%3566:tuple{%3616:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
268150 2024-12-10 17:48:39.912784 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n6,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%3616:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%3616:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
268168 2024-12-10 17:48:39.913574 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n156,rank6)
        - aten::mm:
          inputs: (%3475:<1024x5120xbf16>{5120, 1}, %2939:<5120x1536xbf16>{1, 5120})
          outputs: (%3169:<1024x1536xbf16>{1536,1})
          duration: -1
268323 2024-12-10 17:48:39.921506 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n156,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%3616:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3623:tuple{%3072:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%3072:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%3072:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
268345 2024-12-10 17:48:39.922544 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n157,rank6)
        - aten::mm:
          inputs: (%3518:<1024x1536xbf16>{1536, 1}, %3624:<1536x24576xbf16>{1, 1536})
          outputs: (%2852:<1024x24576xbf16>{24576,1})
          duration: -1
268464 2024-12-10 17:48:39.928691 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n157,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%3072:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%3629:tuple{%3627:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3475:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3625:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3475:<1024x1x128x192xbf16>{24576,24576,192,1},%3625:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3475:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3625:list{128:int, 64:int}, -1:int)
          outputs: (%3575:tuple{%3134:<1024x1x128x128xbf16>{24576,24576,192,1},%3191:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%3616:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%3616:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
268552 2024-12-10 17:48:39.938289 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n158,rank6)
        - aten::mm:
          inputs: (%3635:<1024x5120xbf16>{5120, 1}, %3633:<5120x576xbf16>{1, 5120})
          outputs: (%3636:<1024x576xbf16>{576,1})
          duration: -1
268702 2024-12-10 17:48:39.946169 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n158,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%3616:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3575:tuple{%3637:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3637:<1024x1x576xbf16>{576, 576, 1}, %3639:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3637:<1024x1x576xbf16>{576,576,1},%3639:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3637:<1024x1x576xbf16>{576, 576, 1}, %3639:list{512:int, 64:int}, -1:int)
          outputs: (%3629:tuple{%3072:<1024x1x512xbf16>{576,576,1},%2852:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%3072:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%3072:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
268776 2024-12-10 17:48:39.954464 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n159,rank6)
        - aten::mm:
          inputs: (%3641:<1024x512xbf16>{576, 1}, %3618:<512x32768xbf16>{1, 512})
          outputs: (%3642:<1024x32768xbf16>{32768,1})
          duration: -1
268886 2024-12-10 17:48:39.960634 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n159,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%3072:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%3575:tuple{%3599:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3633:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3640:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%3633:<1024x1x128x256xbf16>{32768,32768,256,1},%3640:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3633:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3640:list{128:int, 128:int}, -1:int)
          outputs: (%3623:tuple{%3645:<1024x1x128x128xbf16>{32768,32768,256,1},%3646:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%3606:tuple{%3641:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%3606:tuple{%3641:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
269005 2024-12-10 17:48:39.972436 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n6,rank6)
269048 2024-12-10 17:48:39.975559 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n6,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%3606:tuple{%3641:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%3587:tuple{%3636:<1024x64xbf16>{64,1},%3635:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%3636:<1024x64xbf16>{64, 1}, %3630:list{%3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3618:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%3635:<1024x64xbf16>{64, 1}, %3644:list{%3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3169:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3388:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3647:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3650:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3652:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%3653:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%3630:list{%3653:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %1979:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%3654:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3654:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3649:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3392:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3650:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3392:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%3652:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3202:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3647:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3650:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3653:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%3655:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%3630:list{%3655:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %3392:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%3656:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3656:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3649:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3653:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3650:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3653:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%3655:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::stack:
          inputs: (%435:list{%369:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%434:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%434:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%3647:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3202:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%3647:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%2160:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3652:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%2160:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%2160:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3648:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%2160:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3597:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3392:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%3597:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%3636:<128x1024x192xbf16>{196608, 192, 1}, %3652:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%3655:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%3202:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%3645:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%3636:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%3627:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%3636:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %3630:list{%3627:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3633:<i32>, False:bool)
          outputs: (%3636:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%3645:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %3636:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%3475:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%3475:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%3475:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%2160:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%3627:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%3475:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %3627:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%3627:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%3657:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3657:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3657:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3657:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%3506:<128x1024x1024xbf16>{1048576, 1024, 1}, %3645:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%2160:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3619:tuple{%2160:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3619:tuple{%2160:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
270540 2024-12-10 17:48:40.094296 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n132,rank6)
        - aten::mm:
          inputs: (%3650:<1024x16384xbf16>{16384, 1}, %3376:<16384x5120xbf16>{1, 16384})
          outputs: (%3660:<1024x5120xbf16>{5120,1})
          duration: -1
270695 2024-12-10 17:48:40.102508 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n132,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3619:tuple{%2160:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%3409:tuple{%3072:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
270710 2024-12-10 17:48:40.103280 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n6,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%3566:tuple{%3616:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3409:tuple{%3072:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%230:list{%404:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%280:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%280:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3663:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3663:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - aten::stack:
          inputs: (%444:list{%413:<1024xf32>{1}}, 0:int, out=%378:<1x1024xf32>{1024, 1})
          outputs: (%378:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3398:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3398:tuple{None:NoneType,)
          duration: -1
        - aten::stack:
          inputs: (%445:list{%414:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%203:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%203:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8674cb0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8674cb0_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3664:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3664:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
273435 2024-12-10 17:48:40.673990 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n19,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1756:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1756:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3388:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3388:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3388:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3388:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3667:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %2852:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3667:tuple{%1756:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%2852:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3388:tuple{%1756:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1779:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1756:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1779:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
273706 2024-12-10 17:48:40.718773 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n19,rank6)
        - ----------->api::MoELayer call:
          inputs: (%3664:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%3664:tuple{%1779:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
273720 2024-12-10 17:48:40.719526 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n6,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%3668:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%3668:tuple{%1779:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
273731 2024-12-10 17:48:40.720272 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n6,rank6)
        - aten::mm:
          inputs: (%3670:<1024x5120xbf16>{5120, 1}, %2595:<5120x160xbf16>{1, 5120})
          outputs: (%3671:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%3599:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%3336:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%3672:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%3673:tuple{%3599:<1024x6xbf16>{6,1},%2978:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%3317:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%3316:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%2854:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%3674:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%3674:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %3145:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%3674:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%3674:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %2854:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3096:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3316:<1024x160xf32>{160, 1}, %2854:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%3080:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3080:<160xf32>{1}, %3096:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%358:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%358:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%3675:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%3675:<i32>, 2_5431315104166666e-07:float)
          outputs: (%3676:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3676:<i32>, 0_01:float)
          outputs: (%358:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2942:<i32>, %2011:<i32>, alpha=1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%2851:<i32>, %2942:<i32>, False:bool)
          outputs: (%2851:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%3678:tuple{%3676:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%3678:tuple{%3676:<i32>}))
          duration: -1
274248 2024-12-10 17:48:40.755693 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n6,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%3668:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3682:tuple{%3680:<1024x6xbf16>{6,1},%2978:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3316:<8192x5120xbf16>{5120, 1}, %3072:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3316:<8192x5120xbf16>{5120,1},%3072:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3316:<8192x5120xbf16>{5120, 1}, %3072:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3673:tuple{%3316:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3676:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2978:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3676:<8192x6xCUSTOM_DATA_TYPE>{6,1},%2978:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3676:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2978:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3682:tuple{%3676:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%3676:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%3411:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%3676:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%2852:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%3411:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2852:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%2942:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%3676:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %2942:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3686:<5817xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3688:<8192x6xbf16>{6, 1}, %3680:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3688:<8192x6xbf16>{6,1},%3680:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3688:<8192x6xbf16>{6, 1}, %3680:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3684:tuple{%3688:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%3688:<8192x6xbf16>{6, 1}, %2942:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%1162:<5817xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%2942:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3689:<5817x2xCUSTOM_DATA_TYPE>{1,5817})
          duration: -1
        - aten::gather:
          inputs: (%3316:<8192x5120xbf16>{5120, 1}, 0:int, %2096:<5817x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3691:<5817x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%3686:<5817xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%3677:tuple{%2852:<5817xCUSTOM_DATA_TYPE>{1},%1238:<5817xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%3169:<5817xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%2113:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%3691:<5817x5120xbf16>{5120, 1}, 0:int, %3692:<5817x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3693:<5817x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%3677:tuple{%3693:<5817x5120xbf16>{5120, 1}, %1424:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%3677:tuple{%3693:<5817x5120xbf16>{5120,1},%1424:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
274983 2024-12-10 17:48:40.862719 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n6,rank6)
        - aten::cumsum:
          inputs: (%1424:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%3676:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%2854:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%3691:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%2854:list{%3691:<1xCUSTOM_DATA_TYPE>{1}, %3676:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%3694:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3517:<69x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3517:<69x5120xbf16>{5120,1}}))
          duration: -1
275154 2024-12-10 17:48:40.873680 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n126,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3517:<69x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3517:<69x5120xbf16>{5120,1}}))
          duration: -1
275174 2024-12-10 17:48:40.874418 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n160,rank6)
        - aten::mm:
          inputs: (%3517:<69x5120xbf16>{5120, 1}, %3523:<5120x3072xbf16>{1, 5120})
          outputs: (%3633:<69x3072xbf16>{3072,1})
          duration: -1
275253 2024-12-10 17:48:40.878298 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n160,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3517:<69x5120xbf16>{5120, 1}})
          outputs: (%3698:tuple{%3633:<69x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3700:<69x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3700:<69x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3700:<69x1536xbf16>{3072, 1})
          outputs: (%3647:<69x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3700:<69x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3647:<69x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3647:<69x1536xbf16>{1536, 1}, %3701:<69x1536xbf16>{3072, 1}+1536)
          outputs: (%3646:<69x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3646:<69x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3646:<69x1536xbf16>{1536,1}}))
          duration: -1
275357 2024-12-10 17:48:40.888299 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n133,rank6)
        - aten::mm:
          inputs: (%3646:<69x1536xbf16>{1536, 1}, %3618:<1536x5120xbf16>{1, 1536})
          outputs: (%3633:<69x5120xbf16>{5120,1})
          duration: -1
275438 2024-12-10 17:48:40.892288 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n133,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3646:<69x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3633:<69x5120xbf16>{5120,1},None:NoneType})
          duration: -1
275457 2024-12-10 17:48:40.893077 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n126,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3517:<69x5120xbf16>{5120, 1}})
          outputs: (%3673:tuple{%3633:<69x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2113:<69x5120xbf16>{5120, 1}, %3633:<69x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2113:<69x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2942:<1022x5120xbf16>{5120, 1}+353280})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2942:<1022x5120xbf16>{5120,1}+353280}))
          duration: -1
275636 2024-12-10 17:48:40.903287 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n127,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2942:<1022x5120xbf16>{5120, 1}+353280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2942:<1022x5120xbf16>{5120,1}+353280}))
          duration: -1
275657 2024-12-10 17:48:40.904017 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n161,rank6)
        - aten::mm:
          inputs: (%2942:<1022x5120xbf16>{5120, 1}+353280, %3647:<5120x3072xbf16>{1, 5120})
          outputs: (%3703:<1022x3072xbf16>{3072,1})
          duration: -1
275740 2024-12-10 17:48:40.907918 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n161,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2942:<1022x5120xbf16>{5120, 1}+353280})
          outputs: (%3682:tuple{%3703:<1022x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3517:<1022x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3517:<1022x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3517:<1022x1536xbf16>{3072, 1})
          outputs: (%3637:<1022x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3517:<1022x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3637:<1022x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3637:<1022x1536xbf16>{1536, 1}, %3705:<1022x1536xbf16>{3072, 1}+1536)
          outputs: (%3647:<1022x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3647:<1022x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3647:<1022x1536xbf16>{1536,1}}))
          duration: -1
275842 2024-12-10 17:48:40.917920 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n134,rank6)
        - aten::mm:
          inputs: (%3647:<1022x1536xbf16>{1536, 1}, %3636:<1536x5120xbf16>{1, 1536})
          outputs: (%2780:<1022x5120xbf16>{5120,1})
          duration: -1
275918 2024-12-10 17:48:40.921758 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n134,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3647:<1022x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%2780:<1022x5120xbf16>{5120,1},None:NoneType})
          duration: -1
275934 2024-12-10 17:48:40.922524 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n127,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2942:<1022x5120xbf16>{5120, 1}+353280})
          outputs: (%3673:tuple{%2780:<1022x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2852:<1022x5120xbf16>{5120, 1}+353280, %2780:<1022x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2852:<1022x5120xbf16>{5120,1}+353280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3701:<257x5120xbf16>{5120, 1}+5585920})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3701:<257x5120xbf16>{5120,1}+5585920}))
          duration: -1
276108 2024-12-10 17:48:40.932815 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n128,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3701:<257x5120xbf16>{5120, 1}+5585920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3701:<257x5120xbf16>{5120,1}+5585920}))
          duration: -1
276131 2024-12-10 17:48:40.933573 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n162,rank6)
        - aten::mm:
          inputs: (%3701:<257x5120xbf16>{5120, 1}+5585920, %3708:<5120x3072xbf16>{1, 5120})
          outputs: (%3597:<257x3072xbf16>{3072,1})
          duration: -1
276212 2024-12-10 17:48:40.937391 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n162,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3701:<257x5120xbf16>{5120, 1}+5585920})
          outputs: (%3696:tuple{%3597:<257x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2942:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2942:<257x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2942:<257x1536xbf16>{3072, 1})
          outputs: (%3452:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2942:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3452:<257x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3452:<257x1536xbf16>{1536, 1}, %3705:<257x1536xbf16>{3072, 1}+1536)
          outputs: (%3711:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3711:<257x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3711:<257x1536xbf16>{1536,1}}))
          duration: -1
276317 2024-12-10 17:48:40.947344 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n135,rank6)
        - aten::mm:
          inputs: (%3711:<257x1536xbf16>{1536, 1}, %3647:<1536x5120xbf16>{1, 1536})
          outputs: (%3712:<257x5120xbf16>{5120,1})
          duration: -1
276392 2024-12-10 17:48:40.951139 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n135,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3711:<257x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3712:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
276409 2024-12-10 17:48:40.951904 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n128,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3701:<257x5120xbf16>{5120, 1}+5585920})
          outputs: (%3673:tuple{%3712:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3714:<257x5120xbf16>{5120, 1}+5585920, %3712:<257x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3714:<257x5120xbf16>{5120,1}+5585920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2852:<396x5120xbf16>{5120, 1}+6901760})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2852:<396x5120xbf16>{5120,1}+6901760}))
          duration: -1
276585 2024-12-10 17:48:40.962199 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n129,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2852:<396x5120xbf16>{5120, 1}+6901760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2852:<396x5120xbf16>{5120,1}+6901760}))
          duration: -1
276609 2024-12-10 17:48:40.962931 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n163,rank6)
        - aten::mm:
          inputs: (%2852:<396x5120xbf16>{5120, 1}+6901760, %3637:<5120x3072xbf16>{1, 5120})
          outputs: (%3715:<396x3072xbf16>{3072,1})
          duration: -1
276695 2024-12-10 17:48:40.966739 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n163,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2852:<396x5120xbf16>{5120, 1}+6901760})
          outputs: (%3698:tuple{%3715:<396x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3645:<396x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3645:<396x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3645:<396x1536xbf16>{3072, 1})
          outputs: (%3523:<396x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3645:<396x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3523:<396x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3523:<396x1536xbf16>{1536, 1}, %3637:<396x1536xbf16>{3072, 1}+1536)
          outputs: (%3618:<396x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3618:<396x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3618:<396x1536xbf16>{1536,1}}))
          duration: -1
276800 2024-12-10 17:48:40.976669 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n136,rank6)
        - aten::mm:
          inputs: (%3618:<396x1536xbf16>{1536, 1}, %3642:<1536x5120xbf16>{1, 1536})
          outputs: (%3707:<396x5120xbf16>{5120,1})
          duration: -1
276877 2024-12-10 17:48:40.980470 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n136,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3618:<396x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3707:<396x5120xbf16>{5120,1},None:NoneType})
          duration: -1
276893 2024-12-10 17:48:40.981241 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n129,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2852:<396x5120xbf16>{5120, 1}+6901760})
          outputs: (%3673:tuple{%3707:<396x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3375:<396x5120xbf16>{5120, 1}+6901760, %3707:<396x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3375:<396x5120xbf16>{5120,1}+6901760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3700:<44x5120xbf16>{5120, 1}+8929280})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3700:<44x5120xbf16>{5120,1}+8929280}))
          duration: -1
277061 2024-12-10 17:48:40.991519 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n130,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3700:<44x5120xbf16>{5120, 1}+8929280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3700:<44x5120xbf16>{5120,1}+8929280}))
          duration: -1
277082 2024-12-10 17:48:40.992350 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n164,rank6)
        - aten::mm:
          inputs: (%3700:<44x5120xbf16>{5120, 1}+8929280, %3452:<5120x3072xbf16>{1, 5120})
          outputs: (%3636:<44x3072xbf16>{3072,1})
          duration: -1
277171 2024-12-10 17:48:40.996253 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n164,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3700:<44x5120xbf16>{5120, 1}+8929280})
          outputs: (%3682:tuple{%3636:<44x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3169:<44x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3169:<44x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3169:<44x1536xbf16>{3072, 1})
          outputs: (%3637:<44x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3169:<44x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3637:<44x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3637:<44x1536xbf16>{1536, 1}, %3714:<44x1536xbf16>{3072, 1}+1536)
          outputs: (%2780:<44x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%2780:<44x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%2780:<44x1536xbf16>{1536,1}}))
          duration: -1
277274 2024-12-10 17:48:41.006250 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n137,rank6)
        - aten::mm:
          inputs: (%2780:<44x1536xbf16>{1536, 1}, %3642:<1536x5120xbf16>{1, 1536})
          outputs: (%3647:<44x5120xbf16>{5120,1})
          duration: -1
277354 2024-12-10 17:48:41.010087 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n137,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%2780:<44x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3647:<44x5120xbf16>{5120,1},None:NoneType})
          duration: -1
277369 2024-12-10 17:48:41.010853 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n130,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3700:<44x5120xbf16>{5120, 1}+8929280})
          outputs: (%3673:tuple{%3647:<44x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2852:<44x5120xbf16>{5120, 1}+8929280, %3647:<44x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2852:<44x5120xbf16>{5120,1}+8929280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2852:<196x5120xbf16>{5120, 1}+9154560})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2852:<196x5120xbf16>{5120,1}+9154560}))
          duration: -1
277541 2024-12-10 17:48:41.021210 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n131,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2852:<196x5120xbf16>{5120, 1}+9154560})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2852:<196x5120xbf16>{5120,1}+9154560}))
          duration: -1
277559 2024-12-10 17:48:41.021947 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n165,rank6)
        - aten::mm:
          inputs: (%2852:<196x5120xbf16>{5120, 1}+9154560, %3642:<5120x3072xbf16>{1, 5120})
          outputs: (%3712:<196x3072xbf16>{3072,1})
          duration: -1
277648 2024-12-10 17:48:41.025782 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n165,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2852:<196x5120xbf16>{5120, 1}+9154560})
          outputs: (%3696:tuple{%3712:<196x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3714:<196x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3714:<196x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3714:<196x1536xbf16>{3072, 1})
          outputs: (%3645:<196x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3714:<196x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3645:<196x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3645:<196x1536xbf16>{1536, 1}, %2113:<196x1536xbf16>{3072, 1}+1536)
          outputs: (%3636:<196x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3636:<196x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3636:<196x1536xbf16>{1536,1}}))
          duration: -1
277754 2024-12-10 17:48:41.035750 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n138,rank6)
        - aten::mm:
          inputs: (%3636:<196x1536xbf16>{1536, 1}, %3723:<1536x5120xbf16>{1, 1536})
          outputs: (%3645:<196x5120xbf16>{5120,1})
          duration: -1
277833 2024-12-10 17:48:41.039549 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n138,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3636:<196x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3645:<196x5120xbf16>{5120,1},None:NoneType})
          duration: -1
277847 2024-12-10 17:48:41.040303 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n131,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2852:<196x5120xbf16>{5120, 1}+9154560})
          outputs: (%3673:tuple{%3645:<196x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3701:<196x5120xbf16>{5120, 1}+9154560, %3645:<196x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3701:<196x5120xbf16>{5120,1}+9154560)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2942:<87x5120xbf16>{5120, 1}+10158080})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2942:<87x5120xbf16>{5120,1}+10158080}))
          duration: -1
278024 2024-12-10 17:48:41.050796 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n132,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2942:<87x5120xbf16>{5120, 1}+10158080})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2942:<87x5120xbf16>{5120,1}+10158080}))
          duration: -1
278041 2024-12-10 17:48:41.051578 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n166,rank6)
        - aten::mm:
          inputs: (%2942:<87x5120xbf16>{5120, 1}+10158080, %3421:<5120x3072xbf16>{1, 5120})
          outputs: (%3714:<87x3072xbf16>{3072,1})
          duration: -1
278133 2024-12-10 17:48:41.055404 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n166,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2942:<87x5120xbf16>{5120, 1}+10158080})
          outputs: (%3698:tuple{%3714:<87x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3169:<87x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3169:<87x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3169:<87x1536xbf16>{3072, 1})
          outputs: (%3636:<87x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3169:<87x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3636:<87x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3636:<87x1536xbf16>{1536, 1}, %3647:<87x1536xbf16>{3072, 1}+1536)
          outputs: (%3707:<87x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3707:<87x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3707:<87x1536xbf16>{1536,1}}))
          duration: -1
278237 2024-12-10 17:48:41.065363 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n139,rank6)
        - aten::mm:
          inputs: (%3707:<87x1536xbf16>{1536, 1}, %3723:<1536x5120xbf16>{1, 1536})
          outputs: (%2852:<87x5120xbf16>{5120,1})
          duration: -1
278318 2024-12-10 17:48:41.069195 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n139,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3707:<87x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%2852:<87x5120xbf16>{5120,1},None:NoneType})
          duration: -1
278330 2024-12-10 17:48:41.070010 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n132,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2942:<87x5120xbf16>{5120, 1}+10158080})
          outputs: (%3673:tuple{%2852:<87x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2780:<87x5120xbf16>{5120, 1}+10158080, %2852:<87x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2780:<87x5120xbf16>{5120,1}+10158080)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3701:<79x5120xbf16>{5120, 1}+10603520})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3701:<79x5120xbf16>{5120,1}+10603520}))
          duration: -1
278506 2024-12-10 17:48:41.080368 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n133,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3701:<79x5120xbf16>{5120, 1}+10603520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3701:<79x5120xbf16>{5120,1}+10603520}))
          duration: -1
278520 2024-12-10 17:48:41.081124 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n167,rank6)
        - aten::mm:
          inputs: (%3701:<79x5120xbf16>{5120, 1}+10603520, %3728:<5120x3072xbf16>{1, 5120})
          outputs: (%3523:<79x3072xbf16>{3072,1})
          duration: -1
278610 2024-12-10 17:48:41.084914 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n167,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3701:<79x5120xbf16>{5120, 1}+10603520})
          outputs: (%3682:tuple{%3523:<79x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3647:<79x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3647:<79x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3647:<79x1536xbf16>{3072, 1})
          outputs: (%3072:<79x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3647:<79x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3072:<79x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<79x1536xbf16>{1536, 1}, %3636:<79x1536xbf16>{3072, 1}+1536)
          outputs: (%3597:<79x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3597:<79x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3597:<79x1536xbf16>{1536,1}}))
          duration: -1
278715 2024-12-10 17:48:41.094891 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n140,rank6)
        - aten::mm:
          inputs: (%3597:<79x1536xbf16>{1536, 1}, %3728:<1536x5120xbf16>{1, 1536})
          outputs: (%3636:<79x5120xbf16>{5120,1})
          duration: -1
278801 2024-12-10 17:48:41.098674 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n140,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3597:<79x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3636:<79x5120xbf16>{5120,1},None:NoneType})
          duration: -1
278814 2024-12-10 17:48:41.099441 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n133,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3701:<79x5120xbf16>{5120, 1}+10603520})
          outputs: (%3673:tuple{%3636:<79x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2942:<79x5120xbf16>{5120, 1}+10603520, %3636:<79x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2942:<79x5120xbf16>{5120,1}+10603520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3618:<587x5120xbf16>{5120, 1}+11008000})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3618:<587x5120xbf16>{5120,1}+11008000}))
          duration: -1
278985 2024-12-10 17:48:41.109722 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n134,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3618:<587x5120xbf16>{5120, 1}+11008000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3618:<587x5120xbf16>{5120,1}+11008000}))
          duration: -1
278999 2024-12-10 17:48:41.110456 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n168,rank6)
        - aten::mm:
          inputs: (%3618:<587x5120xbf16>{5120, 1}+11008000, %3421:<5120x3072xbf16>{1, 5120})
          outputs: (%2113:<587x3072xbf16>{3072,1})
          duration: -1
279081 2024-12-10 17:48:41.114308 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n168,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3618:<587x5120xbf16>{5120, 1}+11008000})
          outputs: (%3696:tuple{%2113:<587x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3517:<587x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3517:<587x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3517:<587x1536xbf16>{3072, 1})
          outputs: (%3072:<587x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3517:<587x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3072:<587x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<587x1536xbf16>{1536, 1}, %3701:<587x1536xbf16>{3072, 1}+1536)
          outputs: (%3523:<587x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3523:<587x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3523:<587x1536xbf16>{1536,1}}))
          duration: -1
279197 2024-12-10 17:48:41.124267 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n141,rank6)
        - aten::mm:
          inputs: (%3523:<587x1536xbf16>{1536, 1}, %3728:<1536x5120xbf16>{1, 1536})
          outputs: (%3700:<587x5120xbf16>{5120,1})
          duration: -1
279273 2024-12-10 17:48:41.128076 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n141,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3523:<587x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3700:<587x5120xbf16>{5120,1},None:NoneType})
          duration: -1
279282 2024-12-10 17:48:41.128838 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n134,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3618:<587x5120xbf16>{5120, 1}+11008000})
          outputs: (%3673:tuple{%3700:<587x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3375:<587x5120xbf16>{5120, 1}+11008000, %3700:<587x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3375:<587x5120xbf16>{5120,1}+11008000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2942:<169x5120xbf16>{5120, 1}+14013440})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2942:<169x5120xbf16>{5120,1}+14013440}))
          duration: -1
279455 2024-12-10 17:48:41.139125 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n135,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2942:<169x5120xbf16>{5120, 1}+14013440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2942:<169x5120xbf16>{5120,1}+14013440}))
          duration: -1
279471 2024-12-10 17:48:41.139862 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n169,rank6)
        - aten::mm:
          inputs: (%2942:<169x5120xbf16>{5120, 1}+14013440, %3421:<5120x3072xbf16>{1, 5120})
          outputs: (%3707:<169x3072xbf16>{3072,1})
          duration: -1
279557 2024-12-10 17:48:41.143690 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n169,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2942:<169x5120xbf16>{5120, 1}+14013440})
          outputs: (%3698:tuple{%3707:<169x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3647:<169x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3647:<169x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3647:<169x1536xbf16>{3072, 1})
          outputs: (%3072:<169x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3647:<169x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3072:<169x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<169x1536xbf16>{1536, 1}, %3618:<169x1536xbf16>{3072, 1}+1536)
          outputs: (%3645:<169x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3645:<169x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3645:<169x1536xbf16>{1536,1}}))
          duration: -1
279675 2024-12-10 17:48:41.153706 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n142,rank6)
        - aten::mm:
          inputs: (%3645:<169x1536xbf16>{1536, 1}, %3736:<1536x5120xbf16>{1, 1536})
          outputs: (%3523:<169x5120xbf16>{5120,1})
          duration: -1
279750 2024-12-10 17:48:41.157521 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n142,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3645:<169x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3523:<169x5120xbf16>{5120,1},None:NoneType})
          duration: -1
279760 2024-12-10 17:48:41.158286 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n135,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2942:<169x5120xbf16>{5120, 1}+14013440})
          outputs: (%3673:tuple{%3523:<169x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3618:<169x5120xbf16>{5120, 1}+14013440, %3523:<169x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3618:<169x5120xbf16>{5120,1}+14013440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2780:<321x5120xbf16>{5120, 1}+14878720})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2780:<321x5120xbf16>{5120,1}+14878720}))
          duration: -1
279929 2024-12-10 17:48:41.168554 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n136,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2780:<321x5120xbf16>{5120, 1}+14878720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2780:<321x5120xbf16>{5120,1}+14878720}))
          duration: -1
279947 2024-12-10 17:48:41.169313 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n170,rank6)
        - aten::mm:
          inputs: (%2780:<321x5120xbf16>{5120, 1}+14878720, %3738:<5120x3072xbf16>{1, 5120})
          outputs: (%1570:<321x3072xbf16>{3072,1})
          duration: -1
280031 2024-12-10 17:48:41.173125 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n170,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2780:<321x5120xbf16>{5120, 1}+14878720})
          outputs: (%3682:tuple{%1570:<321x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2942:<321x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2942:<321x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2942:<321x1536xbf16>{3072, 1})
          outputs: (%3645:<321x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2942:<321x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3645:<321x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3645:<321x1536xbf16>{1536, 1}, %3700:<321x1536xbf16>{3072, 1}+1536)
          outputs: (%3636:<321x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3636:<321x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3636:<321x1536xbf16>{1536,1}}))
          duration: -1
280153 2024-12-10 17:48:41.183095 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n143,rank6)
        - aten::mm:
          inputs: (%3636:<321x1536xbf16>{1536, 1}, %3741:<1536x5120xbf16>{1, 1536})
          outputs: (%3436:<321x5120xbf16>{5120,1})
          duration: -1
280228 2024-12-10 17:48:41.186923 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n143,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3636:<321x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3436:<321x5120xbf16>{5120,1},None:NoneType})
          duration: -1
280239 2024-12-10 17:48:41.187690 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n136,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2780:<321x5120xbf16>{5120, 1}+14878720})
          outputs: (%3673:tuple{%3436:<321x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3375:<321x5120xbf16>{5120, 1}+14878720, %3436:<321x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3375:<321x5120xbf16>{5120,1}+14878720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2852:<34x5120xbf16>{5120, 1}+16522240})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2852:<34x5120xbf16>{5120,1}+16522240}))
          duration: -1
280408 2024-12-10 17:48:41.198065 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n137,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2852:<34x5120xbf16>{5120, 1}+16522240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2852:<34x5120xbf16>{5120,1}+16522240}))
          duration: -1
280425 2024-12-10 17:48:41.198806 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n171,rank6)
        - aten::mm:
          inputs: (%2852:<34x5120xbf16>{5120, 1}+16522240, %3714:<5120x3072xbf16>{1, 5120})
          outputs: (%2113:<34x3072xbf16>{3072,1})
          duration: -1
280512 2024-12-10 17:48:41.202662 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n171,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2852:<34x5120xbf16>{5120, 1}+16522240})
          outputs: (%3696:tuple{%2113:<34x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3597:<34x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3597:<34x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3597:<34x1536xbf16>{3072, 1})
          outputs: (%3647:<34x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3597:<34x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3647:<34x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3647:<34x1536xbf16>{1536, 1}, %3636:<34x1536xbf16>{3072, 1}+1536)
          outputs: (%3707:<34x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3707:<34x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3707:<34x1536xbf16>{1536,1}}))
          duration: -1
280633 2024-12-10 17:48:41.212624 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n144,rank6)
        - aten::mm:
          inputs: (%3707:<34x1536xbf16>{1536, 1}, %3745:<1536x5120xbf16>{1, 1536})
          outputs: (%3645:<34x5120xbf16>{5120,1})
          duration: -1
280709 2024-12-10 17:48:41.216466 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n144,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3707:<34x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3645:<34x5120xbf16>{5120,1},None:NoneType})
          duration: -1
280719 2024-12-10 17:48:41.217238 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n137,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2852:<34x5120xbf16>{5120, 1}+16522240})
          outputs: (%3673:tuple{%3645:<34x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3700:<34x5120xbf16>{5120, 1}+16522240, %3645:<34x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3700:<34x5120xbf16>{5120,1}+16522240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3375:<222x5120xbf16>{5120, 1}+16696320})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3375:<222x5120xbf16>{5120,1}+16696320}))
          duration: -1
280887 2024-12-10 17:48:41.227593 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n138,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3375:<222x5120xbf16>{5120, 1}+16696320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3375:<222x5120xbf16>{5120,1}+16696320}))
          duration: -1
280904 2024-12-10 17:48:41.228329 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n172,rank6)
        - aten::mm:
          inputs: (%3375:<222x5120xbf16>{5120, 1}+16696320, %3745:<5120x3072xbf16>{1, 5120})
          outputs: (%3523:<222x3072xbf16>{3072,1})
          duration: -1
280991 2024-12-10 17:48:41.232200 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n172,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3375:<222x5120xbf16>{5120, 1}+16696320})
          outputs: (%3698:tuple{%3523:<222x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3700:<222x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3700:<222x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3700:<222x1536xbf16>{3072, 1})
          outputs: (%3072:<222x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3700:<222x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3072:<222x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<222x1536xbf16>{1536, 1}, %3169:<222x1536xbf16>{3072, 1}+1536)
          outputs: (%3636:<222x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3636:<222x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3636:<222x1536xbf16>{1536,1}}))
          duration: -1
281113 2024-12-10 17:48:41.242259 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n145,rank6)
        - aten::mm:
          inputs: (%3636:<222x1536xbf16>{1536, 1}, %3745:<1536x5120xbf16>{1, 1536})
          outputs: (%3707:<222x5120xbf16>{5120,1})
          duration: -1
281189 2024-12-10 17:48:41.246143 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n145,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3636:<222x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3707:<222x5120xbf16>{5120,1},None:NoneType})
          duration: -1
281199 2024-12-10 17:48:41.246916 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n138,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3375:<222x5120xbf16>{5120, 1}+16696320})
          outputs: (%3673:tuple{%3707:<222x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2852:<222x5120xbf16>{5120, 1}+16696320, %3707:<222x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2852:<222x5120xbf16>{5120,1}+16696320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2852:<335x5120xbf16>{5120, 1}+17832960})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2852:<335x5120xbf16>{5120,1}+17832960}))
          duration: -1
281375 2024-12-10 17:48:41.257312 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n139,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2852:<335x5120xbf16>{5120, 1}+17832960})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2852:<335x5120xbf16>{5120,1}+17832960}))
          duration: -1
281391 2024-12-10 17:48:41.258054 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n173,rank6)
        - aten::mm:
          inputs: (%2852:<335x5120xbf16>{5120, 1}+17832960, %3736:<5120x3072xbf16>{1, 5120})
          outputs: (%3701:<335x3072xbf16>{3072,1})
          duration: -1
281474 2024-12-10 17:48:41.261909 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n173,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2852:<335x5120xbf16>{5120, 1}+17832960})
          outputs: (%3682:tuple{%3701:<335x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3618:<335x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3618:<335x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3618:<335x1536xbf16>{3072, 1})
          outputs: (%3647:<335x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3618:<335x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3647:<335x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3647:<335x1536xbf16>{1536, 1}, %3700:<335x1536xbf16>{3072, 1}+1536)
          outputs: (%3745:<335x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3745:<335x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3745:<335x1536xbf16>{1536,1}}))
          duration: -1
281601 2024-12-10 17:48:41.271924 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n146,rank6)
        - aten::mm:
          inputs: (%3745:<335x1536xbf16>{1536, 1}, %3738:<1536x5120xbf16>{1, 1536})
          outputs: (%2113:<335x5120xbf16>{5120,1})
          duration: -1
281667 2024-12-10 17:48:41.275797 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n146,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3745:<335x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%2113:<335x5120xbf16>{5120,1},None:NoneType})
          duration: -1
281678 2024-12-10 17:48:41.276560 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n139,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2852:<335x5120xbf16>{5120, 1}+17832960})
          outputs: (%3673:tuple{%2113:<335x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3714:<335x5120xbf16>{5120, 1}+17832960, %2113:<335x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3714:<335x5120xbf16>{5120,1}+17832960)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3375:<331x5120xbf16>{5120, 1}+19548160})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3375:<331x5120xbf16>{5120,1}+19548160}))
          duration: -1
281850 2024-12-10 17:48:41.286934 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n140,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3375:<331x5120xbf16>{5120, 1}+19548160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3375:<331x5120xbf16>{5120,1}+19548160}))
          duration: -1
281866 2024-12-10 17:48:41.287697 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n174,rank6)
        - aten::mm:
          inputs: (%3375:<331x5120xbf16>{5120, 1}+19548160, %3738:<5120x3072xbf16>{1, 5120})
          outputs: (%3523:<331x3072xbf16>{3072,1})
          duration: -1
281948 2024-12-10 17:48:41.291537 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n174,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3375:<331x5120xbf16>{5120, 1}+19548160})
          outputs: (%3696:tuple{%3523:<331x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3645:<331x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3645:<331x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3645:<331x1536xbf16>{3072, 1})
          outputs: (%3636:<331x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3645:<331x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3636:<331x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3636:<331x1536xbf16>{1536, 1}, %3169:<331x1536xbf16>{3072, 1}+1536)
          outputs: (%3741:<331x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3741:<331x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3741:<331x1536xbf16>{1536,1}}))
          duration: -1
282081 2024-12-10 17:48:41.301534 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n147,rank6)
        - aten::mm:
          inputs: (%3741:<331x1536xbf16>{1536, 1}, %1782:<1536x5120xbf16>{1, 1536})
          outputs: (%2852:<331x5120xbf16>{5120,1})
          duration: -1
282148 2024-12-10 17:48:41.305351 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n147,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3741:<331x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%2852:<331x5120xbf16>{5120,1},None:NoneType})
          duration: -1
282159 2024-12-10 17:48:41.306111 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n140,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3375:<331x5120xbf16>{5120, 1}+19548160})
          outputs: (%3673:tuple{%2852:<331x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2942:<331x5120xbf16>{5120, 1}+19548160, %2852:<331x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2942:<331x5120xbf16>{5120,1}+19548160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3645:<191x5120xbf16>{5120, 1}+21242880})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3645:<191x5120xbf16>{5120,1}+21242880}))
          duration: -1
282328 2024-12-10 17:48:41.316410 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n141,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3645:<191x5120xbf16>{5120, 1}+21242880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3645:<191x5120xbf16>{5120,1}+21242880}))
          duration: -1
282345 2024-12-10 17:48:41.317172 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n175,rank6)
        - aten::mm:
          inputs: (%3645:<191x5120xbf16>{5120, 1}+21242880, %3738:<5120x3072xbf16>{1, 5120})
          outputs: (%3723:<191x3072xbf16>{3072,1})
          duration: -1
282428 2024-12-10 17:48:41.321014 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n175,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3645:<191x5120xbf16>{5120, 1}+21242880})
          outputs: (%3698:tuple{%3723:<191x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3647:<191x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3647:<191x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3647:<191x1536xbf16>{3072, 1})
          outputs: (%3618:<191x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3647:<191x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3618:<191x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3618:<191x1536xbf16>{1536, 1}, %3523:<191x1536xbf16>{3072, 1}+1536)
          outputs: (%3636:<191x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3636:<191x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3636:<191x1536xbf16>{1536,1}}))
          duration: -1
282560 2024-12-10 17:48:41.331004 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n148,rank6)
        - aten::mm:
          inputs: (%3636:<191x1536xbf16>{1536, 1}, %3738:<1536x5120xbf16>{1, 1536})
          outputs: (%2160:<191x5120xbf16>{5120,1})
          duration: -1
282626 2024-12-10 17:48:41.334864 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n148,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3636:<191x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%2160:<191x5120xbf16>{5120,1},None:NoneType})
          duration: -1
282639 2024-12-10 17:48:41.335671 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n141,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3645:<191x5120xbf16>{5120, 1}+21242880})
          outputs: (%3673:tuple{%2160:<191x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3169:<191x5120xbf16>{5120, 1}+21242880, %2160:<191x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3169:<191x5120xbf16>{5120,1}+21242880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3700:<257x5120xbf16>{5120, 1}+22220800})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3700:<257x5120xbf16>{5120,1}+22220800}))
          duration: -1
282808 2024-12-10 17:48:41.346007 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n142,rank6)
282824 2024-12-10 17:48:41.346775 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n176,rank6)
        - aten::mm:
          inputs: (%3700:<257x5120xbf16>{5120, 1}+22220800, %3647:<5120x3072xbf16>{1, 5120})
          outputs: (%3636:<257x3072xbf16>{3072,1})
          duration: -1
282906 2024-12-10 17:48:41.350640 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n176,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3700:<257x5120xbf16>{5120, 1}+22220800})
          outputs: (%3682:tuple{%3636:<257x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3647:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3647:<257x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3647:<257x1536xbf16>{3072, 1})
          outputs: (%3645:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3647:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3645:<257x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3645:<257x1536xbf16>{1536, 1}, %3523:<257x1536xbf16>{3072, 1}+1536)
          outputs: (%2780:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%2780:<257x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%2780:<257x1536xbf16>{1536,1}}))
          duration: -1
283039 2024-12-10 17:48:41.360653 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n149,rank6)
        - aten::mm:
          inputs: (%2780:<257x1536xbf16>{1536, 1}, %3738:<1536x5120xbf16>{1, 1536})
          outputs: (%3636:<257x5120xbf16>{5120,1})
          duration: -1
283104 2024-12-10 17:48:41.364467 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n149,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%2780:<257x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3636:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
283118 2024-12-10 17:48:41.365249 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n142,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3700:<257x5120xbf16>{5120, 1}+22220800})
          outputs: (%3673:tuple{%3636:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2780:<257x5120xbf16>{5120, 1}+22220800, %3636:<257x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2780:<257x5120xbf16>{5120,1}+22220800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%3169:<380x5120xbf16>{5120, 1}+23536640})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%3169:<380x5120xbf16>{5120,1}+23536640}))
          duration: -1
283287 2024-12-10 17:48:41.375717 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n143,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%3169:<380x5120xbf16>{5120, 1}+23536640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%3169:<380x5120xbf16>{5120,1}+23536640}))
          duration: -1
283304 2024-12-10 17:48:41.376454 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n177,rank6)
        - aten::mm:
          inputs: (%3169:<380x5120xbf16>{5120, 1}+23536640, %3738:<5120x3072xbf16>{1, 5120})
          outputs: (%3523:<380x3072xbf16>{3072,1})
          duration: -1
283384 2024-12-10 17:48:41.380289 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n177,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%3169:<380x5120xbf16>{5120, 1}+23536640})
          outputs: (%3696:tuple{%3523:<380x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3714:<380x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3714:<380x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3714:<380x1536xbf16>{3072, 1})
          outputs: (%3647:<380x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3714:<380x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3647:<380x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3647:<380x1536xbf16>{1536, 1}, %3645:<380x1536xbf16>{3072, 1}+1536)
          outputs: (%2780:<380x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%2780:<380x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%2780:<380x1536xbf16>{1536,1}}))
          duration: -1
283519 2024-12-10 17:48:41.390270 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n150,rank6)
        - aten::mm:
          inputs: (%2780:<380x1536xbf16>{1536, 1}, %3741:<1536x5120xbf16>{1, 1536})
          outputs: (%3707:<380x5120xbf16>{5120,1})
          duration: -1
283583 2024-12-10 17:48:41.394117 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n150,rank6)
283598 2024-12-10 17:48:41.394930 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n143,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%3169:<380x5120xbf16>{5120, 1}+23536640})
          outputs: (%3673:tuple{%3707:<380x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2113:<380x5120xbf16>{5120, 1}+23536640, %3707:<380x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2113:<380x5120xbf16>{5120,1}+23536640)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2942:<690x5120xbf16>{5120, 1}+25482240})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2942:<690x5120xbf16>{5120,1}+25482240}))
          duration: -1
283765 2024-12-10 17:48:41.405268 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n144,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2942:<690x5120xbf16>{5120, 1}+25482240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2942:<690x5120xbf16>{5120,1}+25482240}))
          duration: -1
283781 2024-12-10 17:48:41.406004 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n178,rank6)
        - aten::mm:
          inputs: (%2942:<690x5120xbf16>{5120, 1}+25482240, %3738:<5120x3072xbf16>{1, 5120})
          outputs: (%3723:<690x3072xbf16>{3072,1})
          duration: -1
283862 2024-12-10 17:48:41.409826 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n178,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2942:<690x5120xbf16>{5120, 1}+25482240})
          outputs: (%3698:tuple{%3723:<690x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3169:<690x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3169:<690x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3169:<690x1536xbf16>{3072, 1})
          outputs: (%3647:<690x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3169:<690x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3647:<690x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3647:<690x1536xbf16>{1536, 1}, %3618:<690x1536xbf16>{3072, 1}+1536)
          outputs: (%3636:<690x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3636:<690x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3636:<690x1536xbf16>{1536,1}}))
          duration: -1
283993 2024-12-10 17:48:41.419809 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n151,rank6)
        - aten::mm:
          inputs: (%3636:<690x1536xbf16>{1536, 1}, %3745:<1536x5120xbf16>{1, 1536})
          outputs: (%3738:<690x5120xbf16>{5120,1})
          duration: -1
284048 2024-12-10 17:48:41.423612 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n151,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3636:<690x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3738:<690x5120xbf16>{5120,1},None:NoneType})
          duration: -1
284061 2024-12-10 17:48:41.424382 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n144,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2942:<690x5120xbf16>{5120, 1}+25482240})
          outputs: (%3673:tuple{%3738:<690x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3375:<690x5120xbf16>{5120, 1}+25482240, %3738:<690x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3375:<690x5120xbf16>{5120,1}+25482240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%2113:<150x5120xbf16>{5120, 1}+29015040})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%2113:<150x5120xbf16>{5120,1}+29015040}))
          duration: -1
284222 2024-12-10 17:48:41.434681 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n145,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%2113:<150x5120xbf16>{5120, 1}+29015040})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%2113:<150x5120xbf16>{5120,1}+29015040}))
          duration: -1
284239 2024-12-10 17:48:41.435439 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n179,rank6)
        - aten::mm:
          inputs: (%2113:<150x5120xbf16>{5120, 1}+29015040, %3745:<5120x3072xbf16>{1, 5120})
          outputs: (%3714:<150x3072xbf16>{3072,1})
          duration: -1
284317 2024-12-10 17:48:41.439255 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n179,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%2113:<150x5120xbf16>{5120, 1}+29015040})
          outputs: (%3682:tuple{%3714:<150x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3645:<150x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3645:<150x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3645:<150x1536xbf16>{3072, 1})
          outputs: (%3618:<150x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3645:<150x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3618:<150x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3618:<150x1536xbf16>{1536, 1}, %3647:<150x1536xbf16>{3072, 1}+1536)
          outputs: (%3707:<150x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3707:<150x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3707:<150x1536xbf16>{1536,1}}))
          duration: -1
284458 2024-12-10 17:48:41.449273 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n152,rank6)
        - aten::mm:
          inputs: (%3707:<150x1536xbf16>{1536, 1}, %3741:<1536x5120xbf16>{1, 1536})
          outputs: (%3636:<150x5120xbf16>{5120,1})
          duration: -1
284521 2024-12-10 17:48:41.453089 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n152,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3707:<150x1536xbf16>{1536, 1}})
          outputs: (%3673:tuple{%3636:<150x5120xbf16>{5120,1},None:NoneType})
          duration: -1
284538 2024-12-10 17:48:41.453864 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n145,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%2113:<150x5120xbf16>{5120, 1}+29015040})
          outputs: (%3673:tuple{%3636:<150x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2942:<150x5120xbf16>{5120, 1}+29015040, %3636:<150x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2942:<150x5120xbf16>{5120,1}+29015040)
          duration: -1
284607 2024-12-10 17:48:41.458803 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n6,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%3677:tuple{%3693:<5817x5120xbf16>{5120, 1}, %1424:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3767:tuple{%3688:<5817x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%2113:<5817x5120xbf16>{5120, 1}, 0:int, %3692:<5817x5120xCUSTOM_DATA_TYPE>{1, 0}, %3688:<5817x5120xbf16>{5120, 1})
          outputs: (%2939:<5817x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%2939:<5817x5120xbf16>{5120, 1}, %2852:<5817x1xbf16>{1, 1})
          outputs: (%2942:<5817x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%3769:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2852:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%2852:<8192x5120xbf16>{5120, 1}, 0:int, %2096:<5817x5120xCUSTOM_DATA_TYPE>{1, 0}, %2942:<5817x5120xbf16>{5120, 1})
          outputs: (%3072:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%3701:<1024x5120xbf16>{5120, 1}, %3072:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%3701:<1024x5120xbf16>{5120,1},%3072:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%3701:<1024x5120xbf16>{5120, 1}, %3072:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%3701:<1024x5120xbf16>{5120,1},%3072:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%3701:<1024x5120xbf16>{5120, 1}, %3072:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%3770:tuple{%3701:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3668:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%3668:tuple{%1779:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
284935 2024-12-10 17:48:41.509862 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n146,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3695:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3695:tuple{%1779:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
284960 2024-12-10 17:48:41.510659 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n180,rank6)
        - aten::mm:
          inputs: (%3700:<1024x5120xbf16>{5120, 1}, %2942:<5120x6144xbf16>{1, 5120})
          outputs: (%3690:<1024x6144xbf16>{6144,1})
          duration: -1
285114 2024-12-10 17:48:41.518837 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n180,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3695:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3677:tuple{%2780:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1570:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1570:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1570:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%3072:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1570:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%3072:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<1024x1x3072xbf16>{3072, 3072, 1}, %2584:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%3375:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3678:tuple{%3375:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3678:tuple{%3375:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
285216 2024-12-10 17:48:41.528819 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n153,rank6)
        - aten::mm:
          inputs: (%3700:<1024x3072xbf16>{3072, 1}, %3714:<3072x5120xbf16>{1, 3072})
          outputs: (%2942:<1024x5120xbf16>{5120,1})
          duration: -1
285368 2024-12-10 17:48:41.536789 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n153,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3678:tuple{%3375:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%3673:tuple{%3072:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
285384 2024-12-10 17:48:41.537580 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n146,rank6)
        - ----------->api::MLP return:
          inputs: (%3668:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3673:tuple{%3072:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%3773:<1024x1x5120xbf16>{5120, 5120, 1}, %3375:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2852:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
285432 2024-12-10 17:48:41.540741 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n6,rank6)
        - ----------->api::MoELayer return:
          inputs: (%3664:tuple{%1779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3673:tuple{%2852:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3782:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3782:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3783:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3783:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55bafe0e70_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55bafe0e70_:_InferenceMode)
          duration: -1
287535 2024-12-10 17:48:42.067665 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n6,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%3785:tuple{%3745:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3786:tuple{%3745:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3786:tuple{%3745:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
287604 2024-12-10 17:48:42.074517 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n20,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3745:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3745:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3745:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3745:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3787:tuple{%3745:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3787:tuple{%3745:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3787:tuple{%3745:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3787:tuple{%3745:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3791:tuple{%3745:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %3790:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3791:tuple{%3745:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%3790:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3745:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3787:tuple{%3745:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3316:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3745:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%3316:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
287786 2024-12-10 17:48:42.119123 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n20,rank6)
287789 2024-12-10 17:48:42.119652 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n6,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%3316:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3792:tuple{%3316:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%3792:tuple{%3316:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
287823 2024-12-10 17:48:42.125996 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n181,rank6)
        - aten::mm:
          inputs: (%3418:<1024x5120xbf16>{5120, 1}, %3701:<5120x102400xbf16>{1, 5120})
          outputs: (%3080:<1024x102400xbf16>{102400,1})
          duration: -1
287974 2024-12-10 17:48:42.138420 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n181,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3792:tuple{%3316:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%3800:tuple{%3078:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%3418:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%3801:tuple{%2942:<1024x1xf32>{1,1},%3067:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%2942:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%2942:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%1816:list{%2942:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3802:tuple{%3592:list{%2942:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%3418:<1024x1x102400xf32>{102400, 102400, 1}, %3707:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%3418:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%1562:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3701:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%1562:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3803:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3701:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3803:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3804:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3693:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%3475:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%3475:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3789:list{%3804:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3803:<i32>, False:bool)
          outputs: (%3475:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3375:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%2914:<1024x102400xf32>{102400, 1}, %3592:list{%3375:<1024xCUSTOM_DATA_TYPE>{1}, %1411:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%1424:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%3597:<1024x1xf32>{1, 1}, %3592:list{%3804:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1424:<i32>, False:bool)
          outputs: (%3597:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%3418:<1024x1x102400xf32>{102400, 102400, 1}, out=%3418:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%3418:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%3418:<1024x1x102400xf32>{102400, 102400, 1}, %3789:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3790:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3597:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3597:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3789:list{%3597:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3800:tuple{%3797:list{%3597:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3790:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3790:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3806:list{%3790:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3807:tuple{%3808:list{%3790:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%3790:<1024x1xf32>{1, 1})
          outputs: (%3773:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3773:<1024x1xf32>{1, 1}, %3597:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3375:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%3418:<1024x1x102400xf32>{102400, 102400, 1}, %2978:<1024x1x1xf32>{1, 1, 1})
          outputs: (%3418:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%3809:tuple{%3418:<1024x1x102400xf32>{102400, 102400, 1}, %3804:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %1411:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%3809:tuple{%3418:<1024x1x102400xf32>{102400,102400,1},%3804:<1024x1xCUSTOM_DATA_TYPE>{1,1},%1411:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2684:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device='cuda_6')_:dict)
          outputs: (%3597:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3597:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3597:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3597:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device='cuda_6')_:dict)
          outputs: (%3597:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3589:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3581:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3516,_19890,___279,_____,____11,__2677,__5008]],_device='cuda_6')_:dict)
          outputs: (%3597:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%1411:<1024xf32>{1}, %3078:<1024xf32>{1})
          outputs: (%2942:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2942:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3475:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%3078:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3714:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3475:<i32>, %3714:<i32>)
          outputs: (%3316:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%3316:<i32>)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%3811:list{%3812:<1xf32>{1}}, 0:int)
          outputs: (%1424:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1424:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1424:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3568:list{%1424:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%3796:tuple{%3794:list{%1424:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%1424:<1xf32>{1}, 8:int)
          outputs: (%1411:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3316:<i32>, 1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%2942:<i32>, 1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%3814:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3701:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%3701:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%3701:<1xf32>{1}))
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %3701:<1xf32>{1})
          outputs: (%3316:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%3316:<1xf32>{1}, 1:int)
          outputs: (%2978:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%1574:<i32>, 0:int, alpha=1:int)
          outputs: (%1574:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3316:<i32>, %3813:<i32>, alpha=1:int)
          outputs: (%3316:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3078:<i32>, %3316:<i32>, False:bool)
          outputs: (%3078:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3375:<i32>, 1:int, alpha=1:int)
          outputs: (%3375:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%1385:<i32>, %3375:<i32>, False:bool)
          outputs: (%1385:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%3794:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%2068:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%433:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %434:list{%433:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%430:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%435:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%430:<1024xCUSTOM_DATA_TYPE>{1}, %434:list{%435:<1024xCUSTOM_DATA_TYPE>{1}}, %431:<i32>, False:bool)
          outputs: (%430:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1, %434:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, %431:<i32>, False:bool)
          outputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - c10d::broadcast_:
          inputs: (%3820:list{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3800:tuple{%3798:list{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1385:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1385:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3821:list{%1385:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3822:tuple{%3794:list{%1385:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3322:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3322:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3820:list{%3322:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3823:tuple{%3592:list{%3322:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3601:list{%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3800:tuple{%3824:list{%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3701:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%3825:list{%3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%3822:tuple{%3798:list{%3701:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3701:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3701:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3701:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2859:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2859:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3701:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_8046,_27781,__4786,_____,____11,___548,___441]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[_8046,_27781,__4786,_____,____11,___548,___441]],_device)
          duration: -1
289993 2024-12-10 17:48:42.329348 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n7,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%3826:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%3826:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
290014 2024-12-10 17:48:42.330102 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n7,rank6)
        - ----------->api::embedding call:
          inputs: (%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%2942:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%3816:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%3816:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
290107 2024-12-10 17:48:42.344967 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n7,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%3826:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3790:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%3826:tuple{%3830:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%3826:tuple{%3830:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
290150 2024-12-10 17:48:42.347037 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n7,rank6)
        - ----------->api::dropout call:
          inputs: (%3830:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3830:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3830:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3830:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
290186 2024-12-10 17:48:42.353382 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n7,rank6)
        - ----------->api::Dropout return:
          inputs: (%3826:tuple{%3830:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3830:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
290200 2024-12-10 17:48:42.354113 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n7,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_8046,_27781,__4786,_____,____11,___548,___441]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%3830:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%3826:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%3826:tuple{1024:int}))
          duration: -1
290232 2024-12-10 17:48:42.355732 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n7,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3831:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%3831:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%3832:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3834:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%3375:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%3820:list{%3375:<1024x20xf32>{20, 1}, %3375:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%3835:<1024x40xf32>{40,1})
          duration: -1
290381 2024-12-10 17:48:42.366757 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n7,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%3826:tuple{1024:int})
          outputs: (%3836:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
290419 2024-12-10 17:48:42.373122 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n7,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
290496 2024-12-10 17:48:42.382110 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n7,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3826:tuple{%3837:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3826:tuple{%3837:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
290516 2024-12-10 17:48:42.382849 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n21,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3837:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3837:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3837:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3837:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3787:tuple{%3837:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3787:tuple{%3837:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3787:tuple{%3837:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3787:tuple{%3837:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3580:tuple{%3837:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3840:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3580:tuple{%3837:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3840:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%437:list{%430:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%436:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3837:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3787:tuple{%3837:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3641:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3837:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%3641:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
290767 2024-12-10 17:48:42.427194 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n21,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%3826:tuple{%3641:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%3826:tuple{%3641:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
290787 2024-12-10 17:48:42.431448 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n7,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3841:tuple{%3641:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3841:tuple{%3641:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
290804 2024-12-10 17:48:42.432220 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n182,rank6)
        - aten::mm:
          inputs: (%3614:<1024x5120xbf16>{5120, 1}, %3840:<5120x1536xbf16>{1, 5120})
          outputs: (%3633:<1024x1536xbf16>{1536,1})
          duration: -1
290945 2024-12-10 17:48:42.440083 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n182,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3841:tuple{%3641:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3796:tuple{%3636:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3841:tuple{%3636:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3841:tuple{%3636:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
290973 2024-12-10 17:48:42.441117 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n183,rank6)
        - aten::mm:
          inputs: (%3840:<1024x1536xbf16>{1536, 1}, %3489:<1536x24576xbf16>{1, 1536})
          outputs: (%3633:<1024x24576xbf16>{24576,1})
          duration: -1
291090 2024-12-10 17:48:42.447253 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n183,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3841:tuple{%3636:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%3850:tuple{%3597:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3078:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3811:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3078:<1024x1x128x192xbf16>{24576,24576,192,1},%3811:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3078:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3811:list{128:int, 64:int}, -1:int)
          outputs: (%3845:tuple{%3773:<1024x1x128x128xbf16>{24576,24576,192,1},%3840:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3841:tuple{%3641:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3841:tuple{%3641:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
291190 2024-12-10 17:48:42.456937 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n184,rank6)
        - aten::mm:
          inputs: (%3526:<1024x5120xbf16>{5120, 1}, %3853:<5120x576xbf16>{1, 5120})
          outputs: (%3452:<1024x576xbf16>{576,1})
          duration: -1
291330 2024-12-10 17:48:42.464854 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n184,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3841:tuple{%3641:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3845:tuple{%3647:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3647:<1024x1x576xbf16>{576, 576, 1}, %3856:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3647:<1024x1x576xbf16>{576,576,1},%3856:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3647:<1024x1x576xbf16>{576, 576, 1}, %3856:list{512:int, 64:int}, -1:int)
          outputs: (%3850:tuple{%3853:<1024x1x512xbf16>{576,576,1},%3636:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3841:tuple{%3853:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3841:tuple{%3853:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
291412 2024-12-10 17:48:42.473163 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n185,rank6)
        - aten::mm:
          inputs: (%3439:<1024x512xbf16>{576, 1}, %3741:<512x32768xbf16>{1, 512})
          outputs: (%3581:<1024x32768xbf16>{32768,1})
          duration: -1
291519 2024-12-10 17:48:42.479357 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n185,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3841:tuple{%3853:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%3845:tuple{%3436:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3452:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3857:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%3452:<1024x1x128x256xbf16>{32768,32768,256,1},%3857:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3452:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %3857:list{128:int, 128:int}, -1:int)
          outputs: (%3796:tuple{%3448:<1024x1x128x128xbf16>{32768,32768,256,1},%3581:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%3827:tuple{%3589:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%3827:tuple{%3589:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
291643 2024-12-10 17:48:42.491213 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n7,rank6)
291684 2024-12-10 17:48:42.494372 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n7,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%3827:tuple{%3589:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%3822:tuple{%3576:<1024x64xbf16>{64,1},%3723:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%3576:<1024x64xbf16>{64, 1}, %3861:list{%3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3840:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%3723:<1024x64xbf16>{64, 1}, %3851:list{%3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3636:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3738:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3862:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3433:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3581:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%3864:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%3861:list{%3864:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %3863:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%3072:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3072:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3421:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3863:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3433:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3863:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%3581:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3441:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3862:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3433:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3072:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%3865:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%3861:list{%3865:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %2852:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%3866:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3866:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3421:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%2852:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3433:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %2852:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%3865:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3741:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%3636:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::stack:
          inputs: (%439:list{%415:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%438:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%438:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%2852:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3581:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%2852:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%3738:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3441:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%3738:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3738:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%1570:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%3647:<128x1024x192xbf16>{196608, 192, 1}, %1570:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%3072:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%3072:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%3647:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%3645:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%3647:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %3861:list{%3645:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3608:<i32>, False:bool)
          outputs: (%3647:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%3072:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %3647:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%3597:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%3597:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%3597:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%3421:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%3608:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%3597:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %3608:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%3608:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout return:
          inputs: (%1570:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%1570:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%3597:<128x1024x1024xbf16>{1048576, 1024, 1}, %3072:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%3441:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3841:tuple{%3597:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3841:tuple{%3597:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
293184 2024-12-10 17:48:42.613753 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n154,rank6)
        - aten::mm:
          inputs: (%3815:<1024x16384xbf16>{16384, 1}, %3441:<16384x5120xbf16>{1, 16384})
          outputs: (%3581:<1024x5120xbf16>{5120,1})
          duration: -1
293332 2024-12-10 17:48:42.621946 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n154,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3841:tuple{%3597:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%3673:tuple{%3441:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
293348 2024-12-10 17:48:42.622713 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n7,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%3826:tuple{%3641:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3673:tuple{%3441:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3872:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3872:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3873:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3873:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b84f8db0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b84f8db0_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3875:tuple{%3874:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3875:tuple{%3874:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
295702 2024-12-10 17:48:43.172267 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n22,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3874:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3874:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3874:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3874:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%3876:tuple{%3874:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%3876:tuple{%3874:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%3876:tuple{%3874:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3876:tuple{%3874:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%3881:tuple{%3874:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %3880:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%3881:tuple{%3874:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%3880:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3874:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %3876:tuple{%3874:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3878:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3874:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%3878:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
295929 2024-12-10 17:48:43.217118 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n22,rank6)
        - ----------->api::MoELayer call:
          inputs: (%3875:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%3875:tuple{%3878:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
295938 2024-12-10 17:48:43.217878 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n7,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%3882:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%3882:tuple{%3878:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
295949 2024-12-10 17:48:43.218621 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n7,rank6)
        - aten::mm:
          inputs: (%3885:<1024x5120xbf16>{5120, 1}, %3883:<5120x160xbf16>{1, 5120})
          outputs: (%3886:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%3441:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%3608:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%3436:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%3889:tuple{%3433:<1024x6xbf16>{6,1},%3608:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%3862:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%3581:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%3553:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%3441:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%3441:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %3439:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%3441:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%3441:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %3553:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3439:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3581:<1024x160xf32>{160, 1}, %3553:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%3589:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3589:<160xf32>{1}, %3439:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%3738:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3738:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%3202:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%3202:<i32>, 2_5431315104166666e-07:float)
          outputs: (%3441:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3441:<i32>, 0_01:float)
          outputs: (%3589:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3738:<i32>, %3891:<i32>, alpha=1:int)
          outputs: (%3738:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3892:<i32>, %3738:<i32>, False:bool)
          outputs: (%3892:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%3894:tuple{%3441:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%3894:tuple{%3441:<i32>}))
          duration: -1
296408 2024-12-10 17:48:43.254437 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n7,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%3882:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3897:tuple{%3072:<1024x6xbf16>{6,1},%3608:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3441:<8192x5120xbf16>{5120, 1}, %3886:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3441:<8192x5120xbf16>{5120,1},%3886:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3441:<8192x5120xbf16>{5120, 1}, %3886:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3889:tuple{%3441:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3436:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3608:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3436:<8192x6xCUSTOM_DATA_TYPE>{6,1},%3608:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3436:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3608:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3897:tuple{%3436:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%3436:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%3576:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%3436:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%3901:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%3576:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3901:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3581:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%3436:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3581:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3902:<8979xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3905:<8192x6xbf16>{6, 1}, %3072:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3899:tuple{%3905:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%3905:<8192x6xbf16>{6, 1}, %3581:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3439:<8979xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%3581:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3906:<8979x2xCUSTOM_DATA_TYPE>{1,8979})
          duration: -1
        - aten::gather:
          inputs: (%3441:<8192x5120xbf16>{5120, 1}, 0:int, %3908:<8979x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3738:<8979x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%3902:<8979xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%3893:tuple{%3909:<8979xCUSTOM_DATA_TYPE>{1},%3892:<8979xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%3891:<8979xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%3910:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%3738:<8979x5120xbf16>{5120, 1}, 0:int, %3914:<8979x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3910:<8979x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%3893:tuple{%3910:<8979x5120xbf16>{5120, 1}, %3912:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%3893:tuple{%3910:<8979x5120xbf16>{5120,1},%3912:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
297438 2024-12-10 17:48:43.657727 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n7,rank6)
        - aten::cumsum:
          inputs: (%3912:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%3581:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%3907:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%3905:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%3907:list{%3905:<1xCUSTOM_DATA_TYPE>{1}, %3581:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%3738:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3576:<162x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3576:<162x5120xbf16>{5120,1}}))
          duration: -1
297609 2024-12-10 17:48:43.668850 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n147,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3576:<162x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3576:<162x5120xbf16>{5120,1}}))
          duration: -1
297631 2024-12-10 17:48:43.669608 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n186,rank6)
        - aten::mm:
          inputs: (%3576:<162x5120xbf16>{5120, 1}, %3919:<5120x3072xbf16>{1, 5120})
          outputs: (%3920:<162x3072xbf16>{3072,1})
          duration: -1
297712 2024-12-10 17:48:43.673492 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n186,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3576:<162x5120xbf16>{5120, 1}})
          outputs: (%3922:tuple{%3920:<162x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3333:<162x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3333:<162x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3333:<162x1536xbf16>{3072, 1})
          outputs: (%3924:<162x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3333:<162x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3924:<162x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3924:<162x1536xbf16>{1536, 1}, %3923:<162x1536xbf16>{3072, 1}+1536)
          outputs: (%3925:<162x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3925:<162x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3925:<162x1536xbf16>{1536,1}}))
          duration: -1
297814 2024-12-10 17:48:43.683470 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n155,rank6)
        - aten::mm:
          inputs: (%3925:<162x1536xbf16>{1536, 1}, %3919:<1536x5120xbf16>{1, 1536})
          outputs: (%3920:<162x5120xbf16>{5120,1})
          duration: -1
297894 2024-12-10 17:48:43.687280 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n155,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3925:<162x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3920:<162x5120xbf16>{5120,1},None:NoneType})
          duration: -1
297910 2024-12-10 17:48:43.688054 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n147,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3576:<162x5120xbf16>{5120, 1}})
          outputs: (%3889:tuple{%3920:<162x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3924:<162x5120xbf16>{5120, 1}, %3920:<162x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3924:<162x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3923:<540x5120xbf16>{5120, 1}+829440})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3923:<540x5120xbf16>{5120,1}+829440}))
          duration: -1
298088 2024-12-10 17:48:43.698448 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n148,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3923:<540x5120xbf16>{5120, 1}+829440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3923:<540x5120xbf16>{5120,1}+829440}))
          duration: -1
298108 2024-12-10 17:48:43.699181 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n187,rank6)
        - aten::mm:
          inputs: (%3923:<540x5120xbf16>{5120, 1}+829440, %1853:<5120x3072xbf16>{1, 5120})
          outputs: (%3689:<540x3072xbf16>{3072,1})
          duration: -1
298191 2024-12-10 17:48:43.703039 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n187,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3923:<540x5120xbf16>{5120, 1}+829440})
          outputs: (%3897:tuple{%3689:<540x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3928:<540x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3928:<540x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3928:<540x1536xbf16>{3072, 1})
          outputs: (%3929:<540x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3928:<540x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3929:<540x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3929:<540x1536xbf16>{1536, 1}, %3891:<540x1536xbf16>{3072, 1}+1536)
          outputs: (%1756:<540x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%1756:<540x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%1756:<540x1536xbf16>{1536,1}}))
          duration: -1
298293 2024-12-10 17:48:43.713127 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n156,rank6)
        - aten::mm:
          inputs: (%1756:<540x1536xbf16>{1536, 1}, %1162:<1536x5120xbf16>{1, 1536})
          outputs: (%3891:<540x5120xbf16>{5120,1})
          duration: -1
298376 2024-12-10 17:48:43.716959 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n156,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%1756:<540x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3891:<540x5120xbf16>{5120,1},None:NoneType})
          duration: -1
298392 2024-12-10 17:48:43.717738 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n148,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3923:<540x5120xbf16>{5120, 1}+829440})
          outputs: (%3889:tuple{%3891:<540x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3929:<540x5120xbf16>{5120, 1}+829440, %3891:<540x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3929:<540x5120xbf16>{5120,1}+829440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3928:<566x5120xbf16>{5120, 1}+3594240})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3928:<566x5120xbf16>{5120,1}+3594240}))
          duration: -1
298570 2024-12-10 17:48:43.728054 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n149,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3928:<566x5120xbf16>{5120, 1}+3594240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3928:<566x5120xbf16>{5120,1}+3594240}))
          duration: -1
298588 2024-12-10 17:48:43.728825 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n188,rank6)
        - aten::mm:
          inputs: (%3928:<566x5120xbf16>{5120, 1}+3594240, %1238:<5120x3072xbf16>{1, 5120})
          outputs: (%3335:<566x3072xbf16>{3072,1})
          duration: -1
298673 2024-12-10 17:48:43.732714 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n188,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3928:<566x5120xbf16>{5120, 1}+3594240})
          outputs: (%3918:tuple{%3335:<566x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3790:<566x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3790:<566x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3790:<566x1536xbf16>{3072, 1})
          outputs: (%1979:<566x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3790:<566x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1979:<566x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1979:<566x1536xbf16>{1536, 1}, %3576:<566x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<566x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%1238:<566x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%1238:<566x1536xbf16>{1536,1}}))
          duration: -1
298772 2024-12-10 17:48:43.742821 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n157,rank6)
        - aten::mm:
          inputs: (%1238:<566x1536xbf16>{1536, 1}, %1539:<1536x5120xbf16>{1, 1536})
          outputs: (%3689:<566x5120xbf16>{5120,1})
          duration: -1
298857 2024-12-10 17:48:43.746630 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n157,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%1238:<566x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3689:<566x5120xbf16>{5120,1},None:NoneType})
          duration: -1
298872 2024-12-10 17:48:43.747386 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n149,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3928:<566x5120xbf16>{5120, 1}+3594240})
          outputs: (%3889:tuple{%3689:<566x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3920:<566x5120xbf16>{5120, 1}+3594240, %3689:<566x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3920:<566x5120xbf16>{5120,1}+3594240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1570:<226x5120xbf16>{5120, 1}+6492160})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1570:<226x5120xbf16>{5120,1}+6492160}))
          duration: -1
299050 2024-12-10 17:48:43.757710 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n150,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1570:<226x5120xbf16>{5120, 1}+6492160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1570:<226x5120xbf16>{5120,1}+6492160}))
          duration: -1
299068 2024-12-10 17:48:43.758451 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n189,rank6)
        - aten::mm:
          inputs: (%1570:<226x5120xbf16>{5120, 1}+6492160, %1979:<5120x3072xbf16>{1, 5120})
          outputs: (%1238:<226x3072xbf16>{3072,1})
          duration: -1
299152 2024-12-10 17:48:43.762319 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n189,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1570:<226x5120xbf16>{5120, 1}+6492160})
          outputs: (%3922:tuple{%1238:<226x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1979:<226x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1979:<226x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1979:<226x1536xbf16>{3072, 1})
          outputs: (%3901:<226x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1979:<226x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3901:<226x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3901:<226x1536xbf16>{1536, 1}, %3335:<226x1536xbf16>{3072, 1}+1536)
          outputs: (%3920:<226x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3920:<226x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3920:<226x1536xbf16>{1536,1}}))
          duration: -1
299253 2024-12-10 17:48:43.772413 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n158,rank6)
        - aten::mm:
          inputs: (%3920:<226x1536xbf16>{1536, 1}, %1979:<1536x5120xbf16>{1, 1536})
          outputs: (%1238:<226x5120xbf16>{5120,1})
          duration: -1
299336 2024-12-10 17:48:43.776320 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n158,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3920:<226x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%1238:<226x5120xbf16>{5120,1},None:NoneType})
          duration: -1
299352 2024-12-10 17:48:43.777097 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n150,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1570:<226x5120xbf16>{5120, 1}+6492160})
          outputs: (%3889:tuple{%1238:<226x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1539:<226x5120xbf16>{5120, 1}+6492160, %1238:<226x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1539:<226x5120xbf16>{5120,1}+6492160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3920:<57x5120xbf16>{5120, 1}+7649280})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3920:<57x5120xbf16>{5120,1}+7649280}))
          duration: -1
299529 2024-12-10 17:48:43.787593 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n151,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3920:<57x5120xbf16>{5120, 1}+7649280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3920:<57x5120xbf16>{5120,1}+7649280}))
          duration: -1
299548 2024-12-10 17:48:43.788331 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n190,rank6)
        - aten::mm:
          inputs: (%3920:<57x5120xbf16>{5120, 1}+7649280, %3636:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<57x3072xbf16>{3072,1})
          duration: -1
299632 2024-12-10 17:48:43.792233 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n190,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3920:<57x5120xbf16>{5120, 1}+7649280})
          outputs: (%3897:tuple{%3891:<57x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<57x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<57x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<57x1536xbf16>{3072, 1})
          outputs: (%3913:<57x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<57x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3913:<57x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3913:<57x1536xbf16>{1536, 1}, %3335:<57x1536xbf16>{3072, 1}+1536)
          outputs: (%3689:<57x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3689:<57x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3689:<57x1536xbf16>{1536,1}}))
          duration: -1
299733 2024-12-10 17:48:43.802206 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n159,rank6)
        - aten::mm:
          inputs: (%3689:<57x1536xbf16>{1536, 1}, %3636:<1536x5120xbf16>{1, 1536})
          outputs: (%1162:<57x5120xbf16>{5120,1})
          duration: -1
299817 2024-12-10 17:48:43.806044 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n159,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3689:<57x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%1162:<57x5120xbf16>{5120,1},None:NoneType})
          duration: -1
299832 2024-12-10 17:48:43.806816 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n151,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3920:<57x5120xbf16>{5120, 1}+7649280})
          outputs: (%3889:tuple{%1162:<57x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<57x5120xbf16>{5120, 1}+7649280, %1162:<57x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<57x5120xbf16>{5120,1}+7649280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1570:<609x5120xbf16>{5120, 1}+7941120})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1570:<609x5120xbf16>{5120,1}+7941120}))
          duration: -1
300010 2024-12-10 17:48:43.817238 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n152,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1570:<609x5120xbf16>{5120, 1}+7941120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1570:<609x5120xbf16>{5120,1}+7941120}))
          duration: -1
300032 2024-12-10 17:48:43.817996 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n191,rank6)
        - aten::mm:
          inputs: (%1570:<609x5120xbf16>{5120, 1}+7941120, %3939:<5120x3072xbf16>{1, 5120})
          outputs: (%3924:<609x3072xbf16>{3072,1})
          duration: -1
300113 2024-12-10 17:48:43.821910 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n191,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1570:<609x5120xbf16>{5120, 1}+7941120})
          outputs: (%3918:tuple{%3924:<609x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3790:<609x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3790:<609x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3790:<609x1536xbf16>{3072, 1})
          outputs: (%3913:<609x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3790:<609x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3913:<609x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3913:<609x1536xbf16>{1536, 1}, %3939:<609x1536xbf16>{3072, 1}+1536)
          outputs: (%3923:<609x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3923:<609x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3923:<609x1536xbf16>{1536,1}}))
          duration: -1
300217 2024-12-10 17:48:43.831921 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n160,rank6)
        - aten::mm:
          inputs: (%3923:<609x1536xbf16>{1536, 1}, %3928:<1536x5120xbf16>{1, 1536})
          outputs: (%3891:<609x5120xbf16>{5120,1})
          duration: -1
300298 2024-12-10 17:48:43.835769 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n160,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3923:<609x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3891:<609x5120xbf16>{5120,1},None:NoneType})
          duration: -1
300313 2024-12-10 17:48:43.836534 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n152,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1570:<609x5120xbf16>{5120, 1}+7941120})
          outputs: (%3889:tuple{%3891:<609x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3901:<609x5120xbf16>{5120, 1}+7941120, %3891:<609x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3901:<609x5120xbf16>{5120,1}+7941120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3923:<391x5120xbf16>{5120, 1}+11059200})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3923:<391x5120xbf16>{5120,1}+11059200}))
          duration: -1
300491 2024-12-10 17:48:43.846862 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n153,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3923:<391x5120xbf16>{5120, 1}+11059200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3923:<391x5120xbf16>{5120,1}+11059200}))
          duration: -1
300510 2024-12-10 17:48:43.847597 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n192,rank6)
        - aten::mm:
          inputs: (%3923:<391x5120xbf16>{5120, 1}+11059200, %3448:<5120x3072xbf16>{1, 5120})
          outputs: (%3913:<391x3072xbf16>{3072,1})
          duration: -1
300593 2024-12-10 17:48:43.851413 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n192,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3923:<391x5120xbf16>{5120, 1}+11059200})
          outputs: (%3922:tuple{%3913:<391x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3901:<391x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3901:<391x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3901:<391x1536xbf16>{3072, 1})
          outputs: (%3928:<391x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3901:<391x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3928:<391x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3928:<391x1536xbf16>{1536, 1}, %3920:<391x1536xbf16>{3072, 1}+1536)
          outputs: (%3840:<391x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3840:<391x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3840:<391x1536xbf16>{1536,1}}))
          duration: -1
300696 2024-12-10 17:48:43.861402 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n161,rank6)
        - aten::mm:
          inputs: (%3840:<391x1536xbf16>{1536, 1}, %3636:<1536x5120xbf16>{1, 1536})
          outputs: (%3901:<391x5120xbf16>{5120,1})
          duration: -1
300777 2024-12-10 17:48:43.865286 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n161,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3840:<391x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3901:<391x5120xbf16>{5120,1},None:NoneType})
          duration: -1
300792 2024-12-10 17:48:43.866049 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n153,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3923:<391x5120xbf16>{5120, 1}+11059200})
          outputs: (%3889:tuple{%3901:<391x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3945:<391x5120xbf16>{5120, 1}+11059200, %3901:<391x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3945:<391x5120xbf16>{5120,1}+11059200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3335:<911x5120xbf16>{5120, 1}+13061120})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3335:<911x5120xbf16>{5120,1}+13061120}))
          duration: -1
300971 2024-12-10 17:48:43.876348 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n154,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3335:<911x5120xbf16>{5120, 1}+13061120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3335:<911x5120xbf16>{5120,1}+13061120}))
          duration: -1
300990 2024-12-10 17:48:43.877113 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n193,rank6)
        - aten::mm:
          inputs: (%3335:<911x5120xbf16>{5120, 1}+13061120, %3928:<5120x3072xbf16>{1, 5120})
          outputs: (%3923:<911x3072xbf16>{3072,1})
          duration: -1
301072 2024-12-10 17:48:43.880906 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n193,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3335:<911x5120xbf16>{5120, 1}+13061120})
          outputs: (%3897:tuple{%3923:<911x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3928:<911x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3928:<911x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3928:<911x1536xbf16>{3072, 1})
          outputs: (%3920:<911x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3928:<911x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3920:<911x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3920:<911x1536xbf16>{1536, 1}, %3924:<911x1536xbf16>{3072, 1}+1536)
          outputs: (%3891:<911x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3891:<911x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3891:<911x1536xbf16>{1536,1}}))
          duration: -1
301174 2024-12-10 17:48:43.890885 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n162,rank6)
        - aten::mm:
          inputs: (%3891:<911x1536xbf16>{1536, 1}, %3928:<1536x5120xbf16>{1, 1536})
          outputs: (%3924:<911x5120xbf16>{5120,1})
          duration: -1
301255 2024-12-10 17:48:43.894780 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n162,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3891:<911x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3924:<911x5120xbf16>{5120,1},None:NoneType})
          duration: -1
301271 2024-12-10 17:48:43.895556 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n154,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3335:<911x5120xbf16>{5120, 1}+13061120})
          outputs: (%3889:tuple{%3924:<911x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<911x5120xbf16>{5120, 1}+13061120, %3924:<911x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<911x5120xbf16>{5120,1}+13061120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3945:<751x5120xbf16>{5120, 1}+17725440})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3945:<751x5120xbf16>{5120,1}+17725440}))
          duration: -1
301449 2024-12-10 17:48:43.905869 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n155,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3945:<751x5120xbf16>{5120, 1}+17725440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3945:<751x5120xbf16>{5120,1}+17725440}))
          duration: -1
301468 2024-12-10 17:48:43.906605 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n194,rank6)
        - aten::mm:
          inputs: (%3945:<751x5120xbf16>{5120, 1}+17725440, %3939:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<751x3072xbf16>{3072,1})
          duration: -1
301551 2024-12-10 17:48:43.910440 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n194,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3945:<751x5120xbf16>{5120, 1}+17725440})
          outputs: (%3918:tuple{%3891:<751x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3939:<751x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3939:<751x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3939:<751x1536xbf16>{3072, 1})
          outputs: (%3928:<751x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3939:<751x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3928:<751x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3928:<751x1536xbf16>{1536, 1}, %3576:<751x1536xbf16>{3072, 1}+1536)
          outputs: (%3913:<751x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3913:<751x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3913:<751x1536xbf16>{1536,1}}))
          duration: -1
301652 2024-12-10 17:48:43.920475 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n163,rank6)
        - aten::mm:
          inputs: (%3913:<751x1536xbf16>{1536, 1}, %3633:<1536x5120xbf16>{1, 1536})
          outputs: (%3928:<751x5120xbf16>{5120,1})
          duration: -1
301733 2024-12-10 17:48:43.924360 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n163,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3913:<751x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3928:<751x5120xbf16>{5120,1},None:NoneType})
          duration: -1
301748 2024-12-10 17:48:43.925142 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n155,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3945:<751x5120xbf16>{5120, 1}+17725440})
          outputs: (%3889:tuple{%3928:<751x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1162:<751x5120xbf16>{5120, 1}+17725440, %3928:<751x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1162:<751x5120xbf16>{5120,1}+17725440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1238:<477x5120xbf16>{5120, 1}+21570560})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1238:<477x5120xbf16>{5120,1}+21570560}))
          duration: -1
301923 2024-12-10 17:48:43.935542 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n156,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1238:<477x5120xbf16>{5120, 1}+21570560})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1238:<477x5120xbf16>{5120,1}+21570560}))
          duration: -1
301946 2024-12-10 17:48:43.936276 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n195,rank6)
        - aten::mm:
          inputs: (%1238:<477x5120xbf16>{5120, 1}+21570560, %3633:<5120x3072xbf16>{1, 5120})
          outputs: (%3920:<477x3072xbf16>{3072,1})
          duration: -1
302028 2024-12-10 17:48:43.940131 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n195,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1238:<477x5120xbf16>{5120, 1}+21570560})
          outputs: (%3922:tuple{%3920:<477x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3924:<477x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3924:<477x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3924:<477x1536xbf16>{3072, 1})
          outputs: (%3576:<477x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3924:<477x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<477x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<477x1536xbf16>{1536, 1}, %3901:<477x1536xbf16>{3072, 1}+1536)
          outputs: (%3891:<477x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3891:<477x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3891:<477x1536xbf16>{1536,1}}))
          duration: -1
302134 2024-12-10 17:48:43.950231 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n164,rank6)
        - aten::mm:
          inputs: (%3891:<477x1536xbf16>{1536, 1}, %3953:<1536x5120xbf16>{1, 1536})
          outputs: (%3913:<477x5120xbf16>{5120,1})
          duration: -1
302213 2024-12-10 17:48:43.954147 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n164,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3891:<477x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3913:<477x5120xbf16>{5120,1},None:NoneType})
          duration: -1
302228 2024-12-10 17:48:43.954947 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n156,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1238:<477x5120xbf16>{5120, 1}+21570560})
          outputs: (%3889:tuple{%3913:<477x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3901:<477x5120xbf16>{5120, 1}+21570560, %3913:<477x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3901:<477x5120xbf16>{5120,1}+21570560)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3923:<207x5120xbf16>{5120, 1}+24012800})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3923:<207x5120xbf16>{5120,1}+24012800}))
          duration: -1
302406 2024-12-10 17:48:43.965417 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n157,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3923:<207x5120xbf16>{5120, 1}+24012800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3923:<207x5120xbf16>{5120,1}+24012800}))
          duration: -1
302427 2024-12-10 17:48:43.966159 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n196,rank6)
        - aten::mm:
          inputs: (%3923:<207x5120xbf16>{5120, 1}+24012800, %3953:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<207x3072xbf16>{3072,1})
          duration: -1
302510 2024-12-10 17:48:43.970129 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n196,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3923:<207x5120xbf16>{5120, 1}+24012800})
          outputs: (%3897:tuple{%3891:<207x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1853:<207x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1853:<207x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1853:<207x1536xbf16>{3072, 1})
          outputs: (%3924:<207x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1853:<207x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3924:<207x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3924:<207x1536xbf16>{1536, 1}, %3953:<207x1536xbf16>{3072, 1}+1536)
          outputs: (%3576:<207x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3576:<207x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3576:<207x1536xbf16>{1536,1}}))
          duration: -1
302615 2024-12-10 17:48:43.980085 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n165,rank6)
        - aten::mm:
          inputs: (%3576:<207x1536xbf16>{1536, 1}, %3633:<1536x5120xbf16>{1, 1536})
          outputs: (%1853:<207x5120xbf16>{5120,1})
          duration: -1
302694 2024-12-10 17:48:43.983910 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n165,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3576:<207x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%1853:<207x5120xbf16>{5120,1},None:NoneType})
          duration: -1
302710 2024-12-10 17:48:43.984707 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n157,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3923:<207x5120xbf16>{5120, 1}+24012800})
          outputs: (%3889:tuple{%1853:<207x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<207x5120xbf16>{5120, 1}+24012800, %1853:<207x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<207x5120xbf16>{5120,1}+24012800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3335:<190x5120xbf16>{5120, 1}+25072640})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3335:<190x5120xbf16>{5120,1}+25072640}))
          duration: -1
302889 2024-12-10 17:48:43.995103 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n158,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3335:<190x5120xbf16>{5120, 1}+25072640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3335:<190x5120xbf16>{5120,1}+25072640}))
          duration: -1
302908 2024-12-10 17:48:43.995840 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n197,rank6)
        - aten::mm:
          inputs: (%3335:<190x5120xbf16>{5120, 1}+25072640, %3928:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<190x3072xbf16>{3072,1})
          duration: -1
302989 2024-12-10 17:48:43.999721 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n197,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3335:<190x5120xbf16>{5120, 1}+25072640})
          outputs: (%3918:tuple{%3891:<190x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3953:<190x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3953:<190x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3953:<190x1536xbf16>{3072, 1})
          outputs: (%3913:<190x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3953:<190x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3913:<190x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3913:<190x1536xbf16>{1536, 1}, %3928:<190x1536xbf16>{3072, 1}+1536)
          outputs: (%3576:<190x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3576:<190x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3576:<190x1536xbf16>{1536,1}}))
          duration: -1
303095 2024-12-10 17:48:44.009770 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n166,rank6)
        - aten::mm:
          inputs: (%3576:<190x1536xbf16>{1536, 1}, %3636:<1536x5120xbf16>{1, 1536})
          outputs: (%3923:<190x5120xbf16>{5120,1})
          duration: -1
303172 2024-12-10 17:48:44.013639 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n166,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3576:<190x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3923:<190x5120xbf16>{5120,1},None:NoneType})
          duration: -1
303189 2024-12-10 17:48:44.014403 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n158,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3335:<190x5120xbf16>{5120, 1}+25072640})
          outputs: (%3889:tuple{%3923:<190x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3920:<190x5120xbf16>{5120, 1}+25072640, %3923:<190x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3920:<190x5120xbf16>{5120,1}+25072640)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3945:<488x5120xbf16>{5120, 1}+26045440})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3945:<488x5120xbf16>{5120,1}+26045440}))
          duration: -1
303367 2024-12-10 17:48:44.024731 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n159,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3945:<488x5120xbf16>{5120, 1}+26045440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3945:<488x5120xbf16>{5120,1}+26045440}))
          duration: -1
303387 2024-12-10 17:48:44.025517 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n198,rank6)
        - aten::mm:
          inputs: (%3945:<488x5120xbf16>{5120, 1}+26045440, %3939:<5120x3072xbf16>{1, 5120})
          outputs: (%3953:<488x3072xbf16>{3072,1})
          duration: -1
303465 2024-12-10 17:48:44.029330 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n198,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3945:<488x5120xbf16>{5120, 1}+26045440})
          outputs: (%3922:tuple{%3953:<488x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3790:<488x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3790:<488x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3790:<488x1536xbf16>{3072, 1})
          outputs: (%3939:<488x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3790:<488x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3939:<488x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3939:<488x1536xbf16>{1536, 1}, %1853:<488x1536xbf16>{3072, 1}+1536)
          outputs: (%3920:<488x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3920:<488x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3920:<488x1536xbf16>{1536,1}}))
          duration: -1
303575 2024-12-10 17:48:44.039382 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n167,rank6)
        - aten::mm:
          inputs: (%3920:<488x1536xbf16>{1536, 1}, %3708:<1536x5120xbf16>{1, 1536})
          outputs: (%3953:<488x5120xbf16>{5120,1})
          duration: -1
303650 2024-12-10 17:48:44.043210 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n167,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3920:<488x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3953:<488x5120xbf16>{5120,1},None:NoneType})
          duration: -1
303668 2024-12-10 17:48:44.043985 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n159,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3945:<488x5120xbf16>{5120, 1}+26045440})
          outputs: (%3889:tuple{%3953:<488x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1162:<488x5120xbf16>{5120, 1}+26045440, %3953:<488x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1162:<488x5120xbf16>{5120,1}+26045440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1238:<1103x5120xbf16>{5120, 1}+28544000})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1238:<1103x5120xbf16>{5120,1}+28544000}))
          duration: -1
303850 2024-12-10 17:48:44.054621 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n160,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1238:<1103x5120xbf16>{5120, 1}+28544000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1238:<1103x5120xbf16>{5120,1}+28544000}))
          duration: -1
303869 2024-12-10 17:48:44.055360 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n199,rank6)
        - aten::mm:
          inputs: (%1238:<1103x5120xbf16>{5120, 1}+28544000, %3939:<5120x3072xbf16>{1, 5120})
          outputs: (%3928:<1103x3072xbf16>{3072,1})
          duration: -1
303945 2024-12-10 17:48:44.059171 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n199,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1238:<1103x5120xbf16>{5120, 1}+28544000})
          outputs: (%3897:tuple{%3928:<1103x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<1103x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<1103x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<1103x1536xbf16>{3072, 1})
          outputs: (%3924:<1103x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<1103x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3924:<1103x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3924:<1103x1536xbf16>{1536, 1}, %3939:<1103x1536xbf16>{3072, 1}+1536)
          outputs: (%3923:<1103x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3923:<1103x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3923:<1103x1536xbf16>{1536,1}}))
          duration: -1
304057 2024-12-10 17:48:44.069264 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n168,rank6)
        - aten::mm:
          inputs: (%3923:<1103x1536xbf16>{1536, 1}, %3928:<1536x5120xbf16>{1, 1536})
          outputs: (%3924:<1103x5120xbf16>{5120,1})
          duration: -1
304132 2024-12-10 17:48:44.073134 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n168,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3923:<1103x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3924:<1103x5120xbf16>{5120,1},None:NoneType})
          duration: -1
304151 2024-12-10 17:48:44.073928 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n160,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1238:<1103x5120xbf16>{5120, 1}+28544000})
          outputs: (%3889:tuple{%3924:<1103x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<1103x5120xbf16>{5120, 1}+28544000, %3924:<1103x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1570:<1103x5120xbf16>{5120,1}+28544000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3335:<377x5120xbf16>{5120, 1}+34191360})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3335:<377x5120xbf16>{5120,1}+34191360}))
          duration: -1
304330 2024-12-10 17:48:44.084420 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n161,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3335:<377x5120xbf16>{5120, 1}+34191360})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3335:<377x5120xbf16>{5120,1}+34191360}))
          duration: -1
304348 2024-12-10 17:48:44.085177 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n200,rank6)
        - aten::mm:
          inputs: (%3335:<377x5120xbf16>{5120, 1}+34191360, %1853:<5120x3072xbf16>{1, 5120})
          outputs: (%3928:<377x3072xbf16>{3072,1})
          duration: -1
304427 2024-12-10 17:48:44.088994 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n200,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3335:<377x5120xbf16>{5120, 1}+34191360})
          outputs: (%3918:tuple{%3928:<377x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3913:<377x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3913:<377x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3913:<377x1536xbf16>{3072, 1})
          outputs: (%3891:<377x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3913:<377x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3891:<377x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3891:<377x1536xbf16>{1536, 1}, %3953:<377x1536xbf16>{3072, 1}+1536)
          outputs: (%3689:<377x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3689:<377x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3689:<377x1536xbf16>{1536,1}}))
          duration: -1
304539 2024-12-10 17:48:44.099113 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n169,rank6)
        - aten::mm:
          inputs: (%3689:<377x1536xbf16>{1536, 1}, %3968:<1536x5120xbf16>{1, 1536})
          outputs: (%3953:<377x5120xbf16>{5120,1})
          duration: -1
304611 2024-12-10 17:48:44.102903 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n169,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3689:<377x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3953:<377x5120xbf16>{5120,1},None:NoneType})
          duration: -1
304631 2024-12-10 17:48:44.103669 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n161,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3335:<377x5120xbf16>{5120, 1}+34191360})
          outputs: (%3889:tuple{%3953:<377x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<377x5120xbf16>{5120, 1}+34191360, %3953:<377x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<377x5120xbf16>{5120,1}+34191360)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1570:<521x5120xbf16>{5120, 1}+36121600})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1570:<521x5120xbf16>{5120,1}+36121600}))
          duration: -1
304809 2024-12-10 17:48:44.114057 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n162,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1570:<521x5120xbf16>{5120, 1}+36121600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1570:<521x5120xbf16>{5120,1}+36121600}))
          duration: -1
304830 2024-12-10 17:48:44.114799 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n201,rank6)
        - aten::mm:
          inputs: (%1570:<521x5120xbf16>{5120, 1}+36121600, %3939:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<521x3072xbf16>{3072,1})
          duration: -1
304905 2024-12-10 17:48:44.118639 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n201,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1570:<521x5120xbf16>{5120, 1}+36121600})
          outputs: (%3922:tuple{%3891:<521x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<521x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<521x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<521x1536xbf16>{3072, 1})
          outputs: (%3920:<521x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<521x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3920:<521x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3920:<521x1536xbf16>{1536, 1}, %3901:<521x1536xbf16>{3072, 1}+1536)
          outputs: (%3928:<521x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3928:<521x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3928:<521x1536xbf16>{1536,1}}))
          duration: -1
305019 2024-12-10 17:48:44.128714 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n170,rank6)
        - aten::mm:
          inputs: (%3928:<521x1536xbf16>{1536, 1}, %3891:<1536x5120xbf16>{1, 1536})
          outputs: (%3924:<521x5120xbf16>{5120,1})
          duration: -1
305091 2024-12-10 17:48:44.132586 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n170,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3928:<521x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3924:<521x5120xbf16>{5120,1},None:NoneType})
          duration: -1
305108 2024-12-10 17:48:44.133364 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n162,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1570:<521x5120xbf16>{5120, 1}+36121600})
          outputs: (%3889:tuple{%3924:<521x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3576:<521x5120xbf16>{5120, 1}+36121600, %3924:<521x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3576:<521x5120xbf16>{5120,1}+36121600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3920:<278x5120xbf16>{5120, 1}+38789120})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3920:<278x5120xbf16>{5120,1}+38789120}))
          duration: -1
305290 2024-12-10 17:48:44.143707 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n163,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3920:<278x5120xbf16>{5120, 1}+38789120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3920:<278x5120xbf16>{5120,1}+38789120}))
          duration: -1
305309 2024-12-10 17:48:44.144449 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n202,rank6)
        - aten::mm:
          inputs: (%3920:<278x5120xbf16>{5120, 1}+38789120, %3939:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<278x3072xbf16>{3072,1})
          duration: -1
305385 2024-12-10 17:48:44.148244 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n202,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3920:<278x5120xbf16>{5120, 1}+38789120})
          outputs: (%3897:tuple{%3891:<278x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3939:<278x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3939:<278x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3939:<278x1536xbf16>{3072, 1})
          outputs: (%3923:<278x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3939:<278x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3923:<278x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3923:<278x1536xbf16>{1536, 1}, %3576:<278x1536xbf16>{3072, 1}+1536)
          outputs: (%3928:<278x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3928:<278x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3928:<278x1536xbf16>{1536,1}}))
          duration: -1
305498 2024-12-10 17:48:44.158356 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n171,rank6)
        - aten::mm:
          inputs: (%3928:<278x1536xbf16>{1536, 1}, %3968:<1536x5120xbf16>{1, 1536})
          outputs: (%3689:<278x5120xbf16>{5120,1})
          duration: -1
305571 2024-12-10 17:48:44.162289 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n171,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3928:<278x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3689:<278x5120xbf16>{5120,1},None:NoneType})
          duration: -1
305586 2024-12-10 17:48:44.163057 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n163,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3920:<278x5120xbf16>{5120, 1}+38789120})
          outputs: (%3889:tuple{%3689:<278x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3945:<278x5120xbf16>{5120, 1}+38789120, %3689:<278x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3945:<278x5120xbf16>{5120,1}+38789120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1238:<257x5120xbf16>{5120, 1}+40212480})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1238:<257x5120xbf16>{5120,1}+40212480}))
          duration: -1
305764 2024-12-10 17:48:44.173417 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n164,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1238:<257x5120xbf16>{5120, 1}+40212480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1238:<257x5120xbf16>{5120,1}+40212480}))
          duration: -1
305787 2024-12-10 17:48:44.174145 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n203,rank6)
        - aten::mm:
          inputs: (%1238:<257x5120xbf16>{5120, 1}+40212480, %3968:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<257x3072xbf16>{3072,1})
          duration: -1
305863 2024-12-10 17:48:44.178054 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n203,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1238:<257x5120xbf16>{5120, 1}+40212480})
          outputs: (%3918:tuple{%3891:<257x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3928:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3928:<257x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3928:<257x1536xbf16>{3072, 1})
          outputs: (%3968:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3928:<257x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3968:<257x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3968:<257x1536xbf16>{1536, 1}, %3920:<257x1536xbf16>{3072, 1}+1536)
          outputs: (%3448:<257x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3448:<257x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3448:<257x1536xbf16>{1536,1}}))
          duration: -1
305977 2024-12-10 17:48:44.187998 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n172,rank6)
        - aten::mm:
          inputs: (%3448:<257x1536xbf16>{1536, 1}, %3840:<1536x5120xbf16>{1, 1536})
          outputs: (%3928:<257x5120xbf16>{5120,1})
          duration: -1
306047 2024-12-10 17:48:44.191886 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n172,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3448:<257x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3928:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
306063 2024-12-10 17:48:44.192657 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n164,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1238:<257x5120xbf16>{5120, 1}+40212480})
          outputs: (%3889:tuple{%3928:<257x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3939:<257x5120xbf16>{5120, 1}+40212480, %3928:<257x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3939:<257x5120xbf16>{5120,1}+40212480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3335:<554x5120xbf16>{5120, 1}+41528320})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3335:<554x5120xbf16>{5120,1}+41528320}))
          duration: -1
306243 2024-12-10 17:48:44.203110 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n165,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3335:<554x5120xbf16>{5120, 1}+41528320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3335:<554x5120xbf16>{5120,1}+41528320}))
          duration: -1
306266 2024-12-10 17:48:44.203876 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n204,rank6)
        - aten::mm:
          inputs: (%3335:<554x5120xbf16>{5120, 1}+41528320, %3636:<5120x3072xbf16>{1, 5120})
          outputs: (%3891:<554x3072xbf16>{3072,1})
          duration: -1
306340 2024-12-10 17:48:44.207692 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n204,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3335:<554x5120xbf16>{5120, 1}+41528320})
          outputs: (%3922:tuple{%3891:<554x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<554x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<554x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<554x1536xbf16>{3072, 1})
          outputs: (%3636:<554x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<554x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3636:<554x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3636:<554x1536xbf16>{1536, 1}, %3901:<554x1536xbf16>{3072, 1}+1536)
          outputs: (%3923:<554x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3923:<554x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3923:<554x1536xbf16>{1536,1}}))
          duration: -1
306454 2024-12-10 17:48:44.217776 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n173,rank6)
        - aten::mm:
          inputs: (%3923:<554x1536xbf16>{1536, 1}, %3616:<1536x5120xbf16>{1, 1536})
          outputs: (%3891:<554x5120xbf16>{5120,1})
          duration: -1
306525 2024-12-10 17:48:44.221694 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n173,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3923:<554x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3891:<554x5120xbf16>{5120,1},None:NoneType})
          duration: -1
306543 2024-12-10 17:48:44.222459 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n165,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3335:<554x5120xbf16>{5120, 1}+41528320})
          outputs: (%3889:tuple{%3891:<554x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1238:<554x5120xbf16>{5120, 1}+41528320, %3891:<554x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1238:<554x5120xbf16>{5120,1}+41528320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%1570:<314x5120xbf16>{5120, 1}+44364800})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%1570:<314x5120xbf16>{5120,1}+44364800}))
          duration: -1
306722 2024-12-10 17:48:44.232975 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n166,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%1570:<314x5120xbf16>{5120, 1}+44364800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%1570:<314x5120xbf16>{5120,1}+44364800}))
          duration: -1
306744 2024-12-10 17:48:44.233784 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n205,rank6)
        - aten::mm:
          inputs: (%1570:<314x5120xbf16>{5120, 1}+44364800, %3924:<5120x3072xbf16>{1, 5120})
          outputs: (%3923:<314x3072xbf16>{3072,1})
          duration: -1
306821 2024-12-10 17:48:44.237714 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n205,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%1570:<314x5120xbf16>{5120, 1}+44364800})
          outputs: (%3897:tuple{%3923:<314x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1853:<314x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1853:<314x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1853:<314x1536xbf16>{3072, 1})
          outputs: (%3920:<314x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1853:<314x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3920:<314x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3920:<314x1536xbf16>{1536, 1}, %3924:<314x1536xbf16>{3072, 1}+1536)
          outputs: (%3928:<314x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3928:<314x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3928:<314x1536xbf16>{1536,1}}))
          duration: -1
306936 2024-12-10 17:48:44.247872 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n174,rank6)
        - aten::mm:
          inputs: (%3928:<314x1536xbf16>{1536, 1}, %3616:<1536x5120xbf16>{1, 1536})
          outputs: (%3636:<314x5120xbf16>{5120,1})
          duration: -1
307005 2024-12-10 17:48:44.251696 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n174,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3928:<314x1536xbf16>{1536, 1}})
          outputs: (%3889:tuple{%3636:<314x5120xbf16>{5120,1},None:NoneType})
          duration: -1
307021 2024-12-10 17:48:44.252493 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n166,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%1570:<314x5120xbf16>{5120, 1}+44364800})
          outputs: (%3889:tuple{%3636:<314x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3335:<314x5120xbf16>{5120, 1}+44364800, %3636:<314x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3335:<314x5120xbf16>{5120,1}+44364800)
          duration: -1
307104 2024-12-10 17:48:44.257543 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n7,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%3893:tuple{%3910:<8979x5120xbf16>{5120, 1}, %3912:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3983:tuple{%3441:<8979x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%1570:<8979x5120xbf16>{5120, 1}, 0:int, %3914:<8979x5120xCUSTOM_DATA_TYPE>{1, 0}, %3441:<8979x5120xbf16>{5120, 1})
          outputs: (%3335:<8979x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%3335:<8979x5120xbf16>{5120, 1}, %1238:<8979x1xbf16>{1, 1})
          outputs: (%3945:<8979x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%3985:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1238:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%1238:<8192x5120xbf16>{5120, 1}, 0:int, %3908:<8979x5120xCUSTOM_DATA_TYPE>{1, 0}, %3945:<8979x5120xbf16>{5120, 1})
          outputs: (%3924:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%3913:<1024x5120xbf16>{5120, 1}, %3924:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%3913:<1024x5120xbf16>{5120,1},%3924:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%3913:<1024x5120xbf16>{5120, 1}, %3924:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%3913:<1024x5120xbf16>{5120,1},%3924:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%3913:<1024x5120xbf16>{5120, 1}, %3924:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%3986:tuple{%3913:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%3882:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%3882:tuple{%3878:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
307339 2024-12-10 17:48:44.300057 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n167,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3917:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3917:tuple{%3878:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
307363 2024-12-10 17:48:44.300840 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n206,rank6)
        - aten::mm:
          inputs: (%3920:<1024x5120xbf16>{5120, 1}, %3928:<5120x6144xbf16>{1, 5120})
          outputs: (%3923:<1024x6144xbf16>{6144,1})
          duration: -1
307516 2024-12-10 17:48:44.308867 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n206,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3917:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3893:tuple{%3993:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3945:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3945:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3945:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%3891:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3945:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%3891:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%3891:<1024x1x3072xbf16>{3072, 3072, 1}, %2625:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%3689:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3894:tuple{%3689:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3894:tuple{%3689:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
307618 2024-12-10 17:48:44.318943 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n175,rank6)
        - aten::mm:
          inputs: (%3891:<1024x3072xbf16>{3072, 1}, %3928:<3072x5120xbf16>{1, 3072})
          outputs: (%3738:<1024x5120xbf16>{5120,1})
          duration: -1
307771 2024-12-10 17:48:44.326948 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n175,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3894:tuple{%3689:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%3889:tuple{%3891:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
307787 2024-12-10 17:48:44.327716 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n167,rank6)
        - ----------->api::MLP return:
          inputs: (%3882:tuple{%3878:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3889:tuple{%3891:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %3689:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%2625:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
307835 2024-12-10 17:48:44.330951 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n7,rank6)
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3998:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3998:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3999:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3999:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b85534f0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b85534f0_:_InferenceMode)
          duration: -1
310058 2024-12-10 17:48:44.881616 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n7,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%4002:tuple{%4001:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4003:tuple{%4001:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4003:tuple{%4001:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
310109 2024-12-10 17:48:44.888934 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n23,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4001:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4001:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4001:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4001:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4004:tuple{%4001:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4004:tuple{%4001:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4004:tuple{%4001:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4004:tuple{%4001:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4006:tuple{%4001:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %3909:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4006:tuple{%4001:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%3909:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4001:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4004:tuple{%4001:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3920:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4001:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%3920:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
310353 2024-12-10 17:48:44.933595 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n23,rank6)
310359 2024-12-10 17:48:44.934122 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n7,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%3920:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4007:tuple{%3920:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%4007:tuple{%3920:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
310426 2024-12-10 17:48:44.940332 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n207,rank6)
        - aten::mm:
          inputs: (%3804:<1024x5120xbf16>{5120, 1}, %2096:<5120x102400xbf16>{1, 5120})
          outputs: (%3121:<1024x102400xbf16>{102400,1})
          duration: -1
310533 2024-12-10 17:48:44.951668 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n207,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4007:tuple{%3920:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%4014:tuple{%3589:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%3335:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%4015:tuple{%3121:<1024x1xf32>{1,1},%1562:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3121:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3121:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4016:list{%3121:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4017:tuple{%4018:list{%3121:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%3335:<1024x1x102400xf32>{102400, 102400, 1}, %3441:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%3335:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%2096:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3576:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%2096:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3581:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3576:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3581:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3441:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3433:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%3902:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%3902:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %4019:list{%3441:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3433:<i32>, False:bool)
          outputs: (%3902:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3738:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3581:<1024x102400xf32>{102400, 1}, %4018:list{%3738:<1024xCUSTOM_DATA_TYPE>{1}, %3433:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4020:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%4021:<1024x1xf32>{1, 1}, %4018:list{%3441:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %1570:<i32>, False:bool)
          outputs: (%4021:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%3335:<1024x1x102400xf32>{102400, 102400, 1}, out=%3335:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%3335:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%3335:<1024x1x102400xf32>{102400, 102400, 1}, %4019:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%4022:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4021:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4021:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4019:list{%4021:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4014:tuple{%4011:list{%4021:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4022:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4022:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2932:list{%4022:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4023:tuple{%4024:list{%4022:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%4022:<1024x1xf32>{1, 1})
          outputs: (%4025:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%4025:<1024x1xf32>{1, 1}, %4021:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3581:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%3335:<1024x1x102400xf32>{102400, 102400, 1}, %1570:<1024x1x1xf32>{1, 1, 1})
          outputs: (%3335:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%4026:tuple{%3335:<1024x1x102400xf32>{102400, 102400, 1}, %3441:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %3433:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%4026:tuple{%3335:<1024x1x102400xf32>{102400,102400,1},%3441:<1024x1xCUSTOM_DATA_TYPE>{1,1},%3433:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2859:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device='cuda_6')_:dict)
          outputs: (%3121:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3121:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3121:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3121:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device='cuda_6')_:dict)
          outputs: (%3121:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%2942:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3701:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2914:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[27781,__4786,___276,_____,___548,___441,___521]],_device='cuda_6')_:dict)
          outputs: (%3121:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%1570:<1024xf32>{1}, %2096:<1024xf32>{1})
          outputs: (%2942:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2942:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3874:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%2096:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%4021:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3874:<i32>, %4021:<i32>)
          outputs: (%4022:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%4022:<i32>)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%4029:list{%3902:<1xf32>{1}}, 0:int)
          outputs: (%1570:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1570:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1570:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3679:list{%1570:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4010:tuple{%2666:list{%1570:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%1570:<1xf32>{1}, 8:int)
          outputs: (%3608:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%4022:<i32>, 1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%2942:<i32>, 1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%4030:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4022:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%4022:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%4022:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %4022:<1xf32>{1})
          outputs: (%3945:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%3945:<1xf32>{1}, 1:int)
          outputs: (%3993:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%2068:<i32>, 0:int, alpha=1:int)
          outputs: (%2068:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2068:<i32>, %3738:<i32>, alpha=1:int)
          outputs: (%2068:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3945:<i32>, %2068:<i32>, False:bool)
          outputs: (%3945:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3945:<i32>, 1:int, alpha=1:int)
          outputs: (%3945:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3121:<i32>, %3945:<i32>, False:bool)
          outputs: (%3121:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%2666:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%2942:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%245:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%434:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %435:list{%434:<1024xCUSTOM_DATA_TYPE>{1}}, %433:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%432:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%422:<1024xCUSTOM_DATA_TYPE>{1}, %435:list{%432:<1024xCUSTOM_DATA_TYPE>{1}}, %433:<i32>, False:bool)
          outputs: (%422:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%245:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%436:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%245:<1024xCUSTOM_DATA_TYPE>{1}+1, %435:list{%436:<1024xCUSTOM_DATA_TYPE>{1}}, %433:<i32>, False:bool)
          outputs: (%245:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - c10d::broadcast_:
          inputs: (%4031:list{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4014:tuple{%4016:list{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - c10d::broadcast_:
          inputs: (%4032:list{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4033:tuple{%2666:list{%3121:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3589:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3589:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4034:list{%3589:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4035:tuple{%4018:list{%3589:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4031:list{%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4014:tuple{%4036:list{%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3436:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4037:list{%3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4033:tuple{%4016:list{%3436:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3436:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3436:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3436:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2684:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2684:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3436:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__338,__6905,___280,_____,__2721,_13969,___372]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[__338,__6905,___280,_____,__2721,_13969,___372]],_device)
          duration: -1
312417 2024-12-10 17:48:45.136725 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n8,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%4038:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%4038:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
312433 2024-12-10 17:48:45.137500 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n8,rank6)
        - ----------->api::embedding call:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%3335:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%3581:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%3581:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
312525 2024-12-10 17:48:45.152523 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n8,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%4038:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%1411:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%4038:tuple{%2068:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%4038:tuple{%2068:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
312569 2024-12-10 17:48:45.154610 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n8,rank6)
        - ----------->api::dropout call:
          inputs: (%2068:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%2068:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%2068:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%2068:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
312605 2024-12-10 17:48:45.160887 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n8,rank6)
        - ----------->api::Dropout return:
          inputs: (%4038:tuple{%2068:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%2068:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
312617 2024-12-10 17:48:45.161617 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n8,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[__338,__6905,___280,_____,__2721,_13969,___372]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%2068:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%4038:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%4038:tuple{1024:int}))
          duration: -1
312649 2024-12-10 17:48:45.163218 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n8,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3576:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%3576:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%3433:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%293:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%3878:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%4043:list{%3878:<1024x20xf32>{20, 1}, %3878:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%4044:<1024x40xf32>{40,1})
          duration: -1
312805 2024-12-10 17:48:45.174441 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n8,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%4038:tuple{1024:int})
          outputs: (%3945:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
312839 2024-12-10 17:48:45.180705 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n8,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
312924 2024-12-10 17:48:45.189879 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n8,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4038:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4038:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1562:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4004:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4004:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4004:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4004:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4027:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3641:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4027:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3641:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%438:list{%422:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%437:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%437:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4004:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3645:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%3645:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
313181 2024-12-10 17:48:45.235169 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n24,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%4038:tuple{%3645:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%4038:tuple{%3645:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
313204 2024-12-10 17:48:45.239404 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n8,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4048:tuple{%3645:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4048:tuple{%3645:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
313228 2024-12-10 17:48:45.240164 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n208,rank6)
        - aten::mm:
          inputs: (%3891:<1024x5120xbf16>{5120, 1}, %3920:<5120x1536xbf16>{1, 5120})
          outputs: (%3910:<1024x1536xbf16>{1536,1})
          duration: -1
313391 2024-12-10 17:48:45.248118 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n208,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4048:tuple{%3645:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4010:tuple{%3924:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4048:tuple{%3924:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4048:tuple{%3924:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
313418 2024-12-10 17:48:45.249158 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n209,rank6)
        - aten::mm:
          inputs: (%3928:<1024x1536xbf16>{1536, 1}, %3912:<1536x24576xbf16>{1, 1536})
          outputs: (%3891:<1024x24576xbf16>{24576,1})
          duration: -1
313539 2024-12-10 17:48:45.255374 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n209,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4048:tuple{%3924:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%4057:tuple{%3468:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%4050:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3964:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%4050:<1024x1x128x192xbf16>{24576,24576,192,1},%3964:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%4050:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3964:list{128:int, 64:int}, -1:int)
          outputs: (%4052:tuple{%3928:<1024x1x128x128xbf16>{24576,24576,192,1},%3924:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4048:tuple{%3645:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4048:tuple{%3645:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
313623 2024-12-10 17:48:45.265069 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n210,rank6)
        - aten::mm:
          inputs: (%3647:<1024x5120xbf16>{5120, 1}, %3840:<5120x576xbf16>{1, 5120})
          outputs: (%3641:<1024x576xbf16>{576,1})
          duration: -1
313778 2024-12-10 17:48:45.272976 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n210,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4048:tuple{%3645:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4052:tuple{%3333:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3333:<1024x1x576xbf16>{576, 576, 1}, %4063:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3333:<1024x1x576xbf16>{576,576,1},%4063:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3333:<1024x1x576xbf16>{576, 576, 1}, %4063:list{512:int, 64:int}, -1:int)
          outputs: (%4057:tuple{%3920:<1024x1x512xbf16>{576,576,1},%4065:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4048:tuple{%3920:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4048:tuple{%3920:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
313847 2024-12-10 17:48:45.281392 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n211,rank6)
        - aten::mm:
          inputs: (%3703:<1024x512xbf16>{576, 1}, %3636:<512x32768xbf16>{1, 512})
          outputs: (%3705:<1024x32768xbf16>{32768,1})
          duration: -1
313962 2024-12-10 17:48:45.287766 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n211,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4048:tuple{%3920:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%4052:tuple{%4068:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3647:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4066:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%3647:<1024x1x128x256xbf16>{32768,32768,256,1},%4066:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3647:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4066:list{128:int, 128:int}, -1:int)
          outputs: (%4010:tuple{%3891:<1024x1x128x128xbf16>{32768,32768,256,1},%3705:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%4039:tuple{%3703:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%4039:tuple{%3703:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
314074 2024-12-10 17:48:45.299629 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n8,rank6)
314121 2024-12-10 17:48:45.302855 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n8,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%4039:tuple{%3703:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%4033:tuple{%3633:<1024x64xbf16>{64,1},%3636:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%3633:<1024x64xbf16>{64, 1}, %4073:list{%3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3840:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%3636:<1024x64xbf16>{64, 1}, %4070:list{%3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3912:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::stack:
          inputs: (%432:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%434:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%434:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%4079:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4074:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4082:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%4084:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%2914:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4073:list{%2914:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %4083:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%4085:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4085:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4075:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4084:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%4082:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4084:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%4086:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3913:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4074:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4082:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%4084:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%3916:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4073:list{%3916:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %4087:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%4088:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4088:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4075:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4084:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%4082:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4084:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%4089:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%3840:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4072:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%3840:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3912:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %4086:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%3912:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::stack:
          inputs: (%440:list{%245:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%439:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%439:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%3928:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4071:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%3928:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%4065:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3928:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%4065:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%3647:<128x1024x192xbf16>{196608, 192, 1}, %3468:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%3333:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%4050:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%3636:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%3468:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%3647:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%3468:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %4073:list{%3647:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3448:<i32>, False:bool)
          outputs: (%3468:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%410:<1024xf32>{1}}, 0:int, out=%436:<1x1024xf32>{1024, 1})
          outputs: (%436:<1x1024xf32>{1024,1})
          duration: -1
        - aten::add:
          inputs: (%3636:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %3468:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%3448:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%3448:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%3448:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%3647:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%4065:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%3448:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %4065:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%4065:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%3647:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3647:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3647:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3647:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%3333:<128x1024x1024xbf16>{1048576, 1024, 1}, %3636:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%3920:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4048:tuple{%3636:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4048:tuple{%3636:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
315661 2024-12-10 17:48:45.421940 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n176,rank6)
        - aten::mm:
          inputs: (%3891:<1024x16384xbf16>{16384, 1}, %4089:<16384x5120xbf16>{1, 16384})
          outputs: (%2914:<1024x5120xbf16>{5120,1})
          duration: -1
315822 2024-12-10 17:48:45.430200 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n176,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4048:tuple{%3636:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%3889:tuple{%3891:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
315836 2024-12-10 17:48:45.430974 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n8,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%4038:tuple{%3645:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3889:tuple{%3891:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - aten::stack:
          inputs: (%435:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%423:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%423:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4095:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4095:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4096:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4096:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b84fac30_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b84fac30_:_InferenceMode)
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4097:tuple{%1238:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4097:tuple{%1238:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
318365 2024-12-10 17:48:45.982975 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n25,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1238:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1238:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4098:tuple{%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4098:tuple{%1238:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4098:tuple{%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4098:tuple{%1238:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4102:tuple{%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %4101:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4102:tuple{%1238:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%4101:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4098:tuple{%1238:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4100:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1238:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%4100:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
318635 2024-12-10 17:48:46.027565 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n25,rank6)
        - ----------->api::MoELayer call:
          inputs: (%4097:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%4097:tuple{%4100:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
318643 2024-12-10 17:48:46.028318 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n8,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%4103:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%4103:tuple{%4100:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
318648 2024-12-10 17:48:46.029073 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n8,rank6)
        - aten::mm:
          inputs: (%3790:<1024x5120xbf16>{5120, 1}, %4104:<5120x160xbf16>{1, 5120})
          outputs: (%3830:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%4107:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%3690:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%3891:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%4108:tuple{%3836:<1024x6xbf16>{6,1},%3690:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%4107:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%3909:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%4109:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%3928:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%3928:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %3714:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%3928:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%3928:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %4109:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%4110:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3909:<1024x160xf32>{160, 1}, %4109:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%3928:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3928:<160xf32>{1}, %4110:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%3920:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%3920:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%4111:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%4111:<i32>, 2_5431315104166666e-07:float)
          outputs: (%4112:<i32>)
          duration: -1
        - aten::div:
          inputs: (%4112:<i32>, 0_01:float)
          outputs: (%3920:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3928:<i32>, %4113:<i32>, alpha=1:int)
          outputs: (%3928:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%4114:<i32>, %3928:<i32>, False:bool)
          outputs: (%4114:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%4116:tuple{%4112:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%4116:tuple{%4112:<i32>}))
          duration: -1
319137 2024-12-10 17:48:46.064487 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n8,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%4103:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%3903:tuple{%4114:<1024x6xbf16>{6,1},%3690:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3928:<8192x5120xbf16>{5120, 1}, %3891:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3928:<8192x5120xbf16>{5120,1},%3891:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3928:<8192x5120xbf16>{5120, 1}, %3891:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4108:tuple{%3928:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3714:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3690:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3714:<8192x6xCUSTOM_DATA_TYPE>{6,1},%3690:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3714:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3690:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%3903:tuple{%3714:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%3714:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%3835:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%3714:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%4112:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%3835:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4112:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4124:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%3714:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4124:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4125:<6649xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4128:<8192x6xbf16>{6, 1}, %4114:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4128:<8192x6xbf16>{6,1},%4114:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4128:<8192x6xbf16>{6, 1}, %4114:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4121:tuple{%4128:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%4128:<8192x6xbf16>{6, 1}, %4124:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3790:<6649xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%4124:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4129:<6649x2xCUSTOM_DATA_TYPE>{1,6649})
          duration: -1
        - aten::gather:
          inputs: (%3928:<8192x5120xbf16>{5120, 1}, 0:int, %4022:<6649x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4133:<6649x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%4125:<6649xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%4115:tuple{%4134:<6649xCUSTOM_DATA_TYPE>{1},%3906:<6649xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%4021:<6649xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%4065:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%4133:<6649x5120xbf16>{5120, 1}, 0:int, %1979:<6649x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%3914:<6649x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%4115:tuple{%3914:<6649x5120xbf16>{5120, 1}, %4113:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%4115:tuple{%3914:<6649x5120xbf16>{5120,1},%4113:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
320015 2024-12-10 17:48:46.184164 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n8,rank6)
        - aten::cumsum:
          inputs: (%4113:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%3928:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%4132:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%3920:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%4132:list{%3920:<1xCUSTOM_DATA_TYPE>{1}, %3928:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%4065:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%4131:<253x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%4131:<253x5120xbf16>{5120,1}}))
          duration: -1
320176 2024-12-10 17:48:46.195063 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n168,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%4131:<253x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%4131:<253x5120xbf16>{5120,1}}))
          duration: -1
320200 2024-12-10 17:48:46.195808 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n212,rank6)
        - aten::mm:
          inputs: (%4131:<253x5120xbf16>{5120, 1}, %3703:<5120x3072xbf16>{1, 5120})
          outputs: (%4111:<253x3072xbf16>{3072,1})
          duration: -1
320285 2024-12-10 17:48:46.199686 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n212,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%4131:<253x5120xbf16>{5120, 1}})
          outputs: (%4139:tuple{%4111:<253x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3714:<253x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3714:<253x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3714:<253x1536xbf16>{3072, 1})
          outputs: (%1570:<253x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3714:<253x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1570:<253x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1570:<253x1536xbf16>{1536, 1}, %4141:<253x1536xbf16>{3072, 1}+1536)
          outputs: (%4068:<253x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4068:<253x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4068:<253x1536xbf16>{1536,1}}))
          duration: -1
320384 2024-12-10 17:48:46.209700 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n177,rank6)
        - aten::mm:
          inputs: (%4068:<253x1536xbf16>{1536, 1}, %3703:<1536x5120xbf16>{1, 1536})
          outputs: (%3333:<253x5120xbf16>{5120,1})
          duration: -1
320469 2024-12-10 17:48:46.213513 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n177,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4068:<253x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3333:<253x5120xbf16>{5120,1},None:NoneType})
          duration: -1
320484 2024-12-10 17:48:46.214278 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n168,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%4131:<253x5120xbf16>{5120, 1}})
          outputs: (%4108:tuple{%3333:<253x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4143:<253x5120xbf16>{5120, 1}, %3333:<253x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4143:<253x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%4128:<415x5120xbf16>{5120, 1}+1295360})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%4128:<415x5120xbf16>{5120,1}+1295360}))
          duration: -1
320653 2024-12-10 17:48:46.224513 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n169,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%4128:<415x5120xbf16>{5120, 1}+1295360})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%4128:<415x5120xbf16>{5120,1}+1295360}))
          duration: -1
320677 2024-12-10 17:48:46.225258 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n213,rank6)
        - aten::mm:
          inputs: (%4128:<415x5120xbf16>{5120, 1}+1295360, %3633:<5120x3072xbf16>{1, 5120})
          outputs: (%4001:<415x3072xbf16>{3072,1})
          duration: -1
320765 2024-12-10 17:48:46.229100 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n213,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%4128:<415x5120xbf16>{5120, 1}+1295360})
          outputs: (%3903:tuple{%4001:<415x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4131:<415x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4131:<415x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4131:<415x1536xbf16>{3072, 1})
          outputs: (%3703:<415x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4131:<415x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3703:<415x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3703:<415x1536xbf16>{1536, 1}, %3636:<415x1536xbf16>{3072, 1}+1536)
          outputs: (%3843:<415x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%3843:<415x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%3843:<415x1536xbf16>{1536,1}}))
          duration: -1
320861 2024-12-10 17:48:46.239056 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n178,rank6)
        - aten::mm:
          inputs: (%3843:<415x1536xbf16>{1536, 1}, %3840:<1536x5120xbf16>{1, 1536})
          outputs: (%3647:<415x5120xbf16>{5120,1})
          duration: -1
320948 2024-12-10 17:48:46.242898 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n178,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%3843:<415x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3647:<415x5120xbf16>{5120,1},None:NoneType})
          duration: -1
320961 2024-12-10 17:48:46.243666 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n169,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%4128:<415x5120xbf16>{5120, 1}+1295360})
          outputs: (%4108:tuple{%3647:<415x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4143:<415x5120xbf16>{5120, 1}+1295360, %3647:<415x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4143:<415x5120xbf16>{5120,1}+1295360)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%2160:<461x5120xbf16>{5120, 1}+3420160})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%2160:<461x5120xbf16>{5120,1}+3420160}))
          duration: -1
321133 2024-12-10 17:48:46.253942 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n170,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%2160:<461x5120xbf16>{5120, 1}+3420160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%2160:<461x5120xbf16>{5120,1}+3420160}))
          duration: -1
321157 2024-12-10 17:48:46.254671 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n214,rank6)
        - aten::mm:
          inputs: (%2160:<461x5120xbf16>{5120, 1}+3420160, %3840:<5120x3072xbf16>{1, 5120})
          outputs: (%3636:<461x3072xbf16>{3072,1})
          duration: -1
321242 2024-12-10 17:48:46.258517 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n214,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%2160:<461x5120xbf16>{5120, 1}+3420160})
          outputs: (%4137:tuple{%3636:<461x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3840:<461x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3840:<461x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3840:<461x1536xbf16>{3072, 1})
          outputs: (%3448:<461x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3840:<461x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3448:<461x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3448:<461x1536xbf16>{1536, 1}, %3843:<461x1536xbf16>{3072, 1}+1536)
          outputs: (%3714:<461x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%3714:<461x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%3714:<461x1536xbf16>{1536,1}}))
          duration: -1
321340 2024-12-10 17:48:46.268432 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n179,rank6)
        - aten::mm:
          inputs: (%3714:<461x1536xbf16>{1536, 1}, %4148:<1536x5120xbf16>{1, 1536})
          outputs: (%3636:<461x5120xbf16>{5120,1})
          duration: -1
321425 2024-12-10 17:48:46.272233 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n179,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%3714:<461x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3636:<461x5120xbf16>{5120,1},None:NoneType})
          duration: -1
321439 2024-12-10 17:48:46.272998 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n170,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%2160:<461x5120xbf16>{5120, 1}+3420160})
          outputs: (%4108:tuple{%3636:<461x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3647:<461x5120xbf16>{5120, 1}+3420160, %3636:<461x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3647:<461x5120xbf16>{5120,1}+3420160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3840:<228x5120xbf16>{5120, 1}+5780480})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3840:<228x5120xbf16>{5120,1}+5780480}))
          duration: -1
321606 2024-12-10 17:48:46.283231 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n171,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3840:<228x5120xbf16>{5120, 1}+5780480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3840:<228x5120xbf16>{5120,1}+5780480}))
          duration: -1
321625 2024-12-10 17:48:46.283961 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n215,rank6)
        - aten::mm:
          inputs: (%3840:<228x5120xbf16>{5120, 1}+5780480, %4150:<5120x3072xbf16>{1, 5120})
          outputs: (%4111:<228x3072xbf16>{3072,1})
          duration: -1
321718 2024-12-10 17:48:46.287785 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n215,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3840:<228x5120xbf16>{5120, 1}+5780480})
          outputs: (%4139:tuple{%4111:<228x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4134:<228x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4134:<228x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4134:<228x1536xbf16>{3072, 1})
          outputs: (%4131:<228x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4134:<228x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4131:<228x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4131:<228x1536xbf16>{1536, 1}, %4125:<228x1536xbf16>{3072, 1}+1536)
          outputs: (%4128:<228x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4128:<228x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4128:<228x1536xbf16>{1536,1}}))
          duration: -1
        - aten::mm:
          inputs: (%4128:<228x1536xbf16>{1536, 1}, %4131:<1536x5120xbf16>{1, 1536})
          outputs: (%3191:<228x5120xbf16>{5120,1})
          duration: -1
321902 2024-12-10 17:48:46.301573 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n180,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4128:<228x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3191:<228x5120xbf16>{5120,1},None:NoneType})
          duration: -1
321916 2024-12-10 17:48:46.302335 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n171,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3840:<228x5120xbf16>{5120, 1}+5780480})
          outputs: (%4108:tuple{%3191:<228x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<228x5120xbf16>{5120, 1}+5780480, %3191:<228x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3636:<228x5120xbf16>{5120,1}+5780480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3448:<125x5120xbf16>{5120, 1}+6947840})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3448:<125x5120xbf16>{5120,1}+6947840}))
          duration: -1
322085 2024-12-10 17:48:46.312551 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n172,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3448:<125x5120xbf16>{5120, 1}+6947840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3448:<125x5120xbf16>{5120,1}+6947840}))
          duration: -1
322103 2024-12-10 17:48:46.313310 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n216,rank6)
        - aten::mm:
          inputs: (%3448:<125x5120xbf16>{5120, 1}+6947840, %4081:<5120x3072xbf16>{1, 5120})
          outputs: (%1385:<125x3072xbf16>{3072,1})
          duration: -1
322197 2024-12-10 17:48:46.317163 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n216,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3448:<125x5120xbf16>{5120, 1}+6947840})
          outputs: (%3903:tuple{%1385:<125x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4143:<125x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4143:<125x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4143:<125x1536xbf16>{3072, 1})
          outputs: (%4128:<125x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4143:<125x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4128:<125x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4128:<125x1536xbf16>{1536, 1}, %4131:<125x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<125x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4134:<125x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4134:<125x1536xbf16>{1536,1}}))
          duration: -1
322292 2024-12-10 17:48:46.327110 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n181,rank6)
        - aten::mm:
          inputs: (%4134:<125x1536xbf16>{1536, 1}, %4148:<1536x5120xbf16>{1, 1536})
          outputs: (%4156:<125x5120xbf16>{5120,1})
          duration: -1
322380 2024-12-10 17:48:46.330932 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n181,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4134:<125x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4156:<125x5120xbf16>{5120,1},None:NoneType})
          duration: -1
322394 2024-12-10 17:48:46.331690 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n172,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3448:<125x5120xbf16>{5120, 1}+6947840})
          outputs: (%4108:tuple{%4156:<125x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3840:<125x5120xbf16>{5120, 1}+6947840, %4156:<125x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3840:<125x5120xbf16>{5120,1}+6947840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3647:<823x5120xbf16>{5120, 1}+7587840})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3647:<823x5120xbf16>{5120,1}+7587840}))
          duration: -1
322562 2024-12-10 17:48:46.341902 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n173,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3647:<823x5120xbf16>{5120, 1}+7587840})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3647:<823x5120xbf16>{5120,1}+7587840}))
          duration: -1
322579 2024-12-10 17:48:46.342626 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n217,rank6)
        - aten::mm:
          inputs: (%3647:<823x5120xbf16>{5120, 1}+7587840, %4081:<5120x3072xbf16>{1, 5120})
          outputs: (%3333:<823x3072xbf16>{3072,1})
          duration: -1
322675 2024-12-10 17:48:46.346448 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n217,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3647:<823x5120xbf16>{5120, 1}+7587840})
          outputs: (%4137:tuple{%3333:<823x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4128:<823x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4128:<823x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4128:<823x1536xbf16>{3072, 1})
          outputs: (%4125:<823x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4128:<823x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4125:<823x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4125:<823x1536xbf16>{1536, 1}, %4131:<823x1536xbf16>{3072, 1}+1536)
          outputs: (%4111:<823x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4111:<823x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4111:<823x1536xbf16>{1536,1}}))
          duration: -1
322769 2024-12-10 17:48:46.356405 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n182,rank6)
        - aten::mm:
          inputs: (%4111:<823x1536xbf16>{1536, 1}, %4160:<1536x5120xbf16>{1, 1536})
          outputs: (%3448:<823x5120xbf16>{5120,1})
          duration: -1
322859 2024-12-10 17:48:46.360212 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n182,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4111:<823x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3448:<823x5120xbf16>{5120,1},None:NoneType})
          duration: -1
322872 2024-12-10 17:48:46.360986 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n173,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3647:<823x5120xbf16>{5120, 1}+7587840})
          outputs: (%4108:tuple{%3448:<823x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<823x5120xbf16>{5120, 1}+7587840, %3448:<823x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3636:<823x5120xbf16>{5120,1}+7587840)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3636:<149x5120xbf16>{5120, 1}+11801600})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3636:<149x5120xbf16>{5120,1}+11801600}))
          duration: -1
323043 2024-12-10 17:48:46.371292 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n174,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3636:<149x5120xbf16>{5120, 1}+11801600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3636:<149x5120xbf16>{5120,1}+11801600}))
          duration: -1
323060 2024-12-10 17:48:46.372028 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n218,rank6)
        - aten::mm:
          inputs: (%3636:<149x5120xbf16>{5120, 1}+11801600, %4162:<5120x3072xbf16>{1, 5120})
          outputs: (%4163:<149x3072xbf16>{3072,1})
          duration: -1
323156 2024-12-10 17:48:46.375830 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n218,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3636:<149x5120xbf16>{5120, 1}+11801600})
          outputs: (%4139:tuple{%4163:<149x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4111:<149x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4111:<149x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4111:<149x1536xbf16>{3072, 1})
          outputs: (%4162:<149x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4111:<149x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4162:<149x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4162:<149x1536xbf16>{1536, 1}, %4128:<149x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<149x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4134:<149x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4134:<149x1536xbf16>{1536,1}}))
          duration: -1
323249 2024-12-10 17:48:46.385754 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n183,rank6)
        - aten::mm:
          inputs: (%4134:<149x1536xbf16>{1536, 1}, %4156:<1536x5120xbf16>{1, 1536})
          outputs: (%4162:<149x5120xbf16>{5120,1})
          duration: -1
323339 2024-12-10 17:48:46.389545 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n183,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4134:<149x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4162:<149x5120xbf16>{5120,1},None:NoneType})
          duration: -1
323350 2024-12-10 17:48:46.390311 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n174,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3636:<149x5120xbf16>{5120, 1}+11801600})
          outputs: (%4108:tuple{%4162:<149x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3647:<149x5120xbf16>{5120, 1}+11801600, %4162:<149x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3647:<149x5120xbf16>{5120,1}+11801600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3840:<279x5120xbf16>{5120, 1}+12564480})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3840:<279x5120xbf16>{5120,1}+12564480}))
          duration: -1
323522 2024-12-10 17:48:46.400565 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n175,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3840:<279x5120xbf16>{5120, 1}+12564480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3840:<279x5120xbf16>{5120,1}+12564480}))
          duration: -1
323538 2024-12-10 17:48:46.401322 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n219,rank6)
        - aten::mm:
          inputs: (%3840:<279x5120xbf16>{5120, 1}+12564480, %4160:<5120x3072xbf16>{1, 5120})
          outputs: (%2160:<279x3072xbf16>{3072,1})
          duration: -1
323633 2024-12-10 17:48:46.405119 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n219,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3840:<279x5120xbf16>{5120, 1}+12564480})
          outputs: (%3903:tuple{%2160:<279x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4125:<279x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4125:<279x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4125:<279x1536xbf16>{3072, 1})
          outputs: (%4084:<279x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4125:<279x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4084:<279x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4084:<279x1536xbf16>{1536, 1}, %4128:<279x1536xbf16>{3072, 1}+1536)
          outputs: (%1385:<279x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%1385:<279x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%1385:<279x1536xbf16>{1536,1}}))
          duration: -1
323729 2024-12-10 17:48:46.415049 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n184,rank6)
        - aten::mm:
          inputs: (%1385:<279x1536xbf16>{1536, 1}, %4148:<1536x5120xbf16>{1, 1536})
          outputs: (%4169:<279x5120xbf16>{5120,1})
          duration: -1
323815 2024-12-10 17:48:46.418835 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n184,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%1385:<279x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4169:<279x5120xbf16>{5120,1},None:NoneType})
          duration: -1
323830 2024-12-10 17:48:46.419600 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n175,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3840:<279x5120xbf16>{5120, 1}+12564480})
          outputs: (%4108:tuple{%4169:<279x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<279x5120xbf16>{5120, 1}+12564480, %4169:<279x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3636:<279x5120xbf16>{5120,1}+12564480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3333:<195x5120xbf16>{5120, 1}+13992960})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3333:<195x5120xbf16>{5120,1}+13992960}))
          duration: -1
323999 2024-12-10 17:48:46.429899 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n176,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3333:<195x5120xbf16>{5120, 1}+13992960})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3333:<195x5120xbf16>{5120,1}+13992960}))
          duration: -1
324014 2024-12-10 17:48:46.430639 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n220,rank6)
        - aten::mm:
          inputs: (%3333:<195x5120xbf16>{5120, 1}+13992960, %3840:<5120x3072xbf16>{1, 5120})
          outputs: (%4080:<195x3072xbf16>{3072,1})
          duration: -1
324110 2024-12-10 17:48:46.434476 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n220,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3333:<195x5120xbf16>{5120, 1}+13992960})
          outputs: (%4137:tuple{%4080:<195x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3840:<195x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3840:<195x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3840:<195x1536xbf16>{3072, 1})
          outputs: (%4128:<195x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3840:<195x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4128:<195x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4128:<195x1536xbf16>{1536, 1}, %4107:<195x1536xbf16>{3072, 1}+1536)
          outputs: (%2160:<195x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%2160:<195x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%2160:<195x1536xbf16>{1536,1}}))
          duration: -1
324208 2024-12-10 17:48:46.444444 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n185,rank6)
        - aten::mm:
          inputs: (%2160:<195x1536xbf16>{1536, 1}, %4087:<1536x5120xbf16>{1, 1536})
          outputs: (%4162:<195x5120xbf16>{5120,1})
          duration: -1
324294 2024-12-10 17:48:46.448220 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n185,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%2160:<195x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4162:<195x5120xbf16>{5120,1},None:NoneType})
          duration: -1
324310 2024-12-10 17:48:46.448990 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n176,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3333:<195x5120xbf16>{5120, 1}+13992960})
          outputs: (%4108:tuple{%4162:<195x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%2160:<195x5120xbf16>{5120, 1}+13992960, %4162:<195x5120xbf16>{5120, 1}, False:bool)
          outputs: (%2160:<195x5120xbf16>{5120,1}+13992960)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3636:<195x5120xbf16>{5120, 1}+14991360})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3636:<195x5120xbf16>{5120,1}+14991360}))
          duration: -1
324479 2024-12-10 17:48:46.459286 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n177,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3636:<195x5120xbf16>{5120, 1}+14991360})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3636:<195x5120xbf16>{5120,1}+14991360}))
          duration: -1
324492 2024-12-10 17:48:46.460024 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n221,rank6)
        - aten::mm:
          inputs: (%3636:<195x5120xbf16>{5120, 1}+14991360, %4087:<5120x3072xbf16>{1, 5120})
          outputs: (%3191:<195x3072xbf16>{3072,1})
          duration: -1
324591 2024-12-10 17:48:46.463840 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n221,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3636:<195x5120xbf16>{5120, 1}+14991360})
          outputs: (%4139:tuple{%3191:<195x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4111:<195x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4111:<195x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4111:<195x1536xbf16>{3072, 1})
          outputs: (%4087:<195x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4111:<195x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4087:<195x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4087:<195x1536xbf16>{1536, 1}, %4128:<195x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<195x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4134:<195x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4134:<195x1536xbf16>{1536,1}}))
          duration: -1
324687 2024-12-10 17:48:46.473797 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n186,rank6)
        - aten::mm:
          inputs: (%4134:<195x1536xbf16>{1536, 1}, %4150:<1536x5120xbf16>{1, 1536})
          outputs: (%4080:<195x5120xbf16>{5120,1})
          duration: -1
324774 2024-12-10 17:48:46.477570 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n186,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4134:<195x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4080:<195x5120xbf16>{5120,1},None:NoneType})
          duration: -1
324786 2024-12-10 17:48:46.478325 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n177,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3636:<195x5120xbf16>{5120, 1}+14991360})
          outputs: (%4108:tuple{%4080:<195x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3333:<195x5120xbf16>{5120, 1}+14991360, %4080:<195x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3333:<195x5120xbf16>{5120,1}+14991360)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3448:<352x5120xbf16>{5120, 1}+15989760})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3448:<352x5120xbf16>{5120,1}+15989760}))
          duration: -1
324957 2024-12-10 17:48:46.488579 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n178,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3448:<352x5120xbf16>{5120, 1}+15989760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3448:<352x5120xbf16>{5120,1}+15989760}))
          duration: -1
324970 2024-12-10 17:48:46.489325 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n222,rank6)
        - aten::mm:
          inputs: (%3448:<352x5120xbf16>{5120, 1}+15989760, %4150:<5120x3072xbf16>{1, 5120})
          outputs: (%3636:<352x3072xbf16>{3072,1})
          duration: -1
325067 2024-12-10 17:48:46.493149 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n222,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3448:<352x5120xbf16>{5120, 1}+15989760})
          outputs: (%3903:tuple{%3636:<352x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4125:<352x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4125:<352x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4125:<352x1536xbf16>{3072, 1})
          outputs: (%4150:<352x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4125:<352x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4150:<352x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4150:<352x1536xbf16>{1536, 1}, %4128:<352x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<352x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4134:<352x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4134:<352x1536xbf16>{1536,1}}))
          duration: -1
325166 2024-12-10 17:48:46.503105 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n187,rank6)
        - aten::mm:
          inputs: (%4134:<352x1536xbf16>{1536, 1}, %4169:<1536x5120xbf16>{1, 1536})
          outputs: (%3689:<352x5120xbf16>{5120,1})
          duration: -1
325251 2024-12-10 17:48:46.506930 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n187,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4134:<352x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3689:<352x5120xbf16>{5120,1},None:NoneType})
          duration: -1
325265 2024-12-10 17:48:46.507694 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n178,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3448:<352x5120xbf16>{5120, 1}+15989760})
          outputs: (%4108:tuple{%3689:<352x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3647:<352x5120xbf16>{5120, 1}+15989760, %3689:<352x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3647:<352x5120xbf16>{5120,1}+15989760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3840:<365x5120xbf16>{5120, 1}+17792000})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3840:<365x5120xbf16>{5120,1}+17792000}))
          duration: -1
325436 2024-12-10 17:48:46.518004 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n179,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3840:<365x5120xbf16>{5120, 1}+17792000})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3840:<365x5120xbf16>{5120,1}+17792000}))
          duration: -1
325450 2024-12-10 17:48:46.518736 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n223,rank6)
        - aten::mm:
          inputs: (%3840:<365x5120xbf16>{5120, 1}+17792000, %4071:<5120x3072xbf16>{1, 5120})
          outputs: (%3191:<365x3072xbf16>{3072,1})
          duration: -1
325543 2024-12-10 17:48:46.522589 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n223,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3840:<365x5120xbf16>{5120, 1}+17792000})
          outputs: (%4137:tuple{%3191:<365x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4111:<365x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4111:<365x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4111:<365x1536xbf16>{3072, 1})
          outputs: (%4071:<365x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4111:<365x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4071:<365x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4071:<365x1536xbf16>{1536, 1}, %4134:<365x1536xbf16>{3072, 1}+1536)
          outputs: (%4128:<365x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4128:<365x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4128:<365x1536xbf16>{1536,1}}))
          duration: -1
325646 2024-12-10 17:48:46.532531 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n188,rank6)
        - aten::mm:
          inputs: (%4128:<365x1536xbf16>{1536, 1}, %4150:<1536x5120xbf16>{1, 1536})
          outputs: (%4072:<365x5120xbf16>{5120,1})
          duration: -1
325726 2024-12-10 17:48:46.536325 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n188,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4128:<365x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4072:<365x5120xbf16>{5120,1},None:NoneType})
          duration: -1
325741 2024-12-10 17:48:46.537121 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n179,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3840:<365x5120xbf16>{5120, 1}+17792000})
          outputs: (%4108:tuple{%4072:<365x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3448:<365x5120xbf16>{5120, 1}+17792000, %4072:<365x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3448:<365x5120xbf16>{5120,1}+17792000)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3636:<475x5120xbf16>{5120, 1}+19660800})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3636:<475x5120xbf16>{5120,1}+19660800}))
          duration: -1
325902 2024-12-10 17:48:46.547378 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n180,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3636:<475x5120xbf16>{5120, 1}+19660800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3636:<475x5120xbf16>{5120,1}+19660800}))
          duration: -1
325914 2024-12-10 17:48:46.548117 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n224,rank6)
        - aten::mm:
          inputs: (%3636:<475x5120xbf16>{5120, 1}+19660800, %4169:<5120x3072xbf16>{1, 5120})
          outputs: (%4150:<475x3072xbf16>{3072,1})
          duration: -1
326000 2024-12-10 17:48:46.551945 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n224,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3636:<475x5120xbf16>{5120, 1}+19660800})
          outputs: (%4139:tuple{%4150:<475x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4134:<475x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4134:<475x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4134:<475x1536xbf16>{3072, 1})
          outputs: (%3840:<475x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4134:<475x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3840:<475x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3840:<475x1536xbf16>{1536, 1}, %4125:<475x1536xbf16>{3072, 1}+1536)
          outputs: (%4128:<475x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4128:<475x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4128:<475x1536xbf16>{1536,1}}))
          duration: -1
326115 2024-12-10 17:48:46.561913 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n189,rank6)
        - aten::mm:
          inputs: (%4128:<475x1536xbf16>{1536, 1}, %4184:<1536x5120xbf16>{1, 1536})
          outputs: (%4104:<475x5120xbf16>{5120,1})
          duration: -1
326190 2024-12-10 17:48:46.565731 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n189,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4128:<475x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4104:<475x5120xbf16>{5120,1},None:NoneType})
          duration: -1
326205 2024-12-10 17:48:46.566497 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n180,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3636:<475x5120xbf16>{5120, 1}+19660800})
          outputs: (%4108:tuple{%4104:<475x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3840:<475x5120xbf16>{5120, 1}+19660800, %4104:<475x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3840:<475x5120xbf16>{5120,1}+19660800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3448:<321x5120xbf16>{5120, 1}+22092800})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3448:<321x5120xbf16>{5120,1}+22092800}))
          duration: -1
326373 2024-12-10 17:48:46.576865 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n181,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3448:<321x5120xbf16>{5120, 1}+22092800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3448:<321x5120xbf16>{5120,1}+22092800}))
          duration: -1
326387 2024-12-10 17:48:46.577630 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n225,rank6)
        - aten::mm:
          inputs: (%3448:<321x5120xbf16>{5120, 1}+22092800, %4184:<5120x3072xbf16>{1, 5120})
          outputs: (%3929:<321x3072xbf16>{3072,1})
          duration: -1
326477 2024-12-10 17:48:46.581445 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n225,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3448:<321x5120xbf16>{5120, 1}+22092800})
          outputs: (%3903:tuple{%3929:<321x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4111:<321x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4111:<321x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4111:<321x1536xbf16>{3072, 1})
          outputs: (%4184:<321x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4111:<321x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4184:<321x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4184:<321x1536xbf16>{1536, 1}, %4125:<321x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<321x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4134:<321x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4134:<321x1536xbf16>{1536,1}}))
          duration: -1
326593 2024-12-10 17:48:46.591403 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n190,rank6)
        - aten::mm:
          inputs: (%4134:<321x1536xbf16>{1536, 1}, %4150:<1536x5120xbf16>{1, 1536})
          outputs: (%1570:<321x5120xbf16>{5120,1})
          duration: -1
326669 2024-12-10 17:48:46.595192 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n190,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4134:<321x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%1570:<321x5120xbf16>{5120,1},None:NoneType})
          duration: -1
326685 2024-12-10 17:48:46.595953 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n181,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3448:<321x5120xbf16>{5120, 1}+22092800})
          outputs: (%4108:tuple{%1570:<321x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<321x5120xbf16>{5120, 1}+22092800, %1570:<321x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3636:<321x5120xbf16>{5120,1}+22092800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3647:<149x5120xbf16>{5120, 1}+23736320})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3647:<149x5120xbf16>{5120,1}+23736320}))
          duration: -1
326850 2024-12-10 17:48:46.606230 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n182,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3647:<149x5120xbf16>{5120, 1}+23736320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3647:<149x5120xbf16>{5120,1}+23736320}))
          duration: -1
326865 2024-12-10 17:48:46.606965 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n226,rank6)
        - aten::mm:
          inputs: (%3647:<149x5120xbf16>{5120, 1}+23736320, %4184:<5120x3072xbf16>{1, 5120})
          outputs: (%4125:<149x3072xbf16>{3072,1})
          duration: -1
326954 2024-12-10 17:48:46.610766 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n226,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3647:<149x5120xbf16>{5120, 1}+23736320})
          outputs: (%4137:tuple{%4125:<149x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1385:<149x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1385:<149x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1385:<149x1536xbf16>{3072, 1})
          outputs: (%4104:<149x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1385:<149x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4104:<149x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4104:<149x1536xbf16>{1536, 1}, %2160:<149x1536xbf16>{3072, 1}+1536)
          outputs: (%4128:<149x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4128:<149x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4128:<149x1536xbf16>{1536,1}}))
          duration: -1
327071 2024-12-10 17:48:46.620717 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n191,rank6)
        - aten::mm:
          inputs: (%4128:<149x1536xbf16>{1536, 1}, %4080:<1536x5120xbf16>{1, 1536})
          outputs: (%3191:<149x5120xbf16>{5120,1})
          duration: -1
327147 2024-12-10 17:48:46.624498 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n191,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4128:<149x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%3191:<149x5120xbf16>{5120,1},None:NoneType})
          duration: -1
327159 2024-12-10 17:48:46.625284 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n182,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3647:<149x5120xbf16>{5120, 1}+23736320})
          outputs: (%4108:tuple{%3191:<149x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%1570:<149x5120xbf16>{5120, 1}+23736320, %3191:<149x5120xbf16>{5120, 1}, False:bool)
          outputs: (%1570:<149x5120xbf16>{5120,1}+23736320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3840:<425x5120xbf16>{5120, 1}+24499200})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3840:<425x5120xbf16>{5120,1}+24499200}))
          duration: -1
327327 2024-12-10 17:48:46.635529 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n183,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3840:<425x5120xbf16>{5120, 1}+24499200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3840:<425x5120xbf16>{5120,1}+24499200}))
          duration: -1
327344 2024-12-10 17:48:46.636262 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n227,rank6)
        - aten::mm:
          inputs: (%3840:<425x5120xbf16>{5120, 1}+24499200, %3647:<5120x3072xbf16>{1, 5120})
          outputs: (%4134:<425x3072xbf16>{3072,1})
          duration: -1
327433 2024-12-10 17:48:46.640072 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n227,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3840:<425x5120xbf16>{5120, 1}+24499200})
          outputs: (%4139:tuple{%4134:<425x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3647:<425x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3647:<425x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3647:<425x1536xbf16>{3072, 1})
          outputs: (%3714:<425x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3647:<425x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3714:<425x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3714:<425x1536xbf16>{1536, 1}, %3468:<425x1536xbf16>{3072, 1}+1536)
          outputs: (%4128:<425x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4128:<425x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4128:<425x1536xbf16>{1536,1}}))
          duration: -1
327551 2024-12-10 17:48:46.650037 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n192,rank6)
327623 2024-12-10 17:48:46.653836 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n192,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4128:<425x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%2160:<425x5120xbf16>{5120,1},None:NoneType})
          duration: -1
327637 2024-12-10 17:48:46.654610 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n183,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3840:<425x5120xbf16>{5120, 1}+24499200})
          outputs: (%4108:tuple{%2160:<425x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4130:<425x5120xbf16>{5120, 1}+24499200, %2160:<425x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4130:<425x5120xbf16>{5120,1}+24499200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3929:<282x5120xbf16>{5120, 1}+26675200})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3929:<282x5120xbf16>{5120,1}+26675200}))
          duration: -1
327807 2024-12-10 17:48:46.664983 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n184,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3929:<282x5120xbf16>{5120, 1}+26675200})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3929:<282x5120xbf16>{5120,1}+26675200}))
          duration: -1
327824 2024-12-10 17:48:46.665738 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n228,rank6)
        - aten::mm:
          inputs: (%3929:<282x5120xbf16>{5120, 1}+26675200, %4184:<5120x3072xbf16>{1, 5120})
          outputs: (%3647:<282x3072xbf16>{3072,1})
          duration: -1
327908 2024-12-10 17:48:46.669545 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n228,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3929:<282x5120xbf16>{5120, 1}+26675200})
          outputs: (%3903:tuple{%3647:<282x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3840:<282x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3840:<282x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3840:<282x1536xbf16>{3072, 1})
          outputs: (%4169:<282x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3840:<282x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4169:<282x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4169:<282x1536xbf16>{1536, 1}, %3636:<282x1536xbf16>{3072, 1}+1536)
          outputs: (%3333:<282x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%3333:<282x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%3333:<282x1536xbf16>{1536,1}}))
          duration: -1
328031 2024-12-10 17:48:46.679517 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n193,rank6)
        - aten::mm:
          inputs: (%3333:<282x1536xbf16>{1536, 1}, %4162:<1536x5120xbf16>{1, 1536})
          outputs: (%4072:<282x5120xbf16>{5120,1})
          duration: -1
328101 2024-12-10 17:48:46.683293 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n193,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%3333:<282x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4072:<282x5120xbf16>{5120,1},None:NoneType})
          duration: -1
328117 2024-12-10 17:48:46.684061 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n184,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3929:<282x5120xbf16>{5120, 1}+26675200})
          outputs: (%4108:tuple{%4072:<282x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<282x5120xbf16>{5120, 1}+26675200, %4072:<282x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3636:<282x5120xbf16>{5120,1}+26675200)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3647:<457x5120xbf16>{5120, 1}+28119040})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3647:<457x5120xbf16>{5120,1}+28119040}))
          duration: -1
328285 2024-12-10 17:48:46.694331 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n185,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3647:<457x5120xbf16>{5120, 1}+28119040})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3647:<457x5120xbf16>{5120,1}+28119040}))
          duration: -1
328300 2024-12-10 17:48:46.695069 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n229,rank6)
        - aten::mm:
          inputs: (%3647:<457x5120xbf16>{5120, 1}+28119040, %4080:<5120x3072xbf16>{1, 5120})
          outputs: (%2160:<457x3072xbf16>{3072,1})
          duration: -1
328386 2024-12-10 17:48:46.698892 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n229,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3647:<457x5120xbf16>{5120, 1}+28119040})
          outputs: (%4137:tuple{%2160:<457x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4125:<457x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4125:<457x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4125:<457x1536xbf16>{3072, 1})
          outputs: (%3714:<457x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4125:<457x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3714:<457x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3714:<457x1536xbf16>{1536, 1}, %1385:<457x1536xbf16>{3072, 1}+1536)
          outputs: (%3929:<457x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%3929:<457x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%3929:<457x1536xbf16>{1536,1}}))
          duration: -1
328510 2024-12-10 17:48:46.708854 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n194,rank6)
        - aten::mm:
          inputs: (%3929:<457x1536xbf16>{1536, 1}, %4184:<1536x5120xbf16>{1, 1536})
          outputs: (%4104:<457x5120xbf16>{5120,1})
          duration: -1
328581 2024-12-10 17:48:46.712678 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n194,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%3929:<457x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4104:<457x5120xbf16>{5120,1},None:NoneType})
          duration: -1
328596 2024-12-10 17:48:46.713461 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n185,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3647:<457x5120xbf16>{5120, 1}+28119040})
          outputs: (%4108:tuple{%4104:<457x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3448:<457x5120xbf16>{5120, 1}+28119040, %4104:<457x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3448:<457x5120xbf16>{5120,1}+28119040)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3840:<592x5120xbf16>{5120, 1}+30458880})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3840:<592x5120xbf16>{5120,1}+30458880}))
          duration: -1
328765 2024-12-10 17:48:46.723746 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n186,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3840:<592x5120xbf16>{5120, 1}+30458880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3840:<592x5120xbf16>{5120,1}+30458880}))
          duration: -1
328781 2024-12-10 17:48:46.724485 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n230,rank6)
        - aten::mm:
          inputs: (%3840:<592x5120xbf16>{5120, 1}+30458880, %4162:<5120x3072xbf16>{1, 5120})
          outputs: (%4128:<592x3072xbf16>{3072,1})
          duration: -1
328864 2024-12-10 17:48:46.728323 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n230,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3840:<592x5120xbf16>{5120, 1}+30458880})
          outputs: (%4139:tuple{%4128:<592x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4134:<592x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4134:<592x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4134:<592x1536xbf16>{3072, 1})
          outputs: (%4080:<592x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4134:<592x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4080:<592x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4080:<592x1536xbf16>{1536, 1}, %4125:<592x1536xbf16>{3072, 1}+1536)
          outputs: (%3647:<592x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%3647:<592x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%3647:<592x1536xbf16>{1536,1}}))
          duration: -1
328990 2024-12-10 17:48:46.738276 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n195,rank6)
        - aten::mm:
          inputs: (%3647:<592x1536xbf16>{1536, 1}, %4162:<1536x5120xbf16>{1, 1536})
          outputs: (%4169:<592x5120xbf16>{5120,1})
          duration: -1
329059 2024-12-10 17:48:46.742067 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n195,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%3647:<592x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4169:<592x5120xbf16>{5120,1},None:NoneType})
          duration: -1
329075 2024-12-10 17:48:46.742834 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n186,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3840:<592x5120xbf16>{5120, 1}+30458880})
          outputs: (%4108:tuple{%4169:<592x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3636:<592x5120xbf16>{5120, 1}+30458880, %4169:<592x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3636:<592x5120xbf16>{5120,1}+30458880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%3448:<108x5120xbf16>{5120, 1}+33489920})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%3448:<108x5120xbf16>{5120,1}+33489920}))
          duration: -1
329244 2024-12-10 17:48:46.753093 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n187,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4136:tuple{%3448:<108x5120xbf16>{5120, 1}+33489920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4136:tuple{%3448:<108x5120xbf16>{5120,1}+33489920}))
          duration: -1
329258 2024-12-10 17:48:46.753828 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n231,rank6)
        - aten::mm:
          inputs: (%3448:<108x5120xbf16>{5120, 1}+33489920, %4087:<5120x3072xbf16>{1, 5120})
          outputs: (%4162:<108x3072xbf16>{3072,1})
          duration: -1
329341 2024-12-10 17:48:46.757657 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n231,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%3448:<108x5120xbf16>{5120, 1}+33489920})
          outputs: (%3903:tuple{%4162:<108x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4134:<108x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4134:<108x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4134:<108x1536xbf16>{3072, 1})
          outputs: (%4128:<108x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4134:<108x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4128:<108x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4128:<108x1536xbf16>{1536, 1}, %4111:<108x1536xbf16>{3072, 1}+1536)
          outputs: (%4125:<108x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%4125:<108x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%4125:<108x1536xbf16>{1536,1}}))
          duration: -1
329469 2024-12-10 17:48:46.767614 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n196,rank6)
        - aten::mm:
          inputs: (%4125:<108x1536xbf16>{1536, 1}, %4072:<1536x5120xbf16>{1, 1536})
          outputs: (%4206:<108x5120xbf16>{5120,1})
          duration: -1
329534 2024-12-10 17:48:46.771397 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n196,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%4125:<108x1536xbf16>{1536, 1}})
          outputs: (%4108:tuple{%4206:<108x5120xbf16>{5120,1},None:NoneType})
          duration: -1
329553 2024-12-10 17:48:46.772164 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n187,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%3448:<108x5120xbf16>{5120, 1}+33489920})
          outputs: (%4108:tuple{%4206:<108x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3840:<108x5120xbf16>{5120, 1}+33489920, %4206:<108x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3840:<108x5120xbf16>{5120,1}+33489920)
          duration: -1
329635 2024-12-10 17:48:46.777094 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n8,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%4115:tuple{%3914:<6649x5120xbf16>{5120, 1}, %4113:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4208:tuple{%4124:<6649x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%3928:<6649x5120xbf16>{5120, 1}, 0:int, %1979:<6649x5120xCUSTOM_DATA_TYPE>{1, 0}, %4124:<6649x5120xbf16>{5120, 1})
          outputs: (%3920:<6649x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%3920:<6649x5120xbf16>{5120, 1}, %4065:<6649x1xbf16>{1, 1})
          outputs: (%3891:<6649x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%4210:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3928:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%3928:<8192x5120xbf16>{5120, 1}, 0:int, %4022:<6649x5120xCUSTOM_DATA_TYPE>{1, 0}, %3891:<6649x5120xbf16>{5120, 1})
          outputs: (%3647:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%3636:<1024x5120xbf16>{5120, 1}, %3647:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%3636:<1024x5120xbf16>{5120,1},%3647:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%3636:<1024x5120xbf16>{5120, 1}, %3647:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%3636:<1024x5120xbf16>{5120,1},%3647:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%3636:<1024x5120xbf16>{5120, 1}, %3647:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%4211:tuple{%3636:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4103:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%4103:tuple{%4100:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
329919 2024-12-10 17:48:46.831000 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n188,rank6)
329940 2024-12-10 17:48:46.831807 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n232,rank6)
        - aten::mm:
          inputs: (%2160:<1024x5120xbf16>{5120, 1}, %3929:<5120x6144xbf16>{1, 5120})
          outputs: (%3714:<1024x6144xbf16>{6144,1})
          duration: -1
330073 2024-12-10 17:48:46.968722 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n232,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4136:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4115:tuple{%4125:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%2160:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%2160:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%2160:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%1539:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%2160:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%1539:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%1539:<1024x1x3072xbf16>{3072, 3072, 1}, %3647:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%3912:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4116:tuple{%3912:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4116:tuple{%3912:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
330177 2024-12-10 17:48:46.978724 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n197,rank6)
        - aten::mm:
          inputs: (%3891:<1024x3072xbf16>{3072, 1}, %3928:<3072x5120xbf16>{1, 3072})
          outputs: (%4128:<1024x5120xbf16>{5120,1})
          duration: -1
330327 2024-12-10 17:48:46.986684 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n197,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4116:tuple{%3912:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%4108:tuple{%3891:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
330340 2024-12-10 17:48:46.987454 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n188,rank6)
        - ----------->api::MLP return:
          inputs: (%4103:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4108:tuple{%3891:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%3840:<1024x1x5120xbf16>{5120, 5120, 1}, %3647:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%3714:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
330386 2024-12-10 17:48:46.990627 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n8,rank6)
        - ----------->api::MoELayer return:
          inputs: (%4097:tuple{%4100:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4108:tuple{%3714:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4221:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4221:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4222:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4222:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8553b30_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8553b30_:_InferenceMode)
          duration: -1
332538 2024-12-10 17:48:47.523485 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n8,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%4223:tuple{%3576:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4224:tuple{%3576:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4224:tuple{%3576:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
332585 2024-12-10 17:48:47.530553 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n26,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3576:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3576:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3576:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3576:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4225:tuple{%3576:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4225:tuple{%3576:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4225:tuple{%3576:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4225:tuple{%3576:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4229:tuple{%3576:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %4228:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4229:tuple{%3576:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%4228:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3576:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4225:tuple{%3576:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3448:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3576:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%3448:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
332786 2024-12-10 17:48:47.575145 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n26,rank6)
332789 2024-12-10 17:48:47.575679 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n8,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%3448:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4230:tuple{%3448:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%4230:tuple{%3448:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
332845 2024-12-10 17:48:47.582007 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n233,rank6)
        - aten::mm:
          inputs: (%1238:<1024x5120xbf16>{5120, 1}, %3874:<5120x102400xbf16>{1, 5120})
          outputs: (%4021:<1024x102400xbf16>{102400,1})
          duration: -1
332972 2024-12-10 17:48:47.594662 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n233,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4230:tuple{%3448:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%4238:tuple{%4236:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%3804:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%4239:tuple{%1238:<1024x1xf32>{1,1},%4240:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1238:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1238:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4241:list{%1238:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4242:tuple{%4037:list{%1238:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%3804:<1024x1x102400xf32>{102400, 102400, 1}, %4243:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%3804:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%293:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%4240:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%293:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%4128:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%4240:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %4128:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3191:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%4110:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%4244:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%4244:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %4245:list{%3191:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %4240:<i32>, False:bool)
          outputs: (%4244:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4240:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%4114:<1024x102400xf32>{102400, 1}, %4037:list{%4240:<1024xCUSTOM_DATA_TYPE>{1}, %4143:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3836:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%4001:<1024x1xf32>{1, 1}, %4037:list{%3191:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3836:<i32>, False:bool)
          outputs: (%4001:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%3804:<1024x1x102400xf32>{102400, 102400, 1}, out=%3804:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%3804:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%3804:<1024x1x102400xf32>{102400, 102400, 1}, %4245:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3647:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4001:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4001:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4245:list{%4001:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4238:tuple{%4234:list{%4001:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3647:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3647:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4246:list{%3647:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4247:tuple{%4248:list{%3647:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%3647:<1024x1xf32>{1, 1})
          outputs: (%3968:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3968:<1024x1xf32>{1, 1}, %4001:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3636:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%3804:<1024x1x102400xf32>{102400, 102400, 1}, %3843:<1024x1x1xf32>{1, 1, 1})
          outputs: (%3804:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%4249:tuple{%3804:<1024x1x102400xf32>{102400, 102400, 1}, %3191:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %4143:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%4249:tuple{%3804:<1024x1x102400xf32>{102400,102400,1},%3191:<1024x1xCUSTOM_DATA_TYPE>{1,1},%4143:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2684:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device='cuda_6')_:dict)
          outputs: (%4244:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%4244:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%4244:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%4244:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device='cuda_6')_:dict)
          outputs: (%4244:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%3335:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_6905,___280,_48109,_____,_13969,___372,___363]],_device='cuda_6')_:dict)
          outputs: (%4244:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%4240:<1024xf32>{1}, %4044:<1024xf32>{1})
          outputs: (%4236:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%4236:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1162:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%4044:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1562:<i32>)
          duration: -1
        - aten::div:
          inputs: (%1162:<i32>, %1562:<i32>)
          outputs: (%1238:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%1238:<i32>)
          outputs: (%1570:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%4252:list{%1162:<1xf32>{1}}, 0:int)
          outputs: (%4240:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4240:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4240:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4253:list{%4240:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4233:tuple{%3784:list{%4240:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%4240:<1xf32>{1}, 8:int)
          outputs: (%1570:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<i32>, 1:int)
          outputs: (%2068:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%2068:<i32>, 1:int)
          outputs: (%2068:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%4254:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4044:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%4044:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%4044:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %4044:<1xf32>{1})
          outputs: (%1562:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%1562:<1xf32>{1}, 1:int)
          outputs: (%4240:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%2942:<i32>, 0:int, alpha=1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2942:<i32>, %4125:<i32>, alpha=1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%4143:<i32>, %2942:<i32>, False:bool)
          outputs: (%4143:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%4143:<i32>, 1:int, alpha=1:int)
          outputs: (%4143:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%412:<i32>, %4143:<i32>, False:bool)
          outputs: (%412:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%3784:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%412:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%429:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %426:list{%429:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%423:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%369:<1024xCUSTOM_DATA_TYPE>{1}, %426:list{%423:<1024xCUSTOM_DATA_TYPE>{1}}, %418:<i32>, False:bool)
          outputs: (%369:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%4259:list{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4238:tuple{%4260:list{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::eq:
          inputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%418:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1, %426:list{%418:<1024xCUSTOM_DATA_TYPE>{1}}, %423:<i32>, False:bool)
          outputs: (%415:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1238:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1238:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4261:list{%1238:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4262:tuple{%3784:list{%1238:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3993:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3993:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4263:list{%3993:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4264:tuple{%4037:list{%3993:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4265:list{%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4238:tuple{%4266:list{%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3813:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4259:list{%3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4262:tuple{%4260:list{%3813:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3813:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3813:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3813:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2859:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2859:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3813:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[21670,___581,___372,_____,____19,___285,___558]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[21670,___581,___372,_____,____19,___285,___558]],_device)
          duration: -1
334994 2024-12-10 17:48:47.799167 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n9,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%4267:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%4267:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
335014 2024-12-10 17:48:47.799930 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n9,rank6)
        - ----------->api::embedding call:
          inputs: (%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%1162:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%4143:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%4143:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
335104 2024-12-10 17:48:47.814889 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n9,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%4267:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%2942:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%4267:tuple{%3690:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%4267:tuple{%3690:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
335150 2024-12-10 17:48:47.816964 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n9,rank6)
        - ----------->api::dropout call:
          inputs: (%3690:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3690:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3690:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3690:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
335184 2024-12-10 17:48:47.823254 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n9,rank6)
        - ----------->api::Dropout return:
          inputs: (%4267:tuple{%3690:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%3690:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
335198 2024-12-10 17:48:47.823992 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n9,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[21670,___581,___372,_____,____19,___285,___558]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%3690:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%4267:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%4267:tuple{1024:int}))
          duration: -1
335230 2024-12-10 17:48:47.825623 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n9,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%2942:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%2942:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%4256:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%293:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%2942:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%4272:list{%2942:<1024x20xf32>{20, 1}, %2942:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%4255:<1024x40xf32>{40,1})
          duration: -1
335384 2024-12-10 17:48:47.836693 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n9,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%4267:tuple{1024:int})
          outputs: (%3874:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
335415 2024-12-10 17:48:47.843020 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n9,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
335495 2024-12-10 17:48:47.852114 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n9,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4267:tuple{%2942:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4267:tuple{%2942:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
335513 2024-12-10 17:48:47.852863 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n27,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%2942:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%2942:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%2942:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%2942:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4225:tuple{%2942:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4225:tuple{%2942:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4225:tuple{%2942:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4225:tuple{%2942:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4250:tuple{%2942:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3581:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4250:tuple{%2942:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3581:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%412:list{%369:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%422:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%422:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%2942:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4225:tuple{%2942:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4044:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%2942:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%4044:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
335756 2024-12-10 17:48:47.897160 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n27,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%4267:tuple{%4044:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%4267:tuple{%4044:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
335782 2024-12-10 17:48:47.901431 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n9,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4275:tuple{%4044:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4275:tuple{%4044:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
335804 2024-12-10 17:48:47.902195 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n234,rank6)
        - aten::mm:
          inputs: (%4065:<1024x5120xbf16>{5120, 1}, %3891:<5120x1536xbf16>{1, 5120})
          outputs: (%3843:<1024x1536xbf16>{1536,1})
          duration: -1
335958 2024-12-10 17:48:47.910108 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n234,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4275:tuple{%4044:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4233:tuple{%3840:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4275:tuple{%3840:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4275:tuple{%3840:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
335987 2024-12-10 17:48:47.911138 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n235,rank6)
        - aten::mm:
          inputs: (%4282:<1024x1536xbf16>{1536, 1}, %4065:<1536x24576xbf16>{1, 1536})
          outputs: (%3891:<1024x24576xbf16>{24576,1})
          duration: -1
336100 2024-12-10 17:48:47.917292 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n235,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4275:tuple{%3840:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%4285:tuple{%3468:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3920:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %4281:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3920:<1024x1x128x192xbf16>{24576,24576,192,1},%4281:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3920:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %4281:list{128:int, 64:int}, -1:int)
          outputs: (%4278:tuple{%4286:<1024x1x128x128xbf16>{24576,24576,192,1},%3910:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4275:tuple{%4044:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4275:tuple{%4044:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
336186 2024-12-10 17:48:47.926852 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n236,rank6)
        - aten::mm:
          inputs: (%3840:<1024x5120xbf16>{5120, 1}, %3448:<5120x576xbf16>{1, 5120})
          outputs: (%3636:<1024x576xbf16>{576,1})
          duration: -1
336337 2024-12-10 17:48:47.934681 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n236,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4275:tuple{%4044:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4278:tuple{%3843:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3843:<1024x1x576xbf16>{576, 576, 1}, %4290:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3843:<1024x1x576xbf16>{576,576,1},%4290:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3843:<1024x1x576xbf16>{576, 576, 1}, %4290:list{512:int, 64:int}, -1:int)
          outputs: (%4285:tuple{%4282:<1024x1x512xbf16>{576,576,1},%3928:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4275:tuple{%4282:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4275:tuple{%4282:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
336409 2024-12-10 17:48:47.942993 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n237,rank6)
        - aten::mm:
          inputs: (%3636:<1024x512xbf16>{576, 1}, %3647:<512x32768xbf16>{1, 512})
          outputs: (%3448:<1024x32768xbf16>{32768,1})
          duration: -1
336517 2024-12-10 17:48:47.949197 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n237,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4275:tuple{%4282:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%4278:tuple{%4293:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3891:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4291:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%3891:<1024x1x128x256xbf16>{32768,32768,256,1},%4291:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3891:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4291:list{128:int, 128:int}, -1:int)
          outputs: (%4233:tuple{%4065:<1024x1x128x128xbf16>{32768,32768,256,1},%3636:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%4268:tuple{%3333:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%4268:tuple{%3333:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
336632 2024-12-10 17:48:47.960970 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n9,rank6)
336674 2024-12-10 17:48:47.964097 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n9,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%4268:tuple{%3333:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%4262:tuple{%3840:<1024x64xbf16>{64,1},%3647:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%3840:<1024x64xbf16>{64, 1}, %4290:list{%3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3636:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%3647:<1024x64xbf16>{64, 1}, %4295:list{%3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3928:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3608:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3441:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3433:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3581:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%4297:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4290:list{%4297:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %4134:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%4148:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4148:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3738:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4134:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3433:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4134:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%3581:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4089:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3441:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3433:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%4298:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%4148:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4290:list{%4148:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %4134:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%4087:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4087:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3738:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3745:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3433:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %3745:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%4298:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::copy_:
          inputs: (%3441:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4111:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%3441:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::stack:
          inputs: (%431:list{%415:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%430:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%430:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%3745:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3581:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%3745:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%3928:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %3441:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%3928:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3928:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %3576:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%3928:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%3468:<128x1024x192xbf16>{196608, 192, 1}, %3843:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%4286:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%3647:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%3468:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%4286:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%3843:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%4286:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %4290:list{%3843:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3840:<i32>, False:bool)
          outputs: (%4286:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%3468:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %4286:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%3647:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%3647:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%3647:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%3840:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%4282:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%3647:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %4282:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%4282:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%3840:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3840:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3840:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3840:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%3928:<128x1024x1024xbf16>{1048576, 1024, 1}, %4065:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%3647:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4275:tuple{%3912:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4275:tuple{%3912:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
338167 2024-12-10 17:48:48.082582 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n198,rank6)
        - aten::mm:
          inputs: (%3581:<1024x16384xbf16>{16384, 1}, %3738:<16384x5120xbf16>{1, 16384})
          outputs: (%4111:<1024x5120xbf16>{5120,1})
          duration: -1
338320 2024-12-10 17:48:48.090799 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n198,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4275:tuple{%3912:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%4108:tuple{%4111:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
338336 2024-12-10 17:48:48.091576 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n9,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%4267:tuple{%4044:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4108:tuple{%4111:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4303:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4303:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4221:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4221:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b84ff030_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b84ff030_:_InferenceMode)
          duration: -1
        - aten::stack:
          inputs: (%415:list{%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%422:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%422:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4305:tuple{%3647:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4305:tuple{%3647:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
340805 2024-12-10 17:48:48.639500 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n28,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3647:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3647:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%3647:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%3647:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4306:tuple{%3647:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4306:tuple{%3647:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4306:tuple{%3647:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4306:tuple{%3647:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4308:tuple{%3647:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %4134:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4308:tuple{%3647:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%4134:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%436:list{%409:<1024xf32>{1}}, 0:int, out=%430:<1x1024xf32>{1024, 1})
          outputs: (%430:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%3647:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4306:tuple{%3647:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%3636:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%3647:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%3636:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
341054 2024-12-10 17:48:48.684035 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n28,rank6)
        - ----------->api::MoELayer call:
          inputs: (%4305:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%4305:tuple{%3636:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
341065 2024-12-10 17:48:48.684786 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n9,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%4309:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%4309:tuple{%3636:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
341077 2024-12-10 17:48:48.685551 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n9,rank6)
        - aten::mm:
          inputs: (%4134:<1024x5120xbf16>{5120, 1}, %4310:<5120x160xbf16>{1, 5120})
          outputs: (%4110:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%4314:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%4315:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%4316:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%4317:tuple{%4318:<1024x6xbf16>{6,1},%4314:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%4319:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%4320:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%4321:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%4322:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%4322:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %3835:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%4322:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%4322:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %4321:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%3928:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%4320:<1024x160xf32>{160, 1}, %4321:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%4282:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%4282:<160xf32>{1}, %3928:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%4286:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%4286:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%3912:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%3912:<i32>, 2_5431315104166666e-07:float)
          outputs: (%3891:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3891:<i32>, 0_01:float)
          outputs: (%3912:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%4243:<i32>, %4324:<i32>, alpha=1:int)
          outputs: (%4243:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%4325:<i32>, %4243:<i32>, False:bool)
          outputs: (%4325:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%4327:tuple{%3891:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%4327:tuple{%3891:<i32>}))
          duration: -1
341591 2024-12-10 17:48:48.720929 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n9,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%4309:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4330:tuple{%4286:<1024x6xbf16>{6,1},%4314:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4313:<8192x5120xbf16>{5120, 1}, %4111:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4313:<8192x5120xbf16>{5120,1},%4111:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4313:<8192x5120xbf16>{5120, 1}, %4111:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4317:tuple{%4313:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%3920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4314:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%3920:<8192x6xCUSTOM_DATA_TYPE>{6,1},%4314:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%3920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4314:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4330:tuple{%3920:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%3920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%4065:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%3920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%3928:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%4065:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3928:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3891:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%3920:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %3891:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%3912:<7226xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::stack:
          inputs: (%437:list{%278:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%439:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%439:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4337:<8192x6xbf16>{6, 1}, %4286:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4337:<8192x6xbf16>{6,1},%4286:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4337:<8192x6xbf16>{6, 1}, %4286:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4332:tuple{%4337:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%4337:<8192x6xbf16>{6, 1}, %3891:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4339:<7226xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%3891:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4315:<7226x2xCUSTOM_DATA_TYPE>{1,7226})
          duration: -1
        - aten::gather:
          inputs: (%4313:<8192x5120xbf16>{5120, 1}, 0:int, %4342:<7226x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4340:<7226x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%3912:<7226xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%4343:tuple{%4129:<7226xCUSTOM_DATA_TYPE>{1},%4344:<7226xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%4129:<7226xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%4345:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%4340:<7226x5120xbf16>{5120, 1}, 0:int, %4346:<7226x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4349:<7226x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%4343:tuple{%4349:<7226x5120xbf16>{5120, 1}, %4347:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%4343:tuple{%4349:<7226x5120xbf16>{5120,1},%4347:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
342475 2024-12-10 17:48:48.838863 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n9,rank6)
        - aten::cumsum:
          inputs: (%4347:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%4350:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%4341:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%3891:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%4341:list{%3891:<1xCUSTOM_DATA_TYPE>{1}, %4350:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%4340:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3441:<281x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3441:<281x5120xbf16>{5120,1}}))
          duration: -1
342645 2024-12-10 17:48:48.849844 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n189,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3441:<281x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3441:<281x5120xbf16>{5120,1}}))
          duration: -1
342667 2024-12-10 17:48:48.850591 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n238,rank6)
        - aten::mm:
          inputs: (%3441:<281x5120xbf16>{5120, 1}, %3745:<5120x3072xbf16>{1, 5120})
          outputs: (%3608:<281x3072xbf16>{3072,1})
          duration: -1
342748 2024-12-10 17:48:48.854463 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n238,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3441:<281x5120xbf16>{5120, 1}})
          outputs: (%4330:tuple{%3608:<281x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3581:<281x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3581:<281x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3581:<281x1536xbf16>{3072, 1})
          outputs: (%4296:<281x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3581:<281x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4296:<281x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4296:<281x1536xbf16>{1536, 1}, %3433:<281x1536xbf16>{3072, 1}+1536)
          outputs: (%3745:<281x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3745:<281x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3745:<281x1536xbf16>{1536,1}}))
          duration: -1
342856 2024-12-10 17:48:48.864506 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n199,rank6)
        - aten::mm:
          inputs: (%3745:<281x1536xbf16>{1536, 1}, %3738:<1536x5120xbf16>{1, 1536})
          outputs: (%3581:<281x5120xbf16>{5120,1})
          duration: -1
342940 2024-12-10 17:48:48.868325 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n199,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3745:<281x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3581:<281x5120xbf16>{5120,1},None:NoneType})
          duration: -1
342956 2024-12-10 17:48:48.869104 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n189,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3441:<281x5120xbf16>{5120, 1}})
          outputs: (%4335:tuple{%3581:<281x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3738:<281x5120xbf16>{5120, 1}, %3581:<281x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3738:<281x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4351:<928x5120xbf16>{5120, 1}+1438720})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4351:<928x5120xbf16>{5120,1}+1438720}))
          duration: -1
343139 2024-12-10 17:48:48.879471 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n190,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4351:<928x5120xbf16>{5120, 1}+1438720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4351:<928x5120xbf16>{5120,1}+1438720}))
          duration: -1
343159 2024-12-10 17:48:48.880210 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n239,rank6)
        - aten::mm:
          inputs: (%4351:<928x5120xbf16>{5120, 1}+1438720, %3738:<5120x3072xbf16>{1, 5120})
          outputs: (%4296:<928x3072xbf16>{3072,1})
          duration: -1
343243 2024-12-10 17:48:48.884050 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n239,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4351:<928x5120xbf16>{5120, 1}+1438720})
          outputs: (%4357:tuple{%4296:<928x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<928x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<928x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<928x1536xbf16>{3072, 1})
          outputs: (%3433:<928x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<928x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3433:<928x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3433:<928x1536xbf16>{1536, 1}, %3738:<928x1536xbf16>{3072, 1}+1536)
          outputs: (%3441:<928x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3441:<928x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3441:<928x1536xbf16>{1536,1}}))
          duration: -1
343343 2024-12-10 17:48:48.894054 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n200,rank6)
        - aten::mm:
          inputs: (%3441:<928x1536xbf16>{1536, 1}, %4360:<1536x5120xbf16>{1, 1536})
          outputs: (%3910:<928x5120xbf16>{5120,1})
          duration: -1
343427 2024-12-10 17:48:48.897853 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n200,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3441:<928x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3910:<928x5120xbf16>{5120,1},None:NoneType})
          duration: -1
343443 2024-12-10 17:48:48.898618 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n190,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4351:<928x5120xbf16>{5120, 1}+1438720})
          outputs: (%4335:tuple{%3910:<928x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3581:<928x5120xbf16>{5120, 1}+1438720, %3910:<928x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3581:<928x5120xbf16>{5120,1}+1438720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4350:<282x5120xbf16>{5120, 1}+6190080})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4350:<282x5120xbf16>{5120,1}+6190080}))
          duration: -1
343624 2024-12-10 17:48:48.908901 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n191,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4350:<282x5120xbf16>{5120, 1}+6190080})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4350:<282x5120xbf16>{5120,1}+6190080}))
          duration: -1
343644 2024-12-10 17:48:48.909659 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n240,rank6)
        - aten::mm:
          inputs: (%4350:<282x5120xbf16>{5120, 1}+6190080, %3433:<5120x3072xbf16>{1, 5120})
          outputs: (%4362:<282x3072xbf16>{3072,1})
          duration: -1
343726 2024-12-10 17:48:48.913453 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n240,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4350:<282x5120xbf16>{5120, 1}+6190080})
          outputs: (%4317:tuple{%4362:<282x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<282x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<282x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<282x1536xbf16>{3072, 1})
          outputs: (%3738:<282x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<282x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3738:<282x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3738:<282x1536xbf16>{1536, 1}, %3433:<282x1536xbf16>{3072, 1}+1536)
          outputs: (%3920:<282x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3920:<282x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3920:<282x1536xbf16>{1536,1}}))
          duration: -1
343826 2024-12-10 17:48:48.923468 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n201,rank6)
        - aten::mm:
          inputs: (%3920:<282x1536xbf16>{1536, 1}, %4087:<1536x5120xbf16>{1, 1536})
          outputs: (%3738:<282x5120xbf16>{5120,1})
          duration: -1
343910 2024-12-10 17:48:48.927252 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n201,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3920:<282x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3738:<282x5120xbf16>{5120,1},None:NoneType})
          duration: -1
343926 2024-12-10 17:48:48.928016 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n191,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4350:<282x5120xbf16>{5120, 1}+6190080})
          outputs: (%4335:tuple{%3738:<282x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3910:<282x5120xbf16>{5120, 1}+6190080, %3738:<282x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3910:<282x5120xbf16>{5120,1}+6190080)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3910:<436x5120xbf16>{5120, 1}+7633920})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3910:<436x5120xbf16>{5120,1}+7633920}))
          duration: -1
344097 2024-12-10 17:48:48.938371 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n192,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3910:<436x5120xbf16>{5120, 1}+7633920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3910:<436x5120xbf16>{5120,1}+7633920}))
          duration: -1
344117 2024-12-10 17:48:48.939107 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n241,rank6)
        - aten::mm:
          inputs: (%3910:<436x5120xbf16>{5120, 1}+7633920, %3433:<5120x3072xbf16>{1, 5120})
          outputs: (%3441:<436x3072xbf16>{3072,1})
          duration: -1
344207 2024-12-10 17:48:48.942946 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n241,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3910:<436x5120xbf16>{5120, 1}+7633920})
          outputs: (%4330:tuple{%3441:<436x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<436x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<436x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<436x1536xbf16>{3072, 1})
          outputs: (%3576:<436x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<436x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<436x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<436x1536xbf16>{1536, 1}, %3433:<436x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<436x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%4134:<436x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%4134:<436x1536xbf16>{1536,1}}))
          duration: -1
344303 2024-12-10 17:48:48.952920 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n202,rank6)
        - aten::mm:
          inputs: (%4134:<436x1536xbf16>{1536, 1}, %4368:<1536x5120xbf16>{1, 1536})
          outputs: (%3608:<436x5120xbf16>{5120,1})
          duration: -1
344391 2024-12-10 17:48:48.956742 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n202,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%4134:<436x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3608:<436x5120xbf16>{5120,1},None:NoneType})
          duration: -1
344404 2024-12-10 17:48:48.957525 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n192,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3910:<436x5120xbf16>{5120, 1}+7633920})
          outputs: (%4335:tuple{%3608:<436x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4256:<436x5120xbf16>{5120, 1}+7633920, %3608:<436x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4256:<436x5120xbf16>{5120,1}+7633920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3745:<39x5120xbf16>{5120, 1}+9866240})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3745:<39x5120xbf16>{5120,1}+9866240}))
          duration: -1
344580 2024-12-10 17:48:48.967831 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n193,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3745:<39x5120xbf16>{5120, 1}+9866240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3745:<39x5120xbf16>{5120,1}+9866240}))
          duration: -1
344599 2024-12-10 17:48:48.968562 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n242,rank6)
        - aten::mm:
          inputs: (%3745:<39x5120xbf16>{5120, 1}+9866240, %3920:<5120x3072xbf16>{1, 5120})
          outputs: (%3441:<39x3072xbf16>{3072,1})
          duration: -1
344688 2024-12-10 17:48:48.972384 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n242,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3745:<39x5120xbf16>{5120, 1}+9866240})
          outputs: (%4357:tuple{%3441:<39x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3468:<39x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3468:<39x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3468:<39x1536xbf16>{3072, 1})
          outputs: (%4296:<39x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3468:<39x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4296:<39x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4296:<39x1536xbf16>{1536, 1}, %4359:<39x1536xbf16>{3072, 1}+1536)
          outputs: (%3738:<39x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3738:<39x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3738:<39x1536xbf16>{1536,1}}))
          duration: -1
344791 2024-12-10 17:48:48.982399 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n203,rank6)
        - aten::mm:
          inputs: (%3738:<39x1536xbf16>{1536, 1}, %3433:<1536x5120xbf16>{1, 1536})
          outputs: (%3581:<39x5120xbf16>{5120,1})
          duration: -1
344876 2024-12-10 17:48:48.986208 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n203,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3738:<39x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3581:<39x5120xbf16>{5120,1},None:NoneType})
          duration: -1
344890 2024-12-10 17:48:48.986983 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n193,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3745:<39x5120xbf16>{5120, 1}+9866240})
          outputs: (%4335:tuple{%3581:<39x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3441:<39x5120xbf16>{5120, 1}+9866240, %3581:<39x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3441:<39x5120xbf16>{5120,1}+9866240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4351:<535x5120xbf16>{5120, 1}+10065920})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4351:<535x5120xbf16>{5120,1}+10065920}))
          duration: -1
345066 2024-12-10 17:48:48.997315 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n194,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4351:<535x5120xbf16>{5120, 1}+10065920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4351:<535x5120xbf16>{5120,1}+10065920}))
          duration: -1
345084 2024-12-10 17:48:48.998051 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n243,rank6)
        - aten::mm:
          inputs: (%4351:<535x5120xbf16>{5120, 1}+10065920, %4360:<5120x3072xbf16>{1, 5120})
          outputs: (%3738:<535x3072xbf16>{3072,1})
          duration: -1
345177 2024-12-10 17:48:49.001871 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n243,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4351:<535x5120xbf16>{5120, 1}+10065920})
          outputs: (%4317:tuple{%3738:<535x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<535x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<535x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<535x1536xbf16>{3072, 1})
          outputs: (%3433:<535x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<535x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3433:<535x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3433:<535x1536xbf16>{1536, 1}, %4296:<535x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<535x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%4134:<535x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%4134:<535x1536xbf16>{1536,1}}))
          duration: -1
345275 2024-12-10 17:48:49.012069 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n204,rank6)
        - aten::mm:
          inputs: (%4134:<535x1536xbf16>{1536, 1}, %3738:<1536x5120xbf16>{1, 1536})
          outputs: (%4087:<535x5120xbf16>{5120,1})
          duration: -1
345362 2024-12-10 17:48:49.015906 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n204,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%4134:<535x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4087:<535x5120xbf16>{5120,1},None:NoneType})
          duration: -1
345378 2024-12-10 17:48:49.016671 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n194,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4351:<535x5120xbf16>{5120, 1}+10065920})
          outputs: (%4335:tuple{%4087:<535x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3581:<535x5120xbf16>{5120, 1}+10065920, %4087:<535x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3581:<535x5120xbf16>{5120,1}+10065920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4350:<97x5120xbf16>{5120, 1}+12805120})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4350:<97x5120xbf16>{5120,1}+12805120}))
          duration: -1
345549 2024-12-10 17:48:49.027085 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n195,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4350:<97x5120xbf16>{5120, 1}+12805120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4350:<97x5120xbf16>{5120,1}+12805120}))
          duration: -1
345564 2024-12-10 17:48:49.027823 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n244,rank6)
        - aten::mm:
          inputs: (%4350:<97x5120xbf16>{5120, 1}+12805120, %3920:<5120x3072xbf16>{1, 5120})
          outputs: (%3910:<97x3072xbf16>{3072,1})
          duration: -1
345657 2024-12-10 17:48:49.031653 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n244,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4350:<97x5120xbf16>{5120, 1}+12805120})
          outputs: (%4330:tuple{%3910:<97x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3745:<97x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3745:<97x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3745:<97x1536xbf16>{3072, 1})
          outputs: (%3738:<97x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3745:<97x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3738:<97x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3738:<97x1536xbf16>{1536, 1}, %4359:<97x1536xbf16>{3072, 1}+1536)
          outputs: (%3920:<97x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3920:<97x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3920:<97x1536xbf16>{1536,1}}))
          duration: -1
345756 2024-12-10 17:48:49.041671 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n205,rank6)
        - aten::mm:
          inputs: (%3920:<97x1536xbf16>{1536, 1}, %4360:<1536x5120xbf16>{1, 1536})
          outputs: (%4296:<97x5120xbf16>{5120,1})
          duration: -1
345839 2024-12-10 17:48:49.045494 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n205,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3920:<97x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4296:<97x5120xbf16>{5120,1},None:NoneType})
          duration: -1
345856 2024-12-10 17:48:49.046289 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n195,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4350:<97x5120xbf16>{5120, 1}+12805120})
          outputs: (%4335:tuple{%4296:<97x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4351:<97x5120xbf16>{5120, 1}+12805120, %4296:<97x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4351:<97x5120xbf16>{5120,1}+12805120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3441:<408x5120xbf16>{5120, 1}+13301760})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3441:<408x5120xbf16>{5120,1}+13301760}))
          duration: -1
346034 2024-12-10 17:48:49.056637 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n196,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3441:<408x5120xbf16>{5120, 1}+13301760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3441:<408x5120xbf16>{5120,1}+13301760}))
          duration: -1
346050 2024-12-10 17:48:49.057393 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n245,rank6)
        - aten::mm:
          inputs: (%3441:<408x5120xbf16>{5120, 1}+13301760, %4206:<5120x3072xbf16>{1, 5120})
          outputs: (%3433:<408x3072xbf16>{3072,1})
          duration: -1
346142 2024-12-10 17:48:49.061222 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n245,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3441:<408x5120xbf16>{5120, 1}+13301760})
          outputs: (%4357:tuple{%3433:<408x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<408x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<408x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<408x1536xbf16>{3072, 1})
          outputs: (%3576:<408x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<408x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<408x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<408x1536xbf16>{1536, 1}, %3745:<408x1536xbf16>{3072, 1}+1536)
          outputs: (%3738:<408x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3738:<408x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3738:<408x1536xbf16>{1536,1}}))
          duration: -1
346246 2024-12-10 17:48:49.071234 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n206,rank6)
        - aten::mm:
          inputs: (%3738:<408x1536xbf16>{1536, 1}, %4087:<1536x5120xbf16>{1, 1536})
          outputs: (%4134:<408x5120xbf16>{5120,1})
          duration: -1
346326 2024-12-10 17:48:49.075089 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n206,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3738:<408x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4134:<408x5120xbf16>{5120,1},None:NoneType})
          duration: -1
346341 2024-12-10 17:48:49.075865 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n196,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3441:<408x5120xbf16>{5120, 1}+13301760})
          outputs: (%4335:tuple{%4134:<408x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4350:<408x5120xbf16>{5120, 1}+13301760, %4134:<408x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4350:<408x5120xbf16>{5120,1}+13301760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3433:<167x5120xbf16>{5120, 1}+15390720})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3433:<167x5120xbf16>{5120,1}+15390720}))
          duration: -1
346514 2024-12-10 17:48:49.086196 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n197,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3433:<167x5120xbf16>{5120, 1}+15390720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3433:<167x5120xbf16>{5120,1}+15390720}))
          duration: -1
346530 2024-12-10 17:48:49.086933 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n246,rank6)
        - aten::mm:
          inputs: (%3433:<167x5120xbf16>{5120, 1}+15390720, %3441:<5120x3072xbf16>{1, 5120})
          outputs: (%3581:<167x3072xbf16>{3072,1})
          duration: -1
346622 2024-12-10 17:48:49.090770 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n246,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3433:<167x5120xbf16>{5120, 1}+15390720})
          outputs: (%4317:tuple{%3581:<167x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<167x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<167x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<167x1536xbf16>{3072, 1})
          outputs: (%3441:<167x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<167x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3441:<167x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3441:<167x1536xbf16>{1536, 1}, %3608:<167x1536xbf16>{3072, 1}+1536)
          outputs: (%3072:<167x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3072:<167x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3072:<167x1536xbf16>{1536,1}}))
          duration: -1
346728 2024-12-10 17:48:49.100763 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n207,rank6)
        - aten::mm:
          inputs: (%3072:<167x1536xbf16>{1536, 1}, %3920:<1536x5120xbf16>{1, 1536})
          outputs: (%4296:<167x5120xbf16>{5120,1})
          duration: -1
346806 2024-12-10 17:48:49.104578 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n207,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3072:<167x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4296:<167x5120xbf16>{5120,1},None:NoneType})
          duration: -1
346820 2024-12-10 17:48:49.105359 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n197,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3433:<167x5120xbf16>{5120, 1}+15390720})
          outputs: (%4335:tuple{%4296:<167x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3920:<167x5120xbf16>{5120, 1}+15390720, %4296:<167x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3920:<167x5120xbf16>{5120,1}+15390720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4351:<308x5120xbf16>{5120, 1}+16245760})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4351:<308x5120xbf16>{5120,1}+16245760}))
          duration: -1
346999 2024-12-10 17:48:49.115679 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n198,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4351:<308x5120xbf16>{5120, 1}+16245760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4351:<308x5120xbf16>{5120,1}+16245760}))
          duration: -1
347015 2024-12-10 17:48:49.116413 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n247,rank6)
        - aten::mm:
          inputs: (%4351:<308x5120xbf16>{5120, 1}+16245760, %4360:<5120x3072xbf16>{1, 5120})
          outputs: (%3441:<308x3072xbf16>{3072,1})
          duration: -1
347104 2024-12-10 17:48:49.120214 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n247,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4351:<308x5120xbf16>{5120, 1}+16245760})
          outputs: (%4330:tuple{%3441:<308x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3920:<308x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3920:<308x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3920:<308x1536xbf16>{3072, 1})
          outputs: (%3576:<308x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3920:<308x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<308x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<308x1536xbf16>{1536, 1}, %3433:<308x1536xbf16>{3072, 1}+1536)
          outputs: (%4134:<308x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%4134:<308x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%4134:<308x1536xbf16>{1536,1}}))
          duration: -1
347220 2024-12-10 17:48:49.130215 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n208,rank6)
        - aten::mm:
          inputs: (%4134:<308x1536xbf16>{1536, 1}, %4362:<1536x5120xbf16>{1, 1536})
          outputs: (%4386:<308x5120xbf16>{5120,1})
          duration: -1
347291 2024-12-10 17:48:49.134013 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n208,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%4134:<308x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4386:<308x5120xbf16>{5120,1},None:NoneType})
          duration: -1
347306 2024-12-10 17:48:49.134776 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n198,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4351:<308x5120xbf16>{5120, 1}+16245760})
          outputs: (%4335:tuple{%4386:<308x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3433:<308x5120xbf16>{5120, 1}+16245760, %4386:<308x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3433:<308x5120xbf16>{5120,1}+16245760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4350:<196x5120xbf16>{5120, 1}+17822720})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4350:<196x5120xbf16>{5120,1}+17822720}))
          duration: -1
347471 2024-12-10 17:48:49.145108 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n199,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4350:<196x5120xbf16>{5120, 1}+17822720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4350:<196x5120xbf16>{5120,1}+17822720}))
          duration: -1
347486 2024-12-10 17:48:49.145854 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n248,rank6)
        - aten::mm:
          inputs: (%4350:<196x5120xbf16>{5120, 1}+17822720, %3920:<5120x3072xbf16>{1, 5120})
          outputs: (%3910:<196x3072xbf16>{3072,1})
          duration: -1
347565 2024-12-10 17:48:49.149662 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n248,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4350:<196x5120xbf16>{5120, 1}+17822720})
          outputs: (%4357:tuple{%3910:<196x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3608:<196x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3608:<196x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3608:<196x1536xbf16>{3072, 1})
          outputs: (%3576:<196x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3608:<196x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<196x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<196x1536xbf16>{1536, 1}, %3433:<196x1536xbf16>{3072, 1}+1536)
          outputs: (%3745:<196x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3745:<196x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3745:<196x1536xbf16>{1536,1}}))
          duration: -1
347688 2024-12-10 17:48:49.159611 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n209,rank6)
        - aten::mm:
          inputs: (%3745:<196x1536xbf16>{1536, 1}, %4087:<1536x5120xbf16>{1, 1536})
          outputs: (%4359:<196x5120xbf16>{5120,1})
          duration: -1
347755 2024-12-10 17:48:49.163394 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n209,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3745:<196x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4359:<196x5120xbf16>{5120,1},None:NoneType})
          duration: -1
347769 2024-12-10 17:48:49.164156 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n199,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4350:<196x5120xbf16>{5120, 1}+17822720})
          outputs: (%4335:tuple{%4359:<196x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4351:<196x5120xbf16>{5120, 1}+17822720, %4359:<196x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4351:<196x5120xbf16>{5120,1}+17822720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4134:<228x5120xbf16>{5120, 1}+18826240})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4134:<228x5120xbf16>{5120,1}+18826240}))
          duration: -1
347939 2024-12-10 17:48:49.174466 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n200,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4134:<228x5120xbf16>{5120, 1}+18826240})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4134:<228x5120xbf16>{5120,1}+18826240}))
          duration: -1
347956 2024-12-10 17:48:49.175208 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n249,rank6)
        - aten::mm:
          inputs: (%4134:<228x5120xbf16>{5120, 1}+18826240, %4072:<5120x3072xbf16>{1, 5120})
          outputs: (%4296:<228x3072xbf16>{3072,1})
          duration: -1
348042 2024-12-10 17:48:49.179034 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n249,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4134:<228x5120xbf16>{5120, 1}+18826240})
          outputs: (%4317:tuple{%4296:<228x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<228x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<228x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<228x1536xbf16>{3072, 1})
          outputs: (%3433:<228x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<228x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3433:<228x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3433:<228x1536xbf16>{1536, 1}, %3581:<228x1536xbf16>{3072, 1}+1536)
          outputs: (%3738:<228x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3738:<228x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3738:<228x1536xbf16>{1536,1}}))
          duration: -1
348159 2024-12-10 17:48:49.188989 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n210,rank6)
        - aten::mm:
          inputs: (%3738:<228x1536xbf16>{1536, 1}, %4150:<1536x5120xbf16>{1, 1536})
          outputs: (%4320:<228x5120xbf16>{5120,1})
          duration: -1
348220 2024-12-10 17:48:49.192756 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n210,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3738:<228x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4320:<228x5120xbf16>{5120,1},None:NoneType})
          duration: -1
348240 2024-12-10 17:48:49.193529 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n200,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4134:<228x5120xbf16>{5120, 1}+18826240})
          outputs: (%4335:tuple{%4320:<228x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4350:<228x5120xbf16>{5120, 1}+18826240, %4320:<228x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4350:<228x5120xbf16>{5120,1}+18826240)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3441:<296x5120xbf16>{5120, 1}+19993600})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3441:<296x5120xbf16>{5120,1}+19993600}))
          duration: -1
348403 2024-12-10 17:48:49.203867 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n201,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3441:<296x5120xbf16>{5120, 1}+19993600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3441:<296x5120xbf16>{5120,1}+19993600}))
          duration: -1
348418 2024-12-10 17:48:49.204600 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n250,rank6)
        - aten::mm:
          inputs: (%3441:<296x5120xbf16>{5120, 1}+19993600, %4386:<5120x3072xbf16>{1, 5120})
          outputs: (%3433:<296x3072xbf16>{3072,1})
          duration: -1
348498 2024-12-10 17:48:49.208428 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n250,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3441:<296x5120xbf16>{5120, 1}+19993600})
          outputs: (%4330:tuple{%3433:<296x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<296x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<296x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<296x1536xbf16>{3072, 1})
          outputs: (%3576:<296x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<296x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<296x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<296x1536xbf16>{1536, 1}, %4296:<296x1536xbf16>{3072, 1}+1536)
          outputs: (%3581:<296x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3581:<296x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3581:<296x1536xbf16>{1536,1}}))
          duration: -1
348637 2024-12-10 17:48:49.218376 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n211,rank6)
        - aten::mm:
          inputs: (%3581:<296x1536xbf16>{1536, 1}, %3433:<1536x5120xbf16>{1, 1536})
          outputs: (%3738:<296x5120xbf16>{5120,1})
          duration: -1
348689 2024-12-10 17:48:49.222168 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n211,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3581:<296x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3738:<296x5120xbf16>{5120,1},None:NoneType})
          duration: -1
348710 2024-12-10 17:48:49.222928 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n201,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3441:<296x5120xbf16>{5120, 1}+19993600})
          outputs: (%4335:tuple{%3738:<296x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3745:<296x5120xbf16>{5120, 1}+19993600, %3738:<296x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3745:<296x5120xbf16>{5120,1}+19993600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4351:<560x5120xbf16>{5120, 1}+21509120})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4351:<560x5120xbf16>{5120,1}+21509120}))
          duration: -1
348881 2024-12-10 17:48:49.233280 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n202,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4351:<560x5120xbf16>{5120, 1}+21509120})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4351:<560x5120xbf16>{5120,1}+21509120}))
          duration: -1
348897 2024-12-10 17:48:49.234012 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n251,rank6)
        - aten::mm:
          inputs: (%4351:<560x5120xbf16>{5120, 1}+21509120, %4368:<5120x3072xbf16>{1, 5120})
          outputs: (%4296:<560x3072xbf16>{3072,1})
          duration: -1
348975 2024-12-10 17:48:49.237827 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n251,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4351:<560x5120xbf16>{5120, 1}+21509120})
          outputs: (%4357:tuple{%4296:<560x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<560x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<560x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<560x1536xbf16>{3072, 1})
          outputs: (%3581:<560x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<560x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3581:<560x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3581:<560x1536xbf16>{1536, 1}, %3433:<560x1536xbf16>{3072, 1}+1536)
          outputs: (%3441:<560x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3441:<560x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3441:<560x1536xbf16>{1536,1}}))
          duration: -1
349117 2024-12-10 17:48:49.247754 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n212,rank6)
        - aten::mm:
          inputs: (%3441:<560x1536xbf16>{1536, 1}, %4072:<1536x5120xbf16>{1, 1536})
          outputs: (%4110:<560x5120xbf16>{5120,1})
          duration: -1
349168 2024-12-10 17:48:49.251551 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n212,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3441:<560x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4110:<560x5120xbf16>{5120,1},None:NoneType})
          duration: -1
349189 2024-12-10 17:48:49.252318 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n202,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4351:<560x5120xbf16>{5120, 1}+21509120})
          outputs: (%4335:tuple{%4110:<560x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3441:<560x5120xbf16>{5120, 1}+21509120, %4110:<560x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3441:<560x5120xbf16>{5120,1}+21509120)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4350:<344x5120xbf16>{5120, 1}+24376320})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4350:<344x5120xbf16>{5120,1}+24376320}))
          duration: -1
349372 2024-12-10 17:48:49.262677 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n203,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4350:<344x5120xbf16>{5120, 1}+24376320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4350:<344x5120xbf16>{5120,1}+24376320}))
          duration: -1
349387 2024-12-10 17:48:49.263416 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n252,rank6)
        - aten::mm:
          inputs: (%4350:<344x5120xbf16>{5120, 1}+24376320, %4080:<5120x3072xbf16>{1, 5120})
          outputs: (%3581:<344x3072xbf16>{3072,1})
          duration: -1
349459 2024-12-10 17:48:49.267238 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n252,rank6)
        - ----------->api::silu call:
          inputs: (%4359:<344x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<344x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<344x1536xbf16>{3072, 1})
          outputs: (%3738:<344x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<344x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3738:<344x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3738:<344x1536xbf16>{1536, 1}, %4296:<344x1536xbf16>{3072, 1}+1536)
          outputs: (%3920:<344x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3920:<344x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3920:<344x1536xbf16>{1536,1}}))
          duration: -1
349602 2024-12-10 17:48:49.277276 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n213,rank6)
        - aten::mm:
          inputs: (%3920:<344x1536xbf16>{1536, 1}, %4072:<1536x5120xbf16>{1, 1536})
          outputs: (%3581:<344x5120xbf16>{5120,1})
          duration: -1
349651 2024-12-10 17:48:49.281095 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n213,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3920:<344x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3581:<344x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4350:<344x5120xbf16>{5120, 1}+24376320})
          outputs: (%4335:tuple{%3581:<344x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4351:<344x5120xbf16>{5120, 1}+24376320, %3581:<344x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4351:<344x5120xbf16>{5120,1}+24376320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3433:<599x5120xbf16>{5120, 1}+26137600})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3433:<599x5120xbf16>{5120,1}+26137600}))
          duration: -1
349841 2024-12-10 17:48:49.292288 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n204,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3433:<599x5120xbf16>{5120, 1}+26137600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3433:<599x5120xbf16>{5120,1}+26137600}))
          duration: -1
349856 2024-12-10 17:48:49.293045 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n253,rank6)
        - aten::mm:
          inputs: (%3433:<599x5120xbf16>{5120, 1}+26137600, %4080:<5120x3072xbf16>{1, 5120})
          outputs: (%3738:<599x3072xbf16>{3072,1})
          duration: -1
349929 2024-12-10 17:48:49.296871 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n253,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3433:<599x5120xbf16>{5120, 1}+26137600})
          outputs: (%4330:tuple{%3738:<599x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<599x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<599x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<599x1536xbf16>{3072, 1})
          outputs: (%3745:<599x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<599x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3745:<599x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3745:<599x1536xbf16>{1536, 1}, %4296:<599x1536xbf16>{3072, 1}+1536)
          outputs: (%3920:<599x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3920:<599x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3920:<599x1536xbf16>{1536,1}}))
          duration: -1
350081 2024-12-10 17:48:49.306843 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n214,rank6)
        - aten::mm:
          inputs: (%3920:<599x1536xbf16>{1536, 1}, %4087:<1536x5120xbf16>{1, 1536})
          outputs: (%3910:<599x5120xbf16>{5120,1})
          duration: -1
350132 2024-12-10 17:48:49.310663 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n214,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3920:<599x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3910:<599x5120xbf16>{5120,1},None:NoneType})
          duration: -1
350147 2024-12-10 17:48:49.311428 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n204,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3433:<599x5120xbf16>{5120, 1}+26137600})
          outputs: (%4335:tuple{%3910:<599x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4350:<599x5120xbf16>{5120, 1}+26137600, %3910:<599x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4350:<599x5120xbf16>{5120,1}+26137600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4351:<512x5120xbf16>{5120, 1}+29204480})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4351:<512x5120xbf16>{5120,1}+29204480}))
          duration: -1
350320 2024-12-10 17:48:49.321839 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n205,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4351:<512x5120xbf16>{5120, 1}+29204480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4351:<512x5120xbf16>{5120,1}+29204480}))
          duration: -1
350335 2024-12-10 17:48:49.322576 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n254,rank6)
        - aten::mm:
          inputs: (%4351:<512x5120xbf16>{5120, 1}+29204480, %4362:<5120x3072xbf16>{1, 5120})
          outputs: (%3738:<512x3072xbf16>{3072,1})
          duration: -1
350411 2024-12-10 17:48:49.326404 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n254,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4351:<512x5120xbf16>{5120, 1}+29204480})
          outputs: (%4357:tuple{%3738:<512x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<512x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<512x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<512x1536xbf16>{3072, 1})
          outputs: (%3576:<512x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<512x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3576:<512x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<512x1536xbf16>{1536, 1}, %3581:<512x1536xbf16>{3072, 1}+1536)
          outputs: (%3433:<512x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3433:<512x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3433:<512x1536xbf16>{1536,1}}))
          duration: -1
350571 2024-12-10 17:48:49.336382 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n215,rank6)
        - aten::mm:
          inputs: (%3433:<512x1536xbf16>{1536, 1}, %4087:<1536x5120xbf16>{1, 1536})
          outputs: (%3581:<512x5120xbf16>{5120,1})
          duration: -1
350620 2024-12-10 17:48:49.340197 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n215,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3433:<512x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3581:<512x5120xbf16>{5120,1},None:NoneType})
          duration: -1
350632 2024-12-10 17:48:49.340961 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n205,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4351:<512x5120xbf16>{5120, 1}+29204480})
          outputs: (%4335:tuple{%3581:<512x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3738:<512x5120xbf16>{5120, 1}+29204480, %3581:<512x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3738:<512x5120xbf16>{5120,1}+29204480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%4350:<354x5120xbf16>{5120, 1}+31825920})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%4350:<354x5120xbf16>{5120,1}+31825920}))
          duration: -1
350808 2024-12-10 17:48:49.351310 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n206,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%4350:<354x5120xbf16>{5120, 1}+31825920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%4350:<354x5120xbf16>{5120,1}+31825920}))
          duration: -1
350823 2024-12-10 17:48:49.352053 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n255,rank6)
        - aten::mm:
          inputs: (%4350:<354x5120xbf16>{5120, 1}+31825920, %4087:<5120x3072xbf16>{1, 5120})
          outputs: (%3433:<354x3072xbf16>{3072,1})
          duration: -1
350897 2024-12-10 17:48:49.355864 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n255,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%4350:<354x5120xbf16>{5120, 1}+31825920})
          outputs: (%4317:tuple{%3433:<354x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<354x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<354x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<354x1536xbf16>{3072, 1})
          outputs: (%3738:<354x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<354x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3738:<354x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3738:<354x1536xbf16>{1536, 1}, %4296:<354x1536xbf16>{3072, 1}+1536)
          outputs: (%3441:<354x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3441:<354x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3441:<354x1536xbf16>{1536,1}}))
          duration: -1
351051 2024-12-10 17:48:49.365844 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n216,rank6)
        - aten::mm:
          inputs: (%3441:<354x1536xbf16>{1536, 1}, %3433:<1536x5120xbf16>{1, 1536})
          outputs: (%4169:<354x5120xbf16>{5120,1})
          duration: -1
351096 2024-12-10 17:48:49.369655 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n216,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3441:<354x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4169:<354x5120xbf16>{5120,1},None:NoneType})
          duration: -1
351113 2024-12-10 17:48:49.370416 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n206,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%4350:<354x5120xbf16>{5120, 1}+31825920})
          outputs: (%4335:tuple{%4169:<354x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4351:<354x5120xbf16>{5120, 1}+31825920, %4169:<354x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4351:<354x5120xbf16>{5120,1}+31825920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3441:<535x5120xbf16>{5120, 1}+33638400})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3441:<535x5120xbf16>{5120,1}+33638400}))
          duration: -1
351299 2024-12-10 17:48:49.380973 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n207,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3441:<535x5120xbf16>{5120, 1}+33638400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3441:<535x5120xbf16>{5120,1}+33638400}))
          duration: -1
351313 2024-12-10 17:48:49.381727 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n256,rank6)
        - aten::mm:
          inputs: (%3441:<535x5120xbf16>{5120, 1}+33638400, %4386:<5120x3072xbf16>{1, 5120})
          outputs: (%3738:<535x3072xbf16>{3072,1})
          duration: -1
351389 2024-12-10 17:48:49.385543 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n256,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3441:<535x5120xbf16>{5120, 1}+33638400})
          outputs: (%4330:tuple{%3738:<535x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4359:<535x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4359:<535x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4359:<535x1536xbf16>{3072, 1})
          outputs: (%3581:<535x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4359:<535x1536xbf16>{3072, 1}, False:bool)
          outputs: (%3581:<535x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%3581:<535x1536xbf16>{1536, 1}, %4296:<535x1536xbf16>{3072, 1}+1536)
          outputs: (%3745:<535x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3745:<535x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3745:<535x1536xbf16>{1536,1}}))
          duration: -1
351542 2024-12-10 17:48:49.395465 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n217,rank6)
        - aten::mm:
          inputs: (%3745:<535x1536xbf16>{1536, 1}, %3738:<1536x5120xbf16>{1, 1536})
          outputs: (%3433:<535x5120xbf16>{5120,1})
          duration: -1
351587 2024-12-10 17:48:49.399253 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n217,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3745:<535x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%3433:<535x5120xbf16>{5120,1},None:NoneType})
          duration: -1
351601 2024-12-10 17:48:49.400018 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n207,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3441:<535x5120xbf16>{5120, 1}+33638400})
          outputs: (%4335:tuple{%3433:<535x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4350:<535x5120xbf16>{5120, 1}+33638400, %3433:<535x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4350:<535x5120xbf16>{5120,1}+33638400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3581:<121x5120xbf16>{5120, 1}+36377600})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3581:<121x5120xbf16>{5120,1}+36377600}))
          duration: -1
351775 2024-12-10 17:48:49.410311 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n208,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3581:<121x5120xbf16>{5120, 1}+36377600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3581:<121x5120xbf16>{5120,1}+36377600}))
          duration: -1
351791 2024-12-10 17:48:49.411046 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n257,rank6)
        - aten::mm:
          inputs: (%3581:<121x5120xbf16>{5120, 1}+36377600, %4072:<5120x3072xbf16>{1, 5120})
          outputs: (%4296:<121x3072xbf16>{3072,1})
          duration: -1
351862 2024-12-10 17:48:49.414843 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n257,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3581:<121x5120xbf16>{5120, 1}+36377600})
          outputs: (%4357:tuple{%4296:<121x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%3576:<121x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%3576:<121x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%3576:<121x1536xbf16>{3072, 1})
          outputs: (%4359:<121x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%3576:<121x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4359:<121x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4359:<121x1536xbf16>{1536, 1}, %3738:<121x1536xbf16>{3072, 1}+1536)
          outputs: (%3441:<121x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3441:<121x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3441:<121x1536xbf16>{1536,1}}))
          duration: -1
352021 2024-12-10 17:48:49.424789 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n218,rank6)
        - aten::mm:
          inputs: (%3441:<121x1536xbf16>{1536, 1}, %4169:<1536x5120xbf16>{1, 1536})
          outputs: (%4320:<121x5120xbf16>{5120,1})
          duration: -1
352065 2024-12-10 17:48:49.428584 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n218,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3441:<121x1536xbf16>{1536, 1}})
          outputs: (%4335:tuple{%4320:<121x5120xbf16>{5120,1},None:NoneType})
          duration: -1
352081 2024-12-10 17:48:49.429355 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n208,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3581:<121x5120xbf16>{5120, 1}+36377600})
          outputs: (%4335:tuple{%4320:<121x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%3441:<121x5120xbf16>{5120, 1}+36377600, %4320:<121x5120xbf16>{5120, 1}, False:bool)
          outputs: (%3441:<121x5120xbf16>{5120,1}+36377600)
          duration: -1
352160 2024-12-10 17:48:49.434302 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n9,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%4343:tuple{%4349:<7226x5120xbf16>{5120, 1}, %4347:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4413:tuple{%4111:<7226x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%4320:<7226x5120xbf16>{5120, 1}, 0:int, %4346:<7226x5120xCUSTOM_DATA_TYPE>{1, 0}, %4111:<7226x5120xbf16>{5120, 1})
          outputs: (%4351:<7226x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%4351:<7226x5120xbf16>{5120, 1}, %4350:<7226x1xbf16>{1, 1})
          outputs: (%4340:<7226x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%4415:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3072:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%3072:<8192x5120xbf16>{5120, 1}, 0:int, %4342:<7226x5120xCUSTOM_DATA_TYPE>{1, 0}, %4340:<7226x5120xbf16>{5120, 1})
          outputs: (%4351:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%3745:<1024x5120xbf16>{5120, 1}, %4351:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%3745:<1024x5120xbf16>{5120,1},%4351:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%3745:<1024x5120xbf16>{5120, 1}, %4351:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%3745:<1024x5120xbf16>{5120,1},%4351:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%3745:<1024x5120xbf16>{5120, 1}, %4351:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%4338:tuple{%3745:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4309:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%4309:tuple{%3636:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
352495 2024-12-10 17:48:49.484217 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n209,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4352:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4352:tuple{%3636:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
352519 2024-12-10 17:48:49.484960 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n258,rank6)
        - aten::mm:
          inputs: (%3738:<1024x5120xbf16>{5120, 1}, %3576:<5120x6144xbf16>{1, 5120})
          outputs: (%3581:<1024x6144xbf16>{6144,1})
          duration: -1
352670 2024-12-10 17:48:49.492845 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n258,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4352:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4335:tuple{%3441:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4422:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4422:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4422:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%3790:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4422:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%3790:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%3790:<1024x1x3072xbf16>{3072, 3072, 1}, %4351:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%3433:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4327:tuple{%3433:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4327:tuple{%3433:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
352774 2024-12-10 17:48:49.502926 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n219,rank6)
        - aten::mm:
          inputs: (%4359:<1024x3072xbf16>{3072, 1}, %3576:<3072x5120xbf16>{1, 3072})
          outputs: (%3441:<1024x5120xbf16>{5120,1})
          duration: -1
352927 2024-12-10 17:48:49.510857 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n219,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4327:tuple{%3433:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%4394:tuple{%4359:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
352943 2024-12-10 17:48:49.511638 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n209,rank6)
        - ----------->api::MLP return:
          inputs: (%4309:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4394:tuple{%4359:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%3072:<1024x1x5120xbf16>{5120, 5120, 1}, %4001:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4351:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
352991 2024-12-10 17:48:49.514819 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n9,rank6)
        - ----------->api::MoELayer return:
          inputs: (%4305:tuple{%3636:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4394:tuple{%4351:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor,0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor, 0_0:float, False:bool, False:bool)
          outputs: (FakeTensor(___,_device='cuda_6',_size=(1024,_1,_5120),_dtype=torch_bfloat16):FakeTensor)
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::disable call:
          inputs: ()
          outputs: (torch.2_3_0.disable())
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.set_multithreading_enabled(False:bool))
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: ()
          outputs: (torch.2_3_0.set_multithreading_enabled())
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%3744:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%3744:tuple{None:NoneType,)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (False:bool)
          outputs: (torch.2_3_0.detect_anomaly(False:bool))
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: ()
          outputs: (torch.2_3_0.detect_anomaly())
          duration: -1
        - ----------->api::dropout call:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (0_0:float, False:bool, False:bool)
          outputs: (NotSurpot:FunctionalTensor)
          duration: -1
        - ----------->api::detect_anomaly call:
          inputs: (%4427:tuple{None:NoneType, None:NoneType, None:NoneType})
          outputs: (torch.2_3_0.detect_anomaly(%4427:tuple{None:NoneType,)
          duration: -1
        - ----------->api::set_multithreading_enabled call:
          inputs: (None:NoneType, None:NoneType, None:NoneType)
          outputs: (torch.2_3_0.set_multithreading_enabled(None:NoneType,)
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::disable call:
          inputs: (None:NoneType)
          outputs: (torch.2_3_0.disable(None:NoneType))
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8552bb0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8552bb0_:_InferenceMode)
          duration: -1
355068 2024-12-10 17:48:50.044212 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n9,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%4429:tuple{%4086:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4430:tuple{%4086:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4430:tuple{%4086:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
355110 2024-12-10 17:48:50.051078 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n29,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4086:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4086:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4086:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4086:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4431:tuple{%4086:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4431:tuple{%4086:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4431:tuple{%4086:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4431:tuple{%4086:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4434:tuple{%4086:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %4044:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4434:tuple{%4086:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%4044:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4086:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4431:tuple{%4086:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1562:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4086:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%1562:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
355344 2024-12-10 17:48:50.095498 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n29,rank6)
355347 2024-12-10 17:48:50.096023 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n9,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%1562:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4435:tuple{%1562:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%4435:tuple{%1562:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
355366 2024-12-10 17:48:50.102340 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n259,rank6)
        - aten::mm:
          inputs: (%4359:<1024x5120xbf16>{5120, 1}, %3608:<5120x102400xbf16>{1, 5120})
          outputs: (%3738:<1024x102400xbf16>{102400,1})
          duration: -1
355464 2024-12-10 17:48:50.114692 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n259,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4435:tuple{%1562:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%4442:tuple{%3441:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%3608:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%4443:tuple{%4444:<1024x1xf32>{1,1},%3581:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4444:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4444:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4445:list{%4444:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4446:tuple{%4447:list{%4444:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%3608:<1024x1x102400xf32>{102400, 102400, 1}, %3433:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%3608:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%2942:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3576:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%2942:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3738:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3576:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3738:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%3581:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3647:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%3433:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%3433:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %4448:list{%3581:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3738:<i32>, False:bool)
          outputs: (%3433:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3636:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3738:<1024x102400xf32>{102400, 1}, %4447:list{%3636:<1024xCUSTOM_DATA_TYPE>{1}, %3468:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3072:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%3647:<1024x1xf32>{1, 1}, %4447:list{%3581:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3072:<i32>, False:bool)
          outputs: (%3647:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%3608:<1024x1x102400xf32>{102400, 102400, 1}, out=%3608:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%3608:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%3608:<1024x1x102400xf32>{102400, 102400, 1}, %4448:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%4449:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3647:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3647:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4448:list{%3647:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4442:tuple{%4439:list{%3647:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4449:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4449:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4450:list{%4449:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4451:tuple{%4030:list{%4449:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%4449:<1024x1xf32>{1, 1})
          outputs: (%3636:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3636:<1024x1xf32>{1, 1}, %3647:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3072:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%3608:<1024x1x102400xf32>{102400, 102400, 1}, %3738:<1024x1x1xf32>{1, 1, 1})
          outputs: (%3608:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%4452:tuple{%3608:<1024x1x102400xf32>{102400, 102400, 1}, %3581:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %3468:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%4452:tuple{%3608:<1024x1x102400xf32>{102400,102400,1},%3581:<1024x1xCUSTOM_DATA_TYPE>{1,1},%3468:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2859:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device='cuda_6')_:dict)
          outputs: (%3433:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3433:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3433:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3433:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device='cuda_6')_:dict)
          outputs: (%3433:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%1162:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3813:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %2068:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_581,__372,__254,_____,__285,__558,_1266]],_device='cuda_6')_:dict)
          outputs: (%3433:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%3608:<1024xf32>{1}, %4143:<1024xf32>{1})
          outputs: (%2942:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%2942:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%4444:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%4143:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3441:<i32>)
          duration: -1
        - aten::div:
          inputs: (%4444:<i32>, %3441:<i32>)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%2942:<i32>)
          outputs: (%3581:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%4455:list{%3581:<1xf32>{1}}, 0:int)
          outputs: (%3608:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3608:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3608:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4456:list{%3608:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4438:tuple{%4457:list{%3608:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%3608:<1xf32>{1}, 8:int)
          outputs: (%3581:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%2942:<i32>, 1:int)
          outputs: (%3738:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%3738:<i32>, 1:int)
          outputs: (%3738:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%4458:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3441:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%3441:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%3441:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %3441:<1xf32>{1})
          outputs: (%3433:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%3433:<1xf32>{1}, 1:int)
          outputs: (%3072:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%412:<i32>, 0:int, alpha=1:int)
          outputs: (%412:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%412:<i32>, %4111:<i32>, alpha=1:int)
          outputs: (%412:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3191:<i32>, %412:<i32>, False:bool)
          outputs: (%3191:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%2942:<i32>, 1:int, alpha=1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3738:<i32>, %2942:<i32>, False:bool)
          outputs: (%3738:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%4457:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%412:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%434:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%410:<1024xf32>{1}, %439:list{%434:<1024xCUSTOM_DATA_TYPE>{1}}, %438:<i32>, False:bool)
          outputs: (%410:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%424:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%428:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::index_put_:
          inputs: (%424:<1024xCUSTOM_DATA_TYPE>{1}, %439:list{%428:<1024xCUSTOM_DATA_TYPE>{1}}, %434:<i32>, False:bool)
          outputs: (%424:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::eq:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%434:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%4461:list{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4442:tuple{%4462:list{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::index_put_:
          inputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}+1, %439:list{%434:<1024xCUSTOM_DATA_TYPE>{1}}, %428:<i32>, False:bool)
          outputs: (%431:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%1238:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%1238:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4463:list{%1238:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4464:tuple{%4457:list{%1238:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3433:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3433:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4461:list{%3433:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4465:tuple{%4447:list{%3433:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4458:list{%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4442:tuple{%3907:list{%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4461:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4464:tuple{%4462:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2684:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2684:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_1228,__3822,_20823,_____,____89,___377,___282]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[_1228,__3822,_20823,_____,____89,___377,___282]],_device)
          duration: -1
357591 2024-12-10 17:48:50.323006 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n10,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%4466:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%4466:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
357594 2024-12-10 17:48:50.323760 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n10,rank6)
        - ----------->api::embedding call:
          inputs: (%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%4125:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%2942:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%2942:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
357718 2024-12-10 17:48:50.338710 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n10,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%4466:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%4459:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%4466:tuple{%293:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%4466:tuple{%293:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
357728 2024-12-10 17:48:50.340777 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n10,rank6)
        - ----------->api::dropout call:
          inputs: (%293:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%293:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%293:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%293:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
357795 2024-12-10 17:48:50.347063 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n10,rank6)
        - ----------->api::Dropout return:
          inputs: (%4466:tuple{%293:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%293:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
357807 2024-12-10 17:48:50.347780 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n10,rank6)
        - ----------->api::LanguageModelEmbedding return:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_1228,__3822,_20823,_____,____89,___377,___282]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (%293:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::RotaryEmbedding call:
          inputs: (%4466:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%4466:tuple{1024:int}))
          duration: -1
357829 2024-12-10 17:48:50.349414 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n10,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4459:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%4459:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%4359:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3576:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%4460:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%4471:list{%4460:<1024x20xf32>{20, 1}, %4460:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%3191:<1024x40xf32>{40,1})
          duration: -1
357970 2024-12-10 17:48:50.360470 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n10,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%4466:tuple{1024:int})
          outputs: (%3468:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
358026 2024-12-10 17:48:50.366902 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n10,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
358100 2024-12-10 17:48:50.375933 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n10,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4466:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4466:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
358105 2024-12-10 17:48:50.376677 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n30,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%1562:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4431:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4431:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4431:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4431:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4453:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %4243:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4453:tuple{%1562:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%4243:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%442:list{%424:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%441:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%441:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4431:tuple{%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%1570:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%1570:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
358346 2024-12-10 17:48:50.420980 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n30,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%4466:tuple{%1570:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%4466:tuple{%1570:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
358401 2024-12-10 17:48:50.425275 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n10,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4474:tuple{%1570:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4474:tuple{%1570:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
358417 2024-12-10 17:48:50.426045 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n260,rank6)
        - aten::mm:
          inputs: (%4298:<1024x5120xbf16>{5120, 1}, %4477:<5120x1536xbf16>{1, 5120})
          outputs: (%4075:<1024x1536xbf16>{1536,1})
          duration: -1
358577 2024-12-10 17:48:50.434051 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n260,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4474:tuple{%1570:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4438:tuple{%4297:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4474:tuple{%4297:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4474:tuple{%4297:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
358604 2024-12-10 17:48:50.435089 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n261,rank6)
        - aten::mm:
          inputs: (%4110:<1024x1536xbf16>{1536, 1}, %4298:<1536x24576xbf16>{1, 1536})
          outputs: (%4148:<1024x24576xbf16>{24576,1})
          duration: -1
358698 2024-12-10 17:48:50.441237 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n261,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4474:tuple{%4297:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%3547:tuple{%4075:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3576:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3988:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3576:<1024x1x128x192xbf16>{24576,24576,192,1},%3988:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3576:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %3988:list{128:int, 64:int}, -1:int)
          outputs: (%4446:tuple{%3439:<1024x1x128x128xbf16>{24576,24576,192,1},%4150:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4474:tuple{%1570:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4474:tuple{%1570:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
358800 2024-12-10 17:48:50.450798 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n262,rank6)
        - aten::mm:
          inputs: (%4084:<1024x5120xbf16>{5120, 1}, %4169:<5120x576xbf16>{1, 5120})
          outputs: (%4072:<1024x576xbf16>{576,1})
          duration: -1
358929 2024-12-10 17:48:50.458632 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n262,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4474:tuple{%1570:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4446:tuple{%3874:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%3874:<1024x1x576xbf16>{576, 576, 1}, %4478:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%3874:<1024x1x576xbf16>{576,576,1},%4478:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%3874:<1024x1x576xbf16>{576, 576, 1}, %4478:list{512:int, 64:int}, -1:int)
          outputs: (%3547:tuple{%4169:<1024x1x512xbf16>{576,576,1},%4368:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4474:tuple{%4169:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4474:tuple{%4169:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
359029 2024-12-10 17:48:50.466958 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n263,rank6)
        - aten::mm:
          inputs: (%4084:<1024x512xbf16>{576, 1}, %4240:<512x32768xbf16>{1, 512})
          outputs: (%4298:<1024x32768xbf16>{32768,1})
          duration: -1
359122 2024-12-10 17:48:50.473157 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n263,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4474:tuple{%4169:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%4446:tuple{%4044:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%4084:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4488:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%4084:<1024x1x128x256xbf16>{32768,32768,256,1},%4488:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%4084:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4488:list{128:int, 128:int}, -1:int)
          outputs: (%4438:tuple{%4072:<1024x1x128x128xbf16>{32768,32768,256,1},%4298:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%4467:tuple{%4243:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%4467:tuple{%4243:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
359231 2024-12-10 17:48:50.484917 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n10,rank6)
359270 2024-12-10 17:48:50.488022 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n10,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%4467:tuple{%4243:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%4464:tuple{%4298:<1024x64xbf16>{64,1},%3993:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%4298:<1024x64xbf16>{64, 1}, %4495:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%4240:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%3993:<1024x64xbf16>{64, 1}, %4491:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%4110:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4500:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4496:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4503:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%3835:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%3813:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4495:list{%3813:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %4499:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%3790:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%3790:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4492:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3813:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%4503:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %3813:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%4340:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4504:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4496:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%3835:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%4506:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%4507:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4495:list{%4507:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %4505:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%4286:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4286:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4492:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4507:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%3835:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4507:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%4506:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%431:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%428:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%428:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%3835:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4492:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%3835:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%4368:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %4340:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%4368:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%4368:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4510:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%4368:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%3945:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %4507:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%3945:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%4169:<128x1024x192xbf16>{196608, 192, 1}, %4075:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%3714:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%4298:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%4169:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%4084:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%4298:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%4084:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %4495:list{%4298:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %3576:<i32>, False:bool)
          outputs: (%4084:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%4169:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %4084:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%3945:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%3945:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%3945:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%3874:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%3576:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%3945:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %3576:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%3576:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%4169:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%4169:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%4169:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%4169:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%3576:<128x1024x1024xbf16>{1048576, 1024, 1}, %3902:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%4072:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4474:tuple{%3714:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4474:tuple{%3714:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
360765 2024-12-10 17:48:50.606617 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n220,rank6)
        - aten::mm:
          inputs: (%3790:<1024x16384xbf16>{16384, 1}, %4110:<16384x5120xbf16>{1, 16384})
          outputs: (%4298:<1024x5120xbf16>{5120,1})
          duration: -1
360901 2024-12-10 17:48:50.614928 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n220,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4474:tuple{%3714:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%4394:tuple{%3790:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
360906 2024-12-10 17:48:50.615718 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n10,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%4466:tuple{%1570:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4394:tuple{%3790:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%3790:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%3790:<1024x1x5120xbf16>{5120,5120,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%3790:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%3790:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b822b870_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b822b870_:_InferenceMode)
          duration: -1
        - aten::stack:
          inputs: (%434:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%439:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%439:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%438:list{%410:<1024xf32>{1}}, 0:int, out=%441:<1x1024xf32>{1024, 1})
          outputs: (%441:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%440:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%443:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%443:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8442e30_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8442e30_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b83cfbb0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b83cfbb0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b81f2db0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b81f2db0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8215330_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8215330_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b827eab0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b827eab0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b866f8f0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b866f8f0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8473430_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8473430_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b82b88f0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b82b88f0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b8217c70_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b8217c70_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b825b370_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b825b370_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b826e470_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b826e470_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b831e2f0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b831e2f0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b82518b0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b82518b0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b80f7970_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b80f7970_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b837f1b0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b837f1b0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b83df0f0_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b83df0f0_:_InferenceMode)
          duration: -1
        - ----------->api::_enter_inference_mode call:
          inputs: (False:bool)
          outputs: (torch.2_3_0._enter_inference_mode(False:bool))
          duration: -1
        - ----------->api::_enter_inference_mode return:
          inputs: (False:bool, _torch__C__InferenceMode_object_at_0_7f55b831f570_:_InferenceMode)
          outputs: (_torch__C__InferenceMode_object_at_0_7f55b831f570_:_InferenceMode)
          duration: -1
        - aten::add:
          inputs: (%1562:<1024x1x5120xbf16>{5120, 5242880, 1}, %3790:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4513:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4514:tuple{%4513:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4514:tuple{%4513:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
364394 2024-12-10 17:48:53.695832 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n31,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4513:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4513:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4513:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4515:tuple{%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4515:tuple{%4513:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4515:tuple{%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4515:tuple{%4513:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4518:tuple{%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %4517:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4518:tuple{%4513:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%4517:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4515:tuple{%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4459:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4513:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%4459:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
364788 2024-12-10 17:48:53.740570 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n31,rank6)
        - ----------->api::MoELayer call:
          inputs: (%4514:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%4514:tuple{%4459:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
364799 2024-12-10 17:48:53.741353 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n10,rank6)
        - ----------->api::TopKRouter call:
          inputs: (%4520:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.TopKRouter(%4520:tuple{%4459:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
364811 2024-12-10 17:48:53.742100 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n10,rank6)
        - aten::mm:
          inputs: (%4522:<1024x5120xbf16>{5120, 1}, %4133:<5120x160xbf16>{1, 5120})
          outputs: (%4244:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%4503:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%4526:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%4527:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%4528:tuple{%4529:<1024x6xbf16>{6,1},%4526:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%4530:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%4531:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%4532:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%4503:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%4503:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %4533:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%4503:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%4503:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %4532:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%4534:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%4531:<1024x160xf32>{160, 1}, %4532:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%4535:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%4535:<160xf32>{1}, %4534:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%4536:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%4536:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%4537:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%4537:<i32>, 2_5431315104166666e-07:float)
          outputs: (%4538:<i32>)
          duration: -1
        - aten::div:
          inputs: (%4538:<i32>, 0_01:float)
          outputs: (%4539:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%4536:<i32>, %4541:<i32>, alpha=1:int)
          outputs: (%4536:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%4540:<i32>, %4536:<i32>, False:bool)
          outputs: (%4540:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%4543:tuple{%4538:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%4543:tuple{%4538:<i32>}))
          duration: -1
365124 2024-12-10 17:48:53.777504 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n10,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%4520:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4546:tuple{%4545:<1024x6xbf16>{6,1},%4526:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4531:<8192x5120xbf16>{5120, 1}, %4548:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4531:<8192x5120xbf16>{5120,1},%4548:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4531:<8192x5120xbf16>{5120, 1}, %4548:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4549:tuple{%4531:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4534:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4526:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4534:<8192x6xCUSTOM_DATA_TYPE>{6,1},%4526:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4534:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4526:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4528:tuple{%4534:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%4534:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%4522:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%4534:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%4492:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%4522:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4492:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4351:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%4534:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4351:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4541:<5118xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4554:<8192x6xbf16>{6, 1}, %4545:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4554:<8192x6xbf16>{6,1},%4545:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4554:<8192x6xbf16>{6, 1}, %4545:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4555:tuple{%4554:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%4554:<8192x6xbf16>{6, 1}, %4351:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4556:<5118xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%4351:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4538:<5118x2xCUSTOM_DATA_TYPE>{1,5118})
          duration: -1
        - aten::gather:
          inputs: (%4531:<8192x5120xbf16>{5120, 1}, 0:int, %4561:<5118x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4557:<5118x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%4541:<5118xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%4562:tuple{%4560:<5118xCUSTOM_DATA_TYPE>{1},%4563:<5118xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%4564:<5118xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%4565:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%4557:<5118x5120xbf16>{5120, 1}, 0:int, %4566:<5118x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4569:<5118x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%4562:tuple{%4569:<5118x5120xbf16>{5120, 1}, %4567:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%4562:tuple{%4569:<5118x5120xbf16>{5120,1},%4567:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
365857 2024-12-10 17:48:53.886169 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n10,rank6)
        - aten::cumsum:
          inputs: (%4567:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%4558:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%4559:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%4541:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%4559:list{%4541:<1xCUSTOM_DATA_TYPE>{1}, %4558:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%4554:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4572:<221x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4572:<221x5120xbf16>{5120,1}}))
          duration: -1
366031 2024-12-10 17:48:53.897107 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n210,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4572:<221x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4572:<221x5120xbf16>{5120,1}}))
          duration: -1
366055 2024-12-10 17:48:53.897896 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n264,rank6)
        - aten::mm:
          inputs: (%4572:<221x5120xbf16>{5120, 1}, %4575:<5120x3072xbf16>{1, 5120})
          outputs: (%4576:<221x3072xbf16>{3072,1})
          duration: -1
366137 2024-12-10 17:48:53.901827 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n264,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4572:<221x5120xbf16>{5120, 1}})
          outputs: (%4546:tuple{%4576:<221x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4579:<221x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4579:<221x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4579:<221x1536xbf16>{3072, 1})
          outputs: (%4580:<221x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4579:<221x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4580:<221x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4580:<221x1536xbf16>{1536, 1}, %4575:<221x1536xbf16>{3072, 1}+1536)
          outputs: (%4581:<221x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4581:<221x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4581:<221x1536xbf16>{1536,1}}))
          duration: -1
366240 2024-12-10 17:48:53.911878 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n221,rank6)
        - aten::mm:
          inputs: (%4581:<221x1536xbf16>{1536, 1}, %4582:<1536x5120xbf16>{1, 1536})
          outputs: (%4583:<221x5120xbf16>{5120,1})
          duration: -1
366323 2024-12-10 17:48:53.915961 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n221,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4581:<221x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4583:<221x5120xbf16>{5120,1},None:NoneType})
          duration: -1
366341 2024-12-10 17:48:53.916743 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n210,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4572:<221x5120xbf16>{5120, 1}})
          outputs: (%4552:tuple{%4583:<221x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4576:<221x5120xbf16>{5120, 1}, %4583:<221x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4576:<221x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4586:<607x5120xbf16>{5120, 1}+1131520})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4586:<607x5120xbf16>{5120,1}+1131520}))
          duration: -1
366518 2024-12-10 17:48:53.927110 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n211,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4586:<607x5120xbf16>{5120, 1}+1131520})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4586:<607x5120xbf16>{5120,1}+1131520}))
          duration: -1
366542 2024-12-10 17:48:53.927850 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n265,rank6)
        - aten::mm:
          inputs: (%4586:<607x5120xbf16>{5120, 1}+1131520, %4587:<5120x3072xbf16>{1, 5120})
          outputs: (%4588:<607x3072xbf16>{3072,1})
          duration: -1
366624 2024-12-10 17:48:53.931704 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n265,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4586:<607x5120xbf16>{5120, 1}+1131520})
          outputs: (%4542:tuple{%4588:<607x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4522:<607x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4522:<607x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4522:<607x1536xbf16>{3072, 1})
          outputs: (%4587:<607x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4522:<607x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4587:<607x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4587:<607x1536xbf16>{1536, 1}, %4572:<607x1536xbf16>{3072, 1}+1536)
          outputs: (%4590:<607x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4590:<607x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4590:<607x1536xbf16>{1536,1}}))
          duration: -1
366726 2024-12-10 17:48:53.941755 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n222,rank6)
        - aten::mm:
          inputs: (%4590:<607x1536xbf16>{1536, 1}, %4591:<1536x5120xbf16>{1, 1536})
          outputs: (%4592:<607x5120xbf16>{5120,1})
          duration: -1
366807 2024-12-10 17:48:53.945609 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n222,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4590:<607x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4592:<607x5120xbf16>{5120,1},None:NoneType})
          duration: -1
366822 2024-12-10 17:48:53.946381 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n211,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4586:<607x5120xbf16>{5120, 1}+1131520})
          outputs: (%4552:tuple{%4592:<607x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4594:<607x5120xbf16>{5120, 1}+1131520, %4592:<607x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4594:<607x5120xbf16>{5120,1}+1131520)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4570:<209x5120xbf16>{5120, 1}+4239360})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4570:<209x5120xbf16>{5120,1}+4239360}))
          duration: -1
366990 2024-12-10 17:48:53.956662 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n212,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4570:<209x5120xbf16>{5120, 1}+4239360})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4570:<209x5120xbf16>{5120,1}+4239360}))
          duration: -1
367014 2024-12-10 17:48:53.957421 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n266,rank6)
        - aten::mm:
          inputs: (%4570:<209x5120xbf16>{5120, 1}+4239360, %4595:<5120x3072xbf16>{1, 5120})
          outputs: (%4596:<209x3072xbf16>{3072,1})
          duration: -1
367099 2024-12-10 17:48:53.961289 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n266,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4570:<209x5120xbf16>{5120, 1}+4239360})
          outputs: (%4574:tuple{%4596:<209x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4133:<209x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4133:<209x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4133:<209x1536xbf16>{3072, 1})
          outputs: (%4583:<209x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4133:<209x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4583:<209x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4583:<209x1536xbf16>{1536, 1}, %4598:<209x1536xbf16>{3072, 1}+1536)
          outputs: (%4595:<209x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4595:<209x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4595:<209x1536xbf16>{1536,1}}))
          duration: -1
367202 2024-12-10 17:48:53.971372 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n223,rank6)
        - aten::stack:
          inputs: (%433:list{%410:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%438:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%438:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::mm:
          inputs: (%4595:<209x1536xbf16>{1536, 1}, %4599:<1536x5120xbf16>{1, 1536})
          outputs: (%4587:<209x5120xbf16>{5120,1})
          duration: -1
367297 2024-12-10 17:48:53.975270 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n223,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4595:<209x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4587:<209x5120xbf16>{5120,1},None:NoneType})
          duration: -1
367310 2024-12-10 17:48:53.976058 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n212,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4570:<209x5120xbf16>{5120, 1}+4239360})
          outputs: (%4552:tuple{%4587:<209x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4579:<209x5120xbf16>{5120, 1}+4239360, %4587:<209x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4579:<209x5120xbf16>{5120,1}+4239360)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4572:<332x5120xbf16>{5120, 1}+5309440})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4572:<332x5120xbf16>{5120,1}+5309440}))
          duration: -1
367492 2024-12-10 17:48:53.986419 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n213,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4572:<332x5120xbf16>{5120, 1}+5309440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4572:<332x5120xbf16>{5120,1}+5309440}))
          duration: -1
367512 2024-12-10 17:48:53.987166 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n267,rank6)
        - aten::mm:
          inputs: (%4572:<332x5120xbf16>{5120, 1}+5309440, %4601:<5120x3072xbf16>{1, 5120})
          outputs: (%4602:<332x3072xbf16>{3072,1})
          duration: -1
367600 2024-12-10 17:48:53.991049 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n267,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4572:<332x5120xbf16>{5120, 1}+5309440})
          outputs: (%4546:tuple{%4602:<332x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4522:<332x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4522:<332x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4522:<332x1536xbf16>{3072, 1})
          outputs: (%4601:<332x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4522:<332x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4601:<332x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4601:<332x1536xbf16>{1536, 1}, %4570:<332x1536xbf16>{3072, 1}+1536)
          outputs: (%4586:<332x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4586:<332x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4586:<332x1536xbf16>{1536,1}}))
          duration: -1
367700 2024-12-10 17:48:54.001109 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n224,rank6)
        - aten::mm:
          inputs: (%4586:<332x1536xbf16>{1536, 1}, %4596:<1536x5120xbf16>{1, 1536})
          outputs: (%4582:<332x5120xbf16>{5120,1})
          duration: -1
367783 2024-12-10 17:48:54.004932 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n224,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4586:<332x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4582:<332x5120xbf16>{5120,1},None:NoneType})
          duration: -1
367800 2024-12-10 17:48:54.005758 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n213,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4572:<332x5120xbf16>{5120, 1}+5309440})
          outputs: (%4552:tuple{%4582:<332x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4534:<332x5120xbf16>{5120, 1}+5309440, %4582:<332x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4534:<332x5120xbf16>{5120,1}+5309440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4606:<192x5120xbf16>{5120, 1}+7009280})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4606:<192x5120xbf16>{5120,1}+7009280}))
          duration: -1
367971 2024-12-10 17:48:54.016118 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n214,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4606:<192x5120xbf16>{5120, 1}+7009280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4606:<192x5120xbf16>{5120,1}+7009280}))
          duration: -1
367994 2024-12-10 17:48:54.016867 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n268,rank6)
        - aten::mm:
          inputs: (%4606:<192x5120xbf16>{5120, 1}+7009280, %4601:<5120x3072xbf16>{1, 5120})
          outputs: (%4586:<192x3072xbf16>{3072,1})
          duration: -1
368086 2024-12-10 17:48:54.020711 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n268,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4606:<192x5120xbf16>{5120, 1}+7009280})
          outputs: (%4542:tuple{%4586:<192x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4522:<192x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4522:<192x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4522:<192x1536xbf16>{3072, 1})
          outputs: (%4601:<192x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4522:<192x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4601:<192x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4601:<192x1536xbf16>{1536, 1}, %4572:<192x1536xbf16>{3072, 1}+1536)
          outputs: (%4608:<192x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4608:<192x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4608:<192x1536xbf16>{1536,1}}))
          duration: -1
368186 2024-12-10 17:48:54.030682 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n225,rank6)
        - aten::mm:
          inputs: (%4608:<192x1536xbf16>{1536, 1}, %4609:<1536x5120xbf16>{1, 1536})
          outputs: (%4339:<192x5120xbf16>{5120,1})
          duration: -1
368269 2024-12-10 17:48:54.034533 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n225,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4608:<192x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4339:<192x5120xbf16>{5120,1},None:NoneType})
          duration: -1
368282 2024-12-10 17:48:54.035296 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n214,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4606:<192x5120xbf16>{5120, 1}+7009280})
          outputs: (%4552:tuple{%4339:<192x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4594:<192x5120xbf16>{5120, 1}+7009280, %4339:<192x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4594:<192x5120xbf16>{5120,1}+7009280)
          duration: -1
        - aten::stack:
          inputs: (%437:list{%408:<1024xf32>{1}}, 0:int, out=%440:<1x1024xf32>{1024, 1})
          outputs: (%440:<1x1024xf32>{1024,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4570:<395x5120xbf16>{5120, 1}+7992320})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4570:<395x5120xbf16>{5120,1}+7992320}))
          duration: -1
368471 2024-12-10 17:48:54.045675 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n215,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4570:<395x5120xbf16>{5120, 1}+7992320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4570:<395x5120xbf16>{5120,1}+7992320}))
          duration: -1
368492 2024-12-10 17:48:54.046410 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n269,rank6)
        - aten::mm:
          inputs: (%4570:<395x5120xbf16>{5120, 1}+7992320, %4601:<5120x3072xbf16>{1, 5120})
          outputs: (%3835:<395x3072xbf16>{3072,1})
          duration: -1
368580 2024-12-10 17:48:54.050252 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n269,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4570:<395x5120xbf16>{5120, 1}+7992320})
          outputs: (%4574:tuple{%3835:<395x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4133:<395x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4133:<395x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4133:<395x1536xbf16>{3072, 1})
          outputs: (%4596:<395x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4133:<395x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4596:<395x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4596:<395x1536xbf16>{1536, 1}, %4595:<395x1536xbf16>{3072, 1}+1536)
          outputs: (%4601:<395x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4601:<395x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4601:<395x1536xbf16>{1536,1}}))
          duration: -1
368681 2024-12-10 17:48:54.060255 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n226,rank6)
        - aten::mm:
          inputs: (%4601:<395x1536xbf16>{1536, 1}, %4592:<1536x5120xbf16>{1, 1536})
          outputs: (%4585:<395x5120xbf16>{5120,1})
          duration: -1
368764 2024-12-10 17:48:54.064067 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n226,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4601:<395x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4585:<395x5120xbf16>{5120,1},None:NoneType})
          duration: -1
368779 2024-12-10 17:48:54.064835 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n215,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4570:<395x5120xbf16>{5120, 1}+7992320})
          outputs: (%4552:tuple{%4585:<395x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4522:<395x5120xbf16>{5120, 1}+7992320, %4585:<395x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4522:<395x5120xbf16>{5120,1}+7992320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4594:<85x5120xbf16>{5120, 1}+10014720})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4594:<85x5120xbf16>{5120,1}+10014720}))
          duration: -1
368951 2024-12-10 17:48:54.075342 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n216,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4594:<85x5120xbf16>{5120, 1}+10014720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4594:<85x5120xbf16>{5120,1}+10014720}))
          duration: -1
368972 2024-12-10 17:48:54.076082 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n270,rank6)
        - aten::mm:
          inputs: (%4594:<85x5120xbf16>{5120, 1}+10014720, %4596:<5120x3072xbf16>{1, 5120})
          outputs: (%4606:<85x3072xbf16>{3072,1})
          duration: -1
369052 2024-12-10 17:48:54.079904 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n270,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4594:<85x5120xbf16>{5120, 1}+10014720})
          outputs: (%4546:tuple{%4606:<85x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4570:<85x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4570:<85x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4570:<85x1536xbf16>{3072, 1})
          outputs: (%4592:<85x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4570:<85x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4592:<85x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4592:<85x1536xbf16>{1536, 1}, %4522:<85x1536xbf16>{3072, 1}+1536)
          outputs: (%4339:<85x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4339:<85x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4339:<85x1536xbf16>{1536,1}}))
          duration: -1
369163 2024-12-10 17:48:54.089922 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n227,rank6)
        - aten::mm:
          inputs: (%4339:<85x1536xbf16>{1536, 1}, %4601:<1536x5120xbf16>{1, 1536})
          outputs: (%4583:<85x5120xbf16>{5120,1})
          duration: -1
369241 2024-12-10 17:48:54.093759 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n227,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4339:<85x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4583:<85x5120xbf16>{5120,1},None:NoneType})
          duration: -1
369253 2024-12-10 17:48:54.094524 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n216,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4594:<85x5120xbf16>{5120, 1}+10014720})
          outputs: (%4552:tuple{%4583:<85x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4570:<85x5120xbf16>{5120, 1}+10014720, %4583:<85x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4570:<85x5120xbf16>{5120,1}+10014720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4585:<154x5120xbf16>{5120, 1}+10449920})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4585:<154x5120xbf16>{5120,1}+10449920}))
          duration: -1
369429 2024-12-10 17:48:54.104822 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n217,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4585:<154x5120xbf16>{5120, 1}+10449920})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4585:<154x5120xbf16>{5120,1}+10449920}))
          duration: -1
369448 2024-12-10 17:48:54.105580 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n271,rank6)
        - aten::mm:
          inputs: (%4585:<154x5120xbf16>{5120, 1}+10449920, %4596:<5120x3072xbf16>{1, 5120})
          outputs: (%4617:<154x3072xbf16>{3072,1})
          duration: -1
369533 2024-12-10 17:48:54.109429 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n271,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4585:<154x5120xbf16>{5120, 1}+10449920})
          outputs: (%4542:tuple{%4617:<154x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4517:<154x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4517:<154x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4517:<154x1536xbf16>{3072, 1})
          outputs: (%4522:<154x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4517:<154x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4522:<154x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4522:<154x1536xbf16>{1536, 1}, %4133:<154x1536xbf16>{3072, 1}+1536)
          outputs: (%4596:<154x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4596:<154x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4596:<154x1536xbf16>{1536,1}}))
          duration: -1
369647 2024-12-10 17:48:54.119443 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n228,rank6)
        - aten::mm:
          inputs: (%4596:<154x1536xbf16>{1536, 1}, %4594:<1536x5120xbf16>{1, 1536})
          outputs: (%4617:<154x5120xbf16>{5120,1})
          duration: -1
369724 2024-12-10 17:48:54.123263 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n228,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4596:<154x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4617:<154x5120xbf16>{5120,1},None:NoneType})
          duration: -1
369737 2024-12-10 17:48:54.124024 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n217,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4585:<154x5120xbf16>{5120, 1}+10449920})
          outputs: (%4552:tuple{%4617:<154x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4596:<154x5120xbf16>{5120, 1}+10449920, %4617:<154x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4596:<154x5120xbf16>{5120,1}+10449920)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4570:<141x5120xbf16>{5120, 1}+11238400})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4570:<141x5120xbf16>{5120,1}+11238400}))
          duration: -1
369910 2024-12-10 17:48:54.134354 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n218,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4570:<141x5120xbf16>{5120, 1}+11238400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4570:<141x5120xbf16>{5120,1}+11238400}))
          duration: -1
369928 2024-12-10 17:48:54.135094 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n272,rank6)
        - aten::mm:
          inputs: (%4570:<141x5120xbf16>{5120, 1}+11238400, %4133:<5120x3072xbf16>{1, 5120})
          outputs: (%4517:<141x3072xbf16>{3072,1})
          duration: -1
370017 2024-12-10 17:48:54.138939 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n272,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4570:<141x5120xbf16>{5120, 1}+11238400})
          outputs: (%4574:tuple{%4517:<141x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4586:<141x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4586:<141x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4586:<141x1536xbf16>{3072, 1})
          outputs: (%4583:<141x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4586:<141x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4583:<141x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4583:<141x1536xbf16>{1536, 1}, %4596:<141x1536xbf16>{3072, 1}+1536)
          outputs: (%4592:<141x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4543:tuple{%4592:<141x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4543:tuple{%4592:<141x1536xbf16>{1536,1}}))
          duration: -1
370134 2024-12-10 17:48:54.148960 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n229,rank6)
        - aten::mm:
          inputs: (%4592:<141x1536xbf16>{1536, 1}, %4596:<1536x5120xbf16>{1, 1536})
          outputs: (%4339:<141x5120xbf16>{5120,1})
          duration: -1
370205 2024-12-10 17:48:54.152772 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n229,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4543:tuple{%4592:<141x1536xbf16>{1536, 1}})
          outputs: (%4552:tuple{%4339:<141x5120xbf16>{5120,1},None:NoneType})
          duration: -1
370217 2024-12-10 17:48:54.153554 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n218,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4570:<141x5120xbf16>{5120, 1}+11238400})
          outputs: (%4552:tuple{%4339:<141x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4592:<141x5120xbf16>{5120, 1}+11238400, %4339:<141x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4592:<141x5120xbf16>{5120,1}+11238400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4517:<207x5120xbf16>{5120, 1}+11960320})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4517:<207x5120xbf16>{5120,1}+11960320}))
          duration: -1
370384 2024-12-10 17:48:54.163740 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n219,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4573:tuple{%4517:<207x5120xbf16>{5120, 1}+11960320})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4573:tuple{%4517:<207x5120xbf16>{5120,1}+11960320}))
          duration: -1
370400 2024-12-10 17:48:54.164476 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n273,rank6)
        - aten::mm:
          inputs: (%4517:<207x5120xbf16>{5120, 1}+11960320, %4596:<5120x3072xbf16>{1, 5120})
          outputs: (%4583:<207x3072xbf16>{3072,1})
          duration: -1
370480 2024-12-10 17:48:54.168290 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n273,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4573:tuple{%4517:<207x5120xbf16>{5120, 1}+11960320})
          outputs: (%4546:tuple{%4583:<207x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - aten::stack:
          inputs: (%439:list{%409:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%442:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%442:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4133:<207x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4133:<207x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4133:<207x1536xbf16>{3072, 1})
          outputs: (%4625:<207x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4133:<207x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4625:<207x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4625:<207x1536xbf16>{1536, 1}, %4522:<207x1536xbf16>{3072, 1}+1536)
          outputs: (%4504:<207x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4504:<207x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4504:<207x1536xbf16>{1536,1}}))
          duration: -1
370628 2024-12-10 17:48:54.178945 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n230,rank6)
        - aten::mm:
          inputs: (%4504:<207x1536xbf16>{1536, 1}, %4625:<1536x5120xbf16>{1, 1536})
          outputs: (%4626:<207x5120xbf16>{5120,1})
          duration: -1
370704 2024-12-10 17:48:54.182848 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n230,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4504:<207x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4626:<207x5120xbf16>{5120,1},None:NoneType})
          duration: -1
370715 2024-12-10 17:48:54.183615 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n219,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4517:<207x5120xbf16>{5120, 1}+11960320})
          outputs: (%4550:tuple{%4626:<207x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4583:<207x5120xbf16>{5120, 1}+11960320, %4626:<207x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4583:<207x5120xbf16>{5120,1}+11960320)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4594:<162x5120xbf16>{5120, 1}+13020160})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4594:<162x5120xbf16>{5120,1}+13020160}))
          duration: -1
370887 2024-12-10 17:48:54.193882 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n220,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4594:<162x5120xbf16>{5120, 1}+13020160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4594:<162x5120xbf16>{5120,1}+13020160}))
          duration: -1
370905 2024-12-10 17:48:54.194617 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n274,rank6)
        - aten::mm:
          inputs: (%4594:<162x5120xbf16>{5120, 1}+13020160, %4629:<5120x3072xbf16>{1, 5120})
          outputs: (%4540:<162x3072xbf16>{3072,1})
          duration: -1
370985 2024-12-10 17:48:54.198439 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n274,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4594:<162x5120xbf16>{5120, 1}+13020160})
          outputs: (%4555:tuple{%4540:<162x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4534:<162x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4534:<162x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4534:<162x1536xbf16>{3072, 1})
          outputs: (%4133:<162x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4534:<162x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4133:<162x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4133:<162x1536xbf16>{1536, 1}, %4522:<162x1536xbf16>{3072, 1}+1536)
          outputs: (%4614:<162x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4614:<162x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4614:<162x1536xbf16>{1536,1}}))
          duration: -1
371110 2024-12-10 17:48:54.208419 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n231,rank6)
        - aten::mm:
          inputs: (%4614:<162x1536xbf16>{1536, 1}, %4632:<1536x5120xbf16>{1, 1536})
          outputs: (%4602:<162x5120xbf16>{5120,1})
          duration: -1
371181 2024-12-10 17:48:54.212232 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n231,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4614:<162x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4602:<162x5120xbf16>{5120,1},None:NoneType})
          duration: -1
371192 2024-12-10 17:48:54.212994 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n220,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4594:<162x5120xbf16>{5120, 1}+13020160})
          outputs: (%4550:tuple{%4602:<162x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4133:<162x5120xbf16>{5120, 1}+13020160, %4602:<162x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4133:<162x5120xbf16>{5120,1}+13020160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4586:<415x5120xbf16>{5120, 1}+13849600})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4586:<415x5120xbf16>{5120,1}+13849600}))
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4586:<415x5120xbf16>{5120, 1}+13849600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4586:<415x5120xbf16>{5120,1}+13849600}))
          duration: -1
371385 2024-12-10 17:48:54.224091 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n275,rank6)
        - aten::mm:
          inputs: (%4586:<415x5120xbf16>{5120, 1}+13849600, %4634:<5120x3072xbf16>{1, 5120})
          outputs: (%4540:<415x3072xbf16>{3072,1})
          duration: -1
371464 2024-12-10 17:48:54.227887 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n275,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4586:<415x5120xbf16>{5120, 1}+13849600})
          outputs: (%4574:tuple{%4540:<415x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4579:<415x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4579:<415x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4579:<415x1536xbf16>{3072, 1})
          outputs: (%4637:<415x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4579:<415x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4637:<415x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4637:<415x1536xbf16>{1536, 1}, %4572:<415x1536xbf16>{3072, 1}+1536)
          outputs: (%4625:<415x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4625:<415x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4625:<415x1536xbf16>{1536,1}}))
          duration: -1
371590 2024-12-10 17:48:54.237872 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n232,rank6)
        - aten::mm:
          inputs: (%4625:<415x1536xbf16>{1536, 1}, %4629:<1536x5120xbf16>{1, 1536})
          outputs: (%4594:<415x5120xbf16>{5120,1})
          duration: -1
371662 2024-12-10 17:48:54.241714 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n232,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4625:<415x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4594:<415x5120xbf16>{5120,1},None:NoneType})
          duration: -1
371673 2024-12-10 17:48:54.242485 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n221,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4586:<415x5120xbf16>{5120, 1}+13849600})
          outputs: (%4550:tuple{%4594:<415x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4583:<415x5120xbf16>{5120, 1}+13849600, %4594:<415x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4583:<415x5120xbf16>{5120,1}+13849600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4522:<138x5120xbf16>{5120, 1}+15974400})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4522:<138x5120xbf16>{5120,1}+15974400}))
          duration: -1
371857 2024-12-10 17:48:54.252775 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n222,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4522:<138x5120xbf16>{5120, 1}+15974400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4522:<138x5120xbf16>{5120,1}+15974400}))
          duration: -1
371872 2024-12-10 17:48:54.253520 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n276,rank6)
        - aten::mm:
          inputs: (%4522:<138x5120xbf16>{5120, 1}+15974400, %4639:<5120x3072xbf16>{1, 5120})
          outputs: (%4540:<138x3072xbf16>{3072,1})
          duration: -1
371951 2024-12-10 17:48:54.257344 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n276,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4522:<138x5120xbf16>{5120, 1}+15974400})
          outputs: (%4628:tuple{%4540:<138x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4133:<138x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4133:<138x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4133:<138x1536xbf16>{3072, 1})
          outputs: (%4534:<138x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4133:<138x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4534:<138x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4534:<138x1536xbf16>{1536, 1}, %4586:<138x1536xbf16>{3072, 1}+1536)
          outputs: (%4517:<138x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4517:<138x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4517:<138x1536xbf16>{1536,1}}))
          duration: -1
372078 2024-12-10 17:48:54.267306 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n233,rank6)
        - aten::mm:
          inputs: (%4517:<138x1536xbf16>{1536, 1}, %4637:<1536x5120xbf16>{1, 1536})
          outputs: (%4642:<138x5120xbf16>{5120,1})
          duration: -1
372154 2024-12-10 17:48:54.271141 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n233,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4517:<138x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4642:<138x5120xbf16>{5120,1},None:NoneType})
          duration: -1
372166 2024-12-10 17:48:54.271905 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n222,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4522:<138x5120xbf16>{5120, 1}+15974400})
          outputs: (%4550:tuple{%4642:<138x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4579:<138x5120xbf16>{5120, 1}+15974400, %4642:<138x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4579:<138x5120xbf16>{5120,1}+15974400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4579:<232x5120xbf16>{5120, 1}+16680960})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4579:<232x5120xbf16>{5120,1}+16680960}))
          duration: -1
372339 2024-12-10 17:48:54.282328 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n223,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4579:<232x5120xbf16>{5120, 1}+16680960})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4579:<232x5120xbf16>{5120,1}+16680960}))
          duration: -1
372354 2024-12-10 17:48:54.283055 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n277,rank6)
        - aten::mm:
          inputs: (%4579:<232x5120xbf16>{5120, 1}+16680960, %4634:<5120x3072xbf16>{1, 5120})
          outputs: (%4629:<232x3072xbf16>{3072,1})
          duration: -1
372433 2024-12-10 17:48:54.286865 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n277,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4579:<232x5120xbf16>{5120, 1}+16680960})
          outputs: (%4555:tuple{%4629:<232x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4594:<232x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4594:<232x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4594:<232x1536xbf16>{3072, 1})
          outputs: (%4534:<232x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4594:<232x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4534:<232x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4534:<232x1536xbf16>{1536, 1}, %4592:<232x1536xbf16>{3072, 1}+1536)
          outputs: (%3835:<232x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%3835:<232x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%3835:<232x1536xbf16>{1536,1}}))
          duration: -1
372560 2024-12-10 17:48:54.296833 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n234,rank6)
        - aten::mm:
          inputs: (%3835:<232x1536xbf16>{1536, 1}, %4634:<1536x5120xbf16>{1, 1536})
          outputs: (%4632:<232x5120xbf16>{5120,1})
          duration: -1
372625 2024-12-10 17:48:54.300653 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n234,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%3835:<232x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4632:<232x5120xbf16>{5120,1},None:NoneType})
          duration: -1
372637 2024-12-10 17:48:54.301439 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n223,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4579:<232x5120xbf16>{5120, 1}+16680960})
          outputs: (%4550:tuple{%4632:<232x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4572:<232x5120xbf16>{5120, 1}+16680960, %4632:<232x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4572:<232x5120xbf16>{5120,1}+16680960)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4540:<409x5120xbf16>{5120, 1}+17868800})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4540:<409x5120xbf16>{5120,1}+17868800}))
          duration: -1
372811 2024-12-10 17:48:54.311743 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n224,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4540:<409x5120xbf16>{5120, 1}+17868800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4540:<409x5120xbf16>{5120,1}+17868800}))
          duration: -1
372828 2024-12-10 17:48:54.312489 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n278,rank6)
        - aten::mm:
          inputs: (%4540:<409x5120xbf16>{5120, 1}+17868800, %4583:<5120x3072xbf16>{1, 5120})
          outputs: (%4602:<409x3072xbf16>{3072,1})
          duration: -1
372906 2024-12-10 17:48:54.316316 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n278,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4540:<409x5120xbf16>{5120, 1}+17868800})
          outputs: (%4574:tuple{%4602:<409x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4583:<409x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4583:<409x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4583:<409x1536xbf16>{3072, 1})
          outputs: (%4614:<409x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4583:<409x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4614:<409x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4614:<409x1536xbf16>{1536, 1}, %4339:<409x1536xbf16>{3072, 1}+1536)
          outputs: (%4649:<409x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4649:<409x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4649:<409x1536xbf16>{1536,1}}))
          duration: -1
373039 2024-12-10 17:48:54.326300 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n235,rank6)
        - aten::mm:
          inputs: (%4649:<409x1536xbf16>{1536, 1}, %4650:<1536x5120xbf16>{1, 1536})
          outputs: (%3835:<409x5120xbf16>{5120,1})
          duration: -1
373103 2024-12-10 17:48:54.330152 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n235,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4649:<409x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%3835:<409x5120xbf16>{5120,1},None:NoneType})
          duration: -1
373117 2024-12-10 17:48:54.330913 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n224,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4540:<409x5120xbf16>{5120, 1}+17868800})
          outputs: (%4550:tuple{%3835:<409x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4517:<409x5120xbf16>{5120, 1}+17868800, %3835:<409x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4517:<409x5120xbf16>{5120,1}+17868800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4586:<214x5120xbf16>{5120, 1}+19962880})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4586:<214x5120xbf16>{5120,1}+19962880}))
          duration: -1
373292 2024-12-10 17:48:54.341216 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n225,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4586:<214x5120xbf16>{5120, 1}+19962880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4586:<214x5120xbf16>{5120,1}+19962880}))
          duration: -1
373308 2024-12-10 17:48:54.341962 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n279,rank6)
        - aten::mm:
          inputs: (%4586:<214x5120xbf16>{5120, 1}+19962880, %4602:<5120x3072xbf16>{1, 5120})
          outputs: (%4534:<214x3072xbf16>{3072,1})
          duration: -1
373384 2024-12-10 17:48:54.345776 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n279,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4586:<214x5120xbf16>{5120, 1}+19962880})
          outputs: (%4628:tuple{%4534:<214x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4602:<214x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4602:<214x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4602:<214x1536xbf16>{3072, 1})
          outputs: (%4614:<214x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4602:<214x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4614:<214x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4614:<214x1536xbf16>{1536, 1}, %4634:<214x1536xbf16>{3072, 1}+1536)
          outputs: (%4572:<214x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4572:<214x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4572:<214x1536xbf16>{1536,1}}))
          duration: -1
373519 2024-12-10 17:48:54.355763 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n236,rank6)
        - aten::mm:
          inputs: (%4572:<214x1536xbf16>{1536, 1}, %4655:<1536x5120xbf16>{1, 1536})
          outputs: (%4517:<214x5120xbf16>{5120,1})
          duration: -1
373579 2024-12-10 17:48:54.359582 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n236,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4572:<214x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4517:<214x5120xbf16>{5120,1},None:NoneType})
          duration: -1
373597 2024-12-10 17:48:54.360342 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n225,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4586:<214x5120xbf16>{5120, 1}+19962880})
          outputs: (%4550:tuple{%4517:<214x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4492:<214x5120xbf16>{5120, 1}+19962880, %4517:<214x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4492:<214x5120xbf16>{5120,1}+19962880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4492:<284x5120xbf16>{5120, 1}+21058560})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4492:<284x5120xbf16>{5120,1}+21058560}))
          duration: -1
373770 2024-12-10 17:48:54.370583 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n226,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4492:<284x5120xbf16>{5120, 1}+21058560})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4492:<284x5120xbf16>{5120,1}+21058560}))
          duration: -1
373787 2024-12-10 17:48:54.371310 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n280,rank6)
        - aten::mm:
          inputs: (%4492:<284x5120xbf16>{5120, 1}+21058560, %4540:<5120x3072xbf16>{1, 5120})
          outputs: (%4594:<284x3072xbf16>{3072,1})
          duration: -1
373862 2024-12-10 17:48:54.375130 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n280,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4492:<284x5120xbf16>{5120, 1}+21058560})
          outputs: (%4555:tuple{%4594:<284x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4534:<284x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4534:<284x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4534:<284x1536xbf16>{3072, 1})
          outputs: (%4632:<284x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4534:<284x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4632:<284x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4632:<284x1536xbf16>{1536, 1}, %4339:<284x1536xbf16>{3072, 1}+1536)
          outputs: (%4642:<284x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4642:<284x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4642:<284x1536xbf16>{1536,1}}))
          duration: -1
374004 2024-12-10 17:48:54.385107 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n237,rank6)
        - aten::mm:
          inputs: (%4642:<284x1536xbf16>{1536, 1}, %4659:<1536x5120xbf16>{1, 1536})
          outputs: (%4522:<284x5120xbf16>{5120,1})
          duration: -1
374062 2024-12-10 17:48:54.388925 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n237,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4642:<284x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4522:<284x5120xbf16>{5120,1},None:NoneType})
          duration: -1
374078 2024-12-10 17:48:54.389712 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n226,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4492:<284x5120xbf16>{5120, 1}+21058560})
          outputs: (%4550:tuple{%4522:<284x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4632:<284x5120xbf16>{5120, 1}+21058560, %4522:<284x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4632:<284x5120xbf16>{5120,1}+21058560)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4632:<72x5120xbf16>{5120, 1}+22512640})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4632:<72x5120xbf16>{5120,1}+22512640}))
          duration: -1
374262 2024-12-10 17:48:54.400034 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n227,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4632:<72x5120xbf16>{5120, 1}+22512640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4632:<72x5120xbf16>{5120,1}+22512640}))
          duration: -1
374274 2024-12-10 17:48:54.400770 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n281,rank6)
        - aten::mm:
          inputs: (%4632:<72x5120xbf16>{5120, 1}+22512640, %4649:<5120x3072xbf16>{1, 5120})
          outputs: (%4642:<72x3072xbf16>{3072,1})
          duration: -1
374353 2024-12-10 17:48:54.404627 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n281,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4632:<72x5120xbf16>{5120, 1}+22512640})
          outputs: (%4574:tuple{%4642:<72x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4583:<72x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4583:<72x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4583:<72x1536xbf16>{3072, 1})
          outputs: (%4594:<72x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4583:<72x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4594:<72x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4594:<72x1536xbf16>{1536, 1}, %4492:<72x1536xbf16>{3072, 1}+1536)
          outputs: (%4133:<72x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4133:<72x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4133:<72x1536xbf16>{1536,1}}))
          duration: -1
374489 2024-12-10 17:48:54.414636 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n238,rank6)
        - aten::mm:
          inputs: (%4133:<72x1536xbf16>{1536, 1}, %4659:<1536x5120xbf16>{1, 1536})
          outputs: (%4592:<72x5120xbf16>{5120,1})
          duration: -1
374549 2024-12-10 17:48:54.418473 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n238,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4133:<72x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4592:<72x5120xbf16>{5120,1},None:NoneType})
          duration: -1
374566 2024-12-10 17:48:54.419238 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n227,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4632:<72x5120xbf16>{5120, 1}+22512640})
          outputs: (%4550:tuple{%4592:<72x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4650:<72x5120xbf16>{5120, 1}+22512640, %4592:<72x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4650:<72x5120xbf16>{5120,1}+22512640)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%3835:<429x5120xbf16>{5120, 1}+22881280})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%3835:<429x5120xbf16>{5120,1}+22881280}))
          duration: -1
374741 2024-12-10 17:48:54.429535 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n228,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%3835:<429x5120xbf16>{5120, 1}+22881280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%3835:<429x5120xbf16>{5120,1}+22881280}))
          duration: -1
374754 2024-12-10 17:48:54.430264 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n282,rank6)
        - aten::mm:
          inputs: (%3835:<429x5120xbf16>{5120, 1}+22881280, %4642:<5120x3072xbf16>{1, 5120})
          outputs: (%4632:<429x3072xbf16>{3072,1})
          duration: -1
374831 2024-12-10 17:48:54.434036 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n282,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%3835:<429x5120xbf16>{5120, 1}+22881280})
          outputs: (%4628:tuple{%4632:<429x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4534:<429x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4534:<429x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4534:<429x1536xbf16>{3072, 1})
          outputs: (%4594:<429x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4534:<429x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4594:<429x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4594:<429x1536xbf16>{1536, 1}, %4540:<429x1536xbf16>{3072, 1}+1536)
          outputs: (%4634:<429x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4634:<429x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4634:<429x1536xbf16>{1536,1}}))
          duration: -1
374968 2024-12-10 17:48:54.443939 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n239,rank6)
        - aten::mm:
          inputs: (%4634:<429x1536xbf16>{1536, 1}, %4655:<1536x5120xbf16>{1, 1536})
          outputs: (%4652:<429x5120xbf16>{5120,1})
          duration: -1
375027 2024-12-10 17:48:54.447732 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n239,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4634:<429x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4652:<429x5120xbf16>{5120,1},None:NoneType})
          duration: -1
375043 2024-12-10 17:48:54.448489 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n228,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%3835:<429x5120xbf16>{5120, 1}+22881280})
          outputs: (%4550:tuple{%4652:<429x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4492:<429x5120xbf16>{5120, 1}+22881280, %4652:<429x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4492:<429x5120xbf16>{5120,1}+22881280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4586:<220x5120xbf16>{5120, 1}+25077760})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4586:<220x5120xbf16>{5120,1}+25077760}))
          duration: -1
375220 2024-12-10 17:48:54.458755 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n229,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4586:<220x5120xbf16>{5120, 1}+25077760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4586:<220x5120xbf16>{5120,1}+25077760}))
          duration: -1
375233 2024-12-10 17:48:54.459485 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n283,rank6)
        - aten::mm:
          inputs: (%4586:<220x5120xbf16>{5120, 1}+25077760, %4667:<5120x3072xbf16>{1, 5120})
          outputs: (%4133:<220x3072xbf16>{3072,1})
          duration: -1
375318 2024-12-10 17:48:54.463294 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n283,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4586:<220x5120xbf16>{5120, 1}+25077760})
          outputs: (%4555:tuple{%4133:<220x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4517:<220x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4517:<220x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4517:<220x1536xbf16>{3072, 1})
          outputs: (%4572:<220x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4517:<220x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4572:<220x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4572:<220x1536xbf16>{1536, 1}, %4534:<220x1536xbf16>{3072, 1}+1536)
          outputs: (%4642:<220x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%3566:tuple{%4642:<220x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%3566:tuple{%4642:<220x1536xbf16>{1536,1}}))
          duration: -1
375456 2024-12-10 17:48:54.473220 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n240,rank6)
        - aten::mm:
          inputs: (%4642:<220x1536xbf16>{1536, 1}, %4650:<1536x5120xbf16>{1, 1536})
          outputs: (%4594:<220x5120xbf16>{5120,1})
          duration: -1
375515 2024-12-10 17:48:54.477049 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n240,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4642:<220x1536xbf16>{1536, 1}})
          outputs: (%4550:tuple{%4594:<220x5120xbf16>{5120,1},None:NoneType})
          duration: -1
375528 2024-12-10 17:48:54.477817 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n229,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4586:<220x5120xbf16>{5120, 1}+25077760})
          outputs: (%4550:tuple{%4594:<220x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4652:<220x5120xbf16>{5120, 1}+25077760, %4594:<220x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4652:<220x5120xbf16>{5120,1}+25077760)
          duration: -1
375609 2024-12-10 17:48:54.482781 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n10,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%4562:tuple{%4569:<5118x5120xbf16>{5120, 1}, %4567:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4670:tuple{%4351:<5118x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%4586:<5118x5120xbf16>{5120, 1}, 0:int, %4566:<5118x5120xCUSTOM_DATA_TYPE>{1, 0}, %4351:<5118x5120xbf16>{5120, 1})
          outputs: (%4554:<5118x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%4554:<5118x5120xbf16>{5120, 1}, %4634:<5118x1xbf16>{1, 1})
          outputs: (%4541:<5118x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%4672:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4554:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%4554:<8192x5120xbf16>{5120, 1}, 0:int, %4561:<5118x5120xCUSTOM_DATA_TYPE>{1, 0}, %4541:<5118x5120xbf16>{5120, 1})
          outputs: (%4558:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%4339:<1024x5120xbf16>{5120, 1}, %4558:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%4339:<1024x5120xbf16>{5120,1},%4558:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%4339:<1024x5120xbf16>{5120, 1}, %4558:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%4339:<1024x5120xbf16>{5120,1},%4558:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%4339:<1024x5120xbf16>{5120, 1}, %4558:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%4551:tuple{%4339:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4520:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%4520:tuple{%4459:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
375907 2024-12-10 17:48:54.537707 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n230,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4543:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4543:tuple{%4459:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
375930 2024-12-10 17:48:54.538619 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n284,rank6)
        - aten::mm:
          inputs: (%4592:<1024x5120xbf16>{5120, 1}, %4572:<5120x6144xbf16>{1, 5120})
          outputs: (%4540:<1024x6144xbf16>{6144,1})
          duration: -1
376082 2024-12-10 17:48:54.546556 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n284,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4543:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4550:tuple{%4534:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4679:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4679:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4679:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%4602:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4679:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%4602:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%4602:<1024x1x3072xbf16>{3072, 3072, 1}, %4554:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%4557:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
376185 2024-12-10 17:48:54.556565 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n241,rank6)
        - aten::mm:
          inputs: (%4540:<1024x3072xbf16>{3072, 1}, %4572:<3072x5120xbf16>{1, 3072})
          outputs: (%3836:<1024x5120xbf16>{5120,1})
          duration: -1
376338 2024-12-10 17:48:54.564481 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n241,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%3566:tuple{%4557:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%4673:tuple{%4602:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
376353 2024-12-10 17:48:54.565273 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n230,rank6)
        - ----------->api::MLP return:
          inputs: (%4520:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4673:tuple{%4602:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%4634:<1024x1x5120xbf16>{5120, 5120, 1}, %4557:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4558:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
376400 2024-12-10 17:48:54.568427 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n10,rank6)
        - ----------->api::MoELayer return:
          inputs: (%4514:tuple{%4459:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4673:tuple{%4558:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%4558:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%4558:<1024x1x5120xbf16>{5120,5120,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%4558:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%4558:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - aten::add:
          inputs: (%4513:<1024x1x5120xbf16>{5120, 5120, 1}, %4558:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4684:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
376516 2024-12-10 17:48:54.785701 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n10,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%4685:tuple{%4684:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4514:tuple{%4684:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4514:tuple{%4684:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
376547 2024-12-10 17:48:54.792635 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n32,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4684:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4684:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4684:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4684:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4515:tuple{%4684:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4515:tuple{%4684:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4515:tuple{%4684:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4515:tuple{%4684:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4689:tuple{%4684:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %4688:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4689:tuple{%4684:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%4688:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4684:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4515:tuple{%4684:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4686:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4684:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%4686:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
376740 2024-12-10 17:48:54.837065 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n32,rank6)
376749 2024-12-10 17:48:54.837589 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n10,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%4686:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3620:tuple{%4686:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%3620:tuple{%4686:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
376827 2024-12-10 17:48:54.843911 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n285,rank6)
        - aten::mm:
          inputs: (%4695:<1024x5120xbf16>{5120, 1}, %4693:<5120x102400xbf16>{1, 5120})
          outputs: (%3636:<1024x102400xbf16>{102400,1})
          duration: -1
376957 2024-12-10 17:48:54.856249 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n285,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3620:tuple{%4686:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%4698:tuple{%3647:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%4359:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%4700:tuple{%4695:<1024x1xf32>{1,1},%4701:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4695:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4695:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4702:list{%4695:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4703:tuple{%4704:list{%4695:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%4359:<1024x1x102400xf32>{102400, 102400, 1}, %3636:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%4359:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%4699:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%3840:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%4699:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%3636:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%3840:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %3636:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%4705:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3840:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%4706:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%4706:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %4707:list{%4705:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3840:<i32>, False:bool)
          outputs: (%4706:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4709:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3840:<1024x102400xf32>{102400, 1}, %4704:list{%4709:<1024xCUSTOM_DATA_TYPE>{1}, %4708:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%3636:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%4650:<1024x1xf32>{1, 1}, %4704:list{%4705:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %3636:<i32>, False:bool)
          outputs: (%4650:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%4359:<1024x1x102400xf32>{102400, 102400, 1}, out=%4359:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%4359:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%4359:<1024x1x102400xf32>{102400, 102400, 1}, %4707:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%4711:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4650:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4650:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4707:list{%4650:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4698:tuple{%4694:list{%4650:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4711:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4711:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4712:list{%4711:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4713:tuple{%4714:list{%4711:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%4711:<1024x1xf32>{1, 1})
          outputs: (%3840:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%3840:<1024x1xf32>{1, 1}, %4650:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%3636:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%4359:<1024x1x102400xf32>{102400, 102400, 1}, %4715:<1024x1x1xf32>{1, 1, 1})
          outputs: (%4359:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%4716:tuple{%4359:<1024x1x102400xf32>{102400, 102400, 1}, %4705:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %4708:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%4716:tuple{%4359:<1024x1x102400xf32>{102400,102400,1},%4705:<1024x1xCUSTOM_DATA_TYPE>{1,1},%4708:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2684:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device='cuda_6')_:dict)
          outputs: (%4715:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%4715:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%4715:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%4715:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device='cuda_6')_:dict)
          outputs: (%4715:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%4125:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[_3822,_20823,___285,_____,___377,___282,_30433]],_device='cuda_6')_:dict)
          outputs: (%4715:<1x1024xf32>{1,1})
          duration: -1
        - aten::mul:
          inputs: (%3441:<1024xf32>{1}, %3738:<1024xf32>{1})
          outputs: (%4695:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%4695:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%3840:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%3738:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%4650:<i32>)
          duration: -1
        - aten::div:
          inputs: (%3840:<i32>, %4650:<i32>)
          outputs: (%3441:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%3441:<i32>)
          outputs: (%4699:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%4719:list{%4720:<1xf32>{1}}, 0:int)
          outputs: (%3840:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%3840:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%3840:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%3798:list{%3840:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4692:tuple{%4721:list{%3840:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%3840:<1xf32>{1}, 8:int)
          outputs: (%3647:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%3441:<i32>, 1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%2942:<i32>, 1:int)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%4722:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3738:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%3738:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%3738:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %3738:<1xf32>{1})
          outputs: (%3636:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%3636:<1xf32>{1}, 1:int)
          outputs: (%3441:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%412:<i32>, 0:int, alpha=1:int)
          outputs: (%412:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3636:<i32>, %3840:<i32>, alpha=1:int)
          outputs: (%3636:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3738:<i32>, %3636:<i32>, False:bool)
          outputs: (%3738:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3636:<i32>, 1:int, alpha=1:int)
          outputs: (%3636:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%2942:<i32>, %3636:<i32>, False:bool)
          outputs: (%2942:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%4721:list{}, dtype=torch_int32:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:bool)
          outputs: (%412:<i32>)
          duration: -1
        - profiler::_record_function_enter_new:
          inputs: (enumerate(DataLoader)#_MultiProcessingDataLoaderIter___ne_t__:str, None:NoneType)
          outputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          duration: -1
        - profiler::_record_function_exit:
          inputs: (ScriptObject____torch___torch_classes_profiler__RecordFunction_:ScriptObject)
          outputs: (None:NoneType)
          duration: -1
        - aten::eq:
          inputs: (%442:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%245:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%409:<1024xf32>{1}, %444:list{%245:<1024xCUSTOM_DATA_TYPE>{1}}, %443:<i32>, False:bool)
          outputs: (%409:<1024xf32>{1})
          duration: -1
        - aten::eq:
          inputs: (%441:<1024xCUSTOM_DATA_TYPE>{1}, -1:int)
          outputs: (%443:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%441:<1024xCUSTOM_DATA_TYPE>{1}, %444:list{%443:<1024xCUSTOM_DATA_TYPE>{1}}, %433:<i32>, False:bool)
          outputs: (%441:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - aten::eq:
          inputs: (%442:<1024xCUSTOM_DATA_TYPE>{1}+1, -1:int)
          outputs: (%433:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - c10d::broadcast_:
          inputs: (%4729:list{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4703:tuple{%4730:list{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::index_put_:
          inputs: (%442:<1024xCUSTOM_DATA_TYPE>{1}+1, %444:list{%433:<1024xCUSTOM_DATA_TYPE>{1}}, %369:<i32>, False:bool)
          outputs: (%442:<1024xCUSTOM_DATA_TYPE>{1}+1)
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3636:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3636:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4731:list{%3636:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4732:tuple{%4721:list{%3636:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%4342:<1x1024xf32>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%4342:<1x1024xf32>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4729:list{%4342:<1x1024xf32>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4713:tuple{%4704:list{%4342:<1x1024xf32>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4731:list{%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4703:tuple{%4733:list{%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::broadcast call:
          inputs: (%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, 6:int, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.broadcast(%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},6:int,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::broadcast_:
          inputs: (%4729:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, 0:int, 0:int, False:bool, -1:int)
          outputs: (%4732:tuple{%4730:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::DistributedDataParallel call:
          inputs: (%2682:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.DistributedDataParallel(%2682:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device)
          duration: -1
        - ----------->api::Float16Module call:
          inputs: (%2683:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.Float16Module(%2683:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (torch.2_3_0.__instancecheck__(%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::GPTModel call:
          inputs: (%2859:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.GPTModel(%2859:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%3738:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1}},_'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device)
          duration: -1
        - ----------->api::LanguageModelEmbedding call:
          inputs: (%455:tuple{}, _'input_ids'__tensor([[_5008,____62,__3027,_____,___883,___185,_53819]],_device='cuda_6'),_'position_ids'__tensor([[___0,____1,____2,_____,_1021,_1022,_1023]],_device='cuda_6')_:dict)
          outputs: (torch.2_3_0.LanguageModelEmbedding(%455:tuple{},_'input_ids'__tensor([[_5008,____62,__3027,_____,___883,___185,_53819]],_device)
          duration: -1
378913 2024-12-10 17:48:55.042095 module_call: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Pre Forward:deepseek_v2.embedding,n11,rank6)
        - ----------->api::VocabParallelEmbedding call:
          inputs: (%3620:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (torch.2_3_0.VocabParallelEmbedding(%3620:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1}}))
          duration: -1
378927 2024-12-10 17:48:55.042844 module_call: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Pre Forward:deepseek_v2.embedding.word_embeddings,n11,rank6)
        - ----------->api::embedding call:
          inputs: (%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, None:NoneType, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.embedding(%4650:<1x1024xCUSTOM_DATA_TYPE>{1024,1},%25:<102400x5120xbf16>{5120,1},None:NoneType,None:NoneType,2_0:float,False:bool,False:bool))
          duration: -1
        - aten::embedding:
          inputs: (%25:<102400x5120xbf16>{5120, 1}, %4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, -1:int, False:bool, False:bool)
          outputs: (%4724:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::embedding return:
          inputs: (%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %25:<102400x5120xbf16>{5120, 1}, -1:int, None:NoneType, 2_0:float, False:bool, False:bool)
          outputs: (%4724:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
379024 2024-12-10 17:48:55.057650 module_return: A37503:None:0 torch.2_3_0.VocabParallelEmbedding(Forward:deepseek_v2.embedding.word_embeddings,n11,rank6)
        - ----------->api::VocabParallelEmbedding return:
          inputs: (%3620:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%3072:<1x1024x5120xbf16>{5242880,5120,1})
          duration: -1
        - ----------->api::Dropout call:
          inputs: (%3620:tuple{%4499:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.Dropout(%3620:tuple{%4499:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
379063 2024-12-10 17:48:55.059709 module_call: A37503:None:0 torch.2_3_0.Dropout(Pre Forward:deepseek_v2.embedding.embedding_dropout,n11,rank6)
        - ----------->api::dropout call:
          inputs: (%4499:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%4499:<1024x1x5120xbf16>{5120,5242880,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%4499:<1024x1x5120xbf16>{5120, 5242880, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%4499:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
379106 2024-12-10 17:48:55.065942 module_return: A37503:None:0 torch.2_3_0.Dropout(Forward:deepseek_v2.embedding.embedding_dropout,n11,rank6)
        - ----------->api::Dropout return:
          inputs: (%3620:tuple{%4499:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4499:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
379117 2024-12-10 17:48:55.066663 module_return: A37503:None:0 torch.2_3_0.LanguageModelEmbedding(Forward:deepseek_v2.embedding,n11,rank6)
        - ----------->api::RotaryEmbedding call:
          inputs: (%3620:tuple{1024:int})
          outputs: (torch.2_3_0.RotaryEmbedding(%3620:tuple{1024:int}))
          duration: -1
379148 2024-12-10 17:48:55.068292 module_call: A37503:None:0 torch.2_3_0.RotaryEmbedding(Pre Forward:deepseek_v2.rotary_pos_emb,n11,rank6)
        - aten::arange:
          inputs: (1024:int, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%3072:<1024xf32>{1})
          duration: -1
        - aten::add:
          inputs: (%3072:<1024xf32>{1}, 0:int, alpha=1:int)
          outputs: (%4725:<1024xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%4738:<1024x1xf32>{1, 1}, %29:<20xf32>{1})
          outputs: (%4726:<1024x20xf32>{20,1})
          duration: -1
        - aten::cat:
          inputs: (%4739:list{%4726:<1024x20xf32>{20, 1}, %4726:<1024x20xf32>{20, 1}}, -1:int)
          outputs: (%1562:<1024x40xf32>{40,1})
          duration: -1
379299 2024-12-10 17:48:55.079282 module_return: A37503:None:0 torch.2_3_0.RotaryEmbedding(Forward:deepseek_v2.rotary_pos_emb,n11,rank6)
        - ----------->api::RotaryEmbedding return:
          inputs: (%3620:tuple{1024:int})
          outputs: (%3581:<1024x1x1x40xf32>{40,40,40,1})
          duration: -1
        - ----------->api::TransformerBlock call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerBlock(%455:tuple{}))
          duration: -1
379337 2024-12-10 17:48:55.085684 module_call: A37503:None:0 torch.2_3_0.TransformerBlock(Pre Forward:deepseek_v2.decoder,n11,rank6)
        - ----------->api::TransformerLayer call:
          inputs: (%455:tuple{})
          outputs: (torch.2_3_0.TransformerLayer(%455:tuple{}))
          duration: -1
379417 2024-12-10 17:48:55.094729 module_call: A37503:None:0 torch.2_3_0.TransformerLayer(Pre Forward:deepseek_v2.decoder.layers.0,n11,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3620:tuple{%4293:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3620:tuple{%4293:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
379429 2024-12-10 17:48:55.095468 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.input_layernorm,n33,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4293:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4293:<1024x1x5120xbf16>{5120,5242880,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4293:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4515:tuple{%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4515:tuple{%4293:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4515:tuple{%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4515:tuple{%4293:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4717:tuple{%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, %3433:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4717:tuple{%4293:<1024x1x5120xbf16>{5120,5242880,1},%33:<5120xbf16>{1},%3433:<1024xf32>{1}}))
          duration: -1
        - aten::stack:
          inputs: (%437:list{%441:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%403:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%403:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4515:tuple{%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %33:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4359:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4293:<1024x1x5120xbf16>{5120, 5242880, 1})
          outputs: (%4359:<1024x1x5120xbf16>{5120,5242880,1})
          duration: -1
379690 2024-12-10 17:48:55.139641 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.input_layernorm,n33,rank6)
        - ----------->api::SelfAttention call:
          inputs: (%3620:tuple{%4359:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.SelfAttention(%3620:tuple{%4359:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
379708 2024-12-10 17:48:55.143971 module_call: A37503:None:0 torch.2_3_0.SelfAttention(Pre Forward:deepseek_v2.decoder.layers.0.self_attention,n11,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4742:tuple{%4359:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4742:tuple{%4359:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
379721 2024-12-10 17:48:55.144732 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n286,rank6)
        - aten::mm:
          inputs: (%4065:<1024x5120xbf16>{5120, 1}, %4745:<5120x1536xbf16>{1, 5120})
          outputs: (%4747:<1024x1536xbf16>{1536,1})
          duration: -1
379867 2024-12-10 17:48:55.152678 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_a_proj,n286,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4742:tuple{%4359:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4749:tuple{%4686:<1024x1x1536xbf16>{1536,1536,1},None:NoneType})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4742:tuple{%4686:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4742:tuple{%4686:<1024x1x1536xbf16>{1536,1536,1}}))
          duration: -1
379891 2024-12-10 17:48:55.153722 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n287,rank6)
        - aten::mm:
          inputs: (%4747:<1024x1536xbf16>{1536, 1}, %4750:<1536x24576xbf16>{1, 1536})
          outputs: (%4752:<1024x24576xbf16>{24576,1})
          duration: -1
380012 2024-12-10 17:48:55.159901 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_q_b_proj,n287,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4742:tuple{%4686:<1024x1x1536xbf16>{1536, 1536, 1}})
          outputs: (%4755:tuple{%4711:<1024x1x24576xbf16>{24576,24576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%4756:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %4751:list{128:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%4756:<1024x1x128x192xbf16>{24576,24576,192,1},%4751:list{128:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%4756:<1024x1x128x192xbf16>{24576, 24576, 192, 1}, %4751:list{128:int, 64:int}, -1:int)
          outputs: (%4698:tuple{%4725:<1024x1x128x128xbf16>{24576,24576,192,1},%4747:<1024x1x128x64xbf16>{24576,24576,192,1}+128})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4742:tuple{%4359:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4742:tuple{%4359:<1024x1x5120xbf16>{5120,5242880,1}}))
          duration: -1
380108 2024-12-10 17:48:55.169459 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n288,rank6)
        - aten::mm:
          inputs: (%4762:<1024x5120xbf16>{5120, 1}, %4708:<5120x576xbf16>{1, 5120})
          outputs: (%4715:<1024x576xbf16>{576,1})
          duration: -1
380249 2024-12-10 17:48:55.177313 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_a_proj_with_mqa,n288,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4742:tuple{%4359:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4698:tuple{%4710:<1024x1x576xbf16>{576,576,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%4710:<1024x1x576xbf16>{576, 576, 1}, %4764:list{512:int, 64:int}, -1:int)
          outputs: (torch.2_3_0.split(%4710:<1024x1x576xbf16>{576,576,1},%4764:list{512:int,64:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%4710:<1024x1x576xbf16>{576, 576, 1}, %4764:list{512:int, 64:int}, -1:int)
          outputs: (%4755:tuple{%4750:<1024x1x512xbf16>{576,576,1},%4686:<1024x1x64xbf16>{576,576,1}+512})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4742:tuple{%4750:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%4742:tuple{%4750:<1024x1x512xbf16>{576,576,1}}))
          duration: -1
380331 2024-12-10 17:48:55.185597 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n289,rank6)
        - aten::mm:
          inputs: (%4768:<1024x512xbf16>{576, 1}, %4766:<512x32768xbf16>{1, 512})
          outputs: (%4762:<1024x32768xbf16>{32768,1})
          duration: -1
380440 2024-12-10 17:48:55.191845 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_kv_b_proj,n289,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4742:tuple{%4750:<1024x1x512xbf16>{576, 576, 1}})
          outputs: (%4698:tuple{%4770:<1024x1x32768xbf16>{32768,32768,1},None:NoneType})
          duration: -1
        - ----------->api::split call:
          inputs: (%4772:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4767:list{128:int, 128:int}, -1:int)
          outputs: (torch.2_3_0.split(%4772:<1024x1x128x256xbf16>{32768,32768,256,1},%4767:list{128:int,128:int},-1:int))
          duration: -1
        - ----------->api::split return:
          inputs: (%4772:<1024x1x128x256xbf16>{32768, 32768, 256, 1}, %4767:list{128:int, 128:int}, -1:int)
          outputs: (%4749:tuple{%4752:<1024x1x128x128xbf16>{32768,32768,256,1},%4759:<1024x1x128x128xbf16>{32768,32768,256,1}+128})
          duration: -1
        - ----------->api::DeepseekV2YarnRotaryEmbedding call:
          inputs: (%4734:tuple{%4762:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (torch.2_3_0.DeepseekV2YarnRotaryEmbedding(%4734:tuple{%4762:<1x128x1024x128xbf16>{32768,256,32768,1}+128},_'seq_len'__1024_:dict))
          duration: -1
380559 2024-12-10 17:48:55.203605 module_call: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n11,rank6)
380605 2024-12-10 17:48:55.206713 module_return: A37503:None:0 torch.2_3_0.DeepseekV2YarnRotaryEmbedding(Forward:deepseek_v2.decoder.layers.0.self_attention.rotary_pos_emb,n11,rank6)
        - ----------->api::DeepseekV2YarnRotaryEmbedding return:
          inputs: (%4734:tuple{%4762:<1x128x1024x128xbf16>{32768, 256, 32768, 1}+128}, _'seq_len'__1024_:dict)
          outputs: (%4713:tuple{%4708:<1024x64xbf16>{64,1},%4768:<1024x64xbf16>{64,1}})
          duration: -1
        - aten::index:
          inputs: (%4708:<1024x64xbf16>{64, 1}, %4776:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%4759:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::index:
          inputs: (%4768:<1024x64xbf16>{64, 1}, %4773:list{%3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}})
          outputs: (%4709:<1x1024x64xbf16>{65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4782:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4777:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4558:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%4786:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}+32)
          outputs: (%4787:<1x128x1024x32xbf16>{4194304,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4776:list{%4787:<1x128x1024x32xbf16>{4194304, 32768, 32, 1}, %4785:<1x128x1024x32xbf16>{8388608, 65536, 64, 1}}, -1:int)
          outputs: (%4788:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4788:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4778:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4785:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%4558:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, %4785:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, alpha=1:int)
          outputs: (%4789:<1x128x1024x64xbf16>{8388608,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4784:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4777:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4583:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::neg:
          inputs: (%4785:<1x1x1024x32xbf16>{65536, 65536, 64, 1}+32)
          outputs: (%4791:<1x1x1024x32xbf16>{32768,32768,32,1})
          duration: -1
        - aten::cat:
          inputs: (%4776:list{%4791:<1x1x1024x32xbf16>{32768, 32768, 32, 1}, %4790:<1x1x1024x32xbf16>{65536, 65536, 64, 1}}, -1:int)
          outputs: (%4792:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::mul:
          inputs: (%4792:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4778:<1x1x1024x64xbf16>{65536, 65536, 64, 1})
          outputs: (%4791:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::add:
          inputs: (%4583:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, %4791:<1x1x1024x64xbf16>{65536, 65536, 64, 1}, alpha=1:int)
          outputs: (%4793:<1x1x1024x64xbf16>{65536,65536,64,1})
          duration: -1
        - aten::stack:
          inputs: (%427:list{%442:<1024xCUSTOM_DATA_TYPE>{1}+1}, 0:int, out=%431:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%431:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::copy_:
          inputs: (%4738:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4759:<1x128x1024x128xbf16>{24576, 192, 24576, 1}, False:bool)
          outputs: (%4738:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%4738:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %4789:<1x128x1024x64xbf16>{8388608, 65536, 64, 1}, False:bool)
          outputs: (%4738:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::copy_:
          inputs: (%4738:<1x128x1024x128xbf16>{25165824, 196608, 192, 1}, %4583:<1x128x1024x128xbf16>{32768, 256, 32768, 1}, False:bool)
          outputs: (%4738:<1x128x1024x128xbf16>{25165824,196608,192,1})
          duration: -1
        - aten::copy_:
          inputs: (%4752:<1x128x1024x64xbf16>{25165824, 196608, 192, 1}+128, %4784:<1x128x1024x64xbf16>{0, 0, 64, 1}, False:bool)
          outputs: (%4752:<1x128x1024x64xbf16>{25165824,196608,192,1}+128)
          duration: -1
        - aten::bmm:
          inputs: (%4708:<128x1024x192xbf16>{196608, 192, 1}, %4710:<128x192x1024xbf16>{196608, 1, 192})
          outputs: (%4711:<128x1024x1024xbf16>{1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%4759:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_1147213867929261:float)
          outputs: (%4711:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::gt:
          inputs: (%4708:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, 0:int)
          outputs: (%4772:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::index_put_:
          inputs: (%4708:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, %4776:list{%4772:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, %4759:<i32>, False:bool)
          outputs: (%4708:<1x1x1024x1024xbf16>{1048576,1048576,1024,1})
          duration: -1
        - aten::add:
          inputs: (%4711:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, %4708:<1x1x1024x1024xbf16>{1048576, 1048576, 1024, 1}, alpha=1:int)
          outputs: (%4756:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax call:
          inputs: (%4756:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype)
          outputs: (torch.2_3_0.softmax(%4756:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},-1:int,3:int,torch_float32:dtype))
          duration: -1
        - aten::_softmax:
          inputs: (%4772:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1}, -1:int, False:bool)
          outputs: (%4709:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::softmax return:
          inputs: (%4756:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, -1:int, 3:int, torch_float32:dtype, %4709:<1x128x1024x1024xf32>{134217728, 1048576, 1024, 1})
          outputs: (%4709:<1x128x1024x1024xf32>{134217728,1048576,1024,1})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%4768:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%4768:<1x128x1024x1024xbf16>{134217728,1048576,1024,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%4768:<1x128x1024x1024xbf16>{134217728, 1048576, 1024, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%4768:<1x128x1024x1024xbf16>{134217728,1048576,1024,1})
          duration: -1
        - aten::bmm:
          inputs: (%4715:<128x1024x1024xbf16>{1048576, 1024, 1}, %4756:<128x1024x128xbf16>{256, 32768, 1}+128)
          outputs: (%4709:<128x1024x128xbf16>{131072,128,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4742:tuple{%4756:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4742:tuple{%4756:<1024x1x16384xbf16>{16384,16384,1}}))
          duration: -1
382092 2024-12-10 17:48:55.325527 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n242,rank6)
        - aten::mm:
          inputs: (%4750:<1024x16384xbf16>{16384, 1}, %4784:<16384x5120xbf16>{1, 16384})
          outputs: (%4752:<1024x5120xbf16>{5120,1})
          duration: -1
382240 2024-12-10 17:48:55.333753 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.self_attention.linear_proj,n242,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4742:tuple{%4756:<1024x1x16384xbf16>{16384, 16384, 1}})
          outputs: (%4673:tuple{%4750:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
382259 2024-12-10 17:48:55.334536 module_return: A37503:None:0 torch.2_3_0.SelfAttention(Forward:deepseek_v2.decoder.layers.0.self_attention,n11,rank6)
        - ----------->api::SelfAttention return:
          inputs: (%3620:tuple{%4359:<1024x1x5120xbf16>{5120, 5242880, 1}})
          outputs: (%4673:tuple{%4750:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%4750:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%4750:<1024x1x5120xbf16>{5120,5120,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%4750:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%4750:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - aten::add:
          inputs: (%4293:<1024x1x5120xbf16>{5120, 5242880, 1}, %4750:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4798:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3620:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3620:tuple{%4798:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
382480 2024-12-10 17:48:55.365628 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n34,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4798:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4798:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4798:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4225:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4225:tuple{%4798:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4225:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4225:tuple{%4798:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4717:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, %4801:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4717:tuple{%4798:<1024x1x5120xbf16>{5120,5120,1},%40:<5120xbf16>{1},%4801:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4225:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %40:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4799:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4798:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%4799:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
382684 2024-12-10 17:48:55.409997 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.layers.0.pre_mlp_layernorm,n34,rank6)
        - ----------->api::MoELayer call:
          inputs: (%3620:tuple{%4799:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MoELayer(%3620:tuple{%4799:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
382699 2024-12-10 17:48:55.410775 module_call: A37503:None:0 torch.2_3_0.MoELayer(Pre Forward:deepseek_v2.decoder.layers.0.mlp,n11,rank6)
382715 2024-12-10 17:48:55.411542 module_call: A37503:None:0 torch.2_3_0.TopKRouter(Pre Forward:deepseek_v2.decoder.layers.0.mlp.router,n11,rank6)
        - aten::mm:
          inputs: (%4503:<1024x5120xbf16>{5120, 1}, %4803:<5120x160xbf16>{1, 5120})
          outputs: (%4805:<1024x160xbf16>{160,1})
          duration: -1
        - aten::_softmax:
          inputs: (%4725:<1024x160xf32>{160, 1}, 1:int, False:bool)
          outputs: (%4803:<1024x160xf32>{160,1})
          duration: -1
        - aten::topk:
          inputs: (%4750:<1024x160xbf16>{160, 1}, 6:int, -1:int, True:bool, True:bool)
          outputs: (%4807:tuple{%4808:<1024x6xbf16>{6,1},%4803:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - aten::_softmax:
          inputs: (%4503:<1024x160xf32>{160, 1}, -1:int, False:bool)
          outputs: (%4779:<1024x160xf32>{160,1})
          duration: -1
        - aten::zeros:
          inputs: (%4809:list{1024:int, 6:int, 160:int}, dtype=torch_int64:dtype, layout=torch_strided:layout, device=cuda_6:device, pin_memory=None:NoneType)
          outputs: (%4725:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::scatter_:
          inputs: (%4725:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, -1:int, %4503:<1024x6x1xCUSTOM_DATA_TYPE>{6, 1, 1}, 1:int)
          outputs: (%4725:<1024x6x160xCUSTOM_DATA_TYPE>{960,160,1})
          duration: -1
        - aten::sum:
          inputs: (%4725:<1024x6x160xCUSTOM_DATA_TYPE>{960, 160, 1}, %4809:list{1:int}, False:bool, dtype=None:NoneType)
          outputs: (%4810:<1024x160xCUSTOM_DATA_TYPE>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%4779:<1024x160xf32>{160, 1}, %4809:list{0:int}, False:bool, dtype=None:NoneType)
          outputs: (%4725:<160xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%4725:<160xf32>{1}, %4810:<1024x160xCUSTOM_DATA_TYPE>{160, 1})
          outputs: (%4811:<1024x160xf32>{160,1})
          duration: -1
        - aten::sum:
          inputs: (%4811:<1024x160xf32>{160, 1}, dtype=None:NoneType)
          outputs: (%4812:<i32>)
          duration: -1
        - aten::mul:
          inputs: (%4812:<i32>, 2_5431315104166666e-07:float)
          outputs: (%4503:<i32>)
          duration: -1
        - aten::div:
          inputs: (%4503:<i32>, 0_01:float)
          outputs: (%4811:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%4725:<i32>, %4814:<i32>, alpha=1:int)
          outputs: (%4725:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%4813:<i32>, %4725:<i32>, False:bool)
          outputs: (%4813:<i32>)
          duration: -1
        - ----------->api::MoEAuxLossAutoScalerBackward call:
          inputs: (%4691:tuple{%4503:<i32>})
          outputs: (torch.2_3_0.MoEAuxLossAutoScalerBackward(%4691:tuple{%4503:<i32>}))
          duration: -1
383257 2024-12-10 17:48:55.446884 module_return: A37503:None:0 torch.2_3_0.TopKRouter(Forward:deepseek_v2.decoder.layers.0.mlp.router,n11,rank6)
        - ----------->api::TopKRouter return:
          inputs: (%4742:tuple{%4799:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4817:tuple{%4813:<1024x6xbf16>{6,1},%4803:<1024x6xCUSTOM_DATA_TYPE>{6,1}})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4779:<8192x5120xbf16>{5120, 1}, %4790:<1024x5120xbf16>{5120, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4779:<8192x5120xbf16>{5120,1},%4790:<1024x5120xbf16>{5120,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4779:<8192x5120xbf16>{5120, 1}, %4790:<1024x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4552:tuple{%4779:<8192x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4525:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4803:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4525:<8192x6xCUSTOM_DATA_TYPE>{6,1},%4803:<1024x6xCUSTOM_DATA_TYPE>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4525:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4803:<1024x6xCUSTOM_DATA_TYPE>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4807:tuple{%4525:<8192x6xCUSTOM_DATA_TYPE>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::ge:
          inputs: (%4525:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 120:int)
          outputs: (%4822:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::le:
          inputs: (%4525:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, 139:int)
          outputs: (%4805:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::bitwise_and:
          inputs: (%4822:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4805:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4810:<8192x6xCUSTOM_DATA_TYPE>{6,1})
          duration: -1
        - aten::masked_select:
          inputs: (%4525:<8192x6xCUSTOM_DATA_TYPE>{6, 1}, %4810:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4822:<6492xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4814:<8192x6xbf16>{6, 1}, %4813:<1024x6xbf16>{6, 1}, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4814:<8192x6xbf16>{6,1},%4813:<1024x6xbf16>{6,1},_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4814:<8192x6xbf16>{6, 1}, %4813:<1024x6xbf16>{6, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4824:tuple{%4814:<8192x6xbf16>{6,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::masked_select:
          inputs: (%4814:<8192x6xbf16>{6, 1}, %4810:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4503:<6492xbf16>{1})
          duration: -1
        - aten::nonzero:
          inputs: (%4810:<8192x6xCUSTOM_DATA_TYPE>{6, 1})
          outputs: (%4805:<6492x2xCUSTOM_DATA_TYPE>{1,6492})
          duration: -1
        - aten::gather:
          inputs: (%4779:<8192x5120xbf16>{5120, 1}, 0:int, %4750:<6492x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4561:<6492x5120xbf16>{5120,1})
          duration: -1
        - aten::sort:
          inputs: (%4822:<6492xCUSTOM_DATA_TYPE>{1}, 0:int, False:bool)
          outputs: (%4826:tuple{%4725:<6492xCUSTOM_DATA_TYPE>{1},%4538:<6492xCUSTOM_DATA_TYPE>{1}})
          duration: -1
        - aten::histc:
          inputs: (%4563:<6492xf32>{1}, 20:int, 120:int, 139:int)
          outputs: (%4725:<20xf32>{1})
          duration: -1
        - aten::gather:
          inputs: (%4561:<6492x5120xbf16>{5120, 1}, 0:int, %4556:<6492x5120xCUSTOM_DATA_TYPE>{1, 0}, sparse_grad=False:bool)
          outputs: (%4566:<6492x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::SequentialMLP call:
          inputs: (%4826:tuple{%4566:<6492x5120xbf16>{5120, 1}, %4747:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0.SequentialMLP(%4826:tuple{%4566:<6492x5120xbf16>{5120,1},%4747:<20xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
384070 2024-12-10 17:48:55.562913 module_call: A37503:None:0 torch.2_3_0.SequentialMLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts,n11,rank6)
        - aten::cumsum:
          inputs: (%4747:<20xCUSTOM_DATA_TYPE>{1}, 0:int, dtype=None:NoneType)
          outputs: (%4822:<20xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::zeros:
          inputs: (%4825:list{1:int}, dtype=torch_int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:bool)
          outputs: (%4810:<1xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::cat:
          inputs: (%4825:list{%4810:<1xCUSTOM_DATA_TYPE>{1}, %4822:<20xCUSTOM_DATA_TYPE>{1}}, 0:int)
          outputs: (%4814:<21xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4563:<144x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4563:<144x5120xbf16>{5120,1}}))
          duration: -1
384238 2024-12-10 17:48:55.573903 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n231,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4563:<144x5120xbf16>{5120, 1}})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4563:<144x5120xbf16>{5120,1}}))
          duration: -1
384262 2024-12-10 17:48:55.574650 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n290,rank6)
        - aten::mm:
          inputs: (%4563:<144x5120xbf16>{5120, 1}, %4829:<5120x3072xbf16>{1, 5120})
          outputs: (%4830:<144x3072xbf16>{3072,1})
          duration: -1
384345 2024-12-10 17:48:55.578579 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc1,n290,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4563:<144x5120xbf16>{5120, 1}})
          outputs: (%4817:tuple{%4830:<144x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4832:<144x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4832:<144x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4832:<144x1536xbf16>{3072, 1})
          outputs: (%4833:<144x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4832:<144x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4833:<144x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4833:<144x1536xbf16>{1536, 1}, %4811:<144x1536xbf16>{3072, 1}+1536)
          outputs: (%4561:<144x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4561:<144x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4561:<144x1536xbf16>{1536,1}}))
          duration: -1
384449 2024-12-10 17:48:55.588604 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n243,rank6)
        - aten::mm:
          inputs: (%4561:<144x1536xbf16>{1536, 1}, %4829:<1536x5120xbf16>{1, 1536})
          outputs: (%4834:<144x5120xbf16>{5120,1})
          duration: -1
384529 2024-12-10 17:48:55.592480 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0.linear_fc2,n243,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4561:<144x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4834:<144x5120xbf16>{5120,1},None:NoneType})
          duration: -1
384543 2024-12-10 17:48:55.593277 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.0,n231,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4563:<144x5120xbf16>{5120, 1}})
          outputs: (%4821:tuple{%4834:<144x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4832:<144x5120xbf16>{5120, 1}, %4834:<144x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4832:<144x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4812:<385x5120xbf16>{5120, 1}+737280})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4812:<385x5120xbf16>{5120,1}+737280}))
          duration: -1
384716 2024-12-10 17:48:55.603591 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n232,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4812:<385x5120xbf16>{5120, 1}+737280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4812:<385x5120xbf16>{5120,1}+737280}))
          duration: -1
384741 2024-12-10 17:48:55.604316 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n291,rank6)
        - aten::mm:
          inputs: (%4812:<385x5120xbf16>{5120, 1}+737280, %1162:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<385x3072xbf16>{3072,1})
          duration: -1
384822 2024-12-10 17:48:55.608173 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc1,n291,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4812:<385x5120xbf16>{5120, 1}+737280})
          outputs: (%4815:tuple{%4459:<385x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4811:<385x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4811:<385x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4811:<385x1536xbf16>{3072, 1})
          outputs: (%4829:<385x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4811:<385x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4829:<385x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4829:<385x1536xbf16>{1536, 1}, %4833:<385x1536xbf16>{3072, 1}+1536)
          outputs: (%4837:<385x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4837:<385x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4837:<385x1536xbf16>{1536,1}}))
          duration: -1
384922 2024-12-10 17:48:55.618160 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n244,rank6)
        - aten::mm:
          inputs: (%4837:<385x1536xbf16>{1536, 1}, %1162:<1536x5120xbf16>{1, 1536})
          outputs: (%1238:<385x5120xbf16>{5120,1})
          duration: -1
385006 2024-12-10 17:48:55.621995 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1.linear_fc2,n244,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4837:<385x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%1238:<385x5120xbf16>{5120,1},None:NoneType})
          duration: -1
385021 2024-12-10 17:48:55.622762 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.1,n232,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4812:<385x5120xbf16>{5120, 1}+737280})
          outputs: (%4821:tuple{%1238:<385x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4709:<385x5120xbf16>{5120, 1}+737280, %1238:<385x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4709:<385x5120xbf16>{5120,1}+737280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4834:<845x5120xbf16>{5120, 1}+2708480})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4834:<845x5120xbf16>{5120,1}+2708480}))
          duration: -1
385193 2024-12-10 17:48:55.633092 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n233,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4834:<845x5120xbf16>{5120, 1}+2708480})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4834:<845x5120xbf16>{5120,1}+2708480}))
          duration: -1
385217 2024-12-10 17:48:55.633870 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n292,rank6)
        - aten::mm:
          inputs: (%4834:<845x5120xbf16>{5120, 1}+2708480, %4840:<5120x3072xbf16>{1, 5120})
          outputs: (%4244:<845x3072xbf16>{3072,1})
          duration: -1
385301 2024-12-10 17:48:55.637734 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc1,n292,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4834:<845x5120xbf16>{5120, 1}+2708480})
          outputs: (%4828:tuple{%4244:<845x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1162:<845x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1162:<845x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1162:<845x1536xbf16>{3072, 1})
          outputs: (%4840:<845x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1162:<845x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4840:<845x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4840:<845x1536xbf16>{1536, 1}, %4723:<845x1536xbf16>{3072, 1}+1536)
          outputs: (%4459:<845x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4459:<845x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4459:<845x1536xbf16>{1536,1}}))
          duration: -1
385401 2024-12-10 17:48:55.647701 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n245,rank6)
        - aten::mm:
          inputs: (%4459:<845x1536xbf16>{1536, 1}, %4843:<1536x5120xbf16>{1, 1536})
          outputs: (%4844:<845x5120xbf16>{5120,1})
          duration: -1
385485 2024-12-10 17:48:55.651564 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2.linear_fc2,n245,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4459:<845x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4844:<845x5120xbf16>{5120,1},None:NoneType})
          duration: -1
385500 2024-12-10 17:48:55.652326 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.2,n233,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4834:<845x5120xbf16>{5120, 1}+2708480})
          outputs: (%4821:tuple{%4844:<845x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4459:<845x5120xbf16>{5120, 1}+2708480, %4844:<845x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4459:<845x5120xbf16>{5120,1}+2708480)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%1162:<319x5120xbf16>{5120, 1}+7034880})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%1162:<319x5120xbf16>{5120,1}+7034880}))
          duration: -1
385670 2024-12-10 17:48:55.662614 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n234,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%1162:<319x5120xbf16>{5120, 1}+7034880})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%1162:<319x5120xbf16>{5120,1}+7034880}))
          duration: -1
385693 2024-12-10 17:48:55.663345 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n293,rank6)
        - aten::mm:
          inputs: (%1162:<319x5120xbf16>{5120, 1}+7034880, %4846:<5120x3072xbf16>{1, 5120})
          outputs: (%4847:<319x3072xbf16>{3072,1})
          duration: -1
385780 2024-12-10 17:48:55.667183 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc1,n293,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%1162:<319x5120xbf16>{5120, 1}+7034880})
          outputs: (%4817:tuple{%4847:<319x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4459:<319x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4459:<319x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4459:<319x1536xbf16>{3072, 1})
          outputs: (%4846:<319x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4459:<319x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4846:<319x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4846:<319x1536xbf16>{1536, 1}, %4832:<319x1536xbf16>{3072, 1}+1536)
          outputs: (%4723:<319x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4723:<319x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4723:<319x1536xbf16>{1536,1}}))
          duration: -1
385878 2024-12-10 17:48:55.677219 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n246,rank6)
        - aten::mm:
          inputs: (%4723:<319x1536xbf16>{1536, 1}, %4843:<1536x5120xbf16>{1, 1536})
          outputs: (%4846:<319x5120xbf16>{5120,1})
          duration: -1
385964 2024-12-10 17:48:55.681055 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3.linear_fc2,n246,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4723:<319x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4846:<319x5120xbf16>{5120,1},None:NoneType})
          duration: -1
385978 2024-12-10 17:48:55.681827 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.3,n234,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%1162:<319x5120xbf16>{5120, 1}+7034880})
          outputs: (%4821:tuple{%4846:<319x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4709:<319x5120xbf16>{5120, 1}+7034880, %4846:<319x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4709:<319x5120xbf16>{5120,1}+7034880)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4244:<44x5120xbf16>{5120, 1}+8668160})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4244:<44x5120xbf16>{5120,1}+8668160}))
          duration: -1
386147 2024-12-10 17:48:55.692150 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n235,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4244:<44x5120xbf16>{5120, 1}+8668160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4244:<44x5120xbf16>{5120,1}+8668160}))
          duration: -1
386170 2024-12-10 17:48:55.692880 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n294,rank6)
        - aten::mm:
          inputs: (%4244:<44x5120xbf16>{5120, 1}+8668160, %4843:<5120x3072xbf16>{1, 5120})
          outputs: (%4851:<44x3072xbf16>{3072,1})
          duration: -1
386257 2024-12-10 17:48:55.696733 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc1,n294,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4244:<44x5120xbf16>{5120, 1}+8668160})
          outputs: (%4815:tuple{%4851:<44x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%1162:<44x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%1162:<44x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%1162:<44x1536xbf16>{3072, 1})
          outputs: (%4832:<44x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%1162:<44x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4832:<44x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4832:<44x1536xbf16>{1536, 1}, %4723:<44x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<44x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1238:<44x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1238:<44x1536xbf16>{1536,1}}))
          duration: -1
386355 2024-12-10 17:48:55.706734 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n247,rank6)
        - aten::mm:
          inputs: (%1238:<44x1536xbf16>{1536, 1}, %4854:<1536x5120xbf16>{1, 1536})
          outputs: (%4667:<44x5120xbf16>{5120,1})
          duration: -1
386441 2024-12-10 17:48:55.710544 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4.linear_fc2,n247,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1238:<44x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4667:<44x5120xbf16>{5120,1},None:NoneType})
          duration: -1
386455 2024-12-10 17:48:55.711313 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.4,n235,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4244:<44x5120xbf16>{5120, 1}+8668160})
          outputs: (%4821:tuple{%4667:<44x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4834:<44x5120xbf16>{5120, 1}+8668160, %4667:<44x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4834:<44x5120xbf16>{5120,1}+8668160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4839:<460x5120xbf16>{5120, 1}+8893440})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4839:<460x5120xbf16>{5120,1}+8893440}))
          duration: -1
386626 2024-12-10 17:48:55.721650 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n236,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4839:<460x5120xbf16>{5120, 1}+8893440})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4839:<460x5120xbf16>{5120,1}+8893440}))
          duration: -1
386648 2024-12-10 17:48:55.722386 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n295,rank6)
        - aten::mm:
          inputs: (%4839:<460x5120xbf16>{5120, 1}+8893440, %4854:<5120x3072xbf16>{1, 5120})
          outputs: (%4843:<460x3072xbf16>{3072,1})
          duration: -1
386737 2024-12-10 17:48:55.726230 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc1,n295,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4839:<460x5120xbf16>{5120, 1}+8893440})
          outputs: (%4828:tuple{%4843:<460x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4723:<460x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4723:<460x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4723:<460x1536xbf16>{3072, 1})
          outputs: (%1238:<460x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4723:<460x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1238:<460x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<460x1536xbf16>{1536, 1}, %4459:<460x1536xbf16>{3072, 1}+1536)
          outputs: (%1162:<460x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1162:<460x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1162:<460x1536xbf16>{1536,1}}))
          duration: -1
386831 2024-12-10 17:48:55.736222 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n248,rank6)
        - aten::mm:
          inputs: (%1162:<460x1536xbf16>{1536, 1}, %4846:<1536x5120xbf16>{1, 1536})
          outputs: (%4858:<460x5120xbf16>{5120,1})
          duration: -1
386917 2024-12-10 17:48:55.740086 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5.linear_fc2,n248,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1162:<460x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4858:<460x5120xbf16>{5120,1},None:NoneType})
          duration: -1
386930 2024-12-10 17:48:55.740855 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.5,n236,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4839:<460x5120xbf16>{5120, 1}+8893440})
          outputs: (%4821:tuple{%4858:<460x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4244:<460x5120xbf16>{5120, 1}+8893440, %4858:<460x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4244:<460x5120xbf16>{5120,1}+8893440)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4834:<159x5120xbf16>{5120, 1}+11248640})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4834:<159x5120xbf16>{5120,1}+11248640}))
          duration: -1
387099 2024-12-10 17:48:55.751211 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n237,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4834:<159x5120xbf16>{5120, 1}+11248640})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4834:<159x5120xbf16>{5120,1}+11248640}))
          duration: -1
387120 2024-12-10 17:48:55.751947 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n296,rank6)
        - aten::mm:
          inputs: (%4834:<159x5120xbf16>{5120, 1}+11248640, %4846:<5120x3072xbf16>{1, 5120})
          outputs: (%1238:<159x3072xbf16>{3072,1})
          duration: -1
387209 2024-12-10 17:48:55.755798 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc1,n296,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4834:<159x5120xbf16>{5120, 1}+11248640})
          outputs: (%4817:tuple{%1238:<159x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<159x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<159x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<159x1536xbf16>{3072, 1})
          outputs: (%1162:<159x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<159x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1162:<159x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1162:<159x1536xbf16>{1536, 1}, %4723:<159x1536xbf16>{3072, 1}+1536)
          outputs: (%4459:<159x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4459:<159x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4459:<159x1536xbf16>{1536,1}}))
          duration: -1
387310 2024-12-10 17:48:55.765793 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n249,rank6)
        - aten::mm:
          inputs: (%4459:<159x1536xbf16>{1536, 1}, %4667:<1536x5120xbf16>{1, 1536})
          outputs: (%4649:<159x5120xbf16>{5120,1})
          duration: -1
387392 2024-12-10 17:48:55.769633 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6.linear_fc2,n249,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4459:<159x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4649:<159x5120xbf16>{5120,1},None:NoneType})
          duration: -1
387409 2024-12-10 17:48:55.770401 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.6,n237,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4834:<159x5120xbf16>{5120, 1}+11248640})
          outputs: (%4821:tuple{%4649:<159x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4839:<159x5120xbf16>{5120, 1}+11248640, %4649:<159x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4839:<159x5120xbf16>{5120,1}+11248640)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4832:<309x5120xbf16>{5120, 1}+12062720})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4832:<309x5120xbf16>{5120,1}+12062720}))
          duration: -1
387579 2024-12-10 17:48:55.780702 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n238,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4832:<309x5120xbf16>{5120, 1}+12062720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4832:<309x5120xbf16>{5120,1}+12062720}))
          duration: -1
387599 2024-12-10 17:48:55.781448 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n297,rank6)
        - aten::mm:
          inputs: (%4832:<309x5120xbf16>{5120, 1}+12062720, %4667:<5120x3072xbf16>{1, 5120})
          outputs: (%1238:<309x3072xbf16>{3072,1})
          duration: -1
387687 2024-12-10 17:48:55.785275 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc1,n297,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4832:<309x5120xbf16>{5120, 1}+12062720})
          outputs: (%4815:tuple{%1238:<309x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<309x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<309x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<309x1536xbf16>{3072, 1})
          outputs: (%4459:<309x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<309x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4459:<309x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4459:<309x1536xbf16>{1536, 1}, %1162:<309x1536xbf16>{3072, 1}+1536)
          outputs: (%4723:<309x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4723:<309x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4723:<309x1536xbf16>{1536,1}}))
          duration: -1
387789 2024-12-10 17:48:55.795224 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n250,rank6)
        - aten::mm:
          inputs: (%4723:<309x1536xbf16>{1536, 1}, %4858:<1536x5120xbf16>{1, 1536})
          outputs: (%4843:<309x5120xbf16>{5120,1})
          duration: -1
387870 2024-12-10 17:48:55.799035 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7.linear_fc2,n250,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4723:<309x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4843:<309x5120xbf16>{5120,1},None:NoneType})
          duration: -1
387887 2024-12-10 17:48:55.799816 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.7,n238,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4832:<309x5120xbf16>{5120, 1}+12062720})
          outputs: (%4821:tuple{%4843:<309x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4834:<309x5120xbf16>{5120, 1}+12062720, %4843:<309x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4834:<309x5120xbf16>{5120,1}+12062720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4244:<73x5120xbf16>{5120, 1}+13644800})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4244:<73x5120xbf16>{5120,1}+13644800}))
          duration: -1
388057 2024-12-10 17:48:55.810158 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n239,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4244:<73x5120xbf16>{5120, 1}+13644800})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4244:<73x5120xbf16>{5120,1}+13644800}))
          duration: -1
388076 2024-12-10 17:48:55.810891 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n298,rank6)
        - aten::mm:
          inputs: (%4244:<73x5120xbf16>{5120, 1}+13644800, %4858:<5120x3072xbf16>{1, 5120})
          outputs: (%1238:<73x3072xbf16>{3072,1})
          duration: -1
388166 2024-12-10 17:48:55.814733 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc1,n298,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4244:<73x5120xbf16>{5120, 1}+13644800})
          outputs: (%4828:tuple{%1238:<73x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<73x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<73x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<73x1536xbf16>{3072, 1})
          outputs: (%4723:<73x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<73x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4723:<73x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4723:<73x1536xbf16>{1536, 1}, %4459:<73x1536xbf16>{3072, 1}+1536)
          outputs: (%1162:<73x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1162:<73x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1162:<73x1536xbf16>{1536,1}}))
          duration: -1
388268 2024-12-10 17:48:55.824718 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n251,rank6)
        - aten::mm:
          inputs: (%1162:<73x1536xbf16>{1536, 1}, %4649:<1536x5120xbf16>{1, 1536})
          outputs: (%4854:<73x5120xbf16>{5120,1})
          duration: -1
388351 2024-12-10 17:48:55.828564 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8.linear_fc2,n251,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1162:<73x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4854:<73x5120xbf16>{5120,1},None:NoneType})
          duration: -1
388368 2024-12-10 17:48:55.829391 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.8,n239,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4244:<73x5120xbf16>{5120, 1}+13644800})
          outputs: (%4821:tuple{%4854:<73x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4832:<73x5120xbf16>{5120, 1}+13644800, %4854:<73x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4832:<73x5120xbf16>{5120,1}+13644800)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4839:<185x5120xbf16>{5120, 1}+14018560})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4839:<185x5120xbf16>{5120,1}+14018560}))
          duration: -1
388539 2024-12-10 17:48:55.839881 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n240,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4839:<185x5120xbf16>{5120, 1}+14018560})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4839:<185x5120xbf16>{5120,1}+14018560}))
          duration: -1
388559 2024-12-10 17:48:55.840620 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n299,rank6)
        - aten::mm:
          inputs: (%4839:<185x5120xbf16>{5120, 1}+14018560, %4649:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<185x3072xbf16>{3072,1})
          duration: -1
388646 2024-12-10 17:48:55.844473 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc1,n299,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4839:<185x5120xbf16>{5120, 1}+14018560})
          outputs: (%4817:tuple{%4459:<185x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<185x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<185x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<185x1536xbf16>{3072, 1})
          outputs: (%1162:<185x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<185x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1162:<185x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1162:<185x1536xbf16>{1536, 1}, %4723:<185x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<185x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1238:<185x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1238:<185x1536xbf16>{1536,1}}))
          duration: -1
388752 2024-12-10 17:48:55.854493 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n252,rank6)
        - aten::mm:
          inputs: (%1238:<185x1536xbf16>{1536, 1}, %4843:<1536x5120xbf16>{1, 1536})
          outputs: (%4844:<185x5120xbf16>{5120,1})
          duration: -1
388831 2024-12-10 17:48:55.858316 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9.linear_fc2,n252,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1238:<185x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4844:<185x5120xbf16>{5120,1},None:NoneType})
          duration: -1
388847 2024-12-10 17:48:55.859089 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.9,n240,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4839:<185x5120xbf16>{5120, 1}+14018560})
          outputs: (%4821:tuple{%4844:<185x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4244:<185x5120xbf16>{5120, 1}+14018560, %4844:<185x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4244:<185x5120xbf16>{5120,1}+14018560)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4834:<121x5120xbf16>{5120, 1}+14965760})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4834:<121x5120xbf16>{5120,1}+14965760}))
          duration: -1
389019 2024-12-10 17:48:55.869430 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n241,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4834:<121x5120xbf16>{5120, 1}+14965760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4834:<121x5120xbf16>{5120,1}+14965760}))
          duration: -1
389038 2024-12-10 17:48:55.870161 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n300,rank6)
        - aten::mm:
          inputs: (%4834:<121x5120xbf16>{5120, 1}+14965760, %4843:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<121x3072xbf16>{3072,1})
          duration: -1
389122 2024-12-10 17:48:55.873998 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc1,n300,rank6)
        - ----------->api::silu call:
          inputs: (%4591:<121x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<121x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<121x1536xbf16>{3072, 1})
          outputs: (%1238:<121x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<121x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1238:<121x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<121x1536xbf16>{1536, 1}, %1162:<121x1536xbf16>{3072, 1}+1536)
          outputs: (%4723:<121x1536xbf16>{1536,1})
          duration: -1
389231 2024-12-10 17:48:55.883977 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n253,rank6)
        - aten::mm:
          inputs: (%4723:<121x1536xbf16>{1536, 1}, %4854:<1536x5120xbf16>{1, 1536})
          outputs: (%4847:<121x5120xbf16>{5120,1})
          duration: -1
389306 2024-12-10 17:48:55.887792 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10.linear_fc2,n253,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4723:<121x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4847:<121x5120xbf16>{5120,1},None:NoneType})
          duration: -1
389325 2024-12-10 17:48:55.888561 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.10,n241,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4834:<121x5120xbf16>{5120, 1}+14965760})
          outputs: (%4821:tuple{%4847:<121x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4839:<121x5120xbf16>{5120, 1}+14965760, %4847:<121x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4839:<121x5120xbf16>{5120,1}+14965760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4832:<226x5120xbf16>{5120, 1}+15585280})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4832:<226x5120xbf16>{5120,1}+15585280}))
          duration: -1
389496 2024-12-10 17:48:55.898928 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n242,rank6)
389516 2024-12-10 17:48:55.899676 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n301,rank6)
        - aten::mm:
          inputs: (%4832:<226x5120xbf16>{5120, 1}+15585280, %4854:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<226x3072xbf16>{3072,1})
          duration: -1
389603 2024-12-10 17:48:55.903547 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc1,n301,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4832:<226x5120xbf16>{5120, 1}+15585280})
          outputs: (%4828:tuple{%4459:<226x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<226x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<226x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<226x1536xbf16>{3072, 1})
          outputs: (%4723:<226x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<226x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4723:<226x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4723:<226x1536xbf16>{1536, 1}, %1238:<226x1536xbf16>{3072, 1}+1536)
          outputs: (%1162:<226x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1162:<226x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1162:<226x1536xbf16>{1536,1}}))
          duration: -1
389712 2024-12-10 17:48:55.913547 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n254,rank6)
        - aten::mm:
          inputs: (%1162:<226x1536xbf16>{1536, 1}, %4844:<1536x5120xbf16>{1, 1536})
          outputs: (%4874:<226x5120xbf16>{5120,1})
          duration: -1
389786 2024-12-10 17:48:55.917369 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11.linear_fc2,n254,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1162:<226x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4874:<226x5120xbf16>{5120,1},None:NoneType})
          duration: -1
389805 2024-12-10 17:48:55.918137 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.11,n242,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4832:<226x5120xbf16>{5120, 1}+15585280})
          outputs: (%4821:tuple{%4874:<226x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4834:<226x5120xbf16>{5120, 1}+15585280, %4874:<226x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4834:<226x5120xbf16>{5120,1}+15585280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4244:<323x5120xbf16>{5120, 1}+16742400})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4244:<323x5120xbf16>{5120,1}+16742400}))
          duration: -1
389973 2024-12-10 17:48:55.928434 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n243,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4244:<323x5120xbf16>{5120, 1}+16742400})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4244:<323x5120xbf16>{5120,1}+16742400}))
          duration: -1
389993 2024-12-10 17:48:55.929184 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n302,rank6)
        - aten::mm:
          inputs: (%4244:<323x5120xbf16>{5120, 1}+16742400, %4844:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<323x3072xbf16>{3072,1})
          duration: -1
390081 2024-12-10 17:48:55.933039 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc1,n302,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4244:<323x5120xbf16>{5120, 1}+16742400})
          outputs: (%4817:tuple{%4459:<323x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<323x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<323x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<323x1536xbf16>{3072, 1})
          outputs: (%1162:<323x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<323x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1162:<323x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1162:<323x1536xbf16>{1536, 1}, %4723:<323x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<323x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1238:<323x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1238:<323x1536xbf16>{1536,1}}))
          duration: -1
390189 2024-12-10 17:48:55.943068 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n255,rank6)
        - aten::mm:
          inputs: (%1238:<323x1536xbf16>{1536, 1}, %4847:<1536x5120xbf16>{1, 1536})
          outputs: (%4840:<323x5120xbf16>{5120,1})
          duration: -1
390264 2024-12-10 17:48:55.946886 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12.linear_fc2,n255,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1238:<323x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4840:<323x5120xbf16>{5120,1},None:NoneType})
          duration: -1
390282 2024-12-10 17:48:55.947660 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.12,n243,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4244:<323x5120xbf16>{5120, 1}+16742400})
          outputs: (%4821:tuple{%4840:<323x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4832:<323x5120xbf16>{5120, 1}+16742400, %4840:<323x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4832:<323x5120xbf16>{5120,1}+16742400)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4839:<476x5120xbf16>{5120, 1}+18396160})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4839:<476x5120xbf16>{5120,1}+18396160}))
          duration: -1
390453 2024-12-10 17:48:55.957992 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n244,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4839:<476x5120xbf16>{5120, 1}+18396160})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4839:<476x5120xbf16>{5120,1}+18396160}))
          duration: -1
390473 2024-12-10 17:48:55.958726 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n303,rank6)
        - aten::mm:
          inputs: (%4839:<476x5120xbf16>{5120, 1}+18396160, %4847:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<476x3072xbf16>{3072,1})
          duration: -1
390558 2024-12-10 17:48:55.962557 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc1,n303,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4839:<476x5120xbf16>{5120, 1}+18396160})
          outputs: (%4815:tuple{%4459:<476x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<476x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<476x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<476x1536xbf16>{3072, 1})
          outputs: (%1238:<476x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<476x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1238:<476x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<476x1536xbf16>{1536, 1}, %1162:<476x1536xbf16>{3072, 1}+1536)
          outputs: (%4723:<476x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4723:<476x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4723:<476x1536xbf16>{1536,1}}))
          duration: -1
390669 2024-12-10 17:48:55.972528 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n256,rank6)
        - aten::mm:
          inputs: (%4723:<476x1536xbf16>{1536, 1}, %4874:<1536x5120xbf16>{1, 1536})
          outputs: (%4846:<476x5120xbf16>{5120,1})
          duration: -1
390744 2024-12-10 17:48:55.976380 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13.linear_fc2,n256,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4723:<476x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4846:<476x5120xbf16>{5120,1},None:NoneType})
          duration: -1
390761 2024-12-10 17:48:55.977166 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.13,n244,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4839:<476x5120xbf16>{5120, 1}+18396160})
          outputs: (%4821:tuple{%4846:<476x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4244:<476x5120xbf16>{5120, 1}+18396160, %4846:<476x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4244:<476x5120xbf16>{5120,1}+18396160)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4834:<595x5120xbf16>{5120, 1}+20833280})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4834:<595x5120xbf16>{5120,1}+20833280}))
          duration: -1
390937 2024-12-10 17:48:55.987509 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n245,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4834:<595x5120xbf16>{5120, 1}+20833280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4834:<595x5120xbf16>{5120,1}+20833280}))
          duration: -1
390955 2024-12-10 17:48:55.988250 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n304,rank6)
        - aten::mm:
          inputs: (%4834:<595x5120xbf16>{5120, 1}+20833280, %4874:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<595x3072xbf16>{3072,1})
          duration: -1
391040 2024-12-10 17:48:55.992100 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc1,n304,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4834:<595x5120xbf16>{5120, 1}+20833280})
          outputs: (%4828:tuple{%4459:<595x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<595x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<595x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<595x1536xbf16>{3072, 1})
          outputs: (%4723:<595x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<595x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4723:<595x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4723:<595x1536xbf16>{1536, 1}, %1238:<595x1536xbf16>{3072, 1}+1536)
          outputs: (%1162:<595x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1162:<595x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1162:<595x1536xbf16>{1536,1}}))
          duration: -1
391151 2024-12-10 17:48:56.002094 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n257,rank6)
        - aten::mm:
          inputs: (%1162:<595x1536xbf16>{1536, 1}, %4840:<1536x5120xbf16>{1, 1536})
          outputs: (%4667:<595x5120xbf16>{5120,1})
          duration: -1
391224 2024-12-10 17:48:56.005937 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14.linear_fc2,n257,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1162:<595x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4667:<595x5120xbf16>{5120,1},None:NoneType})
          duration: -1
391242 2024-12-10 17:48:56.006702 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.14,n245,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4834:<595x5120xbf16>{5120, 1}+20833280})
          outputs: (%4821:tuple{%4667:<595x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4839:<595x5120xbf16>{5120, 1}+20833280, %4667:<595x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4839:<595x5120xbf16>{5120,1}+20833280)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4832:<509x5120xbf16>{5120, 1}+23879680})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4832:<509x5120xbf16>{5120,1}+23879680}))
          duration: -1
391417 2024-12-10 17:48:56.017013 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n246,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4832:<509x5120xbf16>{5120, 1}+23879680})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4832:<509x5120xbf16>{5120,1}+23879680}))
          duration: -1
391434 2024-12-10 17:48:56.017750 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n305,rank6)
        - aten::mm:
          inputs: (%4832:<509x5120xbf16>{5120, 1}+23879680, %4840:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<509x3072xbf16>{3072,1})
          duration: -1
391520 2024-12-10 17:48:56.021577 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc1,n305,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4832:<509x5120xbf16>{5120, 1}+23879680})
          outputs: (%4817:tuple{%4459:<509x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<509x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<509x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<509x1536xbf16>{3072, 1})
          outputs: (%1162:<509x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<509x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1162:<509x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1162:<509x1536xbf16>{1536, 1}, %4723:<509x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<509x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1238:<509x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1238:<509x1536xbf16>{1536,1}}))
          duration: -1
391631 2024-12-10 17:48:56.031537 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n258,rank6)
        - aten::mm:
          inputs: (%1238:<509x1536xbf16>{1536, 1}, %4846:<1536x5120xbf16>{1, 1536})
          outputs: (%4858:<509x5120xbf16>{5120,1})
          duration: -1
391704 2024-12-10 17:48:56.035375 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15.linear_fc2,n258,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1238:<509x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4858:<509x5120xbf16>{5120,1},None:NoneType})
          duration: -1
391716 2024-12-10 17:48:56.036146 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.15,n246,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4832:<509x5120xbf16>{5120, 1}+23879680})
          outputs: (%4821:tuple{%4858:<509x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4834:<509x5120xbf16>{5120, 1}+23879680, %4858:<509x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4834:<509x5120xbf16>{5120,1}+23879680)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4244:<333x5120xbf16>{5120, 1}+26485760})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4244:<333x5120xbf16>{5120,1}+26485760}))
          duration: -1
391889 2024-12-10 17:48:56.046464 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n247,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4244:<333x5120xbf16>{5120, 1}+26485760})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4244:<333x5120xbf16>{5120,1}+26485760}))
          duration: -1
391906 2024-12-10 17:48:56.047203 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n306,rank6)
        - aten::mm:
          inputs: (%4244:<333x5120xbf16>{5120, 1}+26485760, %4846:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<333x3072xbf16>{3072,1})
          duration: -1
391995 2024-12-10 17:48:56.051047 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc1,n306,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4244:<333x5120xbf16>{5120, 1}+26485760})
          outputs: (%4815:tuple{%4459:<333x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<333x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<333x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<333x1536xbf16>{3072, 1})
          outputs: (%1238:<333x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<333x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1238:<333x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<333x1536xbf16>{1536, 1}, %1162:<333x1536xbf16>{3072, 1}+1536)
          outputs: (%4723:<333x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4723:<333x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4723:<333x1536xbf16>{1536,1}}))
          duration: -1
392108 2024-12-10 17:48:56.061015 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n259,rank6)
        - aten::mm:
          inputs: (%4723:<333x1536xbf16>{1536, 1}, %4667:<1536x5120xbf16>{1, 1536})
          outputs: (%4649:<333x5120xbf16>{5120,1})
          duration: -1
392178 2024-12-10 17:48:56.064847 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16.linear_fc2,n259,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4723:<333x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4649:<333x5120xbf16>{5120,1},None:NoneType})
          duration: -1
392194 2024-12-10 17:48:56.065627 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.16,n247,rank6)
        - aten::copy_:
          inputs: (%4832:<333x5120xbf16>{5120, 1}+26485760, %4649:<333x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4832:<333x5120xbf16>{5120,1}+26485760)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4839:<424x5120xbf16>{5120, 1}+28190720})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4839:<424x5120xbf16>{5120,1}+28190720}))
          duration: -1
392367 2024-12-10 17:48:56.075939 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n248,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4839:<424x5120xbf16>{5120, 1}+28190720})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4839:<424x5120xbf16>{5120,1}+28190720}))
          duration: -1
392387 2024-12-10 17:48:56.076685 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n307,rank6)
        - aten::mm:
          inputs: (%4839:<424x5120xbf16>{5120, 1}+28190720, %4667:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<424x3072xbf16>{3072,1})
          duration: -1
392474 2024-12-10 17:48:56.080525 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc1,n307,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4839:<424x5120xbf16>{5120, 1}+28190720})
          outputs: (%4828:tuple{%4459:<424x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<424x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<424x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<424x1536xbf16>{3072, 1})
          outputs: (%4723:<424x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<424x1536xbf16>{3072, 1}, False:bool)
          outputs: (%4723:<424x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%4723:<424x1536xbf16>{1536, 1}, %1238:<424x1536xbf16>{3072, 1}+1536)
          outputs: (%1162:<424x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1162:<424x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1162:<424x1536xbf16>{1536,1}}))
          duration: -1
392603 2024-12-10 17:48:56.091678 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n260,rank6)
        - aten::mm:
          inputs: (%1162:<424x1536xbf16>{1536, 1}, %4858:<1536x5120xbf16>{1, 1536})
          outputs: (%4843:<424x5120xbf16>{5120,1})
          duration: -1
392678 2024-12-10 17:48:56.095563 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17.linear_fc2,n260,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1162:<424x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4843:<424x5120xbf16>{5120,1},None:NoneType})
          duration: -1
392696 2024-12-10 17:48:56.096335 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.17,n248,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4839:<424x5120xbf16>{5120, 1}+28190720})
          outputs: (%4821:tuple{%4843:<424x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4244:<424x5120xbf16>{5120, 1}+28190720, %4843:<424x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4244:<424x5120xbf16>{5120,1}+28190720)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4834:<439x5120xbf16>{5120, 1}+30361600})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4834:<439x5120xbf16>{5120,1}+30361600}))
          duration: -1
392874 2024-12-10 17:48:56.106754 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n249,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4834:<439x5120xbf16>{5120, 1}+30361600})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4834:<439x5120xbf16>{5120,1}+30361600}))
          duration: -1
392891 2024-12-10 17:48:56.107490 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc1,n308,rank6)
        - aten::mm:
          inputs: (%4834:<439x5120xbf16>{5120, 1}+30361600, %4858:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<439x3072xbf16>{3072,1})
          duration: -1
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4834:<439x5120xbf16>{5120, 1}+30361600})
          outputs: (%4817:tuple{%4459:<439x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<439x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<439x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<439x1536xbf16>{3072, 1})
          outputs: (%1162:<439x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<439x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1162:<439x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1162:<439x1536xbf16>{1536, 1}, %4723:<439x1536xbf16>{3072, 1}+1536)
          outputs: (%1238:<439x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%1238:<439x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%1238:<439x1536xbf16>{1536,1}}))
          duration: -1
393085 2024-12-10 17:48:56.121347 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n261,rank6)
        - aten::mm:
          inputs: (%1238:<439x1536xbf16>{1536, 1}, %4649:<1536x5120xbf16>{1, 1536})
          outputs: (%4854:<439x5120xbf16>{5120,1})
          duration: -1
393159 2024-12-10 17:48:56.125168 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18.linear_fc2,n261,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%1238:<439x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4854:<439x5120xbf16>{5120,1},None:NoneType})
          duration: -1
393177 2024-12-10 17:48:56.125932 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.18,n249,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4834:<439x5120xbf16>{5120, 1}+30361600})
          outputs: (%4821:tuple{%4854:<439x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4839:<439x5120xbf16>{5120, 1}+30361600, %4854:<439x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4839:<439x5120xbf16>{5120,1}+30361600)
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4832:<123x5120xbf16>{5120, 1}+32609280})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4832:<123x5120xbf16>{5120,1}+32609280}))
          duration: -1
393354 2024-12-10 17:48:56.136247 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n250,rank6)
        - ----------->api::ColumnParallelLinear call:
          inputs: (%3619:tuple{%4832:<123x5120xbf16>{5120, 1}+32609280})
          outputs: (torch.2_3_0.ColumnParallelLinear(%3619:tuple{%4832:<123x5120xbf16>{5120,1}+32609280}))
          duration: -1
        - aten::mm:
          inputs: (%4832:<123x5120xbf16>{5120, 1}+32609280, %4649:<5120x3072xbf16>{1, 5120})
          outputs: (%4459:<123x3072xbf16>{3072,1})
          duration: -1
393453 2024-12-10 17:48:56.140830 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc1,n309,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4832:<123x5120xbf16>{5120, 1}+32609280})
          outputs: (%4815:tuple{%4459:<123x3072xbf16>{3072,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4591:<123x1536xbf16>{3072, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4591:<123x1536xbf16>{3072,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4591:<123x1536xbf16>{3072, 1})
          outputs: (%1238:<123x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4591:<123x1536xbf16>{3072, 1}, False:bool)
          outputs: (%1238:<123x1536xbf16>{1536,1})
          duration: -1
        - aten::mul:
          inputs: (%1238:<123x1536xbf16>{1536, 1}, %1162:<123x1536xbf16>{3072, 1}+1536)
          outputs: (%4723:<123x1536xbf16>{1536,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4723:<123x1536xbf16>{1536, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4723:<123x1536xbf16>{1536,1}}))
          duration: -1
393565 2024-12-10 17:48:56.150826 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n262,rank6)
        - aten::mm:
          inputs: (%4723:<123x1536xbf16>{1536, 1}, %4843:<1536x5120xbf16>{1, 1536})
          outputs: (%4844:<123x5120xbf16>{5120,1})
          duration: -1
393638 2024-12-10 17:48:56.154670 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19.linear_fc2,n262,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4723:<123x1536xbf16>{1536, 1}})
          outputs: (%4821:tuple{%4844:<123x5120xbf16>{5120,1},None:NoneType})
          duration: -1
393657 2024-12-10 17:48:56.155444 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts.local_experts.19,n250,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4832:<123x5120xbf16>{5120, 1}+32609280})
          outputs: (%4821:tuple{%4844:<123x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::copy_:
          inputs: (%4834:<123x5120xbf16>{5120, 1}+32609280, %4844:<123x5120xbf16>{5120, 1}, False:bool)
          outputs: (%4834:<123x5120xbf16>{5120,1}+32609280)
          duration: -1
393738 2024-12-10 17:48:56.160417 module_return: A37503:None:0 torch.2_3_0.SequentialMLP(Forward:deepseek_v2.decoder.layers.0.mlp.experts,n11,rank6)
        - ----------->api::SequentialMLP return:
          inputs: (%4826:tuple{%4566:<6492x5120xbf16>{5120, 1}, %4747:<20xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4898:tuple{%4779:<6492x5120xbf16>{5120,1},None:NoneType})
          duration: -1
        - aten::scatter:
          inputs: (%4839:<6492x5120xbf16>{5120, 1}, 0:int, %4556:<6492x5120xCUSTOM_DATA_TYPE>{1, 0}, %4779:<6492x5120xbf16>{5120, 1})
          outputs: (%4812:<6492x5120xbf16>{5120,1})
          duration: -1
        - aten::mul:
          inputs: (%4812:<6492x5120xbf16>{5120, 1}, %4832:<6492x1xbf16>{1, 1})
          outputs: (%4709:<6492x5120xbf16>{5120,1})
          duration: -1
        - aten::zeros:
          inputs: (%4900:list{8192:int, 5120:int}, dtype=torch_bfloat16:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4839:<8192x5120xbf16>{5120,1})
          duration: -1
        - aten::scatter_add:
          inputs: (%4839:<8192x5120xbf16>{5120, 1}, 0:int, %4750:<6492x5120xCUSTOM_DATA_TYPE>{1, 0}, %4709:<6492x5120xbf16>{5120, 1})
          outputs: (%4525:<8192x5120xbf16>{5120,1})
          duration: -1
        - ----------->api::_reduce_scatter_base call:
          inputs: (%4837:<1024x5120xbf16>{5120, 1}, %4525:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0._reduce_scatter_base(%4837:<1024x5120xbf16>{5120,1},%4525:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - ----------->api::reduce_scatter_tensor call:
          inputs: (%4837:<1024x5120xbf16>{5120, 1}, %4525:<8192x5120xbf16>{5120, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.reduce_scatter_tensor(%4837:<1024x5120xbf16>{5120,1},%4525:<8192x5120xbf16>{5120,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa2cf0_:ProcessGroup,False:bool))
          duration: -1
        - c10d::_reduce_scatter_base_:
          inputs: (%4837:<1024x5120xbf16>{5120, 1}, %4525:<8192x5120xbf16>{5120, 1}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, False:bool, -1:int)
          outputs: (%4820:tuple{%4837:<1024x5120xbf16>{5120,1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::MLP call:
          inputs: (%4742:tuple{%4799:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.MLP(%4742:tuple{%4799:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
393978 2024-12-10 17:48:56.200795 module_call: A37503:None:0 torch.2_3_0.MLP(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n251,rank6)
394002 2024-12-10 17:48:56.201582 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n310,rank6)
        - aten::mm:
          inputs: (%4814:<1024x5120xbf16>{5120, 1}, %4854:<5120x6144xbf16>{1, 5120})
          outputs: (%4907:<1024x6144xbf16>{6144,1})
          duration: -1
394156 2024-12-10 17:48:56.209655 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc1,n310,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%3619:tuple{%4799:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4821:tuple{%4840:<1024x1x6144xbf16>{6144,6144,1},None:NoneType})
          duration: -1
        - ----------->api::silu call:
          inputs: (%4459:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (torch.2_3_0.silu(%4459:<1024x1x3072xbf16>{6144,6144,1},False:bool))
          duration: -1
        - aten::silu:
          inputs: (%4459:<1024x1x3072xbf16>{6144, 6144, 1})
          outputs: (%4812:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::silu return:
          inputs: (%4459:<1024x1x3072xbf16>{6144, 6144, 1}, False:bool)
          outputs: (%4812:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - aten::mul:
          inputs: (%4812:<1024x1x3072xbf16>{3072, 3072, 1}, %1238:<1024x1x3072xbf16>{6144, 6144, 1}+3072)
          outputs: (%4839:<1024x1x3072xbf16>{3072,3072,1})
          duration: -1
        - ----------->api::RowParallelLinear call:
          inputs: (%4691:tuple{%4839:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (torch.2_3_0.RowParallelLinear(%4691:tuple{%4839:<1024x1x3072xbf16>{3072,3072,1}}))
          duration: -1
394260 2024-12-10 17:48:56.219652 module_call: A37503:None:0 torch.2_3_0.RowParallelLinear(Pre Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n263,rank6)
        - aten::mm:
          inputs: (%4244:<1024x3072xbf16>{3072, 1}, %4844:<3072x5120xbf16>{1, 3072})
          outputs: (%4854:<1024x5120xbf16>{5120,1})
          duration: -1
394413 2024-12-10 17:48:56.227622 module_return: A37503:None:0 torch.2_3_0.RowParallelLinear(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert.linear_fc2,n263,rank6)
        - ----------->api::RowParallelLinear return:
          inputs: (%4691:tuple{%4839:<1024x1x3072xbf16>{3072, 3072, 1}})
          outputs: (%4901:tuple{%4726:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
394428 2024-12-10 17:48:56.228390 module_return: A37503:None:0 torch.2_3_0.MLP(Forward:deepseek_v2.decoder.layers.0.mlp.shared_expert,n251,rank6)
        - ----------->api::MLP return:
          inputs: (%4742:tuple{%4799:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4901:tuple{%4726:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - aten::add:
          inputs: (%4591:<1024x1x5120xbf16>{5120, 5120, 1}, %4839:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4244:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
394476 2024-12-10 17:48:56.231567 module_return: A37503:None:0 torch.2_3_0.MoELayer(Forward:deepseek_v2.decoder.layers.0.mlp,n11,rank6)
        - ----------->api::MoELayer return:
          inputs: (%3620:tuple{%4799:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (%4901:tuple{%4244:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::dropout call:
          inputs: (%4244:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (torch.2_3_0.dropout(%4244:<1024x1x5120xbf16>{5120,5120,1},0_0:float,False:bool,False:bool))
          duration: -1
        - ----------->api::dropout return:
          inputs: (%4244:<1024x1x5120xbf16>{5120, 5120, 1}, 0_0:float, False:bool, False:bool)
          outputs: (%4244:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - aten::add:
          inputs: (%4798:<1024x1x5120xbf16>{5120, 5120, 1}, %4244:<1024x1x5120xbf16>{5120, 5120, 1}, alpha=1:int)
          outputs: (%4779:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
394564 2024-12-10 17:48:56.240551 module_return: A37503:None:0 torch.2_3_0.TransformerLayer(Forward:deepseek_v2.decoder.layers.0,n11,rank6)
        - ----------->api::TransformerLayer return:
          inputs: (%455:tuple{})
          outputs: (%4913:tuple{%4779:<1024x1x5120xbf16>{5120,5120,1},None:NoneType})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%3620:tuple{%4779:<1024x1x5120xbf16>{5120, 5120, 1}})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%3620:tuple{%4779:<1024x1x5120xbf16>{5120,5120,1}}))
          duration: -1
394596 2024-12-10 17:48:56.247281 module_call: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Pre Forward:deepseek_v2.decoder.final_layernorm,n35,rank6)
        - ----------->api::DeepseekV2RMSNorm call:
          inputs: (%4779:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (torch.2_3_0.DeepseekV2RMSNorm(%4779:<1024x1x5120xbf16>{5120,5120,1}))
          duration: -1
        - ----------->api::fused_rms_norm_affine call:
          inputs: (%4779:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool)
          outputs: (torch.2_3_0.fused_rms_norm_affine(%4779:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled call:
          inputs: (%4225:tuple{%4779:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (torch.2_3_0._cast_if_autocast_enabled(%4225:tuple{%4779:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool}))
          duration: -1
        - ----------->api::_cast_if_autocast_enabled return:
          inputs: (%4225:tuple{%4779:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4225:tuple{%4779:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},torch_Size([5120]):Size,1e-06:float,False:bool})
          duration: -1
        - ----------->api::FusedRMSNormAffineFunctionBackward call:
          inputs: (%4717:tuple{%4779:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, %4591:<1024xf32>{1}})
          outputs: (torch.2_3_0.FusedRMSNormAffineFunctionBackward(%4717:tuple{%4779:<1024x1x5120xbf16>{5120,5120,1},%62:<5120xbf16>{1},%4591:<1024xf32>{1}}))
          duration: -1
        - ----------->api::fused_rms_norm_affine return:
          inputs: (%4779:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool, %4225:tuple{%4779:<1024x1x5120xbf16>{5120, 5120, 1}, %62:<5120xbf16>{1}, torch_Size([5120]):Size, 1e-06:float, False:bool})
          outputs: (%4798:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::DeepseekV2RMSNorm return:
          inputs: (%4779:<1024x1x5120xbf16>{5120, 5120, 1})
          outputs: (%4798:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
394771 2024-12-10 17:48:56.291638 module_return: A37503:None:0 torch.2_3_0.DeepseekV2RMSNorm(Forward:deepseek_v2.decoder.final_layernorm,n35,rank6)
394779 2024-12-10 17:48:56.292157 module_return: A37503:None:0 torch.2_3_0.TransformerBlock(Forward:deepseek_v2.decoder,n11,rank6)
        - ----------->api::TransformerBlock return:
          inputs: (%455:tuple{})
          outputs: (%4798:<1024x1x5120xbf16>{5120,5120,1})
          duration: -1
        - ----------->api::ColumnParallelLinear call:
          inputs: (%4734:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (torch.2_3_0.ColumnParallelLinear(%4734:tuple{%4798:<1024x1x5120xbf16>{5120,5120,1}},_'weight'__None_:dict))
          duration: -1
394816 2024-12-10 17:48:56.298541 module_call: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Pre Forward:deepseek_v2.output_layer,n311,rank6)
        - aten::mm:
          inputs: (%4917:<1024x5120xbf16>{5120, 1}, %4747:<5120x102400xbf16>{1, 5120})
          outputs: (%4728:<1024x102400xbf16>{102400,1})
          duration: -1
394966 2024-12-10 17:48:56.310890 module_return: A37503:None:0 torch.2_3_0.ColumnParallelLinear(Forward:deepseek_v2.output_layer,n311,rank6)
        - ----------->api::ColumnParallelLinear return:
          inputs: (%4734:tuple{%4798:<1024x1x5120xbf16>{5120, 5120, 1}}, _'weight'__None_:dict)
          outputs: (%4828:tuple{%4293:<1024x1x102400xbf16>{102400,102400,1},None:NoneType})
          duration: -1
        - aten::max:
          inputs: (%4747:<1024x1x102400xf32>{102400, 102400, 1}, -1:int, False:bool)
          outputs: (%4821:tuple{%4728:<1024x1xf32>{1,1},%4917:<1024x1xCUSTOM_DATA_TYPE>{1,1}})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4728:<1024x1xf32>{1, 1}, RedOpType_MAX:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4728:<1024x1xf32>{1,1},RedOpType_MAX:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4919:list{%4728:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4815:tuple{%4367:list{%4728:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::sub_:
          inputs: (%4747:<1024x1x102400xf32>{102400, 102400, 1}, %4779:<1024x1x1xf32>{1, 1, 1}, alpha=1:int)
          outputs: (%4747:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::lt:
          inputs: (%4566:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int)
          outputs: (%4917:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::ge:
          inputs: (%4566:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 102400:int)
          outputs: (%4725:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::bitwise_or:
          inputs: (%4917:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %4725:<1024x1xCUSTOM_DATA_TYPE>{1, 1024})
          outputs: (%4839:<1024x1xCUSTOM_DATA_TYPE>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%4709:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, 0:int, alpha=1:int)
          outputs: (%4711:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::index_put_:
          inputs: (%4711:<1024x1xCUSTOM_DATA_TYPE>{1, 1024}, %2052:list{%4839:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %4709:<i32>, False:bool)
          outputs: (%4711:<1024x1xCUSTOM_DATA_TYPE>{1,1024})
          duration: -1
        - aten::arange:
          inputs: (0:int, 1024:int, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%1162:<1024xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%3072:<1024x102400xf32>{102400, 1}, %4367:list{%1162:<1024xCUSTOM_DATA_TYPE>{1}, %4832:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4459:<1024xf32>{1})
          duration: -1
        - aten::index_put_:
          inputs: (%1238:<1024x1xf32>{1, 1}, %4367:list{%4839:<1024x1xCUSTOM_DATA_TYPE>{1, 1}}, %4459:<i32>, False:bool)
          outputs: (%1238:<1024x1xf32>{1,1})
          duration: -1
        - aten::exp:
          inputs: (%4747:<1024x1x102400xf32>{102400, 102400, 1}, out=%4747:<1024x1x102400xf32>{102400, 102400, 1})
          outputs: (%4747:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - aten::sum:
          inputs: (%4747:<1024x1x102400xf32>{102400, 102400, 1}, %2052:list{-1:int}, False:bool, dtype=None:NoneType)
          outputs: (%466:<1024x1xf32>{1,1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%1238:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%1238:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%2052:list{%1238:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4828:tuple{%4916:list{%1238:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%466:<1024x1xf32>{1, 1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%466:<1024x1xf32>{1,1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55ebaa1c70_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4920:list{%466:<1024x1xf32>{1, 1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4820:tuple{%4921:list{%466:<1024x1xf32>{1,1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::log:
          inputs: (%466:<1024x1xf32>{1, 1})
          outputs: (%1162:<1024x1xf32>{1,1})
          duration: -1
        - aten::sub:
          inputs: (%1162:<1024x1xf32>{1, 1}, %1238:<1024x1xf32>{1, 1}, alpha=1:int)
          outputs: (%4779:<1024x1xf32>{1,1})
          duration: -1
        - aten::div_:
          inputs: (%4747:<1024x1x102400xf32>{102400, 102400, 1}, %4244:<1024x1x1xf32>{1, 1, 1})
          outputs: (%4747:<1024x1x102400xf32>{102400,102400,1})
          duration: -1
        - ----------->api::_VocabParallelCrossEntropyBackward call:
          inputs: (%4716:tuple{%4747:<1024x1x102400xf32>{102400, 102400, 1}, %4839:<1024x1xCUSTOM_DATA_TYPE>{1, 1}, %4832:<1024xCUSTOM_DATA_TYPE>{1}})
          outputs: (torch.2_3_0._VocabParallelCrossEntropyBackward(%4716:tuple{%4747:<1024x1x102400xf32>{102400,102400,1},%4839:<1024x1xCUSTOM_DATA_TYPE>{1,1},%4832:<1024xCUSTOM_DATA_TYPE>{1}}))
          duration: -1
        - ----------->api::GPTModel return:
          inputs: (%2859:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device='cuda_6')_:dict)
          outputs: (%4832:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::__instancecheck__ call:
          inputs: (%4832:<1x1024xf32>{1, 1})
          outputs: (torch.2_3_0.__instancecheck__(%4832:<1x1024xf32>{1,1}))
          duration: -1
        - ----------->api::__instancecheck__ return:
          inputs: (%4832:<1x1024xf32>{1, 1})
          outputs: (True:bool)
          duration: -1
        - ----------->api::Float16Module return:
          inputs: (%2683:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device='cuda_6')_:dict)
          outputs: (%4832:<1x1024xf32>{1,1})
          duration: -1
        - ----------->api::DistributedDataParallel return:
          inputs: (%2682:tuple{%4650:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %3738:<1x1024xCUSTOM_DATA_TYPE>{1024, 1}, %4727:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1}}, _'labels'__tensor([[___62,__3027,____11,_____,___185,_53819,___285]],_device='cuda_6')_:dict)
          outputs: (%4832:<1x1024xf32>{1,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%295:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%411:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%411:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::mul:
          inputs: (%4803:<1024xf32>{1}, %4293:<1024xf32>{1})
          outputs: (%4566:<1024xf32>{1})
          duration: -1
        - aten::sum:
          inputs: (%4566:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%4709:<i32>)
          duration: -1
        - aten::sum:
          inputs: (%4293:<1024xf32>{1}, dtype=None:NoneType)
          outputs: (%1562:<i32>)
          duration: -1
        - aten::div:
          inputs: (%4709:<i32>, %1562:<i32>)
          outputs: (%4839:<i32>)
          duration: -1
        - aten::isnan:
          inputs: (%4839:<i32>)
          outputs: (%3072:<i32>)
          duration: -1
        - aten::cat:
          inputs: (%4733:list{%4803:<1xf32>{1}}, 0:int)
          outputs: (%4728:<1xf32>{1})
          duration: -1
        - ----------->api::all_reduce call:
          inputs: (%4728:<1xf32>{1}, RedOpType_SUM:RedOpType, _torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup, False:bool)
          outputs: (torch.2_3_0.all_reduce(%4728:<1xf32>{1},RedOpType_SUM:RedOpType,_torch_distributed_distributed_c10d_ProcessGroup_object_at_0_7f55f82c1370_:ProcessGroup,False:bool))
          duration: -1
        - c10d::allreduce_:
          inputs: (%4923:list{%4728:<1xf32>{1}}, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, ScriptObject____torch___torch_classes_c10d_ReduceOp_:ScriptObject, None:NoneType, -1:int)
          outputs: (%4898:tuple{%4924:list{%4728:<1xf32>{1}},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::div:
          inputs: (%4728:<1xf32>{1}, 8:int)
          outputs: (%1562:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%4839:<i32>, 1:int)
          outputs: (%4799:<i32>)
          duration: -1
        - aten::div_:
          inputs: (%4799:<i32>, 1:int)
          outputs: (%4799:<i32>)
          duration: -1
        - aten::ones:
          inputs: (%4925:list{1:int}, dtype=None:NoneType, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4728:<1xf32>{1})
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: (%4728:<1xf32>{1})
          outputs: (torch.2_3_0.ChainedOptimizer(%4728:<1xf32>{1}))
          duration: -1
        - ----------->api::ChainedOptimizer call:
          inputs: ()
          outputs: (torch.2_3_0.ChainedOptimizer())
          duration: -1
        - ----------->api::Float16OptimizerWithFloat16Params call:
          inputs: ()
          outputs: (torch.2_3_0.Float16OptimizerWithFloat16Params())
          duration: -1
        - ----------->api::ChainedOptimizer return:
          inputs: ()
          outputs: (%164:<1xf32>{1})
          duration: -1
        - aten::mul:
          inputs: (%164:<1xf32>{1}, %4728:<1xf32>{1})
          outputs: (%4525:<1xf32>{1})
          duration: -1
        - aten::div:
          inputs: (%4525:<1xf32>{1}, 1:int)
          outputs: (%4839:<1xf32>{1})
          duration: -1
        - aten::add_:
          inputs: (%412:<i32>, 0:int, alpha=1:int)
          outputs: (%412:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%4799:<i32>, %3072:<i32>, alpha=1:int)
          outputs: (%4799:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%3840:<i32>, %4799:<i32>, False:bool)
          outputs: (%3840:<i32>)
          duration: -1
        - aten::add_:
          inputs: (%3840:<i32>, 1:int, alpha=1:int)
          outputs: (%3840:<i32>)
          duration: -1
        - aten::copy_:
          inputs: (%4799:<i32>, %3840:<i32>, False:bool)
          outputs: (%4799:<i32>)
          duration: -1
        - aten::div:
          inputs: (%4293:<i32>, %3647:<i32>)
          outputs: (%3840:<i32>)
          duration: -1
        - aten::zeros:
          inputs: (%4926:list{8:int, 1:int}, dtype=torch_float32:dtype, layout=None:NoneType, device=cuda_6:device, pin_memory=False:bool)
          outputs: (%4799:<8x1xf32>{1,1})
          duration: -1
        - aten::copy_:
          inputs: (%4927:<i32>, %3636:<i32>, False:bool)
          outputs: (%4927:<i32>)
          duration: -1
        - ----------->api::_all_gather_base call:
          inputs: (%4525:<8xf32>{1}, %4927:<1xf32>{1}+6, None:NoneType, False:bool)
          outputs: (torch.2_3_0._all_gather_base(%4525:<8xf32>{1},%4927:<1xf32>{1}+6,None:NoneType,False:bool))
          duration: -1
        - c10d::_allgather_base_:
          inputs: (%4525:<8xf32>{1}, %4927:<1xf32>{1}+6, ScriptObject____torch___torch_classes_c10d_ProcessGroup_:ScriptObject, False:bool, -1:int)
          outputs: (%4819:tuple{%4525:<8xf32>{1},ScriptObject____torch___torch_classes_c10d_Work_:ScriptObject})
          duration: -1
        - aten::stack:
          inputs: (%435:list{%262:<1024xf32>{1}}, 0:int, out=%437:<1x1024xf32>{1024, 1})
          outputs: (%437:<1x1024xf32>{1024,1})
          duration: -1
        - aten::gt:
          inputs: (%4342:<8xf32>{1}, 0_0:float)
          outputs: (%4779:<8xCUSTOM_DATA_TYPE>{1})
          duration: -1
        - aten::index:
          inputs: (%4342:<8xf32>{1}, %4928:list{%4779:<8xCUSTOM_DATA_TYPE>{1}})
          outputs: (%4525:<8xf32>{1})
          duration: -1
        - aten::min:
          inputs: (%4525:<8xf32>{1})
          outputs: (%4927:<i32>)
          duration: -1
        - aten::max:
          inputs: (%4525:<8xf32>{1})
          outputs: (%3636:<i32>)
          duration: -1
        - aten::stack:
          inputs: (%436:list{%280:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%439:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%439:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%440:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%440:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%437:list{%410:<1024xf32>{1}}, 0:int, out=%441:<1x1024xf32>{1024, 1})
          outputs: (%441:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%416:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%443:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%443:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%428:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%441:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%440:list{%410:<1024xf32>{1}}, 0:int, out=%443:<1x1024xf32>{1024, 1})
          outputs: (%443:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%442:list{%364:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%445:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%445:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%414:list{%364:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%432:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%432:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%431:list{%409:<1024xf32>{1}}, 0:int, out=%434:<1x1024xf32>{1024, 1})
          outputs: (%434:<1x1024xf32>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%433:list{%278:<1024xCUSTOM_DATA_TYPE>{1}}, 0:int, out=%436:<1x1024xCUSTOM_DATA_TYPE>{1024, 1})
          outputs: (%436:<1x1024xCUSTOM_DATA_TYPE>{1024,1})
          duration: -1
        - aten::stack:
          inputs: (%440:list{%278:<1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1024, 1}}, 0:int, out=%438:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576, 1048576, 1024, 1})
          outputs: (%438:<1x1x1024x1024xCUSTOM_DATA_TYPE>{1048576,1048576,1024,1})
          duration: -1
        - aten::stack:
          inputs: (%426:list{%410:<1024xf32>{1}}, 0:int, out=%436:<1x1024xf32>{1024, 1})
          outputs: (%436:<1x1024xf32>{1024,1})
          duration: -1
