config:
  env:
    hardware: H20SXM
    n_devices: 8
    n_nodes: 1
    software: nvcr.io/nvidia/pytorch:24.09-py3
  model: qwen2_5_vl_72b
  modules:
    Qwen2_5_VL:
      shape: [num_layers, micro_batch_size, num_attention_heads, seq_length, num_hidden_dim, hidden_size, ffn_hidden_size, vocab_size, kv_cache]
      fwd:
        - layer::GPTDecoder;[num_layers, micro_batch_size, num_attention_heads, seq_length, num_hidden_dim, hidden_size, ffn_hidden_size, -1]
        - layer::ColumnParallelLinear;[micro_batch_size*seq_length, hidden_size, vocab_size]
        - layer::Loss;[micro_batch_size*seq_length, vocab_size]
    optim:
      shape: [9, 1000000000]
      fwd:
        - layer::Adam
  training:
    micro_batch_size: 1
    tensor_model_parallel_size: 4
    gradient_accumulation_steps: 2
    sequence_parallel: True
    virtual_pipeline_model_parallel_size: null
    seq_length: 8192
    input_vision_shape: [1024,1024]
    task_type: Image
    num_frames: 16
  feature:
    transformer_engine: true
    fp8: true
    precision: bf16
module:
  Decoder: